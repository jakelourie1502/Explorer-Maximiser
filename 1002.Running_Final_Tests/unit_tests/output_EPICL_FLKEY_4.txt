append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 40}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lake_KEY.gridWorld'>
same_env_each_time:True
env_size:[7, 7]
observable_size:[7, 7]
game_modes:2
env_map:[['H' 'H' 'H' 'H' 'H' 'F' 'G']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['S' 'F' 'H' 'H' 'H' 'H' 'H']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'K' 'F']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']]
max_steps:120
actions_size:5
optimal_score:1
total_frames:255000
exp_gamma:0.95
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:7
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:episodic
rdn_beta:[0.3333333333333333, 2, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
contrast_vector:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(2, 49)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['H' 'H' 'H' 'H' 'H' 'F' 'G']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['S' 'F' 'H' 'H' 'H' 'H' 'H']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'K' 'F']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
printing an ep nov before normalisation:  7.4915180163504
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[9.404]
 [9.404]
 [9.404]
 [9.404]
 [9.404]] [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  12.240022420883179
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[12.458]
 [12.458]
 [ 7.901]
 [12.458]
 [12.458]] [[1.666]
 [1.666]
 [0.505]
 [1.666]
 [1.666]]
Starting evaluation
siam score:  0.008257237950932573
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.069123983383179
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([0.2891, 0.1915, 0.1484, 0.2134, 0.1575], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.1575, 0.5035, 0.1605, 0.0627, 0.1159], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1067, 0.1920, 0.3072, 0.2548, 0.1394], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1465, 0.0891, 0.2385, 0.3684, 0.1576], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1176, 0.2536, 0.2051, 0.2755, 0.1483], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 2 threads
Frames:  1797 train batches done:  42 episodes:  127
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.5593, 0.0365, 0.0477, 0.2102, 0.1464], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0777, 0.7156, 0.0920, 0.0127, 0.1020], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0829, 0.1242, 0.3988, 0.1795, 0.2147], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1886, 0.0179, 0.1492, 0.4261, 0.2181], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1767, 0.0946, 0.1682, 0.2896, 0.2709], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 1 threads
Frames:  1797 train batches done:  110 episodes:  127
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.5127451
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5347164
actions average: 
K:  1  action  0 :  tensor([0.7909, 0.0314, 0.0034, 0.0778, 0.0966], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.1202, 0.6746, 0.0688, 0.0122, 0.1242], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0161, 0.0503, 0.5199, 0.2671, 0.1466], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0633, 0.0201, 0.1874, 0.4888, 0.2405], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1432, 0.1255, 0.1700, 0.2604, 0.3009], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.552714
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.56582975
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.7716, 0.0436, 0.0020, 0.0901, 0.0927], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0651, 0.7652, 0.0493, 0.0185, 0.1018], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0022, 0.0260, 0.7788, 0.0886, 0.1045], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0337, 0.0021, 0.1130, 0.5602, 0.2911], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0654, 0.1903, 0.0968, 0.3112, 0.3364], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9387,     0.0014,     0.0001,     0.0228,     0.0369],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0789, 0.7852, 0.0097, 0.0292, 0.0970], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0768, 0.0034, 0.7422, 0.0911, 0.0866], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0960, 0.0082, 0.1180, 0.5075, 0.2703], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1430, 0.0155, 0.0767, 0.2820, 0.4828], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.626131
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6333298
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.63278663
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6412187
actions average: 
K:  0  action  0 :  tensor([    0.9105,     0.0015,     0.0005,     0.0360,     0.0514],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0607, 0.8098, 0.0073, 0.0033, 0.1189], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0015,     0.8468,     0.0548,     0.0967],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0292, 0.0013, 0.0935, 0.5808, 0.2953], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0480, 0.0029, 0.0981, 0.3263, 0.5246], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.86226523689581
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.8622,     0.0063,     0.0001,     0.0651,     0.0664],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0603, 0.7025, 0.0572, 0.0128, 0.1672], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0009,     0.8196,     0.1237,     0.0556],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0107, 0.0015, 0.0441, 0.6706, 0.2731], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0235, 0.0144, 0.0787, 0.3045, 0.5789], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  72.43269920349121
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.67746775133025
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  94.93234647178967
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.9583,     0.0038,     0.0000,     0.0094,     0.0285],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0505, 0.8385, 0.0292, 0.0063, 0.0756], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0286,     0.8367,     0.0513,     0.0833],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0228,     0.0003,     0.0912,     0.5479,     0.3378],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0719, 0.0553, 0.0520, 0.2983, 0.5226], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.71690184
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.2 0.2 0.2]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.8564,     0.0005,     0.0000,     0.0657,     0.0774],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0156, 0.9134, 0.0020, 0.0346, 0.0345], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0036,     0.9149,     0.0436,     0.0378],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0191, 0.0021, 0.0841, 0.7055, 0.1892], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0264, 0.0229, 0.0570, 0.5041, 0.3897], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7607,     0.1375,     0.0000,     0.0435,     0.0583],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0261, 0.9496, 0.0031, 0.0010, 0.0203], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0015, 0.0596, 0.6858, 0.1241, 0.1290], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0167, 0.0050, 0.0626, 0.7175, 0.1983], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0428, 0.0509, 0.0749, 0.4800, 0.3515], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8925,     0.0045,     0.0001,     0.0412,     0.0616],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9958,     0.0008,     0.0002,     0.0028],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0118,     0.8505,     0.0488,     0.0890],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0528,     0.0003,     0.1293,     0.5806,     0.2370],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0665, 0.0107, 0.2052, 0.2950, 0.4227], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.231 0.077 0.41  0.154 0.128]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.04 ]
 [78.832]
 [94.209]
 [94.172]
 [86.104]] [[0.595]
 [0.95 ]
 [1.412]
 [1.411]
 [1.168]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  70.13324995045346
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.9515, 0.0088, 0.0131, 0.0246, 0.0020], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0023, 0.9106, 0.0522, 0.0010, 0.0339], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0088,     0.9496,     0.0072,     0.0343],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0021, 0.0020, 0.1475, 0.6216, 0.2269], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0241, 0.0148, 0.0388, 0.3531, 0.5692], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
printing an ep nov before normalisation:  24.796419104050113
siam score:  -0.72158104
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.309]
 [58.775]
 [63.664]
 [56.178]
 [63.855]] [[0.559]
 [0.751]
 [0.877]
 [0.684]
 [0.882]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7152478
printing an ep nov before normalisation:  49.97587325963178
siam score:  -0.7134362
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[91.572]
 [91.572]
 [93.641]
 [94.978]
 [91.572]] [[1.162]
 [1.162]
 [1.212]
 [1.245]
 [1.162]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[95.681]
 [95.681]
 [95.681]
 [95.681]
 [95.681]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
printing an ep nov before normalisation:  47.1772391009659
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.08857845919337
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.331]
 [50.331]
 [50.331]
 [50.331]
 [50.331]] [[0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.47040762146297
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.30781555175781
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.782]
 [44.915]
 [34.252]
 [44.915]
 [44.915]] [[1.43 ]
 [1.095]
 [0.733]
 [1.095]
 [1.095]]
printing an ep nov before normalisation:  54.91673785237118
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.681]
 [52.681]
 [52.681]
 [66.381]
 [52.681]] [[0.955]
 [0.955]
 [0.955]
 [1.349]
 [0.955]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  117.54217624039136
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.387]
 [71.387]
 [78.325]
 [71.387]
 [71.387]] [[1.451]
 [1.451]
 [1.667]
 [1.451]
 [1.451]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.16374312469082
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.73143214
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.72416687
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  87.8095308763644
printing an ep nov before normalisation:  86.23771100546978
printing an ep nov before normalisation:  84.99301196396686
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.0379515751036
printing an ep nov before normalisation:  74.5245318271287
printing an ep nov before normalisation:  81.93497642523904
printing an ep nov before normalisation:  50.968010619578585
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7213623
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.24440164908451
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.04848580950045
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.69419278479917
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 97.10338449575775
siam score:  -0.7448983
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  86.79632618775192
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  71.68453460745573
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.62563765885742
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.60293440513348
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.917113384055
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  11.068661212921143
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.9536,     0.0068,     0.0000,     0.0221,     0.0174],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9599,     0.0110,     0.0002,     0.0286],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0379,     0.8802,     0.0473,     0.0346],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0343, 0.0043, 0.0733, 0.6966, 0.1915], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0471, 0.0985, 0.0303, 0.3896, 0.4345], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.896084366295725
printing an ep nov before normalisation:  81.77493009037346
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.196]
 [68.196]
 [68.196]
 [68.196]
 [68.196]] [[1.86]
 [1.86]
 [1.86]
 [1.86]
 [1.86]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.801]
 [51.481]
 [50.393]
 [49.388]
 [50.393]] [[1.31 ]
 [0.979]
 [0.94 ]
 [0.905]
 [0.94 ]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77730805
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.02]
 [59.02]
 [59.02]
 [59.02]
 [59.02]] [[1.435]
 [1.435]
 [1.435]
 [1.435]
 [1.435]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.52 ]
 [58.983]
 [60.368]
 [59.079]
 [58.446]] [[1.147]
 [1.126]
 [1.182]
 [1.13 ]
 [1.104]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.72998969640662
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.08425158322876
printing an ep nov before normalisation:  76.63612823996273
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.921993085914034
UNIT TEST: sample policy line 217 mcts : [0.231 0.154 0.385 0.154 0.077]
printing an ep nov before normalisation:  46.40096255711147
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[104.068]
 [ 85.527]
 [ 83.385]
 [ 94.273]
 [ 85.527]] [[1.49 ]
 [1.055]
 [1.004]
 [1.26 ]
 [1.055]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  70.03120422363281
printing an ep nov before normalisation:  67.6824436131414
printing an ep nov before normalisation:  57.73683547973633
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.490177154541016
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  89.67258445315547
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 59.74213633488973
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 60.674562974016055
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.87438402852494
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.3893282037767
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[94.491]
 [94.491]
 [94.491]
 [94.491]
 [94.491]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7810043
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.638]
 [57.091]
 [96.741]
 [69.638]
 [64.948]] [[0.747]
 [0.505]
 [1.268]
 [0.747]
 [0.656]]
printing an ep nov before normalisation:  103.66589614580933
printing an ep nov before normalisation:  143.24457016678073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.16738677383328
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  99.41771266184334
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.9771,     0.0002,     0.0001,     0.0087,     0.0139],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0109,     0.9792,     0.0001,     0.0000,     0.0098],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0006,     0.8887,     0.0506,     0.0602],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0483,     0.0005,     0.0448,     0.5958,     0.3106],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0093, 0.0125, 0.0228, 0.4082, 0.5472], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  68.05438130558343
siam score:  -0.79981947
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  69.8018887829381
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.32851096383321
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[81.013]
 [81.013]
 [81.013]
 [81.013]
 [81.013]] [[0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.736513272468386
printing an ep nov before normalisation:  56.16353195628181
printing an ep nov before normalisation:  61.372898788819754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.28971635698618
actions average: 
K:  3  action  0 :  tensor([    0.9924,     0.0001,     0.0000,     0.0029,     0.0046],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9674,     0.0005,     0.0058,     0.0261],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0068,     0.8255,     0.0991,     0.0685],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0028,     0.0007,     0.0266,     0.7329,     0.2370],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0179, 0.0970, 0.0552, 0.2505, 0.5795], grad_fn=<DivBackward0>)
Starting evaluation
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
STARTED EXPV TRAINING ON FRAME NO.  10073
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.433]
 [68.651]
 [57.433]
 [57.433]
 [57.433]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  26.072037940137605
printing an ep nov before normalisation:  103.70097717531087
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[83.914]
 [83.914]
 [83.914]
 [83.914]
 [83.914]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  70.95309257507324
printing an ep nov before normalisation:  82.61124603291658
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  3  action  0 :  tensor([    0.8067,     0.0047,     0.0001,     0.0787,     0.1099],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0008,     0.9080,     0.0029,     0.0181,     0.0701],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0008,     0.0039,     0.8492,     0.0540,     0.0921],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0039, 0.0169, 0.0334, 0.6553, 0.2905], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0365, 0.0482, 0.1207, 0.2504, 0.5443], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.813961029052734
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.552]
 [32.552]
 [32.552]
 [32.552]
 [32.552]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.003]
 [34.744]
 [54.59 ]
 [45.59 ]
 [42.691]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  55.217266891316804
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.64658783767305
printing an ep nov before normalisation:  11.358527883013755
printing an ep nov before normalisation:  56.23907055247377
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.224]
 [33.313]
 [23.224]
 [26.52 ]
 [22.112]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
UNIT TEST: sample policy line 217 mcts : [0.077 0.256 0.487 0.128 0.051]
printing an ep nov before normalisation:  68.14770933266762
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.80552965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 59.6343862706116
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.447]
 [69.447]
 [53.3  ]
 [69.447]
 [69.447]] [[1.281]
 [1.281]
 [0.829]
 [1.281]
 [1.281]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[78.827]
 [58.468]
 [58.468]
 [67.065]
 [58.468]] [[1.076]
 [0.652]
 [0.652]
 [0.831]
 [0.652]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.248]
 [78.464]
 [85.764]
 [81.874]
 [78.464]] [[1.061]
 [1.297]
 [1.484]
 [1.385]
 [1.297]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.142965041638526
printing an ep nov before normalisation:  63.33016325900229
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.33]
 [63.33]
 [63.33]
 [63.33]
 [63.33]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
printing an ep nov before normalisation:  56.938443971318904
actions average: 
K:  4  action  0 :  tensor([    0.9517,     0.0006,     0.0000,     0.0019,     0.0457],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0346,     0.9556,     0.0062,     0.0000,     0.0035],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0002,     0.8279,     0.1001,     0.0718],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0083, 0.0018, 0.0811, 0.6231, 0.2856], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0058, 0.0689, 0.2186, 0.2998, 0.4070], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 1.183381201323435e-06
0.0 2.865062356921553e-07
0.0 0.0
0.0 1.0148093201572787e-06
0.0 0.0
0.0 7.713284811576064e-07
0.0 1.183381201323435e-06
0.0 4.2121274627925803e-07
0.0 2.698839876368436e-07
0.0 6.293520377345918e-07
siam score:  -0.8011155
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.61518769623074
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.051 0.128 0.538 0.179 0.103]
printing an ep nov before normalisation:  58.04452132469653
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 57.4488053335462
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.5268328775281
using explorer policy with actor:  1
printing an ep nov before normalisation:  81.16393480652509
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.784]
 [59.464]
 [70.664]
 [65.99 ]
 [60.69 ]] [[0.789]
 [1.093]
 [1.485]
 [1.321]
 [1.135]]
printing an ep nov before normalisation:  48.058234907697425
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.495670699654305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.398]
 [71.398]
 [61.879]
 [71.398]
 [71.398]] [[1.504]
 [1.504]
 [1.22 ]
 [1.504]
 [1.504]]
siam score:  -0.7893227
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.285]
 [35.63 ]
 [88.926]
 [30.049]
 [27.936]] [[0.128]
 [0.192]
 [0.653]
 [0.144]
 [0.125]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  64.94990960663752
printing an ep nov before normalisation:  75.16975238328993
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.6811957359314
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.75941710263835
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.256 0.    0.564 0.154 0.026]
printing an ep nov before normalisation:  50.40200379651702
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.19563672395293
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.275]
 [72.893]
 [66.629]
 [65.313]
 [62.203]] [[1.307]
 [1.624]
 [1.478]
 [1.447]
 [1.375]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  74.13427527122722
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.084]
 [61.351]
 [62.857]
 [52.508]
 [53.101]] [[1.137]
 [1.26 ]
 [1.317]
 [0.927]
 [0.949]]
printing an ep nov before normalisation:  70.31578914082058
siam score:  -0.8125832
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.83676338195801
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  68.79982753115608
actions average: 
K:  3  action  0 :  tensor([    0.9953,     0.0021,     0.0000,     0.0010,     0.0015],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9942,     0.0002,     0.0001,     0.0053],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0006,     0.8832,     0.0650,     0.0512],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0123, 0.0010, 0.0284, 0.6632, 0.2951], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0094, 0.0068, 0.1390, 0.2556, 0.5891], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  62.666909440218134
printing an ep nov before normalisation:  29.870273784808887
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
actions average: 
K:  1  action  0 :  tensor([    0.9732,     0.0025,     0.0000,     0.0070,     0.0173],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0247,     0.9403,     0.0009,     0.0217,     0.0123],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0002,     0.9744,     0.0119,     0.0135],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0013,     0.0007,     0.0264,     0.7175,     0.2542],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0016,     0.0005,     0.0042,     0.3279,     0.6658],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  60.576614334971126
printing an ep nov before normalisation:  32.67060041427612
siam score:  -0.8095032
actions average: 
K:  4  action  0 :  tensor([    0.9398,     0.0002,     0.0000,     0.0296,     0.0304],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0007,     0.9801,     0.0010,     0.0003,     0.0180],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0042,     0.8636,     0.0785,     0.0535],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0044, 0.0019, 0.0750, 0.7239, 0.1948], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0012, 0.0560, 0.1102, 0.2917, 0.5409], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  83.52725740113006
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.947522850278325
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.291]
 [44.291]
 [38.914]
 [44.291]
 [44.291]] [[1.684]
 [1.684]
 [1.333]
 [1.684]
 [1.684]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.05315262321791
printing an ep nov before normalisation:  63.81183316355669
printing an ep nov before normalisation:  46.865220069885254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.173]
 [80.648]
 [88.35 ]
 [80.14 ]
 [79.27 ]] [[0.274]
 [0.615]
 [0.714]
 [0.608]
 [0.597]]
printing an ep nov before normalisation:  94.76188450138329
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.168]
 [33.799]
 [37.88 ]
 [38.286]
 [33.173]] [[0.862]
 [0.589]
 [0.722]
 [0.736]
 [0.569]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.31817952236038
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.596]
 [49.675]
 [49.675]
 [49.675]
 [49.675]] [[0.788]
 [0.548]
 [0.548]
 [0.548]
 [0.548]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.761270998510525
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[73.75 ]
 [73.75 ]
 [75.61 ]
 [76.806]
 [73.75 ]] [[0.951]
 [0.951]
 [0.981]
 [1.   ]
 [0.951]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[100.861]
 [110.911]
 [114.416]
 [110.911]
 [110.911]] [[1.553]
 [1.812]
 [1.902]
 [1.812]
 [1.812]]
printing an ep nov before normalisation:  91.67446408952986
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.342]
 [54.499]
 [47.928]
 [38.733]
 [37.861]] [[0.193]
 [0.274]
 [0.241]
 [0.195]
 [0.19 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.785]
 [52.313]
 [55.012]
 [30.785]
 [30.785]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.30605145094698
printing an ep nov before normalisation:  81.29104155662968
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 49.24 ]
 [ 88.933]
 [119.245]
 [120.209]
 [118.401]] [[0.317]
 [0.743]
 [1.068]
 [1.079]
 [1.059]]
printing an ep nov before normalisation:  78.22211474689456
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 49.256]
 [ 49.256]
 [112.097]
 [ 49.256]
 [ 49.256]] [[0.341]
 [0.341]
 [1.07 ]
 [0.341]
 [0.341]]
UNIT TEST: sample policy line 217 mcts : [0.436 0.026 0.385 0.128 0.026]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.52650160434081
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.98017949686715
printing an ep nov before normalisation:  51.60012455357321
deleting a thread, now have 1 threads
Frames:  12910 train batches done:  1511 episodes:  588
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81334186
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  129.83177544805181
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.82701737
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  70.24820446968079
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.662550926208496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  86.6041425163431
printing an ep nov before normalisation:  85.40040889373954
actions average: 
K:  2  action  0 :  tensor([    0.9783,     0.0015,     0.0000,     0.0074,     0.0127],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9996,     0.0002,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0024,     0.9562,     0.0220,     0.0195],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0011, 0.0010, 0.0249, 0.7317, 0.2412], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0080, 0.0092, 0.1628, 0.3110, 0.5090], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  48.02397560787312
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.042]
 [40.042]
 [52.291]
 [42.269]
 [40.042]] [[0.459]
 [0.459]
 [0.727]
 [0.508]
 [0.459]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.168]
 [43.858]
 [38.758]
 [36.786]
 [35.908]] [[0.324]
 [0.227]
 [0.167]
 [0.143]
 [0.133]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.3653564453125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.385]
 [40.385]
 [40.385]
 [33.438]
 [40.385]] [[0.315]
 [0.315]
 [0.315]
 [0.215]
 [0.315]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  4  action  0 :  tensor([    0.8999,     0.0003,     0.0001,     0.0519,     0.0478],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0157, 0.8941, 0.0185, 0.0067, 0.0651], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0058, 0.0544, 0.8048, 0.0518, 0.0833], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0065, 0.0040, 0.1881, 0.6723, 0.1290], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1158, 0.0682, 0.2316, 0.1939, 0.3905], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.162]
 [62.135]
 [63.661]
 [53.102]
 [53.102]] [[0.246]
 [0.306]
 [0.319]
 [0.228]
 [0.228]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  84.74629542988772
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83379763
printing an ep nov before normalisation:  45.10003863253082
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.117877589004436
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  84.25627038758901
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.23364530290876
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.01598733827899
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9978,     0.0000,     0.0000,     0.0009,     0.0012],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0018,     0.9944,     0.0003,     0.0001,     0.0035],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0002,     0.9374,     0.0346,     0.0278],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0013,     0.0004,     0.1130,     0.6080,     0.2774],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0002,     0.0506,     0.2304,     0.2871,     0.4317],
       grad_fn=<DivBackward0>)
UNIT TEST: sample policy line 217 mcts : [0.026 0.    0.923 0.026 0.026]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.301]
 [44.529]
 [41.436]
 [57.396]
 [44.529]] [[1.263]
 [0.749]
 [0.641]
 [1.197]
 [0.749]]
printing an ep nov before normalisation:  52.64248739220164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.332131185877785
printing an ep nov before normalisation:  55.599575021952504
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  55.88468766443273
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9992,     0.0000,     0.0000,     0.0003,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0007,     0.9577,     0.0143,     0.0124,     0.0149],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0021,     0.7967,     0.0806,     0.1204],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0008, 0.0022, 0.0274, 0.7148, 0.2548], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0006,     0.0543,     0.0064,     0.2773,     0.6614],
       grad_fn=<DivBackward0>)
siam score:  -0.84240586
printing an ep nov before normalisation:  85.57962414747013
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.30643335519055
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.75537395477295
siam score:  -0.8500644
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.19430374120625
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.84606874
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[86.445]
 [76.11 ]
 [89.216]
 [76.11 ]
 [76.11 ]] [[1.667]
 [1.373]
 [1.746]
 [1.373]
 [1.373]]
siam score:  -0.825846
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.05380058288574
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  112.68769775075107
printing an ep nov before normalisation:  87.94597031938945
printing an ep nov before normalisation:  46.74903121248703
siam score:  -0.82203835
printing an ep nov before normalisation:  19.80491876602173
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.373]
 [53.373]
 [53.373]
 [53.373]
 [53.373]] [[0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.44191769114794
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  84.43439456963381
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.409]
 [42.409]
 [68.315]
 [42.409]
 [42.409]] [[0.945]
 [0.945]
 [1.925]
 [0.945]
 [0.945]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.241]
 [28.588]
 [51.103]
 [28.588]
 [28.588]] [[0.583]
 [0.595]
 [1.421]
 [0.595]
 [0.595]]
printing an ep nov before normalisation:  92.23303741406727
actions average: 
K:  4  action  0 :  tensor([    0.9685,     0.0014,     0.0000,     0.0034,     0.0267],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9966,     0.0001,     0.0002,     0.0027],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0017,     0.9557,     0.0146,     0.0279],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0024, 0.0014, 0.0386, 0.6542, 0.3034], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0163, 0.0761, 0.2611, 0.2680, 0.3785], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.4714994430542
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 52.99032376824543
printing an ep nov before normalisation:  37.87410116981918
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.81554512708654
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.611]
 [49.853]
 [48.5  ]
 [47.894]
 [45.071]] [[1.259]
 [1.271]
 [1.207]
 [1.179]
 [1.046]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  82.41942302106044
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.394]
 [66.394]
 [66.394]
 [66.394]
 [66.394]] [[1.613]
 [1.613]
 [1.613]
 [1.613]
 [1.613]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.091]
 [68.091]
 [75.984]
 [68.091]
 [68.091]] [[0.848]
 [0.848]
 [0.948]
 [0.848]
 [0.848]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.48969160884946
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8307962
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.397082488973915
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9985,     0.0000,     0.0000,     0.0007,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0055, 0.9678, 0.0056, 0.0038, 0.0173], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0007,     0.9534,     0.0218,     0.0240],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0008, 0.0016, 0.0204, 0.6700, 0.3073], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0016, 0.0069, 0.0698, 0.5409, 0.3809], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8403135
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.35371007739148
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8438378
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.37665367126465
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 3.05525385789527e-07
0.0 1.3859434859634043e-06
0.0 4.330853357882602e-07
0.0 4.7400510103658993e-07
0.0 8.253781880159977e-07
0.0 5.445078775177665e-07
0.0 4.2927321637156535e-07
0.0 0.0
0.0 1.3859434859634043e-06
0.0 1.3859434859634043e-06
printing an ep nov before normalisation:  57.77877945168987
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  92.50923156738281
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.544]
 [58.655]
 [58.35 ]
 [55.877]
 [55.921]] [[1.118]
 [1.676]
 [1.664]
 [1.566]
 [1.568]]
printing an ep nov before normalisation:  75.0427982393613
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9955,     0.0022,     0.0000,     0.0013,     0.0010],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0399,     0.9510,     0.0004,     0.0001,     0.0086],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0005,     0.0061,     0.8606,     0.0841,     0.0486],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0457, 0.0039, 0.0691, 0.6631, 0.2183], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0957, 0.0067, 0.0918, 0.3424, 0.4634], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.731]
 [57.794]
 [53.687]
 [64.729]
 [57.794]] [[1.706]
 [1.5  ]
 [1.393]
 [1.68 ]
 [1.5  ]]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  72.27201461791992
printing an ep nov before normalisation:  59.71450988063295
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  40.102007290624314
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  30.778193473815918
siam score:  -0.8370111
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.16044478117243
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.026 0.    0.692 0.256 0.026]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
siam score:  -0.8392828
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9921,     0.0019,     0.0006,     0.0033,     0.0021],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9951,     0.0003,     0.0000,     0.0042],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0136,     0.0000,     0.9296,     0.0330,     0.0237],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0016,     0.0001,     0.0358,     0.6817,     0.2808],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0085,     0.0006,     0.0573,     0.2813,     0.6523],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.19329478766478
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.164]
 [34.987]
 [34.987]
 [34.987]
 [34.987]] [[1.373]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.376]
 [41.053]
 [43.315]
 [48.095]
 [48.193]] [[1.193]
 [0.861]
 [0.98 ]
 [1.231]
 [1.236]]
printing an ep nov before normalisation:  89.62183425081965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.388]
 [50.226]
 [50.241]
 [48.346]
 [48.346]] [[0.147]
 [0.269]
 [0.269]
 [0.251]
 [0.251]]
printing an ep nov before normalisation:  39.33460474014282
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.39739565204285
printing an ep nov before normalisation:  61.17703268310958
printing an ep nov before normalisation:  70.06440476268368
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84417653
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.519]
 [25.623]
 [55.182]
 [23.466]
 [21.815]] [[0.18 ]
 [0.198]
 [0.697]
 [0.162]
 [0.134]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.94841692146662
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.207]
 [38.226]
 [48.395]
 [38.754]
 [35.338]] [[1.185]
 [0.776]
 [1.239]
 [0.8  ]
 [0.645]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.191]
 [37.913]
 [37.236]
 [30.504]
 [27.299]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.84272265
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.41175454469233
printing an ep nov before normalisation:  30.066234207808805
printing an ep nov before normalisation:  48.38381850338396
printing an ep nov before normalisation:  45.159406561927554
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 40.64417067056926
using explorer policy with actor:  1
siam score:  -0.8357549
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.83662826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.221931728505105
printing an ep nov before normalisation:  8.694690069670514
printing an ep nov before normalisation:  45.94217639707156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.695670108031276
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.83209014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  1  action  0 :  tensor([    0.9990,     0.0000,     0.0000,     0.0003,     0.0007],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0070,     0.9864,     0.0026,     0.0000,     0.0040],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0098, 0.0066, 0.9539, 0.0148, 0.0149], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0099, 0.0008, 0.0163, 0.6180, 0.3550], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0412, 0.0014, 0.1623, 0.2472, 0.5479], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 53.12341120021399
printing an ep nov before normalisation:  56.77406441926416
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.395117711518008
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.333645343780518
printing an ep nov before normalisation:  33.8554310798645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0035,     0.9798,     0.0000,     0.0002,     0.0165],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0006,     0.9444,     0.0225,     0.0322],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0028, 0.0013, 0.0414, 0.7377, 0.2168], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0369, 0.0025, 0.0461, 0.3473, 0.5672], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  70.00962341753217
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.74 ]
 [32.712]
 [39.995]
 [39.529]
 [31.646]] [[0.313]
 [0.395]
 [0.545]
 [0.535]
 [0.373]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.3665851020232253
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  114.88183551739604
printing an ep nov before normalisation:  61.71248359006256
printing an ep nov before normalisation:  26.350903511047363
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 60.203046735118576
printing an ep nov before normalisation:  74.98259457578791
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.982]
 [62.982]
 [67.083]
 [62.982]
 [62.982]] [[1.779]
 [1.779]
 [2.   ]
 [1.779]
 [1.779]]
printing an ep nov before normalisation:  67.45289047263137
printing an ep nov before normalisation:  68.61758595625676
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.712]
 [49.712]
 [57.629]
 [49.712]
 [49.712]] [[1.358]
 [1.358]
 [1.667]
 [1.358]
 [1.358]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.912157351432796
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.272]
 [64.572]
 [66.73 ]
 [64.572]
 [64.572]] [[0.942]
 [1.395]
 [1.468]
 [1.395]
 [1.395]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.846 0.026 0.077 0.026 0.026]
printing an ep nov before normalisation:  61.94893901179339
printing an ep nov before normalisation:  59.290861958359784
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.197]
 [53.847]
 [55.07 ]
 [53.533]
 [49.336]] [[1.424]
 [1.615]
 [1.665]
 [1.602]
 [1.43 ]]
printing an ep nov before normalisation:  59.89668641761003
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.011]
 [38.192]
 [41.948]
 [23.688]
 [23.087]] [[0.472]
 [0.623]
 [0.732]
 [0.201]
 [0.184]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.47834514519248
printing an ep nov before normalisation:  68.40250734624757
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.77983010690829
printing an ep nov before normalisation:  46.88742770681324
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8475747
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.77383122381403
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.29270234560504
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.0157958926168
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.491]
 [49.491]
 [49.491]
 [49.491]
 [49.491]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.839862251497735
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[74.073]
 [59.257]
 [59.257]
 [59.257]
 [59.257]] [[0.817]
 [0.579]
 [0.579]
 [0.579]
 [0.579]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.01512622833252
printing an ep nov before normalisation:  53.56186866760254
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.942068483142855
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.726]
 [45.546]
 [48.74 ]
 [45.593]
 [43.432]] [[0.196]
 [0.249]
 [0.284]
 [0.25 ]
 [0.226]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.61883214342093
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9993,     0.0000,     0.0000,     0.0002,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9908,     0.0002,     0.0002,     0.0085],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0003,     0.9614,     0.0137,     0.0246],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0143, 0.0010, 0.0040, 0.7061, 0.2746], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0098,     0.0005,     0.0690,     0.3683,     0.5524],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  20.383032730647496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.92037525581706
printing an ep nov before normalisation:  68.30763169662765
printing an ep nov before normalisation:  31.13874912261963
printing an ep nov before normalisation:  39.62938617205893
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.634]
 [51.148]
 [56.506]
 [51.148]
 [51.148]] [[1.209]
 [0.974]
 [1.168]
 [0.974]
 [0.974]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.024939929911184
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  111.88238012950653
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.653656375382326
printing an ep nov before normalisation:  44.67446290727252
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8434383
siam score:  -0.84234273
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.431]
 [65.267]
 [76.511]
 [73.135]
 [67.319]] [[0.802]
 [0.868]
 [1.274]
 [1.152]
 [0.942]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.694565832255535
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.68311858275945
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[144.161]
 [144.161]
 [144.161]
 [144.161]
 [144.161]] [[0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.901451110839844
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  104.00936998213837
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.57012843420747
siam score:  -0.82768935
printing an ep nov before normalisation:  44.61982250213623
printing an ep nov before normalisation:  53.69336920237487
printing an ep nov before normalisation:  71.72759860647514
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9755,     0.0002,     0.0000,     0.0157,     0.0086],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9764,     0.0023,     0.0003,     0.0209],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0072,     0.9094,     0.0500,     0.0332],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0007,     0.0013,     0.0508,     0.7590,     0.1881],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0028, 0.0017, 0.0636, 0.4571, 0.4748], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.4303032438724
printing an ep nov before normalisation:  73.33361848386744
printing an ep nov before normalisation:  29.65811413791466
printing an ep nov before normalisation:  55.19081906077513
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.14276336949691
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  141.19315054136723
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 54.783872783334324
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.535]
 [45.535]
 [45.535]
 [45.535]
 [45.535]] [[0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.88198471069336
actions average: 
K:  1  action  0 :  tensor([    0.9963,     0.0019,     0.0000,     0.0006,     0.0012],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0049,     0.9831,     0.0017,     0.0000,     0.0103],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9684,     0.0191,     0.0124],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0006,     0.0003,     0.0604,     0.7539,     0.1849],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0007, 0.0318, 0.0643, 0.3645, 0.5388], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.881]
 [37.055]
 [19.993]
 [20.186]
 [37.055]] [[0.692]
 [1.087]
 [0.576]
 [0.582]
 [1.087]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.18454605936198
siam score:  -0.85055524
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.047]
 [60.047]
 [60.047]
 [60.047]
 [60.047]] [[1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.225]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.855]
 [59.22 ]
 [68.007]
 [73.043]
 [64.549]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.33560867594677
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.84863360607162
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.6618665591118
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.57923079458824
printing an ep nov before normalisation:  28.61732006072998
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.82959386922026
printing an ep nov before normalisation:  75.60314124468562
printing an ep nov before normalisation:  70.67676608598963
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  66.22296841269018
printing an ep nov before normalisation:  58.545029837652365
siam score:  -0.8548314
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.742]
 [30.526]
 [37.221]
 [37.036]
 [32.822]] [[0.792]
 [0.609]
 [0.899]
 [0.891]
 [0.708]]
printing an ep nov before normalisation:  47.042187261132874
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.93463039096871
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.741975553582332
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.46138243659804
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.06147015234095
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.99110478988229
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.14123833767815
UNIT TEST: sample policy line 217 mcts : [0.154 0.179 0.077 0.179 0.41 ]
printing an ep nov before normalisation:  49.854472834502936
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.46338684095186
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.881]
 [60.881]
 [60.881]
 [60.881]
 [60.881]] [[1.844]
 [1.844]
 [1.844]
 [1.844]
 [1.844]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.10209894180298
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[77.892]
 [77.892]
 [77.892]
 [77.892]
 [77.892]] [[1.24]
 [1.24]
 [1.24]
 [1.24]
 [1.24]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.440430641174316
actions average: 
K:  3  action  0 :  tensor([    0.9992,     0.0000,     0.0000,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9935,     0.0046,     0.0000,     0.0019],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0001,     0.9321,     0.0469,     0.0208],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0140,     0.0002,     0.0486,     0.7713,     0.1658],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0096, 0.0007, 0.0526, 0.4590, 0.4781], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.79132958896089
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.18626413600003
siam score:  -0.85408133
siam score:  -0.8539898
printing an ep nov before normalisation:  65.26670894487438
printing an ep nov before normalisation:  54.14231954178012
printing an ep nov before normalisation:  70.98133490591701
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  29.004929065704346
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.777288826161794
siam score:  -0.8566788
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.66462516784668
actions average: 
K:  0  action  0 :  tensor([    0.8302,     0.0000,     0.0000,     0.1153,     0.0545],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0197, 0.9228, 0.0037, 0.0036, 0.0501], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0049, 0.0086, 0.9037, 0.0373, 0.0455], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0422, 0.0027, 0.1528, 0.6651, 0.1372], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0327, 0.0071, 0.2562, 0.1857, 0.5184], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 61.77211047292004
printing an ep nov before normalisation:  76.7199682500063
deleting a thread, now have 1 threads
Frames:  21809 train batches done:  2554 episodes:  847
siam score:  -0.85638344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86212236
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.86258227
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.370801063775666
printing an ep nov before normalisation:  8.652043402027232
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 45.143031540521285
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.12521580874663
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.85168527843221
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9973,     0.0001,     0.0000,     0.0017,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.9586,     0.0227,     0.0001,     0.0181],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0001,     0.9147,     0.0256,     0.0596],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0065, 0.0322, 0.0336, 0.7734, 0.1543], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0172, 0.0687, 0.0675, 0.4489, 0.3977], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.959705473732654
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86253816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.13818071079687
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.817]
 [36.817]
 [43.787]
 [55.095]
 [36.817]] [[0.322]
 [0.322]
 [0.449]
 [0.657]
 [0.322]]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.9672,     0.0000,     0.0000,     0.0293,     0.0034],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0055,     0.9580,     0.0081,     0.0004,     0.0279],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0076, 0.0260, 0.7997, 0.0774, 0.0893], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0005,     0.0002,     0.0200,     0.7793,     0.2000],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0048,     0.0005,     0.0047,     0.4464,     0.5436],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  66.0656749898118
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8602646
actions average: 
K:  3  action  0 :  tensor([    0.9735,     0.0004,     0.0000,     0.0095,     0.0166],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0031,     0.9946,     0.0005,     0.0000,     0.0018],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0022,     0.9417,     0.0192,     0.0368],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0020, 0.0022, 0.0512, 0.6452, 0.2994], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0025, 0.0516, 0.0154, 0.2705, 0.6600], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  65.47649063667419
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  85.72489288861716
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  105.89416672483257
printing an ep nov before normalisation:  85.39730524245664
printing an ep nov before normalisation:  41.21866226196289
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.85889894
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.826]
 [33.183]
 [44.45 ]
 [33.183]
 [33.183]] [[0.796]
 [1.016]
 [1.583]
 [1.016]
 [1.016]]
printing an ep nov before normalisation:  39.448838233947754
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.40549735198457
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.54508087005908
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.46847185157191
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.00847055943873
printing an ep nov before normalisation:  34.901161193847656
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  36.625441659984006
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85111696
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.4904431327085
printing an ep nov before normalisation:  56.10347729797923
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  84.48375285634557
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.581]
 [36.18 ]
 [24.875]
 [30.891]
 [29.362]] [[0.91 ]
 [0.895]
 [0.457]
 [0.69 ]
 [0.63 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.3660678089393817
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 35.241555126423535
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 57.8197241530011
printing an ep nov before normalisation:  53.828967724008955
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  97.98812077543741
printing an ep nov before normalisation:  73.87629590977907
line 256 mcts: sample exp_bonus 54.472837079875795
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.805]
 [41.559]
 [32.084]
 [30.247]
 [31.322]] [[0.224]
 [0.194]
 [0.127]
 [0.114]
 [0.122]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
line 256 mcts: sample exp_bonus 45.55947303771973
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.8064515877785
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.929]
 [45.929]
 [57.985]
 [56.272]
 [45.929]] [[0.944]
 [0.944]
 [1.339]
 [1.283]
 [0.944]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.67652605255029
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.25778540064652
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.432516060510466
siam score:  -0.8557623
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8535766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.81547329062331
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.854209
printing an ep nov before normalisation:  33.69981050491333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.69028373676786
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.018940443754616
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8562336
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  88.80053025568562
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.399794193411225
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.385 0.179 0.385 0.051 0.   ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.86856253624619
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.12500425957187
printing an ep nov before normalisation:  56.90537749754448
printing an ep nov before normalisation:  25.632257731950133
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 33.81222879083558
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.5496785925835
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.13545761705573
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.9941,     0.0000,     0.0000,     0.0037,     0.0021],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9994,     0.0001,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0066, 0.0287, 0.8705, 0.0558, 0.0385], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0004,     0.0001,     0.0264,     0.8319,     0.1412],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0117, 0.0972, 0.0904, 0.2425, 0.5582], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.765305042266846
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.469400260091895
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  19.04654567223071
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.41120112996372
printing an ep nov before normalisation:  69.02256950059011
printing an ep nov before normalisation:  43.34702968597412
siam score:  -0.84404373
siam score:  -0.8446133
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  47.501779320072615
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.7906811269588
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.08712017961791
actions average: 
K:  4  action  0 :  tensor([    0.9630,     0.0249,     0.0000,     0.0007,     0.0115],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0020, 0.9396, 0.0447, 0.0042, 0.0096], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0049,     0.9382,     0.0377,     0.0191],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0320,     0.0003,     0.0430,     0.8085,     0.1161],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0419, 0.0342, 0.1101, 0.3639, 0.4499], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9933,     0.0004,     0.0000,     0.0008,     0.0055],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9890,     0.0016,     0.0000,     0.0093],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0001,     0.8356,     0.0552,     0.1088],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0014,     0.0004,     0.0414,     0.6394,     0.3173],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0011, 0.0192, 0.0828, 0.3918, 0.5051], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.967671275138855
printing an ep nov before normalisation:  37.871412268612396
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.16616682148238
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.191]
 [38.191]
 [33.825]
 [39.504]
 [38.191]] [[0.233]
 [0.233]
 [0.183]
 [0.249]
 [0.233]]
printing an ep nov before normalisation:  40.7308997991237
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.98807838799835
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.384503877930214
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.21603276942808
printing an ep nov before normalisation:  54.91465160078906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.20002290807597
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.77]
 [47.77]
 [47.77]
 [47.77]
 [47.77]] [[0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.737527331170355
printing an ep nov before normalisation:  52.43674602509411
printing an ep nov before normalisation:  68.92563059623112
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  100.76734231950111
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.675466060638428
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9952,     0.0001,     0.0000,     0.0025,     0.0022],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9707,     0.0261,     0.0000,     0.0031],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9245,     0.0501,     0.0253],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0177,     0.0003,     0.0872,     0.7373,     0.1575],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0148, 0.0462, 0.0093, 0.2768, 0.6528], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 53.40017482738243
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8681355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.15358054669903
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.22270107269287
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.501]
 [52.541]
 [52.72 ]
 [49.951]
 [49.703]] [[0.403]
 [0.419]
 [0.421]
 [0.381]
 [0.377]]
printing an ep nov before normalisation:  43.320484757749526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.37280606424387
printing an ep nov before normalisation:  26.576784297842828
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([0.9717, 0.0062, 0.0013, 0.0046, 0.0162], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0011, 0.9817, 0.0028, 0.0025, 0.0120], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0006,     0.9235,     0.0494,     0.0265],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0002,     0.0299,     0.8082,     0.1617],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0004,     0.0057,     0.1106,     0.3629,     0.5205],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.007598400115967
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.058]
 [64.107]
 [66.482]
 [64.107]
 [64.107]] [[0.791]
 [1.582]
 [1.667]
 [1.582]
 [1.582]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.27197421866739
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.56130141654842
printing an ep nov before normalisation:  55.92713438852499
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.779161644238993
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.38500045448252
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.0926445362635
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.72994724637381
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86203295
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[70.795]
 [64.207]
 [57.35 ]
 [55.141]
 [60.083]] [[0.429]
 [0.346]
 [0.26 ]
 [0.233]
 [0.295]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.849809098579065
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.9901,     0.0006,     0.0000,     0.0046,     0.0046],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9709,     0.0201,     0.0007,     0.0082],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0012,     0.9460,     0.0226,     0.0302],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0045,     0.0003,     0.0982,     0.6563,     0.2407],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0073,     0.0005,     0.0468,     0.4153,     0.5301],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.462 0.077 0.154 0.256 0.051]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.292097953321154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.69223773113325
printing an ep nov before normalisation:  48.33699331024748
printing an ep nov before normalisation:  22.531485557556152
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.75320830220265
printing an ep nov before normalisation:  65.16521286041086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.971329181280495
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.33396875430353
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.194]
 [48.194]
 [55.764]
 [48.194]
 [48.194]] [[1.283]
 [1.283]
 [1.667]
 [1.283]
 [1.283]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.08060673664903
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.53706795798514
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.75992337993358
printing an ep nov before normalisation:  54.71262348938193
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  63.5307792714465
siam score:  -0.8579722
printing an ep nov before normalisation:  48.66328265005859
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8591962
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.11489922026296
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[74.14 ]
 [47.236]
 [47.236]
 [47.236]
 [75.196]] [[0.916]
 [0.295]
 [0.295]
 [0.295]
 [0.941]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9960,     0.0000,     0.0000,     0.0024,     0.0015],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0005,     0.9950,     0.0015,     0.0000,     0.0030],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0005,     0.9579,     0.0185,     0.0231],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0058, 0.0207, 0.0311, 0.7932, 0.1493], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0082, 0.0753, 0.0490, 0.3234, 0.5442], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  81.67093606218843
UNIT TEST: sample policy line 217 mcts : [0.615 0.051 0.205 0.026 0.103]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.442779412403674
printing an ep nov before normalisation:  43.14871230228711
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86618197
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.36082820774294
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87052625
printing an ep nov before normalisation:  38.60527878710139
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.184733877835555
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.317]
 [27.842]
 [33.948]
 [25.357]
 [24.029]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  62.417837514585216
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.21789707773374
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
siam score:  -0.86849385
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.21661874944697
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.8698027
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9877,     0.0000,     0.0000,     0.0053,     0.0069],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0012,     0.9486,     0.0014,     0.0000,     0.0488],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0033,     0.9356,     0.0342,     0.0269],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0007, 0.0014, 0.0348, 0.6587, 0.3044], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0009, 0.0159, 0.0726, 0.3331, 0.5775], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  73.64238718884009
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.005]
 [43.005]
 [43.005]
 [43.005]
 [43.005]] [[1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.451]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.42 ]
 [44.473]
 [45.755]
 [46.072]
 [44.325]] [[0.816]
 [1.063]
 [1.125]
 [1.141]
 [1.055]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9989,     0.0000,     0.0000,     0.0004,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9954,     0.0000,     0.0000,     0.0045],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0004,     0.9506,     0.0318,     0.0172],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0018,     0.0002,     0.0361,     0.7176,     0.2443],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0028,     0.0004,     0.1334,     0.2758,     0.5875],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.802]
 [58.743]
 [57.535]
 [58.743]
 [58.743]] [[1.35 ]
 [0.982]
 [0.948]
 [0.982]
 [0.982]]
siam score:  -0.8517235
printing an ep nov before normalisation:  62.315795836638834
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.01709737028036784
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.096697998684206
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 82.76011204025245
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.374]
 [49.745]
 [80.078]
 [47.946]
 [41.555]] [[0.362]
 [0.44 ]
 [0.882]
 [0.414]
 [0.321]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.522064273464565
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.324]
 [43.984]
 [40.73 ]
 [45.121]
 [43.984]] [[0.333]
 [0.243]
 [0.214]
 [0.253]
 [0.243]]
printing an ep nov before normalisation:  58.15065500558885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 32.34677791595459
siam score:  -0.8619425
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.95673775767917
printing an ep nov before normalisation:  52.85974102964831
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.89817417086803
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.15680867221361
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[72.53 ]
 [68.624]
 [52.046]
 [69.291]
 [68.624]] [[1.385]
 [1.279]
 [0.829]
 [1.297]
 [1.279]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.367]
 [32.976]
 [49.523]
 [32.976]
 [45.002]] [[1.004]
 [0.401]
 [0.944]
 [0.401]
 [0.795]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.831]
 [32.022]
 [48.129]
 [32.022]
 [32.022]] [[0.841]
 [0.348]
 [0.884]
 [0.348]
 [0.348]]
line 256 mcts: sample exp_bonus 33.343132953958595
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.3973570113396
siam score:  -0.86679125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.9581,     0.0000,     0.0000,     0.0409,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9529,     0.0327,     0.0000,     0.0143],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0126,     0.8512,     0.0707,     0.0653],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0017,     0.0005,     0.0034,     0.7394,     0.2549],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0005,     0.0003,     0.1000,     0.4951,     0.4041],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.38048507793071
printing an ep nov before normalisation:  66.65496714154885
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.78693527268668
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.837]
 [50.271]
 [54.563]
 [55.271]
 [55.982]] [[1.346]
 [1.508]
 [1.711]
 [1.745]
 [1.778]]
printing an ep nov before normalisation:  35.715823111541496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  72.87546873709704
printing an ep nov before normalisation:  52.81333835433855
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.675]
 [57.675]
 [57.675]
 [57.675]
 [57.675]] [[1.271]
 [1.271]
 [1.271]
 [1.271]
 [1.271]]
printing an ep nov before normalisation:  64.84998634400426
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.67974522792691
printing an ep nov before normalisation:  0.03671107865528711
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.43754385129233
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.699]
 [43.699]
 [43.699]
 [43.699]
 [43.699]] [[0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]]
printing an ep nov before normalisation:  88.85078810970772
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.95718936714772
actions average: 
K:  2  action  0 :  tensor([    0.9992,     0.0000,     0.0000,     0.0002,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0018,     0.9511,     0.0001,     0.0025,     0.0444],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0002,     0.9008,     0.0471,     0.0519],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0050, 0.0033, 0.0582, 0.6290, 0.3044], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0065, 0.0007, 0.0695, 0.3645, 0.5588], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  94.29872940739432
printing an ep nov before normalisation:  47.34685472638212
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.7326091206193399
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.467]
 [68.004]
 [60.2  ]
 [68.004]
 [68.004]] [[1.563]
 [1.548]
 [1.29 ]
 [1.548]
 [1.548]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.956]
 [47.956]
 [47.956]
 [47.956]
 [47.956]] [[1.049]
 [1.049]
 [1.049]
 [1.049]
 [1.049]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9478,     0.0004,     0.0000,     0.0255,     0.0263],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0023,     0.9666,     0.0006,     0.0130,     0.0175],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0012,     0.0002,     0.9219,     0.0495,     0.0272],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0263,     0.0002,     0.0377,     0.7601,     0.1756],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0136, 0.0350, 0.0937, 0.2009, 0.6568], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.11880269643399
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.12491777123853
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
printing an ep nov before normalisation:  55.6166959798695
printing an ep nov before normalisation:  29.381370544433594
printing an ep nov before normalisation:  28.01220460134213
printing an ep nov before normalisation:  11.08653728945228
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.62072588741384
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.86120232256196
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.52734256955423
printing an ep nov before normalisation:  34.85317088433597
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.01269266641717
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[18.345]
 [28.111]
 [37.818]
 [17.113]
 [16.071]] [[0.088]
 [0.229]
 [0.37 ]
 [0.07 ]
 [0.055]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8559451
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.361]
 [58.361]
 [58.361]
 [58.361]
 [58.361]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.37192344665527
printing an ep nov before normalisation:  112.55583751076273
printing an ep nov before normalisation:  93.88206532250956
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  53.1126341789419
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.316]
 [32.683]
 [32.683]
 [26.547]
 [32.683]] [[1.   ]
 [1.022]
 [1.022]
 [0.66 ]
 [1.022]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.26156128460378
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.49002901238914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.715535631927143
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.266]
 [35.669]
 [41.784]
 [36.376]
 [35.757]] [[0.998]
 [0.961]
 [1.333]
 [1.004]
 [0.967]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.29 ]
 [51.536]
 [42.29 ]
 [42.29 ]
 [42.29 ]] [[1.093]
 [1.333]
 [1.093]
 [1.093]
 [1.093]]
printing an ep nov before normalisation:  38.279046373472234
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.34441893603116114
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[92.028]
 [92.028]
 [92.028]
 [92.028]
 [92.028]] [[1.379]
 [1.379]
 [1.379]
 [1.379]
 [1.379]]
siam score:  -0.8658167
printing an ep nov before normalisation:  46.763020977921464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  31.157164573669434
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.073425780589396
printing an ep nov before normalisation:  54.80574079394728
printing an ep nov before normalisation:  53.06750454868551
printing an ep nov before normalisation:  41.37533184555779
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.02745192848335
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.62836980819702
printing an ep nov before normalisation:  73.00661045011591
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.868]
 [40.021]
 [28.409]
 [27.514]
 [31.054]] [[0.847]
 [0.994]
 [0.584]
 [0.552]
 [0.677]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  84.3745034955761
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8488295
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0012,     0.9784,     0.0041,     0.0000,     0.0163],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9165,     0.0506,     0.0328],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0048,     0.0004,     0.0397,     0.7042,     0.2509],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0394, 0.0218, 0.0990, 0.3083, 0.5315], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.26415657667637
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.285]
 [45.285]
 [45.285]
 [38.322]
 [45.285]] [[0.898]
 [0.898]
 [0.898]
 [0.667]
 [0.898]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.137]
 [49.448]
 [60.551]
 [48.436]
 [49.137]] [[0.182]
 [0.184]
 [0.252]
 [0.178]
 [0.182]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.625]
 [25.212]
 [28.707]
 [27.936]
 [26.985]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
UNIT TEST: sample policy line 217 mcts : [0.179 0.41  0.103 0.179 0.128]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.45667296219939
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[15.229]
 [15.167]
 [14.572]
 [13.685]
 [14.666]] [[0.663]
 [0.658]
 [0.61 ]
 [0.54 ]
 [0.618]]
printing an ep nov before normalisation:  39.31709244129232
printing an ep nov before normalisation:  41.463801205213436
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.714]
 [37.714]
 [37.714]
 [37.714]
 [37.714]] [[0.921]
 [0.921]
 [0.921]
 [0.921]
 [0.921]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.908]
 [42.931]
 [42.931]
 [42.931]
 [42.931]] [[0.831]
 [0.683]
 [0.683]
 [0.683]
 [0.683]]
printing an ep nov before normalisation:  59.41387887343215
printing an ep nov before normalisation:  41.07363224029541
line 256 mcts: sample exp_bonus 41.614723205566406
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.05140506915357
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.357]
 [40.357]
 [40.357]
 [40.357]
 [40.357]] [[0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.403579809927706
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.44658242519365
printing an ep nov before normalisation:  33.594128399038375
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.101]
 [49.101]
 [45.288]
 [49.101]
 [49.101]] [[1.333]
 [1.333]
 [1.176]
 [1.333]
 [1.333]]
printing an ep nov before normalisation:  37.662951869566434
printing an ep nov before normalisation:  45.446990676034595
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.896]
 [52.468]
 [51.028]
 [52.816]
 [51.9  ]] [[0.894]
 [0.945]
 [0.898]
 [0.956]
 [0.926]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.9958,     0.0000,     0.0000,     0.0037,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0011,     0.9836,     0.0014,     0.0002,     0.0137],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0000,     0.9310,     0.0311,     0.0375],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0195,     0.0002,     0.0737,     0.6985,     0.2081],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0060,     0.0005,     0.1264,     0.3021,     0.5650],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.26295255648356
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.229]
 [32.381]
 [36.72 ]
 [38.872]
 [37.626]] [[0.688]
 [0.937]
 [1.147]
 [1.251]
 [1.191]]
actions average: 
K:  3  action  0 :  tensor([    0.9634,     0.0168,     0.0001,     0.0021,     0.0175],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9884,     0.0000,     0.0000,     0.0116],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0059,     0.9162,     0.0429,     0.0349],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0037, 0.0007, 0.0076, 0.7096, 0.2784], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0083, 0.0058, 0.0863, 0.3983, 0.5012], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.27368268751641
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.98452368630349
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.05997579727436
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.80843842901258
printing an ep nov before normalisation:  58.775930404663086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9984,     0.0000,     0.0000,     0.0005,     0.0011],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9963,     0.0003,     0.0000,     0.0033],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0009,     0.0159,     0.9158,     0.0418,     0.0256],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0002,     0.0274,     0.8083,     0.1640],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0366, 0.0067, 0.0197, 0.3678, 0.5693], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.50905480644418
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.56488342988782
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.4561363921327
printing an ep nov before normalisation:  40.84721826891755
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.669]
 [36.669]
 [35.915]
 [36.669]
 [36.669]] [[0.302]
 [0.302]
 [0.29 ]
 [0.302]
 [0.302]]
printing an ep nov before normalisation:  52.53319391447264
actions average: 
K:  3  action  0 :  tensor([    0.9885,     0.0007,     0.0000,     0.0035,     0.0073],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0017,     0.9220,     0.0000,     0.0004,     0.0759],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0024,     0.9257,     0.0422,     0.0297],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0150, 0.0023, 0.0790, 0.5813, 0.3223], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0017, 0.0397, 0.0535, 0.2658, 0.6394], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.66307783641713
printing an ep nov before normalisation:  78.47436491563641
siam score:  -0.85783124
printing an ep nov before normalisation:  39.93057098737518
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.985]
 [31.52 ]
 [28.201]
 [29.106]
 [31.52 ]] [[0.706]
 [1.667]
 [1.333]
 [1.424]
 [1.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.178391933441162
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.0994555371534
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.374]
 [30.622]
 [36.09 ]
 [36.943]
 [30.137]] [[1.102]
 [0.877]
 [1.205]
 [1.256]
 [0.848]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.58038144885831
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.44 ]
 [40.44 ]
 [65.666]
 [42.228]
 [40.44 ]] [[0.503]
 [0.503]
 [0.817]
 [0.525]
 [0.503]]
printing an ep nov before normalisation:  27.90401123010321
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.03949987180266
using explorer policy with actor:  1
printing an ep nov before normalisation:  20.068831310620574
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.716]
 [35.716]
 [50.771]
 [35.716]
 [35.716]] [[0.745]
 [0.745]
 [1.492]
 [0.745]
 [0.745]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.589]
 [37.047]
 [38.81 ]
 [34.868]
 [34.136]] [[1.754]
 [1.551]
 [1.692]
 [1.377]
 [1.319]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.159]
 [31.271]
 [42.436]
 [31.271]
 [31.271]] [[1.381]
 [0.636]
 [1.336]
 [0.636]
 [0.636]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.711]
 [50.09 ]
 [49.818]
 [47.742]
 [44.85 ]] [[1.839]
 [1.725]
 [1.706]
 [1.559]
 [1.355]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.57174396965919
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.608]
 [51.419]
 [50.677]
 [48.539]
 [44.068]] [[1.617]
 [1.452]
 [1.414]
 [1.303]
 [1.072]]
printing an ep nov before normalisation:  0.024858615503262627
printing an ep nov before normalisation:  57.89039273273458
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.539168161452395
printing an ep nov before normalisation:  60.6101238469871
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.718327918414936
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.267553702319304
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.45549011230469
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.83890934995982
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  83.08416871009256
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.641074383854644
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.22505034863191
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  109.10183194800167
siam score:  -0.853873
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.138563914468214
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[82.136]
 [82.136]
 [82.136]
 [82.136]
 [82.136]] [[1.929]
 [1.929]
 [1.929]
 [1.929]
 [1.929]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.407]
 [36.407]
 [36.407]
 [36.407]
 [36.407]] [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.141568257062612
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.750801847453612
printing an ep nov before normalisation:  36.0914912415453
printing an ep nov before normalisation:  87.35584699760466
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.79001282845842
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.434]
 [34.818]
 [75.582]
 [30.1  ]
 [34.818]] [[0.263]
 [0.243]
 [0.745]
 [0.185]
 [0.243]]
printing an ep nov before normalisation:  59.71997046995218
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.17439735404616
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.077]
 [33.976]
 [35.061]
 [33.976]
 [33.976]] [[1.042]
 [0.64 ]
 [0.679]
 [0.64 ]
 [0.64 ]]
printing an ep nov before normalisation:  42.23283539604905
siam score:  -0.8702305
siam score:  -0.86731386
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.109]
 [54.109]
 [54.109]
 [54.109]
 [54.109]] [[1.091]
 [1.091]
 [1.091]
 [1.091]
 [1.091]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.189]
 [47.189]
 [49.607]
 [45.231]
 [47.189]] [[0.906]
 [0.906]
 [0.952]
 [0.868]
 [0.906]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8649094
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9984,     0.0011,     0.0002,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0014,     0.9901,     0.0013,     0.0000,     0.0072],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9074,     0.0314,     0.0611],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0005,     0.0602,     0.6644,     0.2746],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0031, 0.0098, 0.1332, 0.3794, 0.4745], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.995991706848145
printing an ep nov before normalisation:  16.368491649627686
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.333 0.231 0.077 0.179 0.179]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.56006050109863
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.186]
 [45.226]
 [57.44 ]
 [48.191]
 [51.957]] [[0.647]
 [0.621]
 [0.953]
 [0.702]
 [0.804]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.00255039578631
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8652267
printing an ep nov before normalisation:  40.80887317657471
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.71068468585899
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.676]
 [56.357]
 [56.464]
 [56.357]
 [56.357]] [[0.913]
 [1.097]
 [1.101]
 [1.097]
 [1.097]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.323]
 [48.019]
 [44.525]
 [47.602]
 [50.631]] [[1.66 ]
 [1.271]
 [1.084]
 [1.249]
 [1.41 ]]
printing an ep nov before normalisation:  30.4090084677413
printing an ep nov before normalisation:  29.18442148865597
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.11899692615616
printing an ep nov before normalisation:  52.46809095760317
line 256 mcts: sample exp_bonus 40.568077960984354
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.315]
 [30.686]
 [35.45 ]
 [35.069]
 [29.519]] [[0.607]
 [0.488]
 [0.644]
 [0.632]
 [0.45 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.549145274006335
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9375,     0.0006,     0.0000,     0.0304,     0.0315],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9995,     0.0001,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0026,     0.9679,     0.0140,     0.0152],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0017, 0.0054, 0.0550, 0.6397, 0.2983], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0011, 0.0007, 0.2636, 0.3541, 0.3805], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.562757408983366
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.00450297629328
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9938,     0.0000,     0.0000,     0.0029,     0.0033],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9902,     0.0015,     0.0000,     0.0081],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0002,     0.9296,     0.0250,     0.0452],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0047,     0.0001,     0.0120,     0.7165,     0.2666],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0712, 0.0009, 0.0809, 0.2294, 0.6176], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.67949948479298
printing an ep nov before normalisation:  55.29021855842081
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  50.51430026790216
printing an ep nov before normalisation:  50.93351999951888
printing an ep nov before normalisation:  30.24688429535425
actions average: 
K:  1  action  0 :  tensor([    0.9991,     0.0001,     0.0000,     0.0004,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0722,     0.9248,     0.0003,     0.0005,     0.0021],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0005,     0.9295,     0.0340,     0.0358],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0031, 0.0109, 0.1054, 0.6784, 0.2022], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0154, 0.0334, 0.0458, 0.2882, 0.6172], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.467397709913584
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.333306618800776
printing an ep nov before normalisation:  74.92523599748226
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.67108746202313
printing an ep nov before normalisation:  43.488919125573716
printing an ep nov before normalisation:  37.37974453294505
printing an ep nov before normalisation:  62.36451922662195
printing an ep nov before normalisation:  31.664281129193323
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.48 ]
 [45.047]
 [49.243]
 [53.214]
 [52.603]] [[0.777]
 [0.474]
 [0.586]
 [0.691]
 [0.675]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.09841514008917
printing an ep nov before normalisation:  43.62539960492728
printing an ep nov before normalisation:  45.966563011054106
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.9748,     0.0003,     0.0000,     0.0149,     0.0100],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9972,     0.0002,     0.0000,     0.0025],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0176, 0.0114, 0.8946, 0.0262, 0.0502], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0004,     0.0008,     0.0780,     0.6888,     0.2320],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0383, 0.0054, 0.0820, 0.4061, 0.4683], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9891,     0.0019,     0.0000,     0.0036,     0.0054],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0492,     0.9416,     0.0069,     0.0000,     0.0023],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0348,     0.9026,     0.0291,     0.0336],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0024, 0.0022, 0.0812, 0.6554, 0.2588], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0107, 0.1289, 0.0514, 0.3439, 0.4651], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.316]
 [43.805]
 [43.805]
 [45.9  ]
 [43.805]] [[1.055]
 [0.677]
 [0.677]
 [0.725]
 [0.677]]
printing an ep nov before normalisation:  107.19113021519986
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.896]
 [45.008]
 [48.551]
 [41.887]
 [49.652]] [[0.776]
 [1.165]
 [1.316]
 [1.032]
 [1.364]]
printing an ep nov before normalisation:  73.43518079297785
using explorer policy with actor:  1
printing an ep nov before normalisation:  110.8396190170662
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.98 ]
 [27.946]
 [32.956]
 [37.909]
 [37.907]] [[1.609]
 [1.046]
 [1.233]
 [1.419]
 [1.419]]
printing an ep nov before normalisation:  32.85684108734131
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  16.30492842469893
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.7231094377672
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.48736381530762
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.579]
 [30.579]
 [53.662]
 [36.751]
 [30.579]] [[0.562]
 [0.562]
 [1.297]
 [0.759]
 [0.562]]
printing an ep nov before normalisation:  75.73108318684012
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.914960996718676
actions average: 
K:  1  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9956,     0.0016,     0.0000,     0.0021],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0023,     0.9493,     0.0289,     0.0192],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0018,     0.0002,     0.0147,     0.7609,     0.2224],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0036, 0.0008, 0.0354, 0.4256, 0.5345], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  64.9246176063091
printing an ep nov before normalisation:  43.16110535712144
deleting a thread, now have 1 threads
Frames:  38066 train batches done:  4460 episodes:  1449
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9803,     0.0004,     0.0000,     0.0112,     0.0081],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9984,     0.0004,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0032, 0.0023, 0.8722, 0.0585, 0.0639], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0010,     0.0002,     0.0763,     0.7537,     0.1688],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0064, 0.0148, 0.0511, 0.3392, 0.5885], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.232795453486005
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.999]
 [33.752]
 [33.752]
 [31.303]
 [33.752]] [[1.021]
 [0.686]
 [0.686]
 [0.587]
 [0.686]]
printing an ep nov before normalisation:  26.254660280803396
actions average: 
K:  2  action  0 :  tensor([    0.9968,     0.0001,     0.0000,     0.0017,     0.0014],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0011,     0.9879,     0.0000,     0.0000,     0.0110],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9631,     0.0203,     0.0166],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0015, 0.0021, 0.0463, 0.7094, 0.2407], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0002,     0.0002,     0.0966,     0.5026,     0.4003],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.7909770067
printing an ep nov before normalisation:  37.0290080195944
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.09236717224121
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.15629386901855
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.342]
 [48.369]
 [46.62 ]
 [49.433]
 [51.039]] [[1.784]
 [1.518]
 [1.425]
 [1.575]
 [1.661]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.995318159591665
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.79866032856449
printing an ep nov before normalisation:  53.084425926208496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.715281657811147
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.01618673965552
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.1174766034957
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 59.43130493906746
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.853521
printing an ep nov before normalisation:  47.49602585690673
printing an ep nov before normalisation:  28.323683958033747
actions average: 
K:  3  action  0 :  tensor([    0.9670,     0.0005,     0.0000,     0.0068,     0.0257],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0024, 0.9574, 0.0035, 0.0012, 0.0355], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0002,     0.9207,     0.0427,     0.0364],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0005,     0.0992,     0.7122,     0.1878],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0019, 0.0017, 0.1349, 0.5120, 0.3494], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.46526354969276
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.22318895282487
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.61304187774658
printing an ep nov before normalisation:  39.73668122481834
printing an ep nov before normalisation:  46.672859459884556
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.15785735121767175
printing an ep nov before normalisation:  48.70298015268622
printing an ep nov before normalisation:  29.712104985144343
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.9667,     0.0002,     0.0000,     0.0308,     0.0023],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0014,     0.9950,     0.0002,     0.0000,     0.0035],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0505,     0.9176,     0.0146,     0.0172],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0015, 0.0021, 0.0523, 0.8169, 0.1272], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0225,     0.0005,     0.0008,     0.4254,     0.5508],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 23.16793375545078
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.32031111340895
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.86182885556995
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 34.34484417890427
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.61 ]
 [30.283]
 [52.739]
 [50.348]
 [27.192]] [[0.206]
 [0.179]
 [0.442]
 [0.414]
 [0.143]]
printing an ep nov before normalisation:  76.93245165608596
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  31.18361228922666
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.869411
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.74905418905778
printing an ep nov before normalisation:  27.982106073580752
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.066]
 [39.952]
 [38.126]
 [51.013]
 [41.738]] [[0.615]
 [0.455]
 [0.434]
 [0.58 ]
 [0.475]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.656666959666246
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.09336096725320431
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86166644
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86202234
Starting evaluation
printing an ep nov before normalisation:  32.62537717819214
printing an ep nov before normalisation:  31.23612999916077
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.10257122874785
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.18762721285817
printing an ep nov before normalisation:  40.92043710357615
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.207]
 [31.638]
 [25.713]
 [24.748]
 [27.207]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  33.124942273938316
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.89138189294837
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.453]
 [44.479]
 [45.982]
 [44.737]
 [45.262]] [[1.366]
 [1.181]
 [1.251]
 [1.193]
 [1.217]]
actions average: 
K:  2  action  0 :  tensor([    0.9981,     0.0000,     0.0000,     0.0010,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9898,     0.0018,     0.0002,     0.0080],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0001,     0.9281,     0.0371,     0.0346],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0007,     0.0001,     0.0218,     0.7099,     0.2675],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0639, 0.0138, 0.1110, 0.2987, 0.5127], grad_fn=<DivBackward0>)
siam score:  -0.84902525
printing an ep nov before normalisation:  29.305182376352477
printing an ep nov before normalisation:  36.55153396335661
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.901]
 [47.901]
 [47.901]
 [47.901]
 [47.901]] [[1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.92632494305376
siam score:  -0.85192627
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.59859444123195
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.62 ]
 [44.575]
 [42.517]
 [35.952]
 [41.62 ]] [[1.369]
 [1.467]
 [1.399]
 [1.182]
 [1.369]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.147]
 [37.315]
 [46.963]
 [32.755]
 [37.315]] [[1.064]
 [0.725]
 [1.143]
 [0.527]
 [0.725]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.15368681635453
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8641676
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.52440857754689
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[     0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 3.5442875810157685e-20
0.0 1.1243965340002854e-12
0.0 1.8855265015435864e-12
0.0 -3.892141822592945e-13
0.0 1.2454854022400121e-12
0.0 -1.383872658795688e-12
0.0 6.4441592382104886e-21
0.0 2.0621309562273563e-19
0.0 0.0
0.0 1.7774114470450338e-12
printing an ep nov before normalisation:  77.66534364905495
siam score:  -0.8644684
printing an ep nov before normalisation:  49.22529192478109
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.752]
 [43.752]
 [44.886]
 [43.752]
 [43.752]] [[0.978]
 [0.978]
 [1.018]
 [0.978]
 [0.978]]
printing an ep nov before normalisation:  56.449783076734654
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.569889068603516
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.501]
 [31.208]
 [38.521]
 [32.857]
 [36.08 ]] [[0.815]
 [0.644]
 [0.936]
 [0.71 ]
 [0.839]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.9364, 0.0021, 0.0178, 0.0149, 0.0288], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9700,     0.0016,     0.0022,     0.0262],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9596,     0.0216,     0.0187],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0003,     0.0687,     0.7572,     0.1735],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0009, 0.0049, 0.0652, 0.3505, 0.5785], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.344695806684946
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.61093677131115
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.323]
 [48.323]
 [48.323]
 [48.323]
 [48.323]] [[0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]]
printing an ep nov before normalisation:  51.903493519202115
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  31.624907204895916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.645]
 [46.645]
 [56.064]
 [46.645]
 [46.645]] [[0.211]
 [0.211]
 [0.301]
 [0.211]
 [0.211]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 63.17827555558001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.725]
 [31.941]
 [41.389]
 [36.234]
 [22.633]] [[0.145]
 [0.241]
 [0.353]
 [0.292]
 [0.132]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  0.10600637937506639
printing an ep nov before normalisation:  31.180238723754883
printing an ep nov before normalisation:  55.382215816717014
printing an ep nov before normalisation:  46.5838945257377
printing an ep nov before normalisation:  30.922046891091725
printing an ep nov before normalisation:  37.11574986158111
printing an ep nov before normalisation:  51.02514986601792
printing an ep nov before normalisation:  50.62828635001458
printing an ep nov before normalisation:  44.89381999372066
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.62219429016113
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.904]
 [54.904]
 [54.904]
 [54.904]
 [54.904]] [[0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]]
printing an ep nov before normalisation:  61.28659027445963
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.76544630306051
printing an ep nov before normalisation:  56.198698727032884
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  44.6744441986084
printing an ep nov before normalisation:  68.33259400609627
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.617]
 [36.826]
 [36.826]
 [44.445]
 [36.826]] [[0.411]
 [0.306]
 [0.306]
 [0.408]
 [0.306]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.263]
 [60.046]
 [59.974]
 [56.183]
 [63.733]] [[1.177]
 [1.617]
 [1.613]
 [1.399]
 [1.826]]
printing an ep nov before normalisation:  41.554341316223145
printing an ep nov before normalisation:  48.2934335389963
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.32706757699453
printing an ep nov before normalisation:  50.197574485963656
siam score:  -0.86091566
printing an ep nov before normalisation:  64.05444993090643
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.869879722595215
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  39.9813717766089
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.94924863106451
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.465148244585315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.572057292532385
siam score:  -0.8694373
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.997]
 [39.94 ]
 [41.179]
 [37.28 ]
 [40.566]] [[0.806]
 [0.652]
 [0.69 ]
 [0.572]
 [0.671]]
actions average: 
K:  3  action  0 :  tensor([    0.9554,     0.0023,     0.0000,     0.0140,     0.0284],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9957,     0.0008,     0.0000,     0.0035],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0016, 0.0014, 0.9208, 0.0493, 0.0269], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0014, 0.0032, 0.0257, 0.7883, 0.1815], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0008, 0.0037, 0.1248, 0.2719, 0.5987], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.365]
 [55.11 ]
 [45.936]
 [48.265]
 [42.166]] [[0.277]
 [0.284]
 [0.187]
 [0.212]
 [0.147]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.144]
 [67.144]
 [56.161]
 [67.144]
 [67.144]] [[0.457]
 [0.457]
 [0.333]
 [0.457]
 [0.457]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.78681963912465
printing an ep nov before normalisation:  56.1537822986333
printing an ep nov before normalisation:  0.0072056066862558055
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.051836526220875
printing an ep nov before normalisation:  26.801675527270522
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.24591813467225
printing an ep nov before normalisation:  33.51090131736659
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  38.25891704440847
printing an ep nov before normalisation:  27.49104706597214
printing an ep nov before normalisation:  5.882572891950986
printing an ep nov before normalisation:  75.8868036932423
printing an ep nov before normalisation:  38.87390131179826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.755763436520155
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.77694889789742
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86592156
printing an ep nov before normalisation:  11.803679823551079
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.61827290032007
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.158131384704404
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9829,     0.0137,     0.0000,     0.0007,     0.0027],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0007,     0.9914,     0.0021,     0.0000,     0.0057],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0002,     0.9228,     0.0385,     0.0385],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0004,     0.0004,     0.0575,     0.7525,     0.1892],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0007, 0.0040, 0.1130, 0.3823, 0.4999], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.452531814575195
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.91192000647593
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.32885629100309
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.406323591868084
printing an ep nov before normalisation:  41.25898732120595
printing an ep nov before normalisation:  20.946462981063497
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86143464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.17140322900681
printing an ep nov before normalisation:  0.02681749212131633
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9591,     0.0000,     0.0000,     0.0235,     0.0174],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0081,     0.9830,     0.0013,     0.0000,     0.0076],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.8904,     0.0693,     0.0401],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0045,     0.0006,     0.0699,     0.7267,     0.1984],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0780, 0.0263, 0.0774, 0.4231, 0.3952], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.476117614628805
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.258]
 [28.028]
 [22.581]
 [21.264]
 [23.432]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  68.71747970581055
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8423159
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.377]
 [26.377]
 [26.377]
 [26.377]
 [26.377]] [[0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84234023
printing an ep nov before normalisation:  61.47193192214448
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.470379980620336
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.651570919442925
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  33.243213801026826
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85562044
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.276698776041904
printing an ep nov before normalisation:  46.275103909932994
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  59.69363511667562
printing an ep nov before normalisation:  39.854158020045766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.026 0.051 0.872 0.026 0.026]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.25674036150525
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.576983355012274
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9977,     0.0000,     0.0000,     0.0007,     0.0016],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9728,     0.0161,     0.0000,     0.0110],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0003,     0.9268,     0.0550,     0.0179],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0095, 0.0018, 0.0446, 0.6447, 0.2993], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0011, 0.0164, 0.1558, 0.2309, 0.5959], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  52.76248027749314
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.931]
 [44.083]
 [28.726]
 [28.726]
 [28.726]] [[0.539]
 [0.755]
 [0.348]
 [0.348]
 [0.348]]
actions average: 
K:  4  action  0 :  tensor([    0.9989,     0.0002,     0.0000,     0.0002,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9987,     0.0003,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0158,     0.8723,     0.0420,     0.0695],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0011,     0.0002,     0.0238,     0.7301,     0.2447],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0007, 0.0021, 0.1545, 0.3087, 0.5340], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.12376188883782
printing an ep nov before normalisation:  43.01709959090526
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.86236448752618
printing an ep nov before normalisation:  60.35031252547348
printing an ep nov before normalisation:  47.767387093248125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.136147022247314
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.19818728110395
printing an ep nov before normalisation:  37.73892879486084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.61830877939443
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.9438, 0.0037, 0.0011, 0.0323, 0.0191], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0005,     0.9827,     0.0009,     0.0013,     0.0147],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0002,     0.9706,     0.0108,     0.0184],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0004,     0.0001,     0.0245,     0.7635,     0.2114],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0023, 0.0025, 0.0438, 0.1513, 0.8001], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.73842662998243
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.11 ]
 [26.11 ]
 [45.842]
 [25.234]
 [26.11 ]] [[0.28 ]
 [0.28 ]
 [0.682]
 [0.262]
 [0.28 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.59110611227375
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.25651467386953
printing an ep nov before normalisation:  41.09765666653587
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  37.18175411224365
printing an ep nov before normalisation:  35.52447557449341
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.057322978973396
UNIT TEST: sample policy line 217 mcts : [0.154 0.179 0.179 0.282 0.205]
printing an ep nov before normalisation:  38.16936859396316
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8576483
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9496,     0.0009,     0.0000,     0.0208,     0.0287],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9833,     0.0066,     0.0000,     0.0101],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0254,     0.9311,     0.0198,     0.0236],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0398, 0.0009, 0.0790, 0.6746, 0.2057], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0000,     0.0111,     0.0975,     0.3774,     0.5139],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.729539227103544
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.068046975155276
printing an ep nov before normalisation:  32.14222192764282
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.37297248840332
actions average: 
K:  1  action  0 :  tensor([    0.9973,     0.0000,     0.0000,     0.0012,     0.0015],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9821,     0.0014,     0.0001,     0.0158],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0011,     0.0003,     0.9563,     0.0276,     0.0148],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0043, 0.0010, 0.0293, 0.6447, 0.3207], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0178,     0.0006,     0.0682,     0.2188,     0.6946],
       grad_fn=<DivBackward0>)
siam score:  -0.85766333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.000980622882864
actions average: 
K:  1  action  0 :  tensor([    0.9996,     0.0001,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0009,     0.9981,     0.0000,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0008,     0.0005,     0.9461,     0.0269,     0.0257],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0077,     0.0006,     0.0002,     0.7769,     0.2146],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0125,     0.0022,     0.0005,     0.3178,     0.6670],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.52984611320969
siam score:  -0.8644327
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.838]
 [54.764]
 [54.764]
 [56.442]
 [54.764]] [[1.264]
 [1.036]
 [1.036]
 [1.078]
 [1.036]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  104.58589771789973
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.35 ]
 [42.859]
 [41.412]
 [38.176]
 [38.635]] [[1.241]
 [1.565]
 [1.461]
 [1.229]
 [1.262]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
siam score:  -0.86572677
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.77238246372768
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.59855733668873
printing an ep nov before normalisation:  48.796539306640625
printing an ep nov before normalisation:  52.11501598358154
siam score:  -0.86104125
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.974]
 [75.056]
 [58.058]
 [72.278]
 [74.441]] [[1.679]
 [1.791]
 [1.173]
 [1.69 ]
 [1.769]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.4477467241925
siam score:  -0.8587549
printing an ep nov before normalisation:  63.11208728680489
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.23283523654444
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.292]
 [67.292]
 [67.882]
 [67.292]
 [67.292]] [[1.962]
 [1.962]
 [1.987]
 [1.962]
 [1.962]]
siam score:  -0.86248857
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.57226161300549
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9538,     0.0003,     0.0000,     0.0133,     0.0326],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9975,     0.0014,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9409,     0.0362,     0.0229],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0006,     0.0007,     0.0112,     0.7343,     0.2531],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0022, 0.0074, 0.0340, 0.3090, 0.6474], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.019310045403266
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.37639649527
printing an ep nov before normalisation:  37.39616140935278
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.484]
 [26.302]
 [23.602]
 [28.292]
 [26.302]] [[1.144]
 [0.427]
 [0.343]
 [0.488]
 [0.427]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.28 ]
 [47.28 ]
 [54.683]
 [53.635]
 [47.28 ]] [[1.077]
 [1.077]
 [1.48 ]
 [1.423]
 [1.077]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.76891194056293
printing an ep nov before normalisation:  43.8756479940925
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.566]
 [47.864]
 [41.267]
 [39.779]
 [47.864]] [[1.899]
 [1.922]
 [1.417]
 [1.303]
 [1.922]]
printing an ep nov before normalisation:  39.366797594298305
printing an ep nov before normalisation:  46.682090441061604
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  71.97500596155315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.062]
 [75.491]
 [77.922]
 [75.491]
 [75.491]] [[1.021]
 [1.256]
 [1.333]
 [1.256]
 [1.256]]
printing an ep nov before normalisation:  54.917521976297756
printing an ep nov before normalisation:  42.21494983512679
printing an ep nov before normalisation:  66.35753041075537
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.156]
 [47.156]
 [47.156]
 [47.156]
 [47.156]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.07701351921839
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.25700705729109
printing an ep nov before normalisation:  15.898547829671609
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.634603618535195
printing an ep nov before normalisation:  74.32817035325267
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.57327109780085
siam score:  -0.8681461
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.179 0.385 0.231 0.103 0.103]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8723159
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  57.97270013401964
printing an ep nov before normalisation:  46.7745214131221
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.35161245256539
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.17001335105514
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.72947004399598
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.08714023559554
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.916]
 [39.416]
 [40.274]
 [43.311]
 [39.416]] [[1.271]
 [1.009]
 [1.044]
 [1.166]
 [1.009]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.869]
 [71.869]
 [76.676]
 [81.797]
 [71.869]] [[1.697]
 [1.697]
 [1.844]
 [2.   ]
 [1.697]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  56.82963571589698
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.863]
 [53.78 ]
 [59.134]
 [58.386]
 [58.421]] [[1.14 ]
 [1.386]
 [1.576]
 [1.549]
 [1.55 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.155]
 [58.155]
 [58.441]
 [50.074]
 [58.155]] [[1.394]
 [1.394]
 [1.405]
 [1.062]
 [1.394]]
printing an ep nov before normalisation:  70.02092227886584
printing an ep nov before normalisation:  62.450895428011165
printing an ep nov before normalisation:  1.2204103028892632
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85640764
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.99704021493477
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.307]
 [65.568]
 [68.153]
 [67.734]
 [62.87 ]] [[0.977]
 [1.245]
 [1.329]
 [1.316]
 [1.158]]
printing an ep nov before normalisation:  44.830344252081886
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.469074552136824
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.636]
 [65.717]
 [72.268]
 [65.446]
 [69.265]] [[0.894]
 [1.045]
 [1.208]
 [1.038]
 [1.133]]
printing an ep nov before normalisation:  51.77161326667138
printing an ep nov before normalisation:  36.11077731121575
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.6775040975663
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.98536033211398
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.97949764499011
printing an ep nov before normalisation:  31.851806640625
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.813]
 [47.813]
 [47.813]
 [47.813]
 [47.813]] [[0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]]
printing an ep nov before normalisation:  30.393486504445516
printing an ep nov before normalisation:  25.81284759484028
printing an ep nov before normalisation:  60.4648193374205
printing an ep nov before normalisation:  68.33976225715533
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86979234
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.91501359914356
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  25.375586997818772
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.56810502361802
printing an ep nov before normalisation:  49.98874161857631
printing an ep nov before normalisation:  50.6933518719866
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.60247168849269
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.40407530120436
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.240500660842894
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.236]
 [39.518]
 [40.311]
 [39.925]
 [38.602]] [[0.924]
 [0.756]
 [0.785]
 [0.771]
 [0.724]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.27089732033866
printing an ep nov before normalisation:  64.72989265391253
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  31.720383167266846
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.904401195738092
actions average: 
K:  2  action  0 :  tensor([    0.9992,     0.0000,     0.0000,     0.0002,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9976,     0.0000,     0.0000,     0.0022],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0005,     0.9405,     0.0344,     0.0245],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0010,     0.0006,     0.0627,     0.7340,     0.2017],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0029, 0.0033, 0.0463, 0.3602, 0.5873], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.89665933461616
printing an ep nov before normalisation:  71.75948143005371
printing an ep nov before normalisation:  44.10621568224691
printing an ep nov before normalisation:  44.664979905394844
siam score:  -0.86895585
printing an ep nov before normalisation:  40.103668073007256
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.98 ]
 [42.338]
 [42.338]
 [42.338]
 [42.338]] [[1.266]
 [0.832]
 [0.832]
 [0.832]
 [0.832]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.56474044508394
using explorer policy with actor:  1
siam score:  -0.870121
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.55595348025776
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.64568624217065
printing an ep nov before normalisation:  33.90724383691044
printing an ep nov before normalisation:  66.8763832833527
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.73604390578769
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.73646280326978
printing an ep nov before normalisation:  42.48018574997062
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.859]
 [30.999]
 [35.161]
 [33.831]
 [31.822]] [[0.884]
 [0.584]
 [0.743]
 [0.692]
 [0.615]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.725619594240012
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.63032014115817
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.263]
 [46.684]
 [40.098]
 [45.343]
 [45.263]] [[1.049]
 [1.113]
 [0.816]
 [1.052]
 [1.049]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.97894996793312
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 29.41723480980217
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.35 ]
 [38.206]
 [32.35 ]
 [30.957]
 [32.35 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.88415608130611
printing an ep nov before normalisation:  52.04257546056799
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.174]
 [35.747]
 [37.842]
 [37.397]
 [36.264]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  41.10301285842466
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.386786364285225
printing an ep nov before normalisation:  37.04112370276881
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.277]
 [43.328]
 [38.502]
 [36.026]
 [40.202]] [[1.137]
 [1.438]
 [1.278]
 [1.195]
 [1.334]]
printing an ep nov before normalisation:  17.81790256500244
printing an ep nov before normalisation:  28.736169539100345
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.846]
 [45.602]
 [48.304]
 [45.044]
 [43.334]] [[0.583]
 [1.216]
 [1.319]
 [1.195]
 [1.131]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 43.57769490946574
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.96857833862305
printing an ep nov before normalisation:  41.46467055549203
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.87291815762952
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.983406059917655
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  57.25839623230071
printing an ep nov before normalisation:  50.727612694175576
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.2  ]
 [37.52 ]
 [40.921]
 [37.52 ]
 [37.52 ]] [[0.825]
 [0.838]
 [0.972]
 [0.838]
 [0.838]]
line 256 mcts: sample exp_bonus 42.87624203027312
printing an ep nov before normalisation:  85.35309184560013
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.081]
 [34.941]
 [35.807]
 [34.857]
 [34.516]] [[1.233]
 [1.162]
 [1.216]
 [1.157]
 [1.136]]
printing an ep nov before normalisation:  52.40748794515918
printing an ep nov before normalisation:  64.7164141078834
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.18530015318085
printing an ep nov before normalisation:  40.54254208234642
using explorer policy with actor:  1
siam score:  -0.8642831
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  38.989386558532715
siam score:  -0.87148285
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.58286527996776
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0019,     0.9946,     0.0000,     0.0000,     0.0035],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9215,     0.0401,     0.0383],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0005,     0.0002,     0.0765,     0.6885,     0.2344],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0023, 0.0020, 0.0950, 0.3983, 0.5025], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  27.473105969511813
printing an ep nov before normalisation:  75.5091723113904
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.256 0.205 0.103 0.308 0.128]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.336763381958008
printing an ep nov before normalisation:  24.561314582824707
siam score:  -0.88226473
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  2.348336205258761
siam score:  -0.8786097
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.33513641357422
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87879163
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.879207
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.82843336212334
siam score:  -0.88087845
siam score:  -0.88085294
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.40668628659353
printing an ep nov before normalisation:  41.12563301194548
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.407]
 [32.407]
 [32.407]
 [32.407]
 [32.407]] [[0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]]
printing an ep nov before normalisation:  34.90559740411818
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  24.58364825671907
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.07990611157945
printing an ep nov before normalisation:  53.25304841014476
printing an ep nov before normalisation:  41.83363437652588
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.61274719351827
printing an ep nov before normalisation:  48.547978488260846
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.417379915685366
printing an ep nov before normalisation:  28.513523970775132
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [   -0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 8.40010704132451e-11
0.0 6.601072584990806e-11
0.0 1.729840824934477e-11
0.0 2.7123904191578094e-11
0.0 8.40010704132451e-11
0.0 0.0
0.0 7.957267791025424e-11
0.0 0.0
0.0 0.0
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87013227
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.87422429364464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.076916694641113
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.783]
 [46.752]
 [45.783]
 [48.831]
 [45.783]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.266]
 [42.597]
 [47.737]
 [36.44 ]
 [37.824]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.949]
 [49.522]
 [37.027]
 [48.157]
 [48.157]] [[0.333]
 [0.294]
 [0.219]
 [0.286]
 [0.286]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87286353
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.91730778389007
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.38171096367029
printing an ep nov before normalisation:  44.790218927771136
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.697]
 [48.697]
 [42.419]
 [35.598]
 [37.022]] [[2.137]
 [2.137]
 [1.667]
 [1.156]
 [1.263]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.530166563081707
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.3055317401886
printing an ep nov before normalisation:  24.50392798383533
siam score:  -0.8718348
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.675]
 [43.432]
 [42.699]
 [43.432]
 [43.432]] [[2.   ]
 [1.981]
 [1.922]
 [1.981]
 [1.981]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[17.164]
 [25.767]
 [50.407]
 [19.836]
 [19.254]] [[0.222]
 [0.46 ]
 [1.143]
 [0.296]
 [0.28 ]]
printing an ep nov before normalisation:  45.0121747158462
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -5.903081809442208e-12
0.0 -4.705167042080243e-12
0.0 0.0
0.0 -7.611299621060415e-12
0.0 0.0
0.0 -7.870775745654437e-12
0.0 -9.41898328325227e-12
0.0 -8.277288336166299e-12
0.0 -1.7298407890082892e-13
0.0 -3.373189604216036e-12
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.9972,     0.0007,     0.0000,     0.0005,     0.0016],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9961,     0.0024,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0000,     0.9452,     0.0377,     0.0169],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0003,     0.0220,     0.6410,     0.3364],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0003,     0.0548,     0.1360,     0.2804,     0.5284],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.65404973340358
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.25069820908199
printing an ep nov before normalisation:  49.14338360843318
printing an ep nov before normalisation:  30.344100156362067
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.302]
 [29.967]
 [26.763]
 [31.432]
 [29.631]] [[0.695]
 [0.68 ]
 [0.542]
 [0.743]
 [0.666]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8672355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.07148265838623
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.65269819850507
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.778]
 [21.196]
 [21.196]
 [26.312]
 [21.196]] [[0.991]
 [0.446]
 [0.446]
 [0.651]
 [0.446]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.9345811318966
printing an ep nov before normalisation:  57.41055874857949
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.737]
 [52.037]
 [43.737]
 [43.737]
 [43.737]] [[1.028]
 [1.51 ]
 [1.028]
 [1.028]
 [1.028]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8685084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.47094160252966
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.314977533505726
printing an ep nov before normalisation:  69.57157176644307
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[70.566]
 [70.566]
 [70.566]
 [70.566]
 [70.566]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.294]
 [58.765]
 [57.368]
 [59.372]
 [58.898]] [[1.625]
 [1.819]
 [1.741]
 [1.853]
 [1.826]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.128]
 [27.885]
 [24.919]
 [21.576]
 [23.18 ]] [[0.197]
 [0.279]
 [0.228]
 [0.171]
 [0.198]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.528712246101993
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9988,     0.0000,     0.0000,     0.0003,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0055,     0.9447,     0.0272,     0.0002,     0.0223],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9210,     0.0291,     0.0499],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0007,     0.0005,     0.0594,     0.6403,     0.2990],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0006,     0.0004,     0.0862,     0.3908,     0.5220],
       grad_fn=<DivBackward0>)
siam score:  -0.874361
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.20574913644116
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.05695168500656
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.83604374009083
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.08785135060683
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.85063974380204
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.359972883935725
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.00259865949906
printing an ep nov before normalisation:  81.23770583008448
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.15362597212948
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 42.166356207726174
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.9431759895576874
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.69511198997498
printing an ep nov before normalisation:  68.75605666050213
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.985]
 [36.454]
 [46.642]
 [39.958]
 [37.677]] [[0.487]
 [0.386]
 [0.494]
 [0.423]
 [0.399]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.9578,     0.0001,     0.0000,     0.0247,     0.0173],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0021,     0.9972,     0.0000,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0043,     0.9603,     0.0104,     0.0249],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0015,     0.0003,     0.0731,     0.6544,     0.2707],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0494, 0.0519, 0.1738, 0.1725, 0.5524], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.743]
 [38.393]
 [52.469]
 [48.21 ]
 [35.787]] [[0.059]
 [0.154]
 [0.261]
 [0.229]
 [0.135]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.213]
 [41.213]
 [41.213]
 [45.08 ]
 [41.213]] [[1.329]
 [1.329]
 [1.329]
 [1.558]
 [1.329]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.40384242215054655
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.949]
 [40.949]
 [67.294]
 [34.205]
 [40.949]] [[0.468]
 [0.468]
 [1.07 ]
 [0.314]
 [0.468]]
printing an ep nov before normalisation:  69.60425196592861
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9702,     0.0166,     0.0039,     0.0009,     0.0084],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0009,     0.9922,     0.0004,     0.0000,     0.0065],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0003,     0.9238,     0.0359,     0.0399],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0007,     0.0007,     0.1122,     0.7206,     0.1658],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0003,     0.0651,     0.1005,     0.2190,     0.6151],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.071]
 [49.805]
 [49.207]
 [50.17 ]
 [51.305]] [[0.876]
 [0.721]
 [0.704]
 [0.732]
 [0.765]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8830868
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.027]
 [35.432]
 [32.079]
 [30.267]
 [38.021]] [[1.685]
 [1.562]
 [1.302]
 [1.162]
 [1.762]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.88058615
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.114]
 [66.114]
 [66.114]
 [66.114]
 [66.114]] [[1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[82.42]
 [82.42]
 [82.42]
 [82.42]
 [82.42]] [[1.241]
 [1.241]
 [1.241]
 [1.241]
 [1.241]]
siam score:  -0.87955934
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 38.127184580237014
printing an ep nov before normalisation:  62.398434452061814
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8683042
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.997]
 [71.674]
 [67.509]
 [64.441]
 [68.604]] [[0.377]
 [0.533]
 [0.472]
 [0.427]
 [0.488]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.210706780447914
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.55966281890869
printing an ep nov before normalisation:  66.69446753947935
printing an ep nov before normalisation:  32.4804921318065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.46176052093506
printing an ep nov before normalisation:  43.37292194366455
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.227]
 [58.158]
 [50.345]
 [49.714]
 [56.653]] [[1.295]
 [1.4  ]
 [1.121]
 [1.099]
 [1.346]]
printing an ep nov before normalisation:  25.733677803291002
actions average: 
K:  2  action  0 :  tensor([    0.9975,     0.0000,     0.0000,     0.0014,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9544,     0.0056,     0.0198,     0.0202],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0089,     0.8768,     0.0673,     0.0469],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0009,     0.0006,     0.0296,     0.6778,     0.2911],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0173, 0.0097, 0.1665, 0.2779, 0.5286], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  52.276081209434146
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.08132173343475
actions average: 
K:  2  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0137, 0.9320, 0.0369, 0.0014, 0.0159], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0008,     0.0008,     0.8957,     0.0458,     0.0568],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0005,     0.0633,     0.6945,     0.2415],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0078, 0.0042, 0.0020, 0.4552, 0.5309], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.35411419530753
printing an ep nov before normalisation:  53.50834460766389
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.39318056102877
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.02526064487142321
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.78267478942871
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.88666487
printing an ep nov before normalisation:  49.34008769887108
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9997,     0.0002,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9974,     0.0001,     0.0000,     0.0024],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0004,     0.0009,     0.9508,     0.0189,     0.0290],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0004,     0.0005,     0.0685,     0.7146,     0.2160],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0005, 0.0011, 0.0855, 0.4694, 0.4435], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.188315868377686
printing an ep nov before normalisation:  39.18981755875495
printing an ep nov before normalisation:  30.714490221413055
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[77.244]
 [76.622]
 [77.244]
 [77.244]
 [77.244]] [[1.502]
 [1.483]
 [1.502]
 [1.502]
 [1.502]]
siam score:  -0.8750349
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8751476
printing an ep nov before normalisation:  39.401792237071135
printing an ep nov before normalisation:  63.587391375184666
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.29218864440918
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  93.67883264249961
siam score:  -0.8721069
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.65427032535727
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.779]
 [51.314]
 [52.738]
 [50.033]
 [49.489]] [[0.893]
 [1.224]
 [1.274]
 [1.18 ]
 [1.161]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.736536063660296
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.53345866708369
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.053]
 [47.053]
 [47.053]
 [47.053]
 [47.053]] [[1.755]
 [1.755]
 [1.755]
 [1.755]
 [1.755]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.757]
 [41.092]
 [38.502]
 [35.936]
 [35.484]] [[1.383]
 [1.481]
 [1.291]
 [1.103]
 [1.07 ]]
line 256 mcts: sample exp_bonus 56.35638000285757
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8747328
printing an ep nov before normalisation:  31.99399817839733
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.53258639313283
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0002,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9455,     0.0267,     0.0012,     0.0257],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0016,     0.9426,     0.0209,     0.0346],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0006,     0.0010,     0.0004,     0.7991,     0.1989],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0066, 0.0369, 0.0491, 0.3636, 0.5439], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.26163649961586
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.815]
 [3.102]
 [3.276]
 [4.275]
 [4.496]] [[0.045]
 [0.029]
 [0.03 ]
 [0.04 ]
 [0.042]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.195]
 [50.01 ]
 [42.324]
 [51.105]
 [50.944]] [[1.46 ]
 [1.585]
 [1.243]
 [1.633]
 [1.626]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87057966
printing an ep nov before normalisation:  80.83103332238413
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.914]
 [52.355]
 [57.325]
 [54.231]
 [58.452]] [[0.901]
 [1.425]
 [1.606]
 [1.493]
 [1.647]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.2123073317896
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.33912574916442
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.734035498807216
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.97229969104478
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.02718486874098147
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.8778267
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.67095385485551
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.4216916143175
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.39399346632892
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.108]
 [46.973]
 [53.517]
 [33.894]
 [33.231]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.73811714617102
printing an ep nov before normalisation:  61.18467107362104
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.746]
 [53.767]
 [54.718]
 [50.949]
 [53.767]] [[0.962]
 [1.485]
 [1.527]
 [1.362]
 [1.485]]
siam score:  -0.87350327
printing an ep nov before normalisation:  72.24250547824798
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.37022172993085
printing an ep nov before normalisation:  29.695357478054518
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.8699055
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.21571253383227
printing an ep nov before normalisation:  47.82380693412197
printing an ep nov before normalisation:  60.18033574497371
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.515]
 [37.993]
 [41.461]
 [37.993]
 [42.063]] [[1.326]
 [1.173]
 [1.383]
 [1.173]
 [1.419]]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.94789112317635
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  47.825253227983175
printing an ep nov before normalisation:  46.20152310459024
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.564]
 [48.356]
 [38.957]
 [38.089]
 [38.089]] [[0.457]
 [0.543]
 [0.374]
 [0.359]
 [0.359]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  53.894887833494096
printing an ep nov before normalisation:  52.33216544622562
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.946175409889115
actions average: 
K:  1  action  0 :  tensor([    0.9701,     0.0000,     0.0025,     0.0248,     0.0026],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0014,     0.9858,     0.0038,     0.0001,     0.0090],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0006,     0.0001,     0.9175,     0.0453,     0.0365],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0008,     0.0004,     0.0758,     0.7013,     0.2217],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0153,     0.0016,     0.0006,     0.2865,     0.6961],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  57.698249393535185
printing an ep nov before normalisation:  50.772457122802734
printing an ep nov before normalisation:  36.00953910428605
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.26912478067336
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.80522441864014
printing an ep nov before normalisation:  61.31251637818713
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  47.006764075496115
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.75917964511447
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.131]
 [2.131]
 [2.131]
 [2.131]
 [2.131]] [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8673536
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.26891567470673
actions average: 
K:  2  action  0 :  tensor([    0.9972,     0.0002,     0.0000,     0.0008,     0.0018],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9942,     0.0003,     0.0000,     0.0051],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0019, 0.0042, 0.9251, 0.0268, 0.0419], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0005,     0.0198,     0.6633,     0.3160],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0003,     0.0077,     0.0019,     0.2267,     0.7635],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.70587250286748
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.95399105009574
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.505474922932365
printing an ep nov before normalisation:  44.56948680376252
actions average: 
K:  2  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0107, 0.9528, 0.0015, 0.0025, 0.0325], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0055,     0.9359,     0.0367,     0.0218],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0324, 0.0058, 0.0604, 0.6845, 0.2169], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0796, 0.0018, 0.0665, 0.3403, 0.5118], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.25616407394409
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 4.4782119254601926e-10
0.0 1.3492758469222939e-11
0.0 2.5601644199702338e-12
0.0 2.5601644199702338e-12
0.0 6.331217416814628e-12
0.0 1.9512604495362672e-10
0.0 2.3581190119588476e-10
0.0 6.4696046888765615e-12
0.0 0.0
0.0 1.634699581679987e-12
using explorer policy with actor:  1
printing an ep nov before normalisation:  68.88911920677832
printing an ep nov before normalisation:  31.01371639263526
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87171805
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.5759916851316
printing an ep nov before normalisation:  31.427248706993225
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.28805923461914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  33.77274618800973
printing an ep nov before normalisation:  40.98096762510927
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.487597325247744
printing an ep nov before normalisation:  71.65118938041006
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.427]
 [38.427]
 [38.427]
 [38.427]
 [38.427]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.946]
 [24.946]
 [24.946]
 [24.946]
 [24.946]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  23.337695598602295
actions average: 
K:  2  action  0 :  tensor([    0.9994,     0.0000,     0.0000,     0.0003,     0.0003],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9910,     0.0071,     0.0000,     0.0017],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0037, 0.0228, 0.9170, 0.0273, 0.0293], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0364,     0.0006,     0.0014,     0.7364,     0.2252],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0034, 0.0031, 0.1014, 0.2572, 0.6350], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.91200637817383
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.53 ]
 [42.763]
 [42.763]
 [42.763]
 [42.763]] [[1.043]
 [0.486]
 [0.486]
 [0.486]
 [0.486]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.934487342834473
printing an ep nov before normalisation:  24.703319400519682
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.693]
 [30.707]
 [29.693]
 [29.693]
 [29.693]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  44.871548077783686
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8605063
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.978]
 [35.091]
 [28.784]
 [29.11 ]
 [31.604]] [[1.13 ]
 [0.756]
 [0.49 ]
 [0.504]
 [0.609]]
printing an ep nov before normalisation:  40.12957494433511
printing an ep nov before normalisation:  60.29014188590827
printing an ep nov before normalisation:  49.42408099490292
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.96 ]
 [39.673]
 [25.3  ]
 [25.974]
 [26.087]] [[0.402]
 [0.891]
 [0.414]
 [0.436]
 [0.44 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.199]
 [64.199]
 [64.199]
 [64.199]
 [64.199]] [[0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87027395
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9989,     0.0001,     0.0000,     0.0003,     0.0008],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9984,     0.0001,     0.0000,     0.0014],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0018,     0.9569,     0.0135,     0.0278],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0002,     0.0274,     0.8034,     0.1687],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0013, 0.0128, 0.0496, 0.3300, 0.6062], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.949187310102644
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.97219517329629
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.219718605724026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.1773021282245
siam score:  -0.87387925
actions average: 
K:  2  action  0 :  tensor([    0.9717,     0.0014,     0.0002,     0.0170,     0.0097],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0027,     0.9948,     0.0002,     0.0001,     0.0023],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0054, 0.0115, 0.9139, 0.0333, 0.0359], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0044,     0.0004,     0.0838,     0.6500,     0.2615],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0042, 0.0731, 0.0638, 0.2195, 0.6394], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.712]
 [58.712]
 [58.712]
 [58.712]
 [58.712]] [[117.425]
 [117.425]
 [117.425]
 [117.425]
 [117.425]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.712609085372144
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.8399150499598
printing an ep nov before normalisation:  47.101189662280696
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.80550402318848
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 43.18358244119461
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.895423381724534
printing an ep nov before normalisation:  45.733523449518266
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8590557
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.088984211942147
printing an ep nov before normalisation:  61.58941754208058
printing an ep nov before normalisation:  34.07209396362305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8525589
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.27608889594251
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.9523227460164
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.60883812719162
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.33558940887451
printing an ep nov before normalisation:  40.94772669678963
printing an ep nov before normalisation:  34.282533241180715
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.608]
 [39.608]
 [43.943]
 [39.608]
 [39.608]] [[0.752]
 [0.752]
 [0.909]
 [0.752]
 [0.752]]
printing an ep nov before normalisation:  39.35300852569856
printing an ep nov before normalisation:  61.402346552878534
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.99190188863986
printing an ep nov before normalisation:  50.31667992066629
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.75411234032922
printing an ep nov before normalisation:  50.81638027300593
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9946,     0.0000,     0.0000,     0.0025,     0.0029],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0239,     0.9594,     0.0087,     0.0000,     0.0080],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0028,     0.9620,     0.0020,     0.0331],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0318,     0.0003,     0.0521,     0.7149,     0.2008],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0224, 0.0067, 0.0024, 0.4625, 0.5061], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.322689121276134
actions average: 
K:  2  action  0 :  tensor([0.9743, 0.0029, 0.0067, 0.0029, 0.0132], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0026,     0.9731,     0.0008,     0.0002,     0.0233],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0010,     0.9439,     0.0294,     0.0257],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0008,     0.1397,     0.6107,     0.2485],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0021, 0.0307, 0.1535, 0.3281, 0.4856], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.2406773928935877
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.04813438154437
printing an ep nov before normalisation:  58.996636420009054
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.667]
 [37.349]
 [40.998]
 [37.349]
 [37.349]] [[1.174]
 [0.922]
 [1.095]
 [0.922]
 [0.922]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.279]
 [49.312]
 [44.6  ]
 [49.312]
 [49.312]] [[0.899]
 [0.869]
 [0.726]
 [0.869]
 [0.869]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  72.8994977405283
printing an ep nov before normalisation:  53.44707302597261
siam score:  -0.8671489
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.133977356919026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.02291969752846512
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.620540989734817
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[77.537]
 [77.537]
 [77.537]
 [77.537]
 [77.537]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
siam score:  -0.87234545
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  88.48852663446107
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.987]
 [42.987]
 [42.987]
 [42.987]
 [42.987]] [[0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]]
printing an ep nov before normalisation:  35.76143470335447
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  79.83892021923644
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.76220429366366
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.21387485865914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.28192615509033
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.9707699620048515e-05
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  100.58610166148729
siam score:  -0.8601349
printing an ep nov before normalisation:  22.47504234313965
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.9969,     0.0008,     0.0000,     0.0008,     0.0015],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9568,     0.0007,     0.0006,     0.0419],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0112,     0.9158,     0.0228,     0.0498],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0005,     0.0384,     0.7350,     0.2259],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0098, 0.0887, 0.1030, 0.3318, 0.4667], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.44235720370099
actions average: 
K:  2  action  0 :  tensor([    0.9986,     0.0003,     0.0000,     0.0004,     0.0007],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0012,     0.9835,     0.0034,     0.0001,     0.0119],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0002,     0.9260,     0.0358,     0.0380],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0003,     0.0721,     0.7404,     0.1869],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0039, 0.0092, 0.0328, 0.3281, 0.6260], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8636406
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.58346836138287
actions average: 
K:  0  action  0 :  tensor([    0.9976,     0.0000,     0.0000,     0.0002,     0.0022],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0090,     0.9857,     0.0001,     0.0001,     0.0050],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0003,     0.9331,     0.0302,     0.0363],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0040,     0.0001,     0.0280,     0.8194,     0.1486],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0030, 0.0010, 0.0006, 0.4789, 0.5165], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.526970498021754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.325]
 [46.796]
 [42.325]
 [42.325]
 [42.325]] [[1.367]
 [1.637]
 [1.367]
 [1.367]
 [1.367]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.209]
 [24.209]
 [24.209]
 [41.026]
 [24.209]] [[0.83 ]
 [0.83 ]
 [0.83 ]
 [1.438]
 [0.83 ]]
printing an ep nov before normalisation:  42.192004397308224
actions average: 
K:  0  action  0 :  tensor([    0.9587,     0.0003,     0.0010,     0.0287,     0.0112],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0014,     0.9290,     0.0512,     0.0006,     0.0179],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0003,     0.9150,     0.0532,     0.0314],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0111,     0.0003,     0.0440,     0.6825,     0.2622],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0298, 0.0041, 0.1091, 0.2543, 0.6028], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  86.27356961946806
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.13 ]
 [21.873]
 [23.476]
 [24.898]
 [24.294]] [[0.539]
 [0.527]
 [0.601]
 [0.667]
 [0.639]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.904]
 [44.904]
 [44.904]
 [44.904]
 [44.904]] [[0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]]
printing an ep nov before normalisation:  46.80803163698799
siam score:  -0.87007964
printing an ep nov before normalisation:  38.29744570566362
siam score:  -0.8667389
printing an ep nov before normalisation:  50.463171439762604
printing an ep nov before normalisation:  34.2639303017091
using explorer policy with actor:  1
siam score:  -0.86482614
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.22 ]
 [0.213]
 [0.192]
 [0.18 ]
 [0.225]] [[0.007]
 [0.006]
 [0.005]
 [0.005]
 [0.007]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.155471363042011
printing an ep nov before normalisation:  62.447682205494935
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.121]
 [35.121]
 [35.121]
 [35.121]
 [35.121]] [[0.995]
 [0.995]
 [0.995]
 [0.995]
 [0.995]]
printing an ep nov before normalisation:  44.52635188386461
printing an ep nov before normalisation:  28.487530460866868
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.23316509306738
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.11849184434169
actions average: 
K:  2  action  0 :  tensor([    0.9959,     0.0002,     0.0000,     0.0025,     0.0014],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0108, 0.9512, 0.0067, 0.0011, 0.0302], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0002,     0.9351,     0.0256,     0.0390],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0120, 0.0047, 0.0274, 0.7239, 0.2320], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0030, 0.0009, 0.0435, 0.3385, 0.6140], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 36.552135420776125
printing an ep nov before normalisation:  52.35111965393984
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.253]
 [29.253]
 [29.253]
 [29.253]
 [29.253]] [[0.845]
 [0.845]
 [0.845]
 [0.845]
 [0.845]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.941620639207336
siam score:  -0.8677208
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.89076596495184
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.284430944970474
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.642]
 [39.866]
 [42.919]
 [42.277]
 [41.136]] [[1.123]
 [1.134]
 [1.288]
 [1.255]
 [1.198]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.82607368505035
printing an ep nov before normalisation:  41.67565606342936
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.65517071953026
printing an ep nov before normalisation:  75.34553376170456
printing an ep nov before normalisation:  35.51079511642456
printing an ep nov before normalisation:  47.56292391871685
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.55283738516017
actions average: 
K:  4  action  0 :  tensor([    0.9993,     0.0000,     0.0000,     0.0002,     0.0005],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9868,     0.0060,     0.0002,     0.0070],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0009,     0.0083,     0.9111,     0.0250,     0.0548],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0008,     0.1357,     0.7278,     0.1355],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0030, 0.0008, 0.1228, 0.4442, 0.4291], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.816]
 [69.312]
 [67.42 ]
 [69.208]
 [69.949]] [[1.775]
 [1.875]
 [1.799]
 [1.871]
 [1.901]]
printing an ep nov before normalisation:  51.618332862854004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.75984021224698
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.85320293547795
printing an ep nov before normalisation:  54.641056060791016
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.009246012363739453
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.53060855898937
printing an ep nov before normalisation:  44.30838108062744
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.323]
 [46.333]
 [41.475]
 [39.088]
 [40.57 ]] [[1.314]
 [1.214]
 [0.969]
 [0.849]
 [0.923]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.50408911699387
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  71.21791828997374
printing an ep nov before normalisation:  72.70046195863756
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.52164692851984
printing an ep nov before normalisation:  35.88126012261994
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.9989,     0.0002,     0.0000,     0.0003,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9971,     0.0006,     0.0001,     0.0022],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0024,     0.8594,     0.0614,     0.0766],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0004,     0.0405,     0.6626,     0.2962],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0013, 0.0051, 0.1184, 0.3282, 0.5470], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.064]
 [44.42 ]
 [43.797]
 [34.995]
 [34.995]] [[0.227]
 [0.301]
 [0.293]
 [0.192]
 [0.192]]
printing an ep nov before normalisation:  56.68158634040291
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.221]
 [45.221]
 [45.221]
 [45.221]
 [45.221]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.8733148
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.872143
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.75978771843543
printing an ep nov before normalisation:  23.222226779202852
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.20741331630109
printing an ep nov before normalisation:  20.680696964263916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8758284
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.56958659847834
printing an ep nov before normalisation:  59.30027651618038
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  53.59912844872065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.305]
 [33.305]
 [33.305]
 [33.305]
 [33.305]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.49822482639842
siam score:  -0.8679985
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.38900219008566
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.809]
 [50.133]
 [65.332]
 [61.65 ]
 [50.133]] [[1.271]
 [0.682]
 [1.292]
 [1.145]
 [0.682]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.184]
 [40.184]
 [46.352]
 [40.184]
 [40.184]] [[1.445]
 [1.445]
 [1.667]
 [1.445]
 [1.445]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.461]
 [33.461]
 [36.42 ]
 [35.421]
 [33.461]] [[0.852]
 [0.852]
 [0.995]
 [0.947]
 [0.852]]
printing an ep nov before normalisation:  47.29242293232609
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.028]
 [50.175]
 [46.028]
 [46.028]
 [46.028]] [[1.303]
 [1.506]
 [1.303]
 [1.303]
 [1.303]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.973338398820495
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.940631083598543
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.142253024684436
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.13692791848724
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  91.9839859008789
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.855473092151215
printing an ep nov before normalisation:  55.42808765241952
printing an ep nov before normalisation:  57.06791639950622
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.031]
 [59.129]
 [56.86 ]
 [58.746]
 [58.746]] [[1.291]
 [1.74 ]
 [1.614]
 [1.718]
 [1.718]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.45687845128956
printing an ep nov before normalisation:  36.13462261519798
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.721]
 [52.721]
 [52.721]
 [52.721]
 [52.721]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  44.256103993938666
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.096]
 [38.096]
 [38.096]
 [38.096]
 [38.096]] [[0.921]
 [0.921]
 [0.921]
 [0.921]
 [0.921]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.41411453018182
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.39]
 [32.39]
 [32.39]
 [32.39]
 [32.39]] [[0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9996,     0.0000,     0.0000,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0119, 0.9456, 0.0139, 0.0013, 0.0274], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9629,     0.0232,     0.0138],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0080,     0.0004,     0.0269,     0.7230,     0.2417],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0066,     0.0005,     0.0899,     0.3364,     0.5665],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  73.78863895818223
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.9754319190979
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.717931358589645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.849533081054688
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8700493
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.303512554697164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.64028807219664
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.1592355966568
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.947977418698045
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  108.128511918046
printing an ep nov before normalisation:  56.45576917582064
printing an ep nov before normalisation:  39.92593681205434
printing an ep nov before normalisation:  55.390923888376165
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.059635226299285
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.958]
 [46.363]
 [50.862]
 [48.967]
 [44.994]] [[0.407]
 [0.49 ]
 [0.576]
 [0.54 ]
 [0.464]]
printing an ep nov before normalisation:  36.297776114099925
printing an ep nov before normalisation:  64.60920458013703
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.931483640569446
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8698787
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.10865701673557
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -5.63063187956497e-12
0.0 -1.6433487822619828e-11
0.0 -1.2796497489430759e-11
0.0 0.0
0.0 -4.8781511211824524e-12
0.0 -1.4573908936073032e-11
0.0 -8.294586749856125e-12
0.0 -1.2861366519874434e-11
0.0 -3.451032441352587e-12
0.0 -1.4167396341936332e-11
using explorer policy with actor:  1
siam score:  -0.8769255
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.49699592590332
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.433869615493876
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.15121588042722
printing an ep nov before normalisation:  38.621416091918945
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9988,     0.0008,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9887,     0.0003,     0.0001,     0.0109],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9525,     0.0097,     0.0376],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0058,     0.0004,     0.0366,     0.7691,     0.1881],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0019, 0.0035, 0.0582, 0.3741, 0.5624], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9972,     0.0001,     0.0000,     0.0013,     0.0014],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9601,     0.0027,     0.0000,     0.0372],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0010, 0.0046, 0.8998, 0.0367, 0.0579], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0049,     0.0849,     0.5918,     0.3183],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0019, 0.0158, 0.0299, 0.3392, 0.6132], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.91432093033774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.14483767940028
printing an ep nov before normalisation:  40.31535773032195
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9940,     0.0000,     0.0000,     0.0033,     0.0027],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.9021,     0.0293,     0.0004,     0.0678],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0003,     0.9340,     0.0370,     0.0287],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0005,     0.0010,     0.1077,     0.6366,     0.2542],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0007, 0.0014, 0.0982, 0.2946, 0.6051], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.728]
 [50.728]
 [50.728]
 [50.728]
 [50.728]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.38833746852385
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.89365731603129
printing an ep nov before normalisation:  34.050525500233874
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.595]
 [44.555]
 [44.774]
 [45.291]
 [45.291]] [[0.294]
 [0.282]
 [0.284]
 [0.291]
 [0.291]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.55368227403409
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86647016
printing an ep nov before normalisation:  35.06715035670475
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8643259
printing an ep nov before normalisation:  72.33651981271557
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 61.39877229567271
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.339260956849245
printing an ep nov before normalisation:  27.727119992317487
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.756]
 [49.136]
 [51.392]
 [49.136]
 [49.136]] [[1.048]
 [1.218]
 [1.332]
 [1.218]
 [1.218]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.41888734516773
printing an ep nov before normalisation:  48.64269247784776
siam score:  -0.86861557
printing an ep nov before normalisation:  43.269730654673424
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.256 0.231 0.333 0.077 0.103]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.57243537902832
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[70.921]
 [83.521]
 [83.521]
 [83.521]
 [83.521]] [[0.39 ]
 [0.562]
 [0.562]
 [0.562]
 [0.562]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.8563902585104
printing an ep nov before normalisation:  60.385939987652144
printing an ep nov before normalisation:  5.310028643888245
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.87028563
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  2.9492844297487864
printing an ep nov before normalisation:  50.8424719476862
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.144]
 [48.594]
 [33.109]
 [33.109]
 [33.109]] [[1.411]
 [1.333]
 [0.553]
 [0.553]
 [0.553]]
printing an ep nov before normalisation:  31.539378902426733
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.562]
 [49.079]
 [46.705]
 [41.471]
 [43.828]] [[0.196]
 [0.165]
 [0.149]
 [0.112]
 [0.128]]
printing an ep nov before normalisation:  40.387102016553065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9994,     0.0000,     0.0000,     0.0004,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9937,     0.0041,     0.0000,     0.0022],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0002,     0.9143,     0.0239,     0.0616],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0002,     0.0568,     0.6841,     0.2588],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0073,     0.0003,     0.0733,     0.2655,     0.6537],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.88750028409419
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.37868626923077
using explorer policy with actor:  1
siam score:  -0.8745526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.587901067234856
printing an ep nov before normalisation:  72.74776162302472
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.81937237489088
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.59495067596436
printing an ep nov before normalisation:  65.84671218964537
printing an ep nov before normalisation:  48.63723778048396
printing an ep nov before normalisation:  61.005041096825025
printing an ep nov before normalisation:  39.33109140354383
printing an ep nov before normalisation:  57.05032747004992
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.722]
 [31.808]
 [31.411]
 [41.961]
 [32.511]] [[1.225]
 [0.813]
 [0.789]
 [1.418]
 [0.855]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.829]
 [48.728]
 [52.461]
 [48.728]
 [48.728]] [[1.221]
 [1.578]
 [1.77 ]
 [1.578]
 [1.578]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.519]
 [49.086]
 [49.086]
 [49.086]
 [54.155]] [[1.395]
 [1.09 ]
 [1.09 ]
 [1.09 ]
 [1.33 ]]
printing an ep nov before normalisation:  55.70473798367333
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  53.60361099243164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.100139164628672
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.37117948607229
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86666185
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.71601252098091
printing an ep nov before normalisation:  0.4048695322535423
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.612]
 [46.101]
 [48.021]
 [45.383]
 [46.264]] [[0.902]
 [0.849]
 [0.916]
 [0.824]
 [0.855]]
printing an ep nov before normalisation:  73.25991776576748
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.021774666353096
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.86215806
printing an ep nov before normalisation:  99.36327656269032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8604655
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.779343070525904
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  47.950823723786044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8704117
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.4090940389844
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8717869
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.0542054508887
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.154380500755792
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.08 ]
 [44.279]
 [45.316]
 [37.714]
 [37.93 ]] [[0.963]
 [0.543]
 [0.574]
 [0.343]
 [0.349]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.563984758711484
siam score:  -0.8699294
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.875]
 [34.082]
 [34.082]
 [38.148]
 [34.3  ]] [[0.459]
 [1.078]
 [1.078]
 [1.325]
 [1.091]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.00739288330078
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.00855484268151
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.914]
 [35.45 ]
 [35.45 ]
 [35.45 ]
 [35.45 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  3.8050430976795724
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.23868027897982
printing an ep nov before normalisation:  63.12274335281503
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Starting evaluation
actions average: 
K:  2  action  0 :  tensor([    0.9971,     0.0002,     0.0000,     0.0009,     0.0019],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0012,     0.9332,     0.0230,     0.0003,     0.0423],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0019,     0.8567,     0.0437,     0.0976],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0005,     0.0002,     0.0307,     0.7775,     0.1911],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0005,     0.0238,     0.0681,     0.3838,     0.5238],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.156]
 [49.156]
 [91.47 ]
 [40.602]
 [49.156]] [[0.68 ]
 [0.68 ]
 [1.266]
 [0.562]
 [0.68 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.48580837249756
printing an ep nov before normalisation:  0.0013673881949216593
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.061138898157424
printing an ep nov before normalisation:  0.06905785157329092
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.353318810280705
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.718180962288876
actions average: 
K:  0  action  0 :  tensor([    1.0000,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0019,     0.9700,     0.0175,     0.0001,     0.0105],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0085, 0.0100, 0.8944, 0.0386, 0.0486], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0009,     0.0003,     0.0222,     0.7547,     0.2219],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0041, 0.0020, 0.1239, 0.1640, 0.7060], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 97.8567385324401
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.69 ]
 [44.825]
 [43.179]
 [46.183]
 [45.236]] [[1.059]
 [1.111]
 [1.036]
 [1.172]
 [1.129]]
printing an ep nov before normalisation:  31.008712041911163
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.256 0.308 0.179 0.154 0.103]
printing an ep nov before normalisation:  49.9145074988456
printing an ep nov before normalisation:  118.49496605707846
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.41103639652887
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8578352
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.670596687713825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.752332451838676
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.40018386294094
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.5318629690894
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.78875978167171
actions average: 
K:  3  action  0 :  tensor([    0.9983,     0.0001,     0.0000,     0.0006,     0.0010],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9991,     0.0000,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0091,     0.9182,     0.0113,     0.0614],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0008,     0.0599,     0.7069,     0.2321],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0025, 0.0011, 0.0691, 0.3048, 0.6226], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 44.91008526961046
printing an ep nov before normalisation:  27.43637391499111
printing an ep nov before normalisation:  48.00609601388473
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.22020168969423
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.158349943444684
printing an ep nov before normalisation:  51.7753284323506
printing an ep nov before normalisation:  31.799643708524936
actions average: 
K:  0  action  0 :  tensor([    0.9924,     0.0000,     0.0000,     0.0038,     0.0038],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0009,     0.9942,     0.0000,     0.0000,     0.0049],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0002,     0.9249,     0.0227,     0.0521],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0031, 0.0058, 0.0446, 0.6654, 0.2812], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0022, 0.0014, 0.0994, 0.3182, 0.5788], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.290910720825195
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8571301
printing an ep nov before normalisation:  55.93196661597855
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  65.58260411645462
siam score:  -0.8602774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.9786072913162
printing an ep nov before normalisation:  67.58304523977095
printing an ep nov before normalisation:  54.2350156315286
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.7  ]
 [43.373]
 [52.714]
 [48.381]
 [45.786]] [[0.439]
 [0.493]
 [0.68 ]
 [0.593]
 [0.541]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.85843194689515
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.534044281345516
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.14336485404877
printing an ep nov before normalisation:  43.46147537231445
printing an ep nov before normalisation:  44.53399354217818
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.966]
 [53.966]
 [53.966]
 [53.966]
 [53.966]] [[1.361]
 [1.361]
 [1.361]
 [1.361]
 [1.361]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9967,     0.0003,     0.0000,     0.0021,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9978,     0.0003,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9497,     0.0169,     0.0333],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0024, 0.0006, 0.0498, 0.6433, 0.3039], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0064, 0.0007, 0.0475, 0.3759, 0.5694], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.97267419717407
printing an ep nov before normalisation:  51.20991494626184
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8705621
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.78864097595215
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.143]
 [44.297]
 [43.46 ]
 [44.258]
 [45.903]] [[0.232]
 [0.159]
 [0.152]
 [0.159]
 [0.172]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8692183
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86563385
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.30483964037214
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.119]
 [50.644]
 [43.935]
 [50.268]
 [48.636]] [[1.343]
 [1.368]
 [1.05 ]
 [1.35 ]
 [1.273]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.83656663160729
printing an ep nov before normalisation:  48.38978300911396
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.84386478325973
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.4585606988694
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8578357
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  30.772205643313306
printing an ep nov before normalisation:  39.08727182495795
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  57.95564416547504
printing an ep nov before normalisation:  75.60227291449489
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9992,     0.0000,     0.0000,     0.0002,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0008,     0.9763,     0.0063,     0.0001,     0.0164],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0029,     0.9323,     0.0117,     0.0526],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0005,     0.0537,     0.6460,     0.2996],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0005,     0.0069,     0.1248,     0.2809,     0.5870],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.952508009704047
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.97973543538163
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.49006272829185
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  72.8201608981071
printing an ep nov before normalisation:  39.61838831781581
actions average: 
K:  1  action  0 :  tensor([    0.9965,     0.0001,     0.0000,     0.0033,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0005,     0.9952,     0.0004,     0.0001,     0.0039],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0002,     0.9625,     0.0181,     0.0191],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0005,     0.0005,     0.0519,     0.7114,     0.2358],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0015, 0.0009, 0.1548, 0.3429, 0.4999], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.4214506149292
printing an ep nov before normalisation:  46.02559041015232
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  33.572310743965765
siam score:  -0.86443985
printing an ep nov before normalisation:  62.512753544949504
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.5830570692318
printing an ep nov before normalisation:  41.22197151184082
printing an ep nov before normalisation:  46.99813880801561
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.737]
 [28.814]
 [37.272]
 [32.999]
 [32.656]] [[0.714]
 [0.45 ]
 [0.828]
 [0.637]
 [0.622]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.92561483383179
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.736444879001226
printing an ep nov before normalisation:  48.242437230751264
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86889195
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.21809256074098
siam score:  -0.8720001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.45763969421387
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.555]
 [40.555]
 [40.947]
 [40.555]
 [40.555]] [[0.814]
 [0.814]
 [0.829]
 [0.814]
 [0.814]]
printing an ep nov before normalisation:  52.489265428387284
printing an ep nov before normalisation:  59.12838445085058
printing an ep nov before normalisation:  57.376596311618194
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.09970171424798
printing an ep nov before normalisation:  30.914564856015204
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  112.00725555278053
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.911]
 [42.868]
 [48.225]
 [44.333]
 [42.147]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.459615315267094
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.92669705571232
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86961037
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.698]
 [52.997]
 [52.997]
 [52.997]
 [63.265]] [[0.722]
 [0.535]
 [0.535]
 [0.535]
 [0.872]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.863]
 [36.814]
 [40.123]
 [38.104]
 [34.376]] [[0.636]
 [0.751]
 [0.879]
 [0.801]
 [0.656]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.868437
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.38972236799609
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86757815
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86980027
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.17965644182617
actions average: 
K:  4  action  0 :  tensor([    0.9890,     0.0008,     0.0000,     0.0020,     0.0081],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9971,     0.0013,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0006,     0.8442,     0.0810,     0.0740],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0010,     0.0548,     0.7771,     0.1669],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0007, 0.0717, 0.2012, 0.3559, 0.3704], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.362]
 [44.541]
 [44.741]
 [43.687]
 [44.441]] [[0.856]
 [1.108]
 [1.116]
 [1.073]
 [1.104]]
printing an ep nov before normalisation:  51.44053327799326
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.50424575805664
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8682736
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.04 ]
 [57.64 ]
 [54.225]
 [57.04 ]
 [57.04 ]] [[1.813]
 [1.846]
 [1.659]
 [1.813]
 [1.813]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.038209713900216
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8676117
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.3190661169329
printing an ep nov before normalisation:  35.60954809188843
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.657]
 [51.498]
 [52.399]
 [45.818]
 [46.907]] [[0.812]
 [1.342]
 [1.391]
 [1.037]
 [1.095]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  48.97216450988478
printing an ep nov before normalisation:  47.27205402651911
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [53.539]
 [34.281]] [[-0.195]
 [-0.195]
 [-0.195]
 [ 0.479]
 [ 0.237]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.09023091831238617
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  82.74222565089427
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.03667815575425
printing an ep nov before normalisation:  60.65959183027582
printing an ep nov before normalisation:  35.452090838561126
printing an ep nov before normalisation:  46.10974290074152
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9690,     0.0005,     0.0002,     0.0132,     0.0171],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0005,     0.9664,     0.0177,     0.0001,     0.0153],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0005,     0.0022,     0.8768,     0.0554,     0.0652],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0012,     0.0005,     0.0278,     0.7142,     0.2562],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0028, 0.0014, 0.0561, 0.2191, 0.7206], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8748626
printing an ep nov before normalisation:  40.261650652386685
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.076]
 [33.585]
 [33.585]
 [34.559]
 [33.585]] [[0.522]
 [0.537]
 [0.537]
 [0.564]
 [0.537]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.65649782565707
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  82.39918275448774
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 36.13373405328055
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.379]
 [32.544]
 [58.548]
 [39.746]
 [38.38 ]] [[0.115]
 [0.138]
 [0.655]
 [0.281]
 [0.254]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.30843330660942
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.321]
 [46.232]
 [47.662]
 [46.073]
 [46.394]] [[1.255]
 [1.349]
 [1.419]
 [1.341]
 [1.357]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  103.16478268936558
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8729932
printing an ep nov before normalisation:  44.108162903179476
printing an ep nov before normalisation:  38.825294114242716
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.50182852963027
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.014]
 [42.28 ]
 [40.014]
 [45.728]
 [40.014]] [[1.251]
 [1.408]
 [1.251]
 [1.647]
 [1.251]]
printing an ep nov before normalisation:  42.74110285006419
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.80888636329826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9988,     0.0001,     0.0000,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9923,     0.0005,     0.0010,     0.0057],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0009,     0.9551,     0.0169,     0.0271],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0147,     0.0005,     0.0320,     0.7672,     0.1856],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0190, 0.0372, 0.0831, 0.1935, 0.6673], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.8641161
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.02606173098898
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.44 ]
 [44.033]
 [44.033]
 [44.033]
 [44.033]] [[1.372]
 [0.958]
 [0.958]
 [0.958]
 [0.958]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.479]
 [35.479]
 [37.041]
 [35.479]
 [35.479]] [[0.878]
 [0.878]
 [0.943]
 [0.878]
 [0.878]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.79706501761104
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.747218790466135
siam score:  -0.86108977
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.792031238243645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.27651683982255
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.060967799016694
printing an ep nov before normalisation:  34.69815404774083
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.55503490735869
printing an ep nov before normalisation:  52.95607467790237
printing an ep nov before normalisation:  44.8239174814355
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.924]
 [35.924]
 [35.924]
 [35.924]
 [35.924]] [[1.123]
 [1.123]
 [1.123]
 [1.123]
 [1.123]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.9411785760263456
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.90155356476616
actions average: 
K:  4  action  0 :  tensor([    0.9831,     0.0001,     0.0000,     0.0030,     0.0137],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0014,     0.9713,     0.0007,     0.0001,     0.0265],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0077,     0.9091,     0.0416,     0.0415],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0141, 0.0011, 0.0803, 0.6360, 0.2685], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0029,     0.0006,     0.0021,     0.2665,     0.7279],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.87008935
printing an ep nov before normalisation:  30.025750077813523
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.87028944
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.03736318450066
printing an ep nov before normalisation:  31.26720648759249
printing an ep nov before normalisation:  35.00696747707236
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.86864552796616
printing an ep nov before normalisation:  36.03965548435256
printing an ep nov before normalisation:  53.609905206454876
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.641]
 [29.641]
 [29.641]
 [29.641]
 [29.641]] [[9.87]
 [9.87]
 [9.87]
 [9.87]
 [9.87]]
printing an ep nov before normalisation:  33.700019246348404
printing an ep nov before normalisation:  47.46615321699867
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  94.56800254820739
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86175567
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.32452747576629
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.656]
 [34.656]
 [34.656]
 [34.656]
 [34.656]] [[0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.377]
 [26.377]
 [26.377]
 [26.377]
 [26.377]] [[0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.299]
 [51.717]
 [60.036]
 [58.795]
 [58.795]] [[1.224]
 [1.393]
 [1.711]
 [1.664]
 [1.664]]
printing an ep nov before normalisation:  58.332024521905325
printing an ep nov before normalisation:  36.36526650787867
actions average: 
K:  3  action  0 :  tensor([    0.9975,     0.0016,     0.0000,     0.0004,     0.0005],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9808,     0.0041,     0.0001,     0.0149],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0010,     0.9436,     0.0232,     0.0322],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0001,     0.0057,     0.9280,     0.0661],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0029, 0.0471, 0.1177, 0.2619, 0.5704], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.1490998095943
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.989957613879945
printing an ep nov before normalisation:  2.090825541907577
actions average: 
K:  2  action  0 :  tensor([    0.9981,     0.0004,     0.0000,     0.0002,     0.0013],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9942,     0.0029,     0.0000,     0.0028],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9630,     0.0103,     0.0267],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0005,     0.0006,     0.0392,     0.7195,     0.2403],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0010, 0.0047, 0.0751, 0.3845, 0.5346], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  89.79542865043682
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.538448771713398
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.59059757450423
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.367]
 [24.367]
 [32.293]
 [24.367]
 [24.367]] [[0.123]
 [0.123]
 [0.192]
 [0.123]
 [0.123]]
siam score:  -0.8595616
printing an ep nov before normalisation:  43.78511644199471
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.10095500946045
printing an ep nov before normalisation:  60.46291642557447
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  64.08561147829137
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
siam score:  -0.87112916
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 51.26134281064096
siam score:  -0.86801165
printing an ep nov before normalisation:  50.665060648654865
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  107.26039687499717
printing an ep nov before normalisation:  79.00518783675797
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.802704918799236
printing an ep nov before normalisation:  50.88041064393485
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  126.03180324246657
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.699]
 [20.699]
 [20.699]
 [20.699]
 [20.699]] [[0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]]
printing an ep nov before normalisation:  28.598536358969678
printing an ep nov before normalisation:  0.7311968238389
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.35271353312218
printing an ep nov before normalisation:  46.118484223155725
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.186679872180456
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  89.3275002507625
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.243]
 [52.243]
 [52.243]
 [52.243]
 [52.243]] [[1.07]
 [1.07]
 [1.07]
 [1.07]
 [1.07]]
printing an ep nov before normalisation:  59.171081130160665
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.611]
 [44.186]
 [46.525]
 [51.624]
 [47.554]] [[0.475]
 [0.662]
 [0.72 ]
 [0.846]
 [0.745]]
printing an ep nov before normalisation:  62.51688902722184
printing an ep nov before normalisation:  54.30134765027138
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.648]
 [39.636]
 [46.523]
 [44.532]
 [46.064]] [[0.673]
 [0.708]
 [0.95 ]
 [0.88 ]
 [0.934]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.79644217685836
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8682756
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.649]
 [28.333]
 [36.405]
 [36.071]
 [34.407]] [[0.898]
 [0.827]
 [1.258]
 [1.24 ]
 [1.151]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.15347347574561354
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.44726239884209
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8719243
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.135384817791824
printing an ep nov before normalisation:  83.76771642405805
printing an ep nov before normalisation:  38.715096184543285
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.915]
 [41.419]
 [39.829]
 [46.702]
 [42.637]] [[1.667]
 [1.25 ]
 [1.162]
 [1.544]
 [1.318]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.0602205419021
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.537]
 [56.537]
 [59.667]
 [56.537]
 [56.537]] [[0.515]
 [0.515]
 [0.558]
 [0.515]
 [0.515]]
printing an ep nov before normalisation:  34.39409893408289
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.94786834716797
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.068]
 [41.068]
 [41.068]
 [44.267]
 [41.068]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.145]
 [45.627]
 [49.99 ]
 [39.482]
 [38.9  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.31720071478333
printing an ep nov before normalisation:  85.20628248720666
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.069642246044744
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.73861709196112
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  4.885498197177185
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.296738624572754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9988,     0.0007,     0.0000,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0007,     0.9978,     0.0001,     0.0000,     0.0013],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0002,     0.9549,     0.0202,     0.0248],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0025,     0.0005,     0.0180,     0.7816,     0.1973],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0045, 0.0110, 0.0594, 0.3654, 0.5597], grad_fn=<DivBackward0>)
siam score:  -0.8691382
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.113]
 [33.113]
 [47.124]
 [40.338]
 [33.113]] [[0.411]
 [0.411]
 [0.746]
 [0.584]
 [0.411]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.966]
 [49.812]
 [50.318]
 [49.812]
 [49.812]] [[1.631]
 [1.241]
 [1.261]
 [1.241]
 [1.241]]
printing an ep nov before normalisation:  64.85531351754467
printing an ep nov before normalisation:  63.45387022581979
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.953928701561054
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.28294809046484
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  47.19196369643149
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  57.48122811740526
printing an ep nov before normalisation:  56.308832608429384
using explorer policy with actor:  1
siam score:  -0.86721617
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.8852510487874952
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.237812796547018
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.38790225982666
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.83910608291626
printing an ep nov before normalisation:  59.20171103012966
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.328307399842025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[79.826]
 [79.826]
 [79.826]
 [79.826]
 [79.826]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
printing an ep nov before normalisation:  54.72013203921383
printing an ep nov before normalisation:  48.72757460468897
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  102.73253197805478
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8638414
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.508392284573766
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.304]
 [51.683]
 [50.029]
 [52.346]
 [55.621]] [[0.673]
 [0.56 ]
 [0.527]
 [0.574]
 [0.64 ]]
printing an ep nov before normalisation:  49.20906331120301
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.70380932875629
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.386]
 [33.941]
 [28.193]
 [28.684]
 [28.684]] [[1.021]
 [1.41 ]
 [1.007]
 [1.042]
 [1.042]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.28605071406347
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.04924468580692
printing an ep nov before normalisation:  58.15885478819429
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.5305299552237
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.07370523799088
printing an ep nov before normalisation:  27.027973465592407
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 46.13270073679708
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.175]
 [32.836]
 [26.867]
 [29.802]
 [42.88 ]] [[0.707]
 [0.439]
 [0.309]
 [0.373]
 [0.657]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.30911234631762
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.6475276233925342
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.6469061642212
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.20266802599169
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.062042998714006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  96.7732051508665
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.81490350004283
printing an ep nov before normalisation:  54.05354268049667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.559]
 [37.519]
 [36.73 ]
 [46.138]
 [35.26 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  35.777845506229305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9950,     0.0001,     0.0028,     0.0001,     0.0020],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9392,     0.0315,     0.0029,     0.0260],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0002,     0.9313,     0.0319,     0.0365],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0010,     0.0004,     0.0376,     0.7096,     0.2513],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0033, 0.0067, 0.0288, 0.2967, 0.6646], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8690651
printing an ep nov before normalisation:  111.11754145790263
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.782453400748118
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9947,     0.0004,     0.0000,     0.0001,     0.0048],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9808,     0.0000,     0.0000,     0.0190],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0006,     0.9121,     0.0215,     0.0657],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0010,     0.0005,     0.0275,     0.7533,     0.2177],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0022, 0.1030, 0.0704, 0.1467, 0.6777], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9997,     0.0001,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0007,     0.9960,     0.0007,     0.0000,     0.0025],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9437,     0.0248,     0.0313],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0009,     0.0007,     0.0850,     0.8020,     0.1114],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0532, 0.0152, 0.1654, 0.2111, 0.5550], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.86046326
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.2651234340289
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.478]
 [58.475]
 [48.239]
 [59.329]
 [60.267]] [[0.726]
 [0.626]
 [0.422]
 [0.643]
 [0.662]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  83.99250984191895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.119477853159616
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.281870760454524
printing an ep nov before normalisation:  0.001725180450193875
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.19436011995828
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.93737485079032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.15366374693939
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.70899696861985
printing an ep nov before normalisation:  36.67498815324141
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9990,     0.0000,     0.0000,     0.0003,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9998,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0002,     0.9214,     0.0350,     0.0433],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0005,     0.0006,     0.0009,     0.7875,     0.2105],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0002,     0.0012,     0.0862,     0.3677,     0.5447],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.290397751526193
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
siam score:  -0.85892487
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.607]
 [56.974]
 [61.959]
 [56.155]
 [58.759]] [[0.698]
 [0.627]
 [0.761]
 [0.605]
 [0.675]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.847]
 [49.847]
 [55.675]
 [49.847]
 [49.847]] [[1.002]
 [1.002]
 [1.158]
 [1.002]
 [1.002]]
printing an ep nov before normalisation:  59.830839483440016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  103.85406953823235
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.38915146697464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  78.21506771934195
printing an ep nov before normalisation:  69.23005380760404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.321685943122986
siam score:  -0.8672411
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9969,     0.0007,     0.0000,     0.0005,     0.0019],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9926,     0.0027,     0.0000,     0.0046],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9751,     0.0127,     0.0121],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0007,     0.1032,     0.6804,     0.2154],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0002,     0.0346,     0.0483,     0.3541,     0.5627],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.32082964722704
printing an ep nov before normalisation:  17.908308903376263
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.783796369460255
printing an ep nov before normalisation:  43.28598505540574
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.86615044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.513 0.128 0.051 0.128 0.179]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  4.765549761032162e-05
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.147]
 [34.044]
 [42.187]
 [41.019]
 [42.91 ]] [[1.266]
 [0.784]
 [1.215]
 [1.153]
 [1.253]]
siam score:  -0.8710472
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86900526
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.523]
 [35.523]
 [35.523]
 [35.523]
 [35.523]] [[1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]]
printing an ep nov before normalisation:  43.01181163093513
printing an ep nov before normalisation:  55.093784895539144
printing an ep nov before normalisation:  49.654467355799866
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.055]
 [63.275]
 [40.38 ]
 [43.876]
 [44.68 ]] [[0.561]
 [0.882]
 [0.326]
 [0.411]
 [0.43 ]]
printing an ep nov before normalisation:  44.41661348659975
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.21782964692383
printing an ep nov before normalisation:  32.02720716937664
siam score:  -0.8630127
printing an ep nov before normalisation:  71.43742738497878
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.86076486
actions average: 
K:  0  action  0 :  tensor([    0.9961,     0.0000,     0.0000,     0.0012,     0.0027],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0025,     0.9936,     0.0002,     0.0000,     0.0037],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0008,     0.0050,     0.9184,     0.0284,     0.0474],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0006,     0.0833,     0.6528,     0.2631],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0011, 0.0163, 0.0496, 0.3026, 0.6305], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.79886951014481
printing an ep nov before normalisation:  55.335296568935696
printing an ep nov before normalisation:  33.12879541930932
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.10013771057129
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.06329548998213
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  65.94094276428223
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.623279283898285
printing an ep nov before normalisation:  61.61335891638649
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8619577
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.23735171605509
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.9981,     0.0004,     0.0000,     0.0007,     0.0007],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0016,     0.9304,     0.0185,     0.0000,     0.0495],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0088,     0.9050,     0.0275,     0.0586],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0006,     0.0012,     0.0175,     0.8611,     0.1196],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0003,     0.0283,     0.2476,     0.2721,     0.4516],
       grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.861861
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86691546
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.86415416111983
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  61.09622545952371
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.12696551047661
printing an ep nov before normalisation:  40.638739701528436
printing an ep nov before normalisation:  47.31693908288759
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.696]
 [57.953]
 [52.696]
 [52.696]
 [52.696]] [[0.944]
 [1.113]
 [0.944]
 [0.944]
 [0.944]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.106]
 [46.106]
 [55.718]
 [46.106]
 [46.106]] [[1.052]
 [1.052]
 [1.421]
 [1.052]
 [1.052]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.65027560003519
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.00996056488685
using explorer policy with actor:  1
printing an ep nov before normalisation:  83.58405114440598
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[74.071]
 [75.348]
 [85.692]
 [82.155]
 [88.484]] [[1.137]
 [1.181]
 [1.539]
 [1.417]
 [1.635]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.195]
 [51.878]
 [44.36 ]
 [42.771]
 [46.603]] [[0.756]
 [1.069]
 [0.826]
 [0.775]
 [0.898]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.617]
 [36.576]
 [47.935]
 [35.018]
 [34.237]] [[0.656]
 [0.728]
 [1.143]
 [0.671]
 [0.643]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.118]
 [51.945]
 [51.118]
 [51.118]
 [50.298]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  62.10131860964088
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.73204697733521
printing an ep nov before normalisation:  50.33314601299714
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.873]
 [36.654]
 [41.395]
 [35.865]
 [34.962]] [[0.18 ]
 [0.228]
 [0.276]
 [0.22 ]
 [0.211]]
printing an ep nov before normalisation:  69.28329170246249
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.28127813925372
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85770816
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.277478662360416
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.288]
 [35.288]
 [46.712]
 [35.288]
 [35.288]] [[0.676]
 [0.676]
 [1.132]
 [0.676]
 [0.676]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.58590187555267
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.663849210858565
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.648311614990234
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.4173232852225
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 30.544620615105767
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  92.48593435959474
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.61799470336358
printing an ep nov before normalisation:  58.38709669377175
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.73031234741211
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.03954029071413
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8682652
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.822848699562435
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8654364
printing an ep nov before normalisation:  57.140829103829674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8622693
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.445115819769434
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.45124002968158
line 256 mcts: sample exp_bonus 41.000263494130124
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  64.88656041055768
line 256 mcts: sample exp_bonus 41.78535565717308
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.75113063745489
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 34.73687126821422
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.22809898295375
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.97842572512532
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.49242593968019
printing an ep nov before normalisation:  96.2321783567197
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[135.769]
 [123.34 ]
 [123.34 ]
 [123.34 ]
 [123.34 ]] [[1.333]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.364]
 [55.54 ]
 [41.626]
 [49.221]
 [53.008]] [[0.392]
 [0.504]
 [0.286]
 [0.405]
 [0.464]]
printing an ep nov before normalisation:  40.536203384399414
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  52.516636601492614
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.964476934870078
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  97.18513528128352
printing an ep nov before normalisation:  35.422079631457166
printing an ep nov before normalisation:  26.678267547062468
printing an ep nov before normalisation:  55.33245943272095
actions average: 
K:  3  action  0 :  tensor([    0.9890,     0.0007,     0.0087,     0.0005,     0.0012],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9801,     0.0016,     0.0000,     0.0179],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0005,     0.8761,     0.0381,     0.0852],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0008, 0.0009, 0.0360, 0.7570, 0.2053], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0031, 0.0233, 0.0994, 0.1946, 0.6796], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  67.95395635491674
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.29849334574718
printing an ep nov before normalisation:  56.18456011314341
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.56591314270747
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.44757627952796
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.08834648132324
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.56529116301245
printing an ep nov before normalisation:  65.67565333015611
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.19905126799831
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84680724
line 256 mcts: sample exp_bonus 35.06111454528843
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.40470426253301
printing an ep nov before normalisation:  61.64107468138703
printing an ep nov before normalisation:  38.419189185531955
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.088987973677554
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.17929553985596
line 256 mcts: sample exp_bonus 41.291934967041016
siam score:  -0.86624056
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.56761050222985
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  36.129650073975256
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.632363862141965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.743665837476755
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8716411
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.50254022745678
printing an ep nov before normalisation:  63.31320567057921
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.87100846
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  61.95644084823637
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.435]
 [49.415]
 [38.435]
 [38.435]
 [38.435]] [[0.229]
 [0.333]
 [0.229]
 [0.229]
 [0.229]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.620530738222534
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.363]
 [43.777]
 [44.901]
 [38.442]
 [43.019]] [[0.442]
 [0.468]
 [0.489]
 [0.37 ]
 [0.454]]
printing an ep nov before normalisation:  45.627284713477366
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.157]
 [49.783]
 [39.826]
 [53.563]
 [49.81 ]] [[0.29 ]
 [0.267]
 [0.169]
 [0.304]
 [0.267]]
printing an ep nov before normalisation:  39.26426887512207
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.302]
 [37.598]
 [66.941]
 [42.824]
 [35.928]] [[0.261]
 [0.45 ]
 [1.118]
 [0.569]
 [0.412]]
printing an ep nov before normalisation:  66.76414474136224
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.495495252254194
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.21282174433754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.86044144630432
printing an ep nov before normalisation:  43.21632910657566
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.26689338684082
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.736]
 [34.347]
 [33.787]
 [36.047]
 [33.119]] [[0.302]
 [0.468]
 [0.454]
 [0.511]
 [0.437]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.760392460581656
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  30.78239443584811
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.599382400512695
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86125064
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  69.27588676280953
printing an ep nov before normalisation:  58.50869054615104
printing an ep nov before normalisation:  35.46114921569824
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  19.37313468788664
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.024]
 [44.802]
 [44.802]
 [44.802]
 [44.802]] [[1.479]
 [0.903]
 [0.903]
 [0.903]
 [0.903]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.905]
 [51.905]
 [51.533]
 [57.249]
 [51.905]] [[1.667]
 [1.667]
 [1.651]
 [1.905]
 [1.667]]
printing an ep nov before normalisation:  46.39120048728482
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86351645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  83.36948666656559
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.479516777101836
actions average: 
K:  2  action  0 :  tensor([    0.9976,     0.0004,     0.0000,     0.0007,     0.0013],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9873,     0.0001,     0.0000,     0.0125],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0004,     0.0003,     0.8955,     0.0446,     0.0591],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0001,     0.0036,     0.8454,     0.1507],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0026, 0.0008, 0.0014, 0.3376, 0.6576], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.423628072410157
siam score:  -0.8577064
siam score:  -0.8587453
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.85949033
printing an ep nov before normalisation:  39.05200650733518
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  75.71808149612622
printing an ep nov before normalisation:  56.82059804221707
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.00604963190472
printing an ep nov before normalisation:  68.34409058910866
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.85609946336691
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.38215273396442
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.18 ]
 [53.7  ]
 [43.995]
 [43.995]
 [43.995]] [[1.193]
 [1.483]
 [1.109]
 [1.109]
 [1.109]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.  ]
 [37.08]
 [ 0.  ]
 [ 0.  ]
 [ 0.  ]] [[-1.81]
 [ 2.  ]
 [-1.81]
 [-1.81]
 [-1.81]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  47.22761965422717
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.815]
 [59.715]
 [59.715]
 [59.715]
 [59.715]] [[2.   ]
 [1.816]
 [1.816]
 [1.816]
 [1.816]]
printing an ep nov before normalisation:  64.40694970826186
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.915927049419345
printing an ep nov before normalisation:  55.020786359877405
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.122]
 [53.152]
 [59.927]
 [59.753]
 [40.105]] [[0.203]
 [0.372]
 [0.46 ]
 [0.458]
 [0.203]]
Starting evaluation
printing an ep nov before normalisation:  0.04867825787565987
printing an ep nov before normalisation:  25.87769197145906
printing an ep nov before normalisation:  0.0034684266211115755
printing an ep nov before normalisation:  55.175312307140445
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[10.756]
 [21.169]
 [14.047]
 [24.293]
 [23.154]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  55.88090437513824
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[73.432]
 [57.776]
 [58.901]
 [58.901]
 [58.901]] [[1.14 ]
 [0.752]
 [0.78 ]
 [0.78 ]
 [0.78 ]]
printing an ep nov before normalisation:  42.94782330855945
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.573]
 [59.573]
 [59.573]
 [59.573]
 [59.573]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  33.199065841755456
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.6010598622717
printing an ep nov before normalisation:  50.41434139808073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.337]
 [39.749]
 [32.272]
 [34.053]
 [33.898]] [[1.383]
 [1.346]
 [0.876]
 [0.988]
 [0.978]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.2697868347168
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.52294898286625
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.366]
 [59.453]
 [58.402]
 [55.366]
 [55.366]] [[1.643]
 [1.81 ]
 [1.767]
 [1.643]
 [1.643]]
siam score:  -0.8579575
siam score:  -0.8587189
printing an ep nov before normalisation:  31.42892837524414
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.36610893324621
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.17299270629883
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8666975
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.069092664838564
printing an ep nov before normalisation:  72.560904974232
line 256 mcts: sample exp_bonus 48.01105721800638
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.947]
 [36.947]
 [54.218]
 [36.947]
 [36.947]] [[0.802]
 [0.802]
 [1.517]
 [0.802]
 [0.802]]
siam score:  -0.8700423
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9873,     0.0016,     0.0000,     0.0014,     0.0097],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9988,     0.0001,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0008,     0.0029,     0.8353,     0.0311,     0.1299],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0004,     0.0005,     0.0619,     0.6485,     0.2887],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0004,     0.0376,     0.0787,     0.2657,     0.6176],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  78.57523778531129
printing an ep nov before normalisation:  80.93623607132359
actions average: 
K:  2  action  0 :  tensor([    0.9984,     0.0001,     0.0000,     0.0004,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0018,     0.9887,     0.0008,     0.0006,     0.0081],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0011,     0.9566,     0.0114,     0.0309],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0020,     0.0003,     0.0591,     0.6858,     0.2528],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0196, 0.0018, 0.0803, 0.2740, 0.6243], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.396]
 [38.739]
 [53.968]
 [65.939]
 [39.07 ]] [[0.089]
 [0.308]
 [0.667]
 [0.949]
 [0.316]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.9981317735416
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.055389404296875
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.529]
 [33.703]
 [43.078]
 [28.08 ]
 [31.249]] [[0.071]
 [0.392]
 [0.662]
 [0.231]
 [0.322]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.426072621597875
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.310508357298595
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  15.162806709607443
printing an ep nov before normalisation:  75.45142803709238
printing an ep nov before normalisation:  32.84013748168945
printing an ep nov before normalisation:  66.28255844116211
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.4135022942217
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.00942131049386
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  91.9176034970066
printing an ep nov before normalisation:  52.999747582234406
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.227]
 [67.962]
 [64.929]
 [64.929]
 [64.929]] [[0.692]
 [0.942]
 [0.864]
 [0.864]
 [0.864]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  64.81901501444287
actions average: 
K:  4  action  0 :  tensor([    0.9873,     0.0001,     0.0001,     0.0067,     0.0058],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9808,     0.0082,     0.0021,     0.0087],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9509,     0.0149,     0.0341],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0801,     0.0003,     0.1144,     0.5584,     0.2467],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0011, 0.0210, 0.0599, 0.3097, 0.6083], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  108.14727914499996
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.86081666
printing an ep nov before normalisation:  68.55347295532096
printing an ep nov before normalisation:  41.626473385809646
printing an ep nov before normalisation:  48.32624413909165
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.934]
 [31.891]
 [67.357]
 [65.134]
 [39.939]] [[0.245]
 [0.244]
 [1.081]
 [1.029]
 [0.434]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.194596790530476
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.264835223025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.91735130242169
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.469]
 [43.469]
 [43.469]
 [43.469]
 [43.469]] [[0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.432]
 [32.432]
 [49.796]
 [32.432]
 [32.432]] [[0.163]
 [0.163]
 [0.329]
 [0.163]
 [0.163]]
printing an ep nov before normalisation:  98.46663732253249
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.495162566880154
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.065399861202465
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.351]
 [68.   ]
 [78.408]
 [74.82 ]
 [72.607]] [[0.38 ]
 [0.613]
 [0.822]
 [0.75 ]
 [0.705]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.20496439933777
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.55021276213084
printing an ep nov before normalisation:  0.0025685216485271667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.997]
 [63.319]
 [54.315]
 [54.315]
 [54.315]] [[1.53 ]
 [1.404]
 [1.205]
 [1.205]
 [1.205]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.61728146763353
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.415033444787014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.00019418873989707208
actions average: 
K:  0  action  0 :  tensor([    0.9992,     0.0000,     0.0000,     0.0003,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9977,     0.0002,     0.0001,     0.0018],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9653,     0.0106,     0.0240],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0008,     0.0002,     0.0141,     0.7711,     0.2138],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0073, 0.0147, 0.0549, 0.2495, 0.6737], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.8383077773922
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 77.76416385394423
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9880,     0.0002,     0.0066,     0.0004,     0.0049],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9172,     0.0313,     0.0001,     0.0513],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0003,     0.9495,     0.0135,     0.0367],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0006,     0.0509,     0.7116,     0.2367],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0006,     0.0697,     0.1008,     0.1750,     0.6539],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.304]
 [24.204]
 [24.889]
 [26.85 ]
 [25.476]] [[0.631]
 [0.604]
 [0.621]
 [0.67 ]
 [0.635]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.73991656435786
printing an ep nov before normalisation:  46.903768834357805
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.851]
 [57.851]
 [57.851]
 [57.851]
 [57.851]] [[1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.345]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9821,     0.0047,     0.0000,     0.0046,     0.0085],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9949,     0.0006,     0.0001,     0.0044],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0007,     0.0196,     0.9196,     0.0013,     0.0588],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0001,     0.0031,     0.7527,     0.2441],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0001,     0.0003,     0.0061,     0.4669,     0.5266],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.194]
 [33.68 ]
 [34.026]
 [35.193]
 [31.415]] [[1.33 ]
 [1.02 ]
 [1.039]
 [1.105]
 [0.892]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8714029
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.14 ]
 [60.31 ]
 [45.548]
 [45.548]
 [45.548]] [[0.475]
 [0.592]
 [0.381]
 [0.381]
 [0.381]]
siam score:  -0.8685497
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.723098278045654
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.15662305225176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.95228328409711
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0020901835887343623
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.667845514085556
printing an ep nov before normalisation:  20.788781722806554
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.856865420807146
printing an ep nov before normalisation:  33.237262427881
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.719]
 [27.719]
 [27.719]
 [27.719]
 [27.719]] [[0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.5694532291244
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8664557
printing an ep nov before normalisation:  53.34933248368195
printing an ep nov before normalisation:  32.66571691671654
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.334517394756233
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.6314763196981
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  68.50419931280877
printing an ep nov before normalisation:  69.84377686582715
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.36590099334717
printing an ep nov before normalisation:  62.86931118248033
printing an ep nov before normalisation:  46.37612168784418
actions average: 
K:  0  action  0 :  tensor([    0.9989,     0.0000,     0.0000,     0.0006,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0026,     0.9587,     0.0155,     0.0003,     0.0230],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0004,     0.0004,     0.9305,     0.0256,     0.0431],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0033,     0.0006,     0.0434,     0.7299,     0.2228],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0046, 0.0013, 0.1027, 0.2733, 0.6180], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[82.331]
 [82.331]
 [82.331]
 [82.331]
 [82.331]] [[1.25]
 [1.25]
 [1.25]
 [1.25]
 [1.25]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[87.597]
 [77.045]
 [77.045]
 [77.045]
 [77.045]] [[1.529]
 [1.241]
 [1.241]
 [1.241]
 [1.241]]
siam score:  -0.86536026
using explorer policy with actor:  1
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.9971,     0.0000,     0.0000,     0.0015,     0.0014],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9989,     0.0000,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0003,     0.8969,     0.0239,     0.0789],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0012,     0.0807,     0.6789,     0.2389],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0018, 0.0311, 0.0548, 0.2773, 0.6349], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8656932
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 51.870125346511145
actions average: 
K:  1  action  0 :  tensor([    0.9993,     0.0001,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9984,     0.0001,     0.0000,     0.0013],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9341,     0.0319,     0.0339],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0017,     0.0002,     0.0546,     0.6922,     0.2513],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0099, 0.0366, 0.0588, 0.3737, 0.5210], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.59946670097214
printing an ep nov before normalisation:  89.24178641980912
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.025]
 [30.025]
 [41.433]
 [30.025]
 [30.025]] [[0.205]
 [0.205]
 [0.372]
 [0.205]
 [0.205]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.05531536684565
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.825]
 [53.825]
 [62.339]
 [53.825]
 [53.825]] [[1.287]
 [1.287]
 [1.632]
 [1.287]
 [1.287]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.511]
 [46.371]
 [45.03 ]
 [44.701]
 [47.727]] [[0.716]
 [0.636]
 [0.602]
 [0.593]
 [0.671]]
printing an ep nov before normalisation:  57.990358606210464
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.369]
 [35.705]
 [32.003]
 [52.647]
 [34.204]] [[0.16 ]
 [0.195]
 [0.156]
 [0.374]
 [0.18 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.54742021328442
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.896]
 [56.693]
 [56.693]
 [56.693]
 [56.693]] [[1.512]
 [1.357]
 [1.357]
 [1.357]
 [1.357]]
printing an ep nov before normalisation:  58.09341195161935
printing an ep nov before normalisation:  57.674486561063816
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.80990120021836
printing an ep nov before normalisation:  68.40074403568205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[117.118]
 [117.118]
 [117.118]
 [117.118]
 [117.118]] [[1.563]
 [1.563]
 [1.563]
 [1.563]
 [1.563]]
printing an ep nov before normalisation:  59.05390739440918
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 47.89719865889982
printing an ep nov before normalisation:  61.503202505029165
printing an ep nov before normalisation:  49.56933973404337
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.026 0.026 0.385 0.538 0.026]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  84.2179655846259
actions average: 
K:  2  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0016,     0.9966,     0.0001,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0002,     0.9281,     0.0408,     0.0309],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0008,     0.0145,     0.7881,     0.1962],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0055, 0.0029, 0.0576, 0.2962, 0.6378], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  53.13778877258301
printing an ep nov before normalisation:  58.55328559875488
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.743574332937822
printing an ep nov before normalisation:  56.85650821232454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8647137
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.50972235269917
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.27550380464139
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.786]
 [62.786]
 [66.495]
 [62.786]
 [62.786]] [[1.128]
 [1.128]
 [1.232]
 [1.128]
 [1.128]]
printing an ep nov before normalisation:  65.36166003884993
printing an ep nov before normalisation:  65.46923928183848
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.180284469361126
printing an ep nov before normalisation:  59.51415388816971
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.56017485659059
printing an ep nov before normalisation:  44.74149703979492
siam score:  -0.8679836
siam score:  -0.8686588
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  93.81363198615831
printing an ep nov before normalisation:  42.717695236206055
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.74697590544457
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  89.31994706137795
printing an ep nov before normalisation:  57.69683410352612
printing an ep nov before normalisation:  79.41865897426126
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.7386061183755
siam score:  -0.8659346
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.05753402931033
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 96.337]
 [ 96.337]
 [104.19 ]
 [ 96.337]
 [ 96.337]] [[1.134]
 [1.134]
 [1.266]
 [1.134]
 [1.134]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86389464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86159116
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.586]
 [59.586]
 [59.586]
 [59.586]
 [59.586]] [[1.85]
 [1.85]
 [1.85]
 [1.85]
 [1.85]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.621732642610944
printing an ep nov before normalisation:  46.072454366984715
siam score:  -0.8583892
printing an ep nov before normalisation:  31.14253520965576
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.139]
 [61.296]
 [61.296]
 [61.954]
 [61.296]] [[1.833]
 [1.755]
 [1.755]
 [1.783]
 [1.755]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.064]
 [48.95 ]
 [54.919]
 [55.99 ]
 [48.95 ]] [[1.325]
 [1.205]
 [1.435]
 [1.477]
 [1.205]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.60175201533528
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 47.48262562167719
printing an ep nov before normalisation:  121.21977515752273
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.639016299282936
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  3.4327651324669465e-06
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  91.88726385390432
printing an ep nov before normalisation:  68.80215474400967
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.36373043060303
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.257]
 [47.55 ]
 [49.232]
 [46.053]
 [48.208]] [[0.752]
 [1.033]
 [1.109]
 [0.966]
 [1.063]]
siam score:  -0.85943574
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.278879308521596
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.633199547907495
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.19968716115781
printing an ep nov before normalisation:  32.13383872158781
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.432597569056924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9973,     0.0002,     0.0000,     0.0001,     0.0025],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9967,     0.0000,     0.0000,     0.0030],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0002,     0.8662,     0.0572,     0.0763],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0010,     0.0001,     0.0042,     0.8121,     0.1826],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0057, 0.0634, 0.0973, 0.2428, 0.5907], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  68.13438828690145
printing an ep nov before normalisation:  56.898179054260254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.50629838974639
printing an ep nov before normalisation:  48.56667689509458
printing an ep nov before normalisation:  61.1690144576957
siam score:  -0.86414033
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  33.71293272649819
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.095]
 [56.095]
 [56.095]
 [56.095]
 [56.095]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
printing an ep nov before normalisation:  45.2790800723998
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.185484668175874
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.698]
 [34.698]
 [34.698]
 [30.292]
 [34.698]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8726581
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.611109433470105
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.87013793
printing an ep nov before normalisation:  58.002173199206155
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.37613146777884
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.72834085542728
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.32974114065482
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.611]
 [47.642]
 [40.497]
 [36.475]
 [34.405]] [[0.707]
 [0.955]
 [0.735]
 [0.611]
 [0.547]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.893]
 [37.728]
 [36.597]
 [35.888]
 [30.073]] [[0.908]
 [1.185]
 [1.15 ]
 [1.128]
 [0.945]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.69085645675659
printing an ep nov before normalisation:  26.437187194824222
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9968,     0.0002,     0.0000,     0.0011,     0.0020],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9955,     0.0001,     0.0000,     0.0043],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9662,     0.0293,     0.0045],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0002,     0.0165,     0.6206,     0.3625],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0028,     0.0003,     0.1241,     0.1649,     0.7079],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  61.26190004748821
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.604735310225955
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.91702938079834
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.19382780156139
actions average: 
K:  4  action  0 :  tensor([    0.9844,     0.0039,     0.0000,     0.0062,     0.0054],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9614,     0.0004,     0.0000,     0.0379],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0010,     0.9849,     0.0003,     0.0137],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0014, 0.0042, 0.1069, 0.6486, 0.2390], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0003,     0.0069,     0.1347,     0.3194,     0.5387],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  54.16887760346847
printing an ep nov before normalisation:  54.780956136989595
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.08008303511447
siam score:  -0.8613944
printing an ep nov before normalisation:  52.32851459385973
siam score:  -0.86125535
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 43.879174437718824
printing an ep nov before normalisation:  44.63844768648854
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.797]
 [46.056]
 [43.869]
 [43.869]
 [43.869]] [[0.699]
 [0.884]
 [0.807]
 [0.807]
 [0.807]]
printing an ep nov before normalisation:  54.29953870492144
printing an ep nov before normalisation:  46.41037464141846
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  42.16251864985872
printing an ep nov before normalisation:  0.026099566142931963
printing an ep nov before normalisation:  56.56008780769464
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.25512218475342
printing an ep nov before normalisation:  115.27419316782674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.14 ]
 [38.148]
 [39.764]
 [37.335]
 [37.335]] [[0.343]
 [0.343]
 [0.369]
 [0.33 ]
 [0.33 ]]
printing an ep nov before normalisation:  62.934013024346655
printing an ep nov before normalisation:  45.09000197560317
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.5709922397723
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8652414
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.023207840004865
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.352]
 [45.838]
 [44.451]
 [40.84 ]
 [40.798]] [[0.946]
 [1.29 ]
 [1.216]
 [1.025]
 [1.022]]
printing an ep nov before normalisation:  44.51024391498963
using explorer policy with actor:  1
siam score:  -0.86470884
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.339744308014325
printing an ep nov before normalisation:  47.06731096269986
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.825]
 [27.825]
 [27.825]
 [27.825]
 [27.825]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  33.54886174201965
printing an ep nov before normalisation:  36.37875056713021
printing an ep nov before normalisation:  38.91586389901262
printing an ep nov before normalisation:  37.352585792541504
printing an ep nov before normalisation:  27.550559043884277
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.575]
 [56.605]
 [69.838]
 [69.455]
 [68.505]] [[0.337]
 [0.476]
 [0.705]
 [0.698]
 [0.682]]
printing an ep nov before normalisation:  50.9246293075814
printing an ep nov before normalisation:  35.39416790008545
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.446]
 [24.082]
 [22.079]
 [24.994]
 [21.768]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.266242952788744
printing an ep nov before normalisation:  30.57111280908215
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.312148180374265
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.189605349494975
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.56024897248435
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.69832654448761
printing an ep nov before normalisation:  42.57761745372063
printing an ep nov before normalisation:  38.68295970443213
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.79531503271703
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.226]
 [50.107]
 [53.588]
 [49.233]
 [49.233]] [[1.412]
 [1.645]
 [1.854]
 [1.592]
 [1.592]]
siam score:  -0.86903596
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.08922664481602
siam score:  -0.8670352
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0072, 0.9517, 0.0065, 0.0011, 0.0334], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9531,     0.0233,     0.0234],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0134, 0.0009, 0.0263, 0.7015, 0.2579], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1045, 0.0017, 0.0527, 0.2240, 0.6172], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.14752655521788
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.087669273553665
printing an ep nov before normalisation:  50.880141258239746
printing an ep nov before normalisation:  73.57405766329838
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.775]
 [36.205]
 [43.572]
 [36.205]
 [42.693]] [[0.589]
 [0.742]
 [1.208]
 [0.742]
 [1.152]]
printing an ep nov before normalisation:  51.12093250926291
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.083]
 [45.368]
 [40.798]
 [43.977]
 [43.977]] [[1.726]
 [1.606]
 [1.289]
 [1.51 ]
 [1.51 ]]
printing an ep nov before normalisation:  59.77371614815702
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.506]
 [50.082]
 [41.963]
 [50.082]
 [50.082]] [[1.669]
 [1.477]
 [1.021]
 [1.477]
 [1.477]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.647611417825786
siam score:  -0.86603594
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.578033956966785
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.031178226620334
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.86852809766696
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  64.8962407075285
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.81198751525591
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.12285634032579
printing an ep nov before normalisation:  41.902332406001335
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.11398548368645
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.9991,     0.0000,     0.0000,     0.0003,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9993,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0002,     0.8815,     0.0331,     0.0852],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0003,     0.0139,     0.7934,     0.1923],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0025, 0.0567, 0.0807, 0.2748, 0.5854], grad_fn=<DivBackward0>)
siam score:  -0.8625029
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.4380499045205
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.504]
 [33.207]
 [27.166]
 [32.648]
 [36.609]] [[1.038]
 [0.971]
 [0.658]
 [0.942]
 [1.147]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.38720775682065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.823513968725955
printing an ep nov before normalisation:  43.28863177366964
printing an ep nov before normalisation:  42.18192134027306
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.15512445694791
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.23836149721671
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.86689428127677
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.103 0.179 0.128 0.513 0.077]
actions average: 
K:  0  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9976,     0.0002,     0.0000,     0.0014],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0041,     0.9663,     0.0013,     0.0281],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0010,     0.0004,     0.0419,     0.7251,     0.2316],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0015,     0.0005,     0.0840,     0.3543,     0.5597],
       grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([    0.9744,     0.0235,     0.0000,     0.0001,     0.0020],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9502,     0.0207,     0.0001,     0.0290],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0002,     0.9332,     0.0136,     0.0530],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0002,     0.0002,     0.8553,     0.1441],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0013, 0.0026, 0.0782, 0.3680, 0.5499], grad_fn=<DivBackward0>)
siam score:  -0.869057
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.965]
 [37.785]
 [30.289]
 [31.62 ]
 [32.175]] [[0.375]
 [0.409]
 [0.268]
 [0.293]
 [0.303]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.48615298105068
printing an ep nov before normalisation:  43.78897696942227
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.56351670749551
printing an ep nov before normalisation:  52.05932376288122
printing an ep nov before normalisation:  61.78800017368841
printing an ep nov before normalisation:  0.004975148133894436
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.25482299674315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.22121576119986
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.40674505802175
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.552]
 [69.987]
 [77.621]
 [78.864]
 [78.62 ]] [[1.453]
 [1.531]
 [1.773]
 [1.812]
 [1.805]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.668]
 [43.668]
 [54.691]
 [43.668]
 [43.668]] [[0.904]
 [0.904]
 [1.309]
 [0.904]
 [0.904]]
line 256 mcts: sample exp_bonus 71.90492175623821
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.76169984193799
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.36901722699409
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[78.034]
 [86.76 ]
 [93.51 ]
 [92.07 ]
 [92.319]] [[0.44 ]
 [0.525]
 [0.591]
 [0.577]
 [0.579]]
printing an ep nov before normalisation:  36.19819401668984
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  112.73833999001319
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9963,     0.0000,     0.0000,     0.0011,     0.0026],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0040,     0.9300,     0.0227,     0.0000,     0.0433],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0004,     0.9104,     0.0349,     0.0542],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0013, 0.0007, 0.0563, 0.6481, 0.2936], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0133, 0.0108, 0.0825, 0.3115, 0.5820], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.63008975982666
actions average: 
K:  1  action  0 :  tensor([    0.9987,     0.0003,     0.0000,     0.0002,     0.0009],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0010,     0.9927,     0.0006,     0.0000,     0.0057],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0003,     0.9265,     0.0162,     0.0570],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0017,     0.0002,     0.0254,     0.7721,     0.2007],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0051, 0.0036, 0.0718, 0.2956, 0.6239], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.474617156011394
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.15455612941722
printing an ep nov before normalisation:  54.89301257587022
siam score:  -0.87344205
printing an ep nov before normalisation:  0.12979628375489938
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.08661534234173
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.45661906218252
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.337]
 [22.387]
 [22.387]
 [22.387]
 [22.387]] [[0.601]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]]
printing an ep nov before normalisation:  56.53763997865767
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9995,     0.0002,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9989,     0.0000,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0075,     0.9455,     0.0081,     0.0388],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0006,     0.0005,     0.0667,     0.6859,     0.2462],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0021, 0.0238, 0.1371, 0.2014, 0.6356], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  62.064481789087466
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.662]
 [28.674]
 [28.674]
 [28.674]
 [28.674]] [[0.678]
 [0.309]
 [0.309]
 [0.309]
 [0.309]]
printing an ep nov before normalisation:  38.21202977934959
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.003857429823028724
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.9956,     0.0028,     0.0001,     0.0001,     0.0013],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9795,     0.0045,     0.0000,     0.0158],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9509,     0.0143,     0.0348],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0003,     0.0297,     0.6779,     0.2919],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0104, 0.0017, 0.0814, 0.3178, 0.5887], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.206]
 [47.906]
 [50.78 ]
 [45.208]
 [46.187]] [[1.187]
 [1.223]
 [1.37 ]
 [1.085]
 [1.135]]
printing an ep nov before normalisation:  28.81394985434629
siam score:  -0.86994845
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.833]
 [57.833]
 [57.833]
 [57.833]
 [57.833]] [[1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.40452238689372
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.861371113310135
printing an ep nov before normalisation:  35.247781336014995
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.6225528717041
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.034]
 [35.034]
 [35.034]
 [35.034]
 [35.034]] [[0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]]
printing an ep nov before normalisation:  90.32554627616264
printing an ep nov before normalisation:  45.42243431565302
printing an ep nov before normalisation:  52.06335451593359
printing an ep nov before normalisation:  30.57360354705678
siam score:  -0.86451614
printing an ep nov before normalisation:  107.98695496146777
line 256 mcts: sample exp_bonus 50.60045284568552
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.53940792513157
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.134596344949756
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.864026
printing an ep nov before normalisation:  48.837575912475586
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.40723671340263
printing an ep nov before normalisation:  48.16548902921027
printing an ep nov before normalisation:  43.522462791545685
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 69.52568047993576
actions average: 
K:  0  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0001,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0056,     0.9477,     0.0039,     0.0001,     0.0426],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0054, 0.0087, 0.9557, 0.0092, 0.0211], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0034, 0.0008, 0.0281, 0.7438, 0.2240], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0082, 0.0282, 0.0872, 0.1994, 0.6771], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  79.57171401852221
printing an ep nov before normalisation:  72.56124812276069
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.43504720506053
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.61663413640859
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.42]
 [42.42]
 [42.42]
 [42.42]
 [42.42]] [[0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9995,     0.0003,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0037,     0.9729,     0.0003,     0.0000,     0.0230],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0002,     0.9182,     0.0291,     0.0524],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0047,     0.0004,     0.0288,     0.7849,     0.1812],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0138, 0.0112, 0.0369, 0.3405, 0.5976], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.415853106238636
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.536]
 [29.536]
 [45.237]
 [29.536]
 [29.536]] [[0.059]
 [0.059]
 [0.199]
 [0.059]
 [0.059]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.67715789063364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.74788862160745
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.929]
 [39.929]
 [39.929]
 [39.929]
 [39.929]] [[1.968]
 [1.968]
 [1.968]
 [1.968]
 [1.968]]
printing an ep nov before normalisation:  32.205700407889516
printing an ep nov before normalisation:  58.49123851841327
actions average: 
K:  1  action  0 :  tensor([    0.9928,     0.0001,     0.0000,     0.0021,     0.0050],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9622,     0.0089,     0.0000,     0.0287],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9516,     0.0181,     0.0301],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0008,     0.0133,     0.5539,     0.4318],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0004,     0.0010,     0.0386,     0.3122,     0.6477],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.819372346220604
printing an ep nov before normalisation:  49.98677909927888
printing an ep nov before normalisation:  35.59283032033294
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.44899241684803
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 54.003504037857056
using explorer policy with actor:  1
siam score:  -0.87544525
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.40976390615995
siam score:  -0.8733189
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.75063063566494
printing an ep nov before normalisation:  34.258026611185
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.31464161628465
printing an ep nov before normalisation:  40.68071767318312
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.16870576289452
printing an ep nov before normalisation:  50.563896561859956
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.9656, 0.0042, 0.0138, 0.0021, 0.0144], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9528,     0.0332,     0.0000,     0.0137],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9312,     0.0126,     0.0561],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0006,     0.0424,     0.8213,     0.1356],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0003,     0.0037,     0.1089,     0.2403,     0.6468],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.257]
 [31.257]
 [67.475]
 [31.257]
 [31.257]] [[0.428]
 [0.428]
 [1.315]
 [0.428]
 [0.428]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8731926
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  90.43179097280026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8699733
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.854491975279515
printing an ep nov before normalisation:  44.39018772580857
printing an ep nov before normalisation:  41.41549110412598
printing an ep nov before normalisation:  45.113621050401406
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.938545200107036
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 56.499274980129194
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.73022270202637
printing an ep nov before normalisation:  17.412404679219
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9996,     0.0000,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9975,     0.0005,     0.0000,     0.0019],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0090,     0.9142,     0.0199,     0.0568],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0003,     0.0765,     0.7667,     0.1563],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0004,     0.0039,     0.0026,     0.2174,     0.7758],
       grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.707]
 [48.707]
 [48.707]
 [48.707]
 [48.707]] [[0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[74.998]
 [74.998]
 [74.998]
 [74.998]
 [74.998]] [[1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.951]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  43.05407480788442
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.119]
 [37.854]
 [39.218]
 [27.315]
 [33.857]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  51.16039764128204
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.342]
 [34.334]
 [34.824]
 [34.301]
 [35.814]] [[0.909]
 [1.021]
 [1.049]
 [1.019]
 [1.105]]
printing an ep nov before normalisation:  29.395644208167187
siam score:  -0.8677881
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8676043
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.391]
 [39.391]
 [39.391]
 [39.391]
 [39.391]] [[52.509]
 [52.509]
 [52.509]
 [52.509]
 [52.509]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.10140068006049
printing an ep nov before normalisation:  46.136458458209155
printing an ep nov before normalisation:  50.27197291206602
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.719]
 [37.525]
 [37.525]
 [37.525]
 [37.525]] [[1.294]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9829,     0.0002,     0.0000,     0.0077,     0.0093],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0500, 0.7859, 0.1171, 0.0308, 0.0161], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0123,     0.8644,     0.0376,     0.0856],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0029,     0.0006,     0.1257,     0.7180,     0.1528],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0093, 0.0208, 0.1204, 0.3181, 0.5314], grad_fn=<DivBackward0>)
siam score:  -0.8477159
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.715]
 [60.664]
 [60.664]
 [57.915]
 [60.664]] [[1.603]
 [1.297]
 [1.297]
 [1.204]
 [1.297]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.00165825816905
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  106.39541589958222
printing an ep nov before normalisation:  59.75480071481139
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 35.84852529067641
printing an ep nov before normalisation:  37.904223068225384
printing an ep nov before normalisation:  43.14600181888494
printing an ep nov before normalisation:  47.19088077545166
printing an ep nov before normalisation:  38.06498765275502
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.098]
 [49.743]
 [50.084]
 [51.319]
 [51.195]] [[0.519]
 [0.576]
 [0.583]
 [0.61 ]
 [0.607]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9980,     0.0015,     0.0000,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0255, 0.9212, 0.0147, 0.0019, 0.0366], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0016, 0.0241, 0.8372, 0.0414, 0.0956], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0478, 0.0017, 0.0175, 0.5988, 0.3342], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0565, 0.0375, 0.0612, 0.2496, 0.5952], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  67.05614989487302
printing an ep nov before normalisation:  82.97456785820818
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  60.51982243855794
printing an ep nov before normalisation:  55.090171043763625
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.093523962916024
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.49573136287471
siam score:  -0.8585173
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.87129555611344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.35126495125051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.8165474893919
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.918839181083136
printing an ep nov before normalisation:  82.69839021489497
printing an ep nov before normalisation:  30.892012072320203
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.61923313140869
printing an ep nov before normalisation:  35.771366219682236
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[70.603]
 [70.603]
 [70.603]
 [70.603]
 [70.603]] [[0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9963,     0.0004,     0.0001,     0.0003,     0.0030],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0034,     0.9545,     0.0026,     0.0003,     0.0392],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0125,     0.9405,     0.0010,     0.0457],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0011, 0.0009, 0.0891, 0.6505, 0.2584], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0044, 0.0251, 0.1000, 0.2444, 0.6261], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  60.76343802514522
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.463]
 [48.944]
 [81.813]
 [30.995]
 [31.584]] [[0.147]
 [0.338]
 [0.644]
 [0.171]
 [0.176]]
siam score:  -0.86816096
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.251]
 [39.328]
 [35.251]
 [35.251]
 [35.251]] [[0.514]
 [0.617]
 [0.514]
 [0.514]
 [0.514]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.596195652091392
line 256 mcts: sample exp_bonus 14.979476928710938
printing an ep nov before normalisation:  52.695185504957955
siam score:  -0.867574
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.97175543710549
printing an ep nov before normalisation:  32.94477462768555
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9986,     0.0003,     0.0000,     0.0002,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9950,     0.0004,     0.0000,     0.0044],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0038,     0.9144,     0.0276,     0.0542],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0007,     0.0006,     0.0358,     0.7410,     0.2218],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0014, 0.0011, 0.1295, 0.3105, 0.5575], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.539166745968465
printing an ep nov before normalisation:  69.61553690631112
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.68284158534136
printing an ep nov before normalisation:  71.27727820814852
printing an ep nov before normalisation:  54.54588110254884
printing an ep nov before normalisation:  62.473170035436084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.36141846478516
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8757747
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.54010034191639
printing an ep nov before normalisation:  46.63660311992624
printing an ep nov before normalisation:  63.02813682441381
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.020302357390236
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  86.72137085318936
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.092]
 [58.869]
 [50.83 ]
 [47.47 ]
 [52.719]] [[0.873]
 [1.037]
 [0.809]
 [0.713]
 [0.862]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.072]
 [53.261]
 [51.997]
 [53.072]
 [53.072]] [[1.657]
 [1.667]
 [1.599]
 [1.657]
 [1.657]]
printing an ep nov before normalisation:  54.98546720693918
printing an ep nov before normalisation:  41.860485351028366
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.25812489989571
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
line 256 mcts: sample exp_bonus 45.4702537207523
printing an ep nov before normalisation:  28.985543576715322
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.277]
 [42.277]
 [42.277]
 [42.277]
 [42.277]] [[56.356]
 [56.356]
 [56.356]
 [56.356]
 [56.356]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87049216
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  47.77752004336546
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.300021523312516
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.55664253234863
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.719366678368207
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.48928352330486
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.149]
 [40.47 ]
 [38.13 ]
 [34.517]
 [39.149]] [[1.007]
 [1.187]
 [1.06 ]
 [0.864]
 [1.115]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8760717
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.70515755795264
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.648621559143066
printing an ep nov before normalisation:  61.57398521206722
printing an ep nov before normalisation:  49.57106726540235
printing an ep nov before normalisation:  37.80356407165527
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86868507
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 38.85130261498578
printing an ep nov before normalisation:  47.540586449558184
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[81.571]
 [81.571]
 [81.571]
 [81.571]
 [81.571]] [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
printing an ep nov before normalisation:  118.62744029726238
printing an ep nov before normalisation:  45.92246055603027
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0005410366213709494
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  44.146232428272114
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.042054095805781344
printing an ep nov before normalisation:  68.74060630570169
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.69912004470825
printing an ep nov before normalisation:  34.47842935019391
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.6061763449846
printing an ep nov before normalisation:  30.7822585105896
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.15957063527058
printing an ep nov before normalisation:  50.39332772233617
line 256 mcts: sample exp_bonus 48.7078995086101
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.31315593922622
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.16122341156006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.399]
 [38.127]
 [44.282]
 [37.194]
 [37.147]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  58.161282539367676
printing an ep nov before normalisation:  33.700810193312485
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 40.60745179633404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.48446304787766
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.257]
 [49.335]
 [49.335]
 [45.232]
 [49.335]] [[0.53 ]
 [0.419]
 [0.419]
 [0.353]
 [0.419]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.59912998501319
printing an ep nov before normalisation:  54.879753316806735
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.04289674758911
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.030277095926862785
printing an ep nov before normalisation:  54.26515754407706
actions average: 
K:  3  action  0 :  tensor([    0.9943,     0.0000,     0.0022,     0.0013,     0.0023],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9990,     0.0000,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9707,     0.0136,     0.0156],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0043, 0.0009, 0.0444, 0.7375, 0.2129], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0012, 0.0174, 0.1093, 0.2971, 0.5749], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  77.34587516883077
printing an ep nov before normalisation:  25.580390522586875
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  66.64210585646322
printing an ep nov before normalisation:  66.87933331287442
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.5436623094598
printing an ep nov before normalisation:  39.923356147790585
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.099]
 [26.099]
 [26.099]
 [38.409]
 [26.099]] [[0.906]
 [0.906]
 [0.906]
 [1.333]
 [0.906]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.68473314923506
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.06 ]
 [30.731]
 [31.06 ]
 [31.06 ]
 [31.06 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  97.38926563647829
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.635]
 [37.584]
 [32.32 ]
 [38.372]
 [37.283]] [[0.259]
 [0.2  ]
 [0.148]
 [0.208]
 [0.197]]
line 256 mcts: sample exp_bonus 51.210511424091756
printing an ep nov before normalisation:  65.04918520467453
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.172]
 [49.281]
 [55.631]
 [55.631]
 [58.659]] [[1.158]
 [1.521]
 [1.845]
 [1.845]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.781695766576604
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.596]
 [50.748]
 [42.724]
 [43.596]
 [41.693]] [[1.249]
 [1.633]
 [1.202]
 [1.249]
 [1.147]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.674]
 [54.954]
 [49.674]
 [49.674]
 [49.674]] [[1.135]
 [1.374]
 [1.135]
 [1.135]
 [1.135]]
siam score:  -0.8643433
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8651862
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.7408146732878151
printing an ep nov before normalisation:  35.82427347638569
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.076]
 [40.076]
 [89.09 ]
 [37.364]
 [39.518]] [[0.201]
 [0.201]
 [0.668]
 [0.175]
 [0.195]]
printing an ep nov before normalisation:  47.96649932861328
printing an ep nov before normalisation:  56.0980371311705
printing an ep nov before normalisation:  40.29168119006606
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.392]
 [46.457]
 [64.871]
 [41.159]
 [64.392]] [[1.314]
 [0.612]
 [1.333]
 [0.405]
 [1.314]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.051]
 [33.051]
 [33.051]
 [29.25 ]
 [33.051]] [[1.663]
 [1.663]
 [1.663]
 [1.333]
 [1.663]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.9978,     0.0012,     0.0000,     0.0001,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9989,     0.0002,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9766,     0.0102,     0.0132],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0004,     0.0195,     0.7060,     0.2738],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0023, 0.0299, 0.1355, 0.2658, 0.5665], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.85443664969008
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.14854501434058
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.471]
 [45.135]
 [47.699]
 [44.865]
 [45.104]] [[1.384]
 [1.564]
 [1.738]
 [1.546]
 [1.562]]
printing an ep nov before normalisation:  48.03161467792984
siam score:  -0.87123644
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.853397386536265
printing an ep nov before normalisation:  47.858755192586095
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.263457260469703
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  60.693705340003504
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.64896869659424
actions average: 
K:  0  action  0 :  tensor([    0.9843,     0.0047,     0.0002,     0.0005,     0.0103],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9969,     0.0002,     0.0002,     0.0022],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0121,     0.8935,     0.0314,     0.0629],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0003,     0.0179,     0.7382,     0.2434],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0004,     0.0342,     0.0835,     0.3182,     0.5637],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.21385296307495
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 1.5982345337906107e-09
0.0 0.0
0.0 1.0653743665021295e-09
0.0 1.1035000582424557e-09
0.0 7.350439627753234e-10
0.0 1.3282409780229812e-09
0.0 1.3861214520143209e-09
0.0 1.0973764217089642e-09
0.0 9.749382882318179e-10
0.0 1.513679914329394e-09
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.095722885486495
printing an ep nov before normalisation:  38.14486026763916
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  56.21859090731548
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 49.15309696749193
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.129]
 [30.129]
 [41.283]
 [30.129]
 [31.69 ]] [[0.57 ]
 [0.57 ]
 [0.946]
 [0.57 ]
 [0.623]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.385504722595215
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.64269004587868
printing an ep nov before normalisation:  97.37072194390576
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.87675583
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  4  action  0 :  tensor([    0.9960,     0.0006,     0.0000,     0.0011,     0.0024],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9991,     0.0001,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0002,     0.9225,     0.0381,     0.0392],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0155,     0.0003,     0.0321,     0.8102,     0.1419],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0005,     0.0036,     0.0533,     0.3143,     0.6283],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.423734699052034
siam score:  -0.86855125
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.305]
 [45.324]
 [63.77 ]
 [35.294]
 [35.294]] [[0.456]
 [0.426]
 [0.702]
 [0.277]
 [0.277]]
printing an ep nov before normalisation:  40.241275502999365
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.90835547200501
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  117.00251617918146
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.063]
 [39.377]
 [31.612]
 [33.136]
 [37.859]] [[1.236]
 [1.254]
 [0.828]
 [0.912]
 [1.17 ]]
printing an ep nov before normalisation:  37.65506364560747
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9989,     0.0000,     0.0000,     0.0001,     0.0010],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9994,     0.0001,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0010,     0.9318,     0.0286,     0.0386],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0003,     0.0005,     0.0580,     0.7248,     0.2165],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0018, 0.0107, 0.0798, 0.2206, 0.6870], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.558129447047385
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  96.50043437837479
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8696753
printing an ep nov before normalisation:  41.766164024008866
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.6]
 [39.6]
 [39.6]
 [39.6]
 [39.6]] [[1.087]
 [1.087]
 [1.087]
 [1.087]
 [1.087]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.019821832596904
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.57 ]
 [55.57 ]
 [55.57 ]
 [43.327]
 [55.57 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.87011826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.24113506043819
printing an ep nov before normalisation:  70.72199263527325
printing an ep nov before normalisation:  0.012958595814325236
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.076851320728295
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.081]
 [36.028]
 [31.081]
 [31.345]
 [32.356]] [[0.721]
 [0.938]
 [0.721]
 [0.732]
 [0.777]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.90055188644033
printing an ep nov before normalisation:  40.826736336136776
printing an ep nov before normalisation:  41.27016308261492
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  59.890293988305714
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.27691472749231
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.812847985161675
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  83.76910604587344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.033397126011906
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[105.115]
 [105.115]
 [110.569]
 [105.115]
 [105.115]] [[0.846]
 [0.846]
 [0.903]
 [0.846]
 [0.846]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.047]
 [33.675]
 [39.266]
 [38.452]
 [43.575]] [[0.599]
 [0.656]
 [0.85 ]
 [0.822]
 [1.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.50152693783752
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.43712958717962
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.808]
 [39.808]
 [39.808]
 [39.808]
 [39.808]] [[1.666]
 [1.666]
 [1.666]
 [1.666]
 [1.666]]
printing an ep nov before normalisation:  42.87191004506066
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.086508340455254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.67232189819376
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.495]
 [72.148]
 [76.454]
 [78.897]
 [78.997]] [[0.646]
 [0.797]
 [0.844]
 [0.871]
 [0.872]]
printing an ep nov before normalisation:  68.10137367193205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.9966684017526
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.037]
 [82.153]
 [84.698]
 [84.146]
 [75.52 ]] [[0.754]
 [1.185]
 [1.242]
 [1.23 ]
 [1.035]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.180680450296926
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.999575926462217
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.00534156512003392
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.305568810931426
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.81011887796203
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.9990,     0.0000,     0.0000,     0.0003,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0007,     0.9578,     0.0054,     0.0001,     0.0359],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0002,     0.9015,     0.0221,     0.0761],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0003,     0.0448,     0.7060,     0.2486],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0012, 0.0106, 0.0504, 0.3211, 0.6167], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.74502604732126
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.953]
 [53.953]
 [53.953]
 [53.953]
 [53.953]] [[1.764]
 [1.764]
 [1.764]
 [1.764]
 [1.764]]
printing an ep nov before normalisation:  62.572393542280864
printing an ep nov before normalisation:  56.26346409419571
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  67.83538049401017
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.567]
 [49.567]
 [49.567]
 [49.567]
 [49.567]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.015740609749173018
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.62459373474121
printing an ep nov before normalisation:  41.31198460469505
printing an ep nov before normalisation:  31.48289680480957
printing an ep nov before normalisation:  57.28146648472814
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.82314580781982
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.12641143798828
printing an ep nov before normalisation:  36.15401374915672
printing an ep nov before normalisation:  33.60192683059928
printing an ep nov before normalisation:  54.084648389444
siam score:  -0.8684352
printing an ep nov before normalisation:  29.41450772452639
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.421]
 [31.421]
 [31.421]
 [31.421]
 [31.421]] [[0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8710276
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.784]
 [45.784]
 [45.784]
 [45.784]
 [45.784]] [[0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.717]
 [36.845]
 [47.494]
 [41.105]
 [41.344]] [[0.484]
 [0.541]
 [0.828]
 [0.656]
 [0.662]]
printing an ep nov before normalisation:  35.15307426452637
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.62003976212227
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.930124673153166
actions average: 
K:  1  action  0 :  tensor([    0.9917,     0.0006,     0.0000,     0.0004,     0.0073],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9989,     0.0000,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0036,     0.8494,     0.0474,     0.0995],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0044,     0.0004,     0.0577,     0.6520,     0.2854],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0020, 0.0271, 0.0494, 0.2660, 0.6556], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.189759223678884
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.704411370413645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.36677312908633
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.543887831289645
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 59.1322205069525
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.107]
 [58.44 ]
 [62.196]
 [61.272]
 [60.749]] [[1.062]
 [1.108]
 [1.237]
 [1.205]
 [1.187]]
printing an ep nov before normalisation:  36.92853573695578
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.79500476416693
printing an ep nov before normalisation:  39.27423376119511
printing an ep nov before normalisation:  36.75630969806013
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8688159
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9941,     0.0011,     0.0000,     0.0044],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0045,     0.9267,     0.0203,     0.0482],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0003,     0.0158,     0.7632,     0.2203],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0051, 0.0016, 0.0961, 0.2831, 0.6142], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.11924795782552641
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.454]
 [53.568]
 [45.733]
 [48.454]
 [45.947]] [[1.317]
 [1.456]
 [1.243]
 [1.317]
 [1.249]]
printing an ep nov before normalisation:  61.42178430172624
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.26244953583454
printing an ep nov before normalisation:  30.567207293657574
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.821767585166484
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.4460701674216
printing an ep nov before normalisation:  55.95174586781157
printing an ep nov before normalisation:  69.20846799076052
actions average: 
K:  3  action  0 :  tensor([    0.9199,     0.0089,     0.0000,     0.0197,     0.0515],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0007,     0.9597,     0.0110,     0.0002,     0.0284],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0069,     0.8932,     0.0385,     0.0614],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0000,     0.0001,     0.0014,     0.8236,     0.1748],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0006, 0.0620, 0.1444, 0.2333, 0.5597], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.678488039265716
printing an ep nov before normalisation:  38.78962394698427
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.832]
 [34.86 ]
 [26.832]
 [26.832]
 [26.832]] [[0.384]
 [0.602]
 [0.384]
 [0.384]
 [0.384]]
printing an ep nov before normalisation:  54.47528564282837
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  50.676155888625004
printing an ep nov before normalisation:  32.941511116021644
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.251572608947754
printing an ep nov before normalisation:  96.38045293346937
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.02492470872327
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.004]
 [56.547]
 [56.223]
 [51.65 ]
 [59.589]] [[0.509]
 [0.565]
 [0.559]
 [0.488]
 [0.612]]
actions average: 
K:  0  action  0 :  tensor([    0.9941,     0.0003,     0.0008,     0.0021,     0.0028],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9961,     0.0001,     0.0000,     0.0034],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0001,     0.9225,     0.0372,     0.0401],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0003,     0.0004,     0.0009,     0.8048,     0.1936],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0008, 0.0009, 0.0529, 0.2749, 0.6705], grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([    0.9960,     0.0001,     0.0002,     0.0005,     0.0033],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9993,     0.0001,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9375,     0.0312,     0.0313],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0003,     0.0177,     0.8315,     0.1504],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0004,     0.0010,     0.0075,     0.2507,     0.7404],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  0.4457264801946792
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.6279821395874
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.696]
 [29.696]
 [29.696]
 [43.006]
 [29.696]] [[0.384]
 [0.384]
 [0.384]
 [0.713]
 [0.384]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.37815856108939
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.758]
 [43.565]
 [36.349]
 [43.471]
 [42.992]] [[0.545]
 [0.672]
 [0.481]
 [0.67 ]
 [0.657]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  47.401239718821
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.14274513362005
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  44.01274074896466
siam score:  -0.8616523
printing an ep nov before normalisation:  46.28359337121495
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.135]
 [24.159]
 [66.067]
 [26.207]
 [26.728]] [[0.089]
 [0.132]
 [1.033]
 [0.176]
 [0.188]]
printing an ep nov before normalisation:  38.51762771606445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.981898712488636
siam score:  -0.85829985
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.295]
 [30.942]
 [26.558]
 [40.841]
 [33.236]] [[0.424]
 [0.648]
 [0.474]
 [1.04 ]
 [0.739]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.855436196803495
printing an ep nov before normalisation:  1.1571659978699245
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.861]
 [34.557]
 [31.308]
 [34.242]
 [30.226]] [[1.111]
 [0.47 ]
 [0.385]
 [0.462]
 [0.356]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 2.9469568273135143e-10
0.0 1.1232548404306386e-09
0.0 7.032148916630972e-10
0.0 6.676839611002421e-10
0.0 6.2357302013111e-10
0.0 1.2392579660958274e-09
0.0 3.8288296790708035e-10
0.0 4.594457227660831e-10
0.0 1.2098852688950518e-09
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8625414
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.9991,     0.0001,     0.0000,     0.0001,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9900,     0.0019,     0.0011,     0.0068],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9826,     0.0004,     0.0169],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0005,     0.0009,     0.7655,     0.2329],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0122, 0.0488, 0.1225, 0.2629, 0.5535], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8620143
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.78228769692286
printing an ep nov before normalisation:  45.56117682294941
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.778]
 [38.305]
 [38.877]
 [22.41 ]
 [22.41 ]] [[0.226]
 [0.496]
 [0.506]
 [0.2  ]
 [0.2  ]]
printing an ep nov before normalisation:  0.005699685428908197
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.99836195612506
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.14760036751685
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8702824
printing an ep nov before normalisation:  39.68263650909402
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.9874,     0.0016,     0.0000,     0.0003,     0.0107],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0011,     0.9707,     0.0092,     0.0001,     0.0189],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0148,     0.0031,     0.9677,     0.0009,     0.0135],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0004,     0.0189,     0.7805,     0.2002],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0495, 0.0474, 0.1600, 0.1164, 0.6268], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 24.51341152191162
printing an ep nov before normalisation:  42.243937419867386
printing an ep nov before normalisation:  30.818806701790848
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8562462
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.740698011067515
deleting a thread, now have 1 threads
Frames:  119202 train batches done:  13968 episodes:  4718
printing an ep nov before normalisation:  32.404433514227485
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.733]
 [38.733]
 [37.922]
 [38.733]
 [38.733]] [[1.595]
 [1.595]
 [1.562]
 [1.595]
 [1.595]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.603]
 [44.585]
 [44.096]
 [33.672]
 [40.174]] [[0.45 ]
 [0.312]
 [0.306]
 [0.175]
 [0.256]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.238]
 [26.238]
 [26.238]
 [29.827]
 [26.238]] [[0.589]
 [0.589]
 [0.589]
 [0.737]
 [0.589]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.62449060177812
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.162]
 [4.348]
 [4.891]
 [7.24 ]
 [6.645]] [[0.17 ]
 [0.119]
 [0.134]
 [0.2  ]
 [0.184]]
siam score:  -0.871886
UNIT TEST: sample policy line 217 mcts : [0.128 0.256 0.41  0.051 0.154]
printing an ep nov before normalisation:  36.601242227385356
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.512551032701246
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9997,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0002,     0.8950,     0.0369,     0.0678],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0004,     0.0008,     0.0554,     0.6603,     0.2832],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0167, 0.0212, 0.1036, 0.2943, 0.5642], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.412087800399455
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.59822197543325
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.66197714956857
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  30.687368375269156
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.323]
 [28.323]
 [28.323]
 [29.15 ]
 [28.323]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  30.28420925140381
using explorer policy with actor:  1
siam score:  -0.8700524
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.101]
 [27.407]
 [30.8  ]
 [29.107]
 [30.212]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.410177266185201e-05
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.839260036609716
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.514]
 [57.615]
 [63.262]
 [62.18 ]
 [64.782]] [[1.356]
 [1.177]
 [1.348]
 [1.315]
 [1.394]]
printing an ep nov before normalisation:  72.24092593918223
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.73673152923584
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.681]
 [59.823]
 [56.422]
 [62.144]
 [62.881]] [[0.495]
 [0.957]
 [0.853]
 [1.028]
 [1.051]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  93.39250747119635
printing an ep nov before normalisation:  54.43120712293712
printing an ep nov before normalisation:  51.6503111359025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.87027013
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.31647777557373
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.203]
 [32.203]
 [38.212]
 [32.203]
 [32.203]] [[0.449]
 [0.449]
 [0.533]
 [0.449]
 [0.449]]
siam score:  -0.87394285
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.707]
 [31.847]
 [22.019]
 [23.005]
 [22.019]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87054473
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.95756244659424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86338013
printing an ep nov before normalisation:  122.71188293245118
printing an ep nov before normalisation:  70.54644356561752
printing an ep nov before normalisation:  70.21846995822769
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8580097
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.32723263938897
printing an ep nov before normalisation:  90.53044356358802
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.86494611714584
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.399]
 [42.01 ]
 [47.269]
 [44.473]
 [40.493]] [[1.351]
 [0.998]
 [1.219]
 [1.101]
 [0.934]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[17.097]
 [20.932]
 [32.429]
 [19.463]
 [20.376]] [[0.239]
 [0.357]
 [0.712]
 [0.312]
 [0.34 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.70723738780025
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.915]
 [35.7  ]
 [72.261]
 [31.756]
 [33.857]] [[0.042]
 [0.079]
 [0.219]
 [0.064]
 [0.072]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.503319336073886
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85663193
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85501724
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  31.857894481045648
siam score:  -0.85521317
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.96734897001291
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.5804642846768
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.411]
 [34.482]
 [28.411]
 [28.411]
 [28.411]] [[0.243]
 [0.296]
 [0.243]
 [0.243]
 [0.243]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9941,     0.0004,     0.0000,     0.0035,     0.0020],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9975,     0.0009,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0019,     0.8995,     0.0319,     0.0666],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0013, 0.0015, 0.0504, 0.6835, 0.2632], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0051, 0.0094, 0.1179, 0.3304, 0.5373], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [54.916]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.833]
 [ 0.7  ]
 [-0.833]
 [-0.833]
 [-0.833]]
siam score:  -0.8601253
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.773857089676575
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.852955256887235
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.86133814
printing an ep nov before normalisation:  46.22271818963645
printing an ep nov before normalisation:  36.769477720730556
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9986,     0.0000,     0.0000,     0.0006,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9976,     0.0000,     0.0002,     0.0017],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0145,     0.9035,     0.0173,     0.0645],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0006,     0.0690,     0.7426,     0.1876],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0186, 0.0068, 0.0584, 0.2495, 0.6667], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.520936419078595
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.329506700470144
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  58.95027654820135
printing an ep nov before normalisation:  57.89389817982603
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.03309951587927
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 98.509]
 [ 98.509]
 [100.521]
 [ 98.509]
 [ 98.509]] [[1.53 ]
 [1.53 ]
 [1.565]
 [1.53 ]
 [1.53 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.983]
 [34.933]
 [39.812]
 [35.647]
 [35.647]] [[0.654]
 [0.587]
 [0.746]
 [0.61 ]
 [0.61 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8640678
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86360687
siam score:  -0.86444354
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.16977571196979
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.49894408092328
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.8706834
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.9733,     0.0001,     0.0008,     0.0164,     0.0095],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9917,     0.0078,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0003,     0.8738,     0.0595,     0.0664],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0003,     0.0438,     0.7556,     0.2001],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0006,     0.0064,     0.1190,     0.2361,     0.6379],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  67.40460762556312
printing an ep nov before normalisation:  58.449264240812646
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.29743055647231
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.11021947008554
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.13856596710296
printing an ep nov before normalisation:  38.143910009482816
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.87378347
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.866]
 [39.538]
 [30.866]
 [30.866]
 [30.866]] [[0.802]
 [1.232]
 [0.802]
 [0.802]
 [0.802]]
printing an ep nov before normalisation:  59.56832998300303
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.056]
 [62.056]
 [97.935]
 [65.763]
 [62.056]] [[0.159]
 [0.159]
 [0.284]
 [0.172]
 [0.159]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  61.78671565008255
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.885]
 [37.325]
 [37.325]
 [37.325]
 [37.325]] [[1.157]
 [0.638]
 [0.638]
 [0.638]
 [0.638]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.9970,     0.0009,     0.0000,     0.0007,     0.0014],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9985,     0.0001,     0.0001,     0.0013],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0002,     0.9536,     0.0108,     0.0353],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0014,     0.0006,     0.0004,     0.6868,     0.3108],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0027, 0.0009, 0.1092, 0.2527, 0.6344], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8607384
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.314]
 [52.389]
 [52.389]
 [52.389]
 [52.389]] [[1.933]
 [1.634]
 [1.634]
 [1.634]
 [1.634]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.56643093973177
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.04981327056885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.64182448387146
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9994,     0.0003,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9982,     0.0002,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0053,     0.8831,     0.0354,     0.0761],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0003,     0.0280,     0.7896,     0.1818],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0135, 0.0530, 0.0691, 0.2372, 0.6272], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  52.43669154434585
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.161]
 [36.161]
 [36.161]
 [58.883]
 [36.161]] [[0.788]
 [0.788]
 [0.788]
 [1.283]
 [0.788]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.28646310001386
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 42.9927838661915
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  30.893434478552265
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.868130580325438
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.513]
 [30.513]
 [30.914]
 [30.513]
 [30.513]] [[0.309]
 [0.309]
 [0.317]
 [0.309]
 [0.309]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.53169759179822
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.724]
 [35.724]
 [39.553]
 [38.898]
 [35.724]] [[1.167]
 [1.167]
 [1.372]
 [1.337]
 [1.167]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.899939049673016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  96.66920017457203
printing an ep nov before normalisation:  32.61997065670962
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.73734059174295
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.652]
 [44.746]
 [41.822]
 [42.865]
 [44.652]] [[1.583]
 [1.589]
 [1.396]
 [1.465]
 [1.583]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.47273719910342
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.756]
 [43.933]
 [24.256]
 [26.512]
 [27.052]] [[0.359]
 [0.739]
 [0.277]
 [0.33 ]
 [0.343]]
printing an ep nov before normalisation:  43.57903003692627
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.301]
 [45.592]
 [45.592]
 [45.592]
 [45.592]] [[1.594]
 [1.611]
 [1.611]
 [1.611]
 [1.611]]
printing an ep nov before normalisation:  23.487610816955566
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.17613744735718
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[80.493]
 [84.766]
 [97.016]
 [97.036]
 [95.839]] [[1.476]
 [1.574]
 [1.854]
 [1.855]
 [1.827]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  53.800410658778794
printing an ep nov before normalisation:  57.272986500265745
printing an ep nov before normalisation:  39.783370082024945
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9912,     0.0001,     0.0001,     0.0012,     0.0074],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9835,     0.0007,     0.0001,     0.0155],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9232,     0.0186,     0.0581],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0033,     0.0004,     0.0180,     0.8355,     0.1428],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0045, 0.0995, 0.0945, 0.2824, 0.5191], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.451]
 [49.818]
 [38.56 ]
 [42.544]
 [44.043]] [[0.617]
 [0.919]
 [0.591]
 [0.707]
 [0.751]]
printing an ep nov before normalisation:  51.56902187470077
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.063]
 [39.415]
 [33.755]
 [36.599]
 [38.86 ]] [[0.579]
 [0.588]
 [0.429]
 [0.509]
 [0.573]]
printing an ep nov before normalisation:  32.67203895208903
printing an ep nov before normalisation:  49.75546755381363
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.729]
 [36.037]
 [43.281]
 [42.115]
 [40.388]] [[0.427]
 [0.373]
 [0.518]
 [0.494]
 [0.46 ]]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.9985,     0.0007,     0.0000,     0.0001,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9953,     0.0002,     0.0000,     0.0043],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0002,     0.9357,     0.0424,     0.0216],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0283,     0.0002,     0.0103,     0.7519,     0.2093],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0175, 0.0664, 0.1273, 0.2274, 0.5614], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.97724976578058
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.05785238270701
siam score:  -0.8647234
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.513720989227295
printing an ep nov before normalisation:  36.53181312759872
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.758]
 [45.409]
 [51.302]
 [47.547]
 [43.879]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.897143227305467
siam score:  -0.867469
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9228,     0.0021,     0.0000,     0.0057,     0.0694],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9732,     0.0104,     0.0000,     0.0161],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9421,     0.0172,     0.0407],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0055, 0.0008, 0.0382, 0.6265, 0.3291], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0001,     0.0005,     0.1066,     0.1731,     0.7198],
       grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.80783754598193
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.922]
 [51.922]
 [51.922]
 [51.922]
 [51.922]] [[0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.477]
 [47.594]
 [47.486]
 [41.548]
 [48.236]] [[1.612]
 [1.583]
 [1.579]
 [1.381]
 [1.604]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  41.90364947301404
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  91.04334169042848
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.608]
 [47.901]
 [44.827]
 [43.973]
 [43.054]] [[1.048]
 [1.242]
 [1.103]
 [1.064]
 [1.023]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.729400634765625
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.224]
 [32.224]
 [42.86 ]
 [32.224]
 [32.224]] [[0.749]
 [0.749]
 [1.376]
 [0.749]
 [0.749]]
siam score:  -0.85385805
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.24437568035299
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  71.58171111543481
printing an ep nov before normalisation:  34.17766594185036
printing an ep nov before normalisation:  47.35232362015092
printing an ep nov before normalisation:  71.59234006022689
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.807]
 [32.282]
 [32.282]
 [30.438]
 [32.773]] [[1.708]
 [1.563]
 [1.563]
 [1.387]
 [1.609]]
printing an ep nov before normalisation:  51.504031218668935
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  3.426338372267992
printing an ep nov before normalisation:  46.08654610644669
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.993]
 [54.143]
 [68.428]
 [72.112]
 [57.993]] [[1.122]
 [1.019]
 [1.401]
 [1.499]
 [1.122]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.849]
 [35.978]
 [39.099]
 [40.849]
 [40.849]] [[2.17 ]
 [1.696]
 [2.   ]
 [2.17 ]
 [2.17 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.7829371676094
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.841]
 [31.957]
 [33.306]
 [34.268]
 [31.841]] [[1.543]
 [1.553]
 [1.68 ]
 [1.771]
 [1.543]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.705635383950835
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.162]
 [45.664]
 [42.488]
 [46.738]
 [46.275]] [[1.671]
 [1.579]
 [1.383]
 [1.645]
 [1.616]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.895]
 [30.989]
 [30.989]
 [30.989]
 [30.989]] [[1.523]
 [0.502]
 [0.502]
 [0.502]
 [0.502]]
siam score:  -0.8736272
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.15456128149836
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.8926491363494
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.43294006604619
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.94950485229492
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.353058275247044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.09839033694202
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9857,     0.0001,     0.0000,     0.0140],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0014,     0.8459,     0.0678,     0.0847],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0073, 0.0009, 0.0136, 0.7164, 0.2618], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0142, 0.0018, 0.0786, 0.3375, 0.5679], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.919]
 [35.427]
 [32.147]
 [40.374]
 [33.714]] [[0.282]
 [0.219]
 [0.181]
 [0.276]
 [0.199]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.70125610679275
printing an ep nov before normalisation:  43.55867070969433
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.4548152638723
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  35.36507606506348
siam score:  -0.87961996
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.231822851455625
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[19.758]
 [24.201]
 [29.141]
 [19.682]
 [19.311]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  106.94701494255594
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.86743225664058
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.67181111949805
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.285710527460836
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.926]
 [33.029]
 [49.893]
 [46.715]
 [39.601]] [[1.524]
 [1.072]
 [1.62 ]
 [1.517]
 [1.286]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.822046756744385
printing an ep nov before normalisation:  45.72846578903409
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.78848254533198
printing an ep nov before normalisation:  69.45996026662455
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.9348965883255
printing an ep nov before normalisation:  69.84370680170912
siam score:  -0.8556379
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.27 ]
 [59.464]
 [62.76 ]
 [43.847]
 [37.472]] [[0.239]
 [0.745]
 [0.804]
 [0.465]
 [0.35 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.391]
 [24.511]
 [24.511]
 [24.511]
 [24.511]] [[1.264]
 [0.548]
 [0.548]
 [0.548]
 [0.548]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8570974
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  83.99426361264389
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.459198706010135
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86300296
printing an ep nov before normalisation:  38.69400716099176
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.447]
 [58.367]
 [61.111]
 [59.092]
 [56.904]] [[0.255]
 [0.209]
 [0.225]
 [0.213]
 [0.201]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.82337070864578
printing an ep nov before normalisation:  67.32704292167809
actions average: 
K:  4  action  0 :  tensor([    0.9974,     0.0001,     0.0000,     0.0015,     0.0010],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9948,     0.0017,     0.0000,     0.0035],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0003,     0.9048,     0.0412,     0.0536],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0005,     0.0506,     0.7893,     0.1594],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0010, 0.0008, 0.1949, 0.2828, 0.5205], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.41722848416929
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.135]
 [19.57 ]
 [19.57 ]
 [19.57 ]
 [19.57 ]] [[1.035]
 [0.331]
 [0.331]
 [0.331]
 [0.331]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.459]
 [55.459]
 [55.459]
 [55.459]
 [55.459]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  43.15556125412969
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  83.11106279467914
printing an ep nov before normalisation:  47.115083674026316
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.398478031158447
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86441785
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[79.633]
 [79.633]
 [79.633]
 [83.989]
 [79.633]] [[1.843]
 [1.843]
 [1.843]
 [1.974]
 [1.843]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.40866978186022
printing an ep nov before normalisation:  48.926963806152344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  18.3727387412291
printing an ep nov before normalisation:  35.971164523831725
printing an ep nov before normalisation:  28.722385761184857
printing an ep nov before normalisation:  34.65571459165042
printing an ep nov before normalisation:  0.04626121607429923
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.971]
 [48.372]
 [48.372]
 [48.372]
 [48.372]] [[2.   ]
 [2.688]
 [2.688]
 [2.688]
 [2.688]]
line 256 mcts: sample exp_bonus 47.38791356350967
line 256 mcts: sample exp_bonus 91.6946269960367
printing an ep nov before normalisation:  51.97739601135254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.98 ]
 [49.188]
 [49.972]
 [48.466]
 [48.466]] [[1.667]
 [1.308]
 [1.344]
 [1.275]
 [1.275]]
printing an ep nov before normalisation:  52.212281096857566
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.314875892828475
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.967]
 [61.965]
 [51.967]
 [51.967]
 [51.967]] [[1.262]
 [1.667]
 [1.262]
 [1.262]
 [1.262]]
printing an ep nov before normalisation:  0.0108985866910416
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.336]
 [43.816]
 [51.157]
 [58.872]
 [52.98 ]] [[0.601]
 [0.812]
 [1.018]
 [1.236]
 [1.07 ]]
printing an ep nov before normalisation:  22.305946350097656
printing an ep nov before normalisation:  53.60446235083361
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.60050928677447
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.909658363198176
printing an ep nov before normalisation:  61.071168380516205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.198]
 [34.631]
 [34.965]
 [34.863]
 [36.204]] [[0.369]
 [0.309]
 [0.315]
 [0.313]
 [0.336]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.569]
 [20.462]
 [20.891]
 [17.174]
 [20.393]] [[0.59 ]
 [0.409]
 [0.424]
 [0.293]
 [0.407]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.98103845459216
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  25.736271493801542
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.30157089224653
printing an ep nov before normalisation:  66.57492026832038
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  0.18105217876609458
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.003]
 [58.86 ]
 [57.142]
 [54.966]
 [58.623]] [[1.12 ]
 [1.276]
 [1.206]
 [1.118]
 [1.266]]
printing an ep nov before normalisation:  0.009532413579904642
printing an ep nov before normalisation:  61.819835565622114
line 256 mcts: sample exp_bonus 46.336198803015556
printing an ep nov before normalisation:  44.87442464895984
UNIT TEST: sample policy line 217 mcts : [0.154 0.103 0.256 0.256 0.231]
siam score:  -0.86785924
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.878]
 [48.685]
 [48.685]
 [48.685]
 [48.685]] [[1.641]
 [1.584]
 [1.584]
 [1.584]
 [1.584]]
printing an ep nov before normalisation:  41.43921039460828
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.601]
 [50.907]
 [52.829]
 [48.955]
 [50.5  ]] [[0.736]
 [1.298]
 [1.393]
 [1.201]
 [1.278]]
printing an ep nov before normalisation:  33.46619653862105
printing an ep nov before normalisation:  68.59073131632599
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0007280716999957804
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.485]
 [50.179]
 [30.484]
 [33.494]
 [33.973]] [[0.525]
 [1.024]
 [0.436]
 [0.526]
 [0.54 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.791]
 [25.147]
 [22.791]
 [23.368]
 [22.559]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.87736356
printing an ep nov before normalisation:  44.40338327366465
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.127]
 [65.055]
 [56.41 ]
 [53.895]
 [57.409]] [[1.312]
 [1.571]
 [1.248]
 [1.154]
 [1.285]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.613]
 [47.639]
 [52.335]
 [36.317]
 [42.992]] [[0.56 ]
 [0.634]
 [0.721]
 [0.426]
 [0.549]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.16671913914821
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.00827405206244
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.745]
 [57.028]
 [60.834]
 [57.118]
 [61.29 ]] [[0.6  ]
 [1.074]
 [1.192]
 [1.077]
 [1.206]]
actions average: 
K:  1  action  0 :  tensor([    0.9977,     0.0008,     0.0000,     0.0000,     0.0014],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0027,     0.9626,     0.0158,     0.0003,     0.0185],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9403,     0.0098,     0.0497],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0005,     0.0012,     0.0845,     0.7549,     0.1589],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0017, 0.0013, 0.1022, 0.2964, 0.5984], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.712]
 [48.897]
 [47.066]
 [55.543]
 [57.603]] [[1.191]
 [0.864]
 [0.787]
 [1.142]
 [1.228]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.50433158874512
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.46770929965967
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.984]
 [60.664]
 [57.984]
 [57.984]
 [57.984]] [[1.34 ]
 [1.441]
 [1.34 ]
 [1.34 ]
 [1.34 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.573249340057373
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  66.24093392713912
printing an ep nov before normalisation:  38.25319766998291
printing an ep nov before normalisation:  59.53762622134017
printing an ep nov before normalisation:  60.63424510438953
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.06120167835235
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  89.73000611027351
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.85382358187654
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.433055889288816
printing an ep nov before normalisation:  66.66738499197294
printing an ep nov before normalisation:  33.41917615554937
printing an ep nov before normalisation:  82.4630581325216
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.987]
 [56.987]
 [56.987]
 [56.987]
 [56.987]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
printing an ep nov before normalisation:  33.204513132045385
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8762087
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.822062569775014
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  3  action  0 :  tensor([    0.9983,     0.0001,     0.0000,     0.0007,     0.0009],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0005,     0.9713,     0.0017,     0.0000,     0.0265],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9656,     0.0182,     0.0161],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0003,     0.0337,     0.8150,     0.1509],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0006, 0.0041, 0.0609, 0.4053, 0.5291], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[16.276]
 [ 9.937]
 [ 4.228]
 [ 8.203]
 [10.335]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  1.0256093172301917
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 45.05877040676165
printing an ep nov before normalisation:  38.10522794723511
printing an ep nov before normalisation:  37.78936464173081
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.010118123559976766
printing an ep nov before normalisation:  76.08625313642979
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.025]
 [25.679]
 [47.437]
 [32.34 ]
 [32.23 ]] [[0.368]
 [0.256]
 [0.817]
 [0.428]
 [0.425]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.6498630377974
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.96069394856151
siam score:  -0.87101835
printing an ep nov before normalisation:  56.69595623315861
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.73686111948952
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.667]
 [57.628]
 [61.893]
 [73.024]
 [60.355]] [[0.354]
 [0.377]
 [0.427]
 [0.557]
 [0.409]]
printing an ep nov before normalisation:  47.26458021369712
printing an ep nov before normalisation:  31.838851827408924
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.342]
 [57.342]
 [55.646]
 [59.855]
 [57.342]] [[1.796]
 [1.796]
 [1.698]
 [1.94 ]
 [1.796]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.86132161047063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8769761
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.72416670661316
printing an ep nov before normalisation:  60.192756420470175
printing an ep nov before normalisation:  37.947820995433545
siam score:  -0.87442535
printing an ep nov before normalisation:  86.77944251499954
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.29104673093757
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.88917163759853
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.434]
 [63.592]
 [53.014]
 [55.997]
 [64.525]] [[1.213]
 [1.308]
 [0.991]
 [1.08 ]
 [1.336]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  87.2225532890869
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.17910586875332
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.54404120942515
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.378]
 [64.773]
 [55.378]
 [55.378]
 [55.378]] [[1.521]
 [1.889]
 [1.521]
 [1.521]
 [1.521]]
printing an ep nov before normalisation:  35.11185169219971
printing an ep nov before normalisation:  58.77592146954036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.61284695075486
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.036]
 [46.65 ]
 [31.661]
 [39.192]
 [36.833]] [[0.259]
 [0.237]
 [0.1  ]
 [0.169]
 [0.147]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.44793243595647
printing an ep nov before normalisation:  28.929383754730225
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.29789733886719
printing an ep nov before normalisation:  63.8968844039518
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.116]
 [64.249]
 [60.116]
 [60.116]
 [60.116]] [[0.841]
 [0.946]
 [0.841]
 [0.841]
 [0.841]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.126]
 [53.418]
 [42.167]
 [47.599]
 [48.75 ]] [[0.415]
 [0.575]
 [0.358]
 [0.463]
 [0.485]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.918064150000056
printing an ep nov before normalisation:  66.58393999915054
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9595,     0.0151,     0.0002,     0.0249],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0007,     0.0181,     0.8547,     0.0465,     0.0800],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0005,     0.0002,     0.0024,     0.8750,     0.1219],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0111, 0.0073, 0.1049, 0.1936, 0.6832], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.351935554096585
printing an ep nov before normalisation:  42.033200316660526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.56635173955119
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87106276
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.270144204233574
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 46.987829461539484
printing an ep nov before normalisation:  70.19988203746554
actions average: 
K:  2  action  0 :  tensor([    0.9994,     0.0001,     0.0000,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9988,     0.0003,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0002,     0.9285,     0.0129,     0.0584],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0003,     0.0145,     0.8717,     0.1133],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0010, 0.0025, 0.0316, 0.3035, 0.6614], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.04598135686933347
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.48069083808758
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.408180713653564
printing an ep nov before normalisation:  57.639610491209744
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[74.224]
 [68.355]
 [75.082]
 [74.224]
 [74.224]] [[0.986]
 [0.891]
 [1.   ]
 [0.986]
 [0.986]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.80311080033188
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9923,     0.0003,     0.0000,     0.0021,     0.0053],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9944,     0.0001,     0.0000,     0.0055],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0004,     0.9408,     0.0109,     0.0478],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0016,     0.0153,     0.7757,     0.2073],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0004,     0.0709,     0.0473,     0.1498,     0.7316],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.27375417434852
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.516]
 [34.091]
 [30.859]
 [38.145]
 [28.909]] [[0.951]
 [1.085]
 [0.917]
 [1.295]
 [0.816]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.863929782502865
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.13919862226689
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8712162
line 256 mcts: sample exp_bonus 35.980306413409345
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.86793214
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.479]
 [23.34 ]
 [22.484]
 [35.351]
 [25.223]] [[0.608]
 [0.297]
 [0.275]
 [0.605]
 [0.346]]
actions average: 
K:  3  action  0 :  tensor([    0.9937,     0.0000,     0.0047,     0.0003,     0.0014],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0187,     0.9504,     0.0013,     0.0009,     0.0287],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9528,     0.0177,     0.0293],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0347, 0.0117, 0.0533, 0.6854, 0.2149], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0568, 0.0023, 0.0937, 0.3155, 0.5317], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.615]
 [22.424]
 [41.59 ]
 [20.928]
 [21.2  ]] [[0.321]
 [0.29 ]
 [0.784]
 [0.252]
 [0.259]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.799939587979196
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.2  ]
 [33.431]
 [40.61 ]
 [35.876]
 [38.783]] [[0.996]
 [0.856]
 [1.219]
 [0.98 ]
 [1.127]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.231 0.154 0.179 0.282 0.154]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.12202725824377
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.359]
 [23.78 ]
 [15.716]
 [ 0.   ]
 [45.256]] [[ 0.125]
 [ 0.16 ]
 [ 0.077]
 [-0.083]
 [ 0.379]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  92.80744448241454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.9971,     0.0003,     0.0002,     0.0003,     0.0022],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0009,     0.9726,     0.0096,     0.0000,     0.0169],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.8650,     0.0408,     0.0940],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0015, 0.0073, 0.0126, 0.7534, 0.2252], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0079, 0.0217, 0.0606, 0.2095, 0.7003], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.44582092988842
printing an ep nov before normalisation:  47.83947944641113
printing an ep nov before normalisation:  39.12218297892087
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.85679164720155
printing an ep nov before normalisation:  61.34924863152589
printing an ep nov before normalisation:  31.518673319241103
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.15559357586716
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.736937022856154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.26546625190635
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.233405113220215
printing an ep nov before normalisation:  55.0043028922115
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.099]
 [38.823]
 [39.621]
 [33.57 ]
 [37.578]] [[0.394]
 [0.247]
 [0.256]
 [0.189]
 [0.234]]
printing an ep nov before normalisation:  62.59937580560303
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.22234604505327
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  66.8018245810589
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9993,     0.0000,     0.0005,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9990,     0.0001,     0.0001,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9987,     0.0007,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0018, 0.0026, 0.0243, 0.7645, 0.2067], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0039, 0.0060, 0.1010, 0.2826, 0.6065], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.461631320037064
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8674673
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.12303922664506
printing an ep nov before normalisation:  34.47210525137106
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.8539, 0.0533, 0.0433, 0.0017, 0.0478], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0040,     0.9863,     0.0024,     0.0004,     0.0070],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9687,     0.0142,     0.0170],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0375, 0.0099, 0.0344, 0.7009, 0.2172], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0601, 0.0168, 0.1125, 0.1855, 0.6251], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.976818347180235
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.8673279
printing an ep nov before normalisation:  45.18987950566454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.62471294403076
printing an ep nov before normalisation:  41.239204504894644
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.26 ]
 [37.522]
 [42.26 ]
 [42.26 ]
 [42.26 ]] [[1.667]
 [1.354]
 [1.667]
 [1.667]
 [1.667]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.977]
 [40.977]
 [18.236]
 [40.977]
 [40.977]] [[5.627]
 [5.627]
 [1.667]
 [5.627]
 [5.627]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.822]
 [53.124]
 [46.334]
 [45.7  ]
 [49.687]] [[1.064]
 [1.237]
 [0.965]
 [0.939]
 [1.099]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.74018955230713
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.7951714523984
printing an ep nov before normalisation:  59.487357777216914
line 256 mcts: sample exp_bonus 30.659455558021758
printing an ep nov before normalisation:  17.68181129209918
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.9696697362297
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.90949111057347
actions average: 
K:  1  action  0 :  tensor([    0.9993,     0.0002,     0.0001,     0.0002,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9989,     0.0002,     0.0001,     0.0007],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0003,     0.8873,     0.0235,     0.0888],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0004,     0.0024,     0.0125,     0.7858,     0.1990],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0481, 0.0237, 0.0972, 0.1745, 0.6565], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.272604065226965
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.614886738123495
printing an ep nov before normalisation:  42.465810775756836
printing an ep nov before normalisation:  29.71930742263794
UNIT TEST: sample policy line 217 mcts : [0.051 0.846 0.026 0.051 0.026]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.583]
 [39.583]
 [42.499]
 [39.583]
 [39.583]] [[0.877]
 [0.877]
 [1.01 ]
 [0.877]
 [0.877]]
printing an ep nov before normalisation:  59.094794219868334
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.462]
 [48.462]
 [48.462]
 [48.462]
 [48.462]] [[0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.876]]
printing an ep nov before normalisation:  55.0670850932198
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.559]
 [ 0.007]
 [ 0.006]
 [ 0.006]
 [ 0.006]] [[1.892]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.866785
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.31692383221235
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.49024090798942
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.189627599849345
siam score:  -0.8693213
siam score:  -0.8709555
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.103570723998736
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.44942569732666
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.23008255164571
printing an ep nov before normalisation:  53.77954335259947
printing an ep nov before normalisation:  47.88957211366795
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.57283203589235
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.9995,     0.0002,     0.0000,     0.0002,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0131,     0.9546,     0.0078,     0.0002,     0.0243],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0016, 0.0043, 0.8915, 0.0416, 0.0610], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0002,     0.0312,     0.8136,     0.1546],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0049, 0.0055, 0.0518, 0.3665, 0.5713], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.53711478047114
siam score:  -0.87807786
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9967,     0.0001,     0.0016,     0.0015],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0011,     0.9563,     0.0119,     0.0306],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0007,     0.0004,     0.0748,     0.7459,     0.1782],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0022, 0.0028, 0.1179, 0.2772, 0.5999], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.427]
 [45.24 ]
 [52.232]
 [45.24 ]
 [31.912]] [[0.   ]
 [1.005]
 [1.3  ]
 [1.005]
 [0.443]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.629]
 [53.674]
 [41.629]
 [41.629]
 [46.251]] [[0.841]
 [1.244]
 [0.841]
 [0.841]
 [0.995]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
UNIT TEST: sample policy line 217 mcts : [0.103 0.513 0.205 0.077 0.103]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8732573
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  104.56918150129448
printing an ep nov before normalisation:  85.62067731297321
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  17.64068129718485
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0261,     0.9424,     0.0108,     0.0008,     0.0200],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0002,     0.9149,     0.0234,     0.0615],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0006,     0.0004,     0.0584,     0.7221,     0.2185],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0007, 0.0013, 0.1165, 0.2941, 0.5874], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.89067239908546
printing an ep nov before normalisation:  60.98667484389226
printing an ep nov before normalisation:  41.03313566905956
printing an ep nov before normalisation:  38.36933363471462
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  106.26558939242881
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.31886393473917
printing an ep nov before normalisation:  39.95180033607155
printing an ep nov before normalisation:  56.10904717548006
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.14265060424805
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.636]
 [43.286]
 [43.286]
 [43.286]
 [43.286]] [[1.224]
 [0.625]
 [0.625]
 [0.625]
 [0.625]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.56539289522644
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.25984422846363486
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.79254572545333
line 256 mcts: sample exp_bonus 48.49948895153578
printing an ep nov before normalisation:  36.43702900867531
printing an ep nov before normalisation:  73.979005216018
printing an ep nov before normalisation:  56.76801905175107
printing an ep nov before normalisation:  34.19112682342529
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.262]
 [53.314]
 [45.262]
 [45.262]
 [45.262]] [[1.283]
 [1.796]
 [1.283]
 [1.283]
 [1.283]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.296]
 [54.386]
 [50.791]
 [42.296]
 [60.426]] [[0.65 ]
 [1.139]
 [0.994]
 [0.65 ]
 [1.383]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.067909380712486
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.085452140268735
siam score:  -0.869905
using explorer policy with actor:  1
printing an ep nov before normalisation:  21.413084649721853
siam score:  -0.8725058
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.66425318932951
printing an ep nov before normalisation:  54.146271073118164
printing an ep nov before normalisation:  37.92630236326191
printing an ep nov before normalisation:  52.01348449046798
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 42.05336852900077
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.47793614299826
printing an ep nov before normalisation:  51.66627756758883
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.12541013977056
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.965]
 [45.72 ]
 [45.005]
 [43.945]
 [45.491]] [[0.741]
 [1.679]
 [1.63 ]
 [1.558]
 [1.664]]
printing an ep nov before normalisation:  32.730088233947754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.97570276940665
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.86598104
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.948]
 [52.316]
 [56.283]
 [57.826]
 [49.7  ]] [[0.302]
 [0.672]
 [0.774]
 [0.814]
 [0.605]]
printing an ep nov before normalisation:  42.20007452988345
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.616]
 [47.098]
 [36.415]
 [35.54 ]
 [43.264]] [[0.505]
 [0.877]
 [0.531]
 [0.503]
 [0.753]]
printing an ep nov before normalisation:  36.79978937325523
UNIT TEST: sample policy line 217 mcts : [0.026 0.026 0.897 0.026 0.026]
printing an ep nov before normalisation:  54.16225215982521
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.385]
 [26.548]
 [27.364]
 [28.74 ]
 [29.44 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  84.60637561143567
printing an ep nov before normalisation:  48.96383940549915
printing an ep nov before normalisation:  31.691364511846132
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.94207253795079
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.7051305770874
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 33.1465691845214
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.89996147155762
printing an ep nov before normalisation:  51.09637749001267
printing an ep nov before normalisation:  43.65644593288075
siam score:  -0.86734986
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  78.23259194801233
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.844]
 [32.491]
 [36.167]
 [32.777]
 [33.7  ]] [[0.83 ]
 [0.729]
 [0.887]
 [0.742]
 [0.781]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.921545445286974
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
siam score:  -0.8723438
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.9987,     0.0004,     0.0000,     0.0003,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9472,     0.0017,     0.0002,     0.0508],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0002,     0.9271,     0.0333,     0.0392],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0009,     0.0006,     0.0215,     0.7342,     0.2428],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0016, 0.0010, 0.1250, 0.4430, 0.4295], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.918]
 [35.999]
 [38.948]
 [34.592]
 [36.898]] [[0.438]
 [0.316]
 [0.361]
 [0.294]
 [0.33 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8739305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.01687527462608
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.846297696893075
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.022]
 [48.929]
 [57.523]
 [23.079]
 [24.43 ]] [[0.19 ]
 [0.348]
 [0.424]
 [0.12 ]
 [0.132]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.683]
 [62.865]
 [59.655]
 [62.606]
 [65.253]] [[1.678]
 [1.606]
 [1.524]
 [1.599]
 [1.667]]
siam score:  -0.87683576
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.597]
 [45.835]
 [40.692]
 [36.295]
 [41.284]] [[1.433]
 [1.671]
 [1.383]
 [1.136]
 [1.416]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.873]
 [50.75 ]
 [72.53 ]
 [39.133]
 [43.066]] [[0.293]
 [0.853]
 [1.467]
 [0.526]
 [0.637]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87132406
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.878]
 [54.92 ]
 [44.282]
 [39.865]
 [48.602]] [[0.787]
 [1.208]
 [0.836]
 [0.682]
 [0.987]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  89.91670687481876
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  38.834385292079816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.45185099735059
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  59.30165292653928
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.88294107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.222]
 [68.222]
 [68.222]
 [68.222]
 [68.222]] [[1.108]
 [1.108]
 [1.108]
 [1.108]
 [1.108]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.42134263888699
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.159622551555586
printing an ep nov before normalisation:  56.01914272644089
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  86.2149716535453
printing an ep nov before normalisation:  81.53878748467531
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  66.41016568948169
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  91.1485671574557
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.9182071685791
printing an ep nov before normalisation:  0.0037708267086600245
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.351]
 [41.351]
 [64.121]
 [41.351]
 [41.351]] [[0.286]
 [0.286]
 [0.503]
 [0.286]
 [0.286]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 48.35413689703003
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.66122795470986
printing an ep nov before normalisation:  53.21679533912505
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.20566567835373917
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.77887472288533
printing an ep nov before normalisation:  58.907173417157544
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.05]
 [37.05]
 [37.05]
 [37.05]
 [37.05]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86781603
printing an ep nov before normalisation:  0.0019479482148199168
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.9995,     0.0000,     0.0000,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0017,     0.9100,     0.0109,     0.0002,     0.0771],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0005,     0.0007,     0.8632,     0.0255,     0.1101],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0010,     0.0004,     0.0034,     0.8332,     0.1620],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0010, 0.0009, 0.1066, 0.1849, 0.7066], grad_fn=<DivBackward0>)
siam score:  -0.8711485
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  73.79676926966637
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9986,     0.0000,     0.0000,     0.0001,     0.0013],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0177,     0.9541,     0.0000,     0.0001,     0.0281],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0016, 0.0012, 0.8883, 0.0286, 0.0803], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0012,     0.0005,     0.0675,     0.7786,     0.1522],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0023, 0.0022, 0.0757, 0.2509, 0.6689], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.30490017725151
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.082]
 [65.082]
 [65.082]
 [65.082]
 [65.082]] [[1.142]
 [1.142]
 [1.142]
 [1.142]
 [1.142]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.39967652862106
printing an ep nov before normalisation:  27.15956687927246
Sims:  40 1 epoch:  139319 pick best:  False frame count:  139319
printing an ep nov before normalisation:  28.79989067065633
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  57.75141069448576
printing an ep nov before normalisation:  40.711864829063416
printing an ep nov before normalisation:  52.41192448044167
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.229]
 [26.962]
 [19.852]
 [26.563]
 [18.064]] [[0.565]
 [0.604]
 [0.444]
 [0.595]
 [0.404]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.3709724099271625
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.85139187829289
printing an ep nov before normalisation:  54.78339484430163
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.874424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  0.03871666562572175
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.106]
 [42.699]
 [52.503]
 [27.4  ]
 [27.872]] [[0.157]
 [0.29 ]
 [0.393]
 [0.129]
 [0.134]]
printing an ep nov before normalisation:  33.07997465133667
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  64.09053866794827
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.797]
 [44.656]
 [38.339]
 [38.339]
 [38.339]] [[1.014]
 [1.048]
 [0.803]
 [0.803]
 [0.803]]
siam score:  -0.87233675
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.62670660018921
Starting evaluation
Printing some Q and Qe and total Qs values:  [[1.]
 [1.]
 [1.]
 [0.]
 [1.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.]
 [1.]
 [1.]
 [0.]
 [1.]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.862]
 [32.664]
 [24.344]
 [25.509]
 [26.031]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.40647152252433
siam score:  -0.86970514
printing an ep nov before normalisation:  54.55647476014759
printing an ep nov before normalisation:  33.853025261258274
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.698]
 [21.698]
 [21.698]
 [21.698]
 [21.698]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  23.994724166718637
printing an ep nov before normalisation:  55.26769783922324
printing an ep nov before normalisation:  42.77012825012207
printing an ep nov before normalisation:  20.315318359793647
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.279]
 [53.36 ]
 [60.561]
 [56.085]
 [53.408]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  68.37895883797219
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.806]
 [56.806]
 [56.806]
 [56.806]
 [56.806]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
printing an ep nov before normalisation:  27.312619669534048
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.593925996364185
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9986,     0.0000,     0.0000,     0.0004,     0.0009],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0005,     0.9691,     0.0029,     0.0002,     0.0273],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0001,     0.9477,     0.0254,     0.0267],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0007,     0.0025,     0.7998,     0.1969],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0005,     0.0180,     0.0912,     0.2431,     0.6472],
       grad_fn=<DivBackward0>)
actions average: 
K:  2  action  0 :  tensor([    0.9954,     0.0001,     0.0001,     0.0023,     0.0022],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0083,     0.9011,     0.0154,     0.0004,     0.0749],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0002,     0.9367,     0.0119,     0.0509],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0004,     0.0563,     0.8111,     0.1319],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0172, 0.0032, 0.0930, 0.3075, 0.5791], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  77.06371623047593
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9937,     0.0000,     0.0060,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9961,     0.0000,     0.0001,     0.0036],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0002,     0.8892,     0.0396,     0.0709],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0037, 0.0008, 0.0916, 0.7040, 0.1999], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0245, 0.0023, 0.0606, 0.3517, 0.5609], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  75.40372298271748
printing an ep nov before normalisation:  56.210198163894106
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.001]
 [28.706]
 [29.57 ]
 [31.705]
 [31.383]] [[0.638]
 [0.952]
 [1.01 ]
 [1.153]
 [1.131]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.408]
 [40.408]
 [40.408]
 [40.408]
 [40.408]] [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.79680712393924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.63422152216258
printing an ep nov before normalisation:  41.895811916376864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.866647397576315
printing an ep nov before normalisation:  61.74305712317429
printing an ep nov before normalisation:  57.578538316946094
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.439]
 [58.63 ]
 [49.744]
 [49.744]
 [49.744]] [[1.272]
 [1.241]
 [0.895]
 [0.895]
 [0.895]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.90730914814616
siam score:  -0.87608385
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  64.26672521990756
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8694633
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.86514527
siam score:  -0.86655533
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.836]
 [37.81 ]
 [29.154]
 [32.687]
 [32.524]] [[1.027]
 [0.885]
 [0.477]
 [0.643]
 [0.635]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  96.26134803451473
printing an ep nov before normalisation:  75.56897376188738
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.62606178112921
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9980,     0.0001,     0.0000,     0.0003,     0.0016],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0231,     0.9119,     0.0162,     0.0001,     0.0488],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9181,     0.0462,     0.0356],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0020,     0.0004,     0.0519,     0.8184,     0.1273],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0010, 0.0046, 0.1623, 0.2742, 0.5579], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  64.1850597584563
printing an ep nov before normalisation:  66.18082216563967
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.14653491973877
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[11.043]
 [ 9.275]
 [14.294]
 [13.951]
 [15.187]] [[0.059]
 [0.049]
 [0.076]
 [0.074]
 [0.081]]
printing an ep nov before normalisation:  66.41419442204179
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.47554767547113
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.802]
 [46.802]
 [46.802]
 [34.525]
 [46.802]] [[2.796]
 [2.796]
 [2.796]
 [1.667]
 [2.796]]
printing an ep nov before normalisation:  76.25225247885953
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.694]
 [46.694]
 [60.407]
 [46.694]
 [46.694]] [[0.796]
 [0.796]
 [1.199]
 [0.796]
 [0.796]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.272394282055227
actions average: 
K:  2  action  0 :  tensor([    0.9956,     0.0001,     0.0000,     0.0018,     0.0025],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0046,     0.9506,     0.0003,     0.0001,     0.0444],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0002,     0.9224,     0.0345,     0.0427],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0007,     0.0548,     0.6863,     0.2578],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0082, 0.0711, 0.1250, 0.2767, 0.5189], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  30.28731535646913
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.101903915405273
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9490,     0.0002,     0.0000,     0.0018,     0.0490],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0022, 0.9623, 0.0086, 0.0139, 0.0129], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.8776,     0.0428,     0.0795],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0006,     0.0010,     0.0284,     0.7521,     0.2178],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0004,     0.0234,     0.1325,     0.2453,     0.5984],
       grad_fn=<DivBackward0>)
siam score:  -0.8691292
printing an ep nov before normalisation:  25.697406903898983
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8687466
printing an ep nov before normalisation:  46.89847486850041
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  89.3594283286507
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.498]
 [54.252]
 [51.511]
 [54.35 ]
 [54.265]] [[0.246]
 [0.281]
 [0.255]
 [0.282]
 [0.281]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  109.91702953166816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.47298980905574
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[70.901]
 [61.364]
 [61.364]
 [66.159]
 [61.364]] [[1.194]
 [0.976]
 [0.976]
 [1.085]
 [0.976]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.601181804196074
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.17205975795002
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  24.9420166015625
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.62812805175781
printing an ep nov before normalisation:  61.04609525141892
printing an ep nov before normalisation:  30.06053690130532
printing an ep nov before normalisation:  57.99115934562111
printing an ep nov before normalisation:  56.82565820132623
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.93205785751343
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87463164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.242]
 [26.78 ]
 [26.78 ]
 [26.78 ]
 [26.78 ]] [[1.667]
 [1.732]
 [1.732]
 [1.732]
 [1.732]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
siam score:  -0.87735295
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.25327037246372
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.3312310354232295
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.75406551361084
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.093]
 [63.138]
 [65.093]
 [65.093]
 [67.809]] [[1.881]
 [1.795]
 [1.881]
 [1.881]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.803867082362096
line 256 mcts: sample exp_bonus 20.985605600907345
printing an ep nov before normalisation:  28.76097559928894
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.892]
 [48.892]
 [59.149]
 [59.425]
 [48.892]] [[1.004]
 [1.004]
 [1.321]
 [1.33 ]
 [1.004]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.369]
 [48.369]
 [48.369]
 [62.237]
 [60.754]] [[0.961]
 [0.961]
 [0.961]
 [1.393]
 [1.347]]
printing an ep nov before normalisation:  59.698398042553755
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.12 ]
 [64.362]
 [64.48 ]
 [67.651]
 [68.119]] [[1.414]
 [1.362]
 [1.366]
 [1.458]
 [1.472]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.59276197581413
printing an ep nov before normalisation:  25.089457035064697
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  75.05150732780021
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.62147045135498
line 256 mcts: sample exp_bonus 29.612481162442112
printing an ep nov before normalisation:  29.332609618130043
printing an ep nov before normalisation:  50.97966444454069
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.116]
 [38.132]
 [38.132]
 [38.132]
 [38.132]] [[1.471]
 [0.899]
 [0.899]
 [0.899]
 [0.899]]
printing an ep nov before normalisation:  40.976909829050825
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  78.674253669713
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.86089450718394
printing an ep nov before normalisation:  32.223991095553465
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.98639103333979
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.077]
 [51.906]
 [39.086]
 [39.086]
 [39.086]] [[1.158]
 [1.25 ]
 [0.941]
 [0.941]
 [0.941]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86383104
printing an ep nov before normalisation:  0.011812889866291698
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.228939724786706
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.364]
 [44.364]
 [61.11 ]
 [44.364]
 [44.364]] [[1.139]
 [1.139]
 [1.683]
 [1.139]
 [1.139]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.9901, 0.0016, 0.0019, 0.0027, 0.0037], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9757,     0.0016,     0.0001,     0.0226],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0056, 0.0018, 0.8778, 0.0473, 0.0675], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0009,     0.0007,     0.0099,     0.8267,     0.1619],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0064, 0.0075, 0.1266, 0.2469, 0.6126], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.630789023899275
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.865466
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.070399949770824
siam score:  -0.8656052
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.069]
 [54.215]
 [50.209]
 [52.396]
 [52.396]] [[1.604]
 [1.509]
 [1.303]
 [1.415]
 [1.415]]
printing an ep nov before normalisation:  46.40886663598977
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9947,     0.0003,     0.0002,     0.0004,     0.0044],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0489,     0.9489,     0.0000,     0.0000,     0.0021],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0024,     0.0003,     0.8911,     0.0300,     0.0761],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0551, 0.0008, 0.0242, 0.6838, 0.2360], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0004,     0.0050,     0.0495,     0.3210,     0.6241],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.382515997283118
printing an ep nov before normalisation:  53.990919249398374
printing an ep nov before normalisation:  65.70187190532349
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.530123998284715
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.887]
 [34.936]
 [40.642]
 [43.521]
 [38.344]] [[0.746]
 [0.688]
 [0.8  ]
 [0.857]
 [0.755]]
printing an ep nov before normalisation:  41.79117653131255
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.18 ]
 [40.18 ]
 [37.622]
 [40.18 ]
 [40.938]] [[1.744]
 [1.744]
 [1.517]
 [1.744]
 [1.811]]
siam score:  -0.8706393
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  75.37425914420687
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.74566235477347
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.71362701581525
printing an ep nov before normalisation:  43.41965416324908
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.31027602374617
printing an ep nov before normalisation:  34.17896571498622
printing an ep nov before normalisation:  46.997575968747526
printing an ep nov before normalisation:  56.71087139169296
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.435565948486328
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.07558792678511
line 256 mcts: sample exp_bonus 46.93203870928846
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.71562805052125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.531335861926344
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.21778311002865
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 41.422067942944636
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.26805664794906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.92016537871981
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.99377245528917
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.354]
 [44.176]
 [45.564]
 [44.447]
 [45.255]] [[1.249]
 [1.355]
 [1.436]
 [1.371]
 [1.418]]
printing an ep nov before normalisation:  68.1056424578502
actions average: 
K:  3  action  0 :  tensor([    0.9940,     0.0003,     0.0000,     0.0008,     0.0049],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9990,     0.0001,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0001,     0.9439,     0.0117,     0.0442],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0005,     0.0736,     0.6203,     0.3052],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0013, 0.0041, 0.1191, 0.1747, 0.7009], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87203157
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.803]
 [27.366]
 [40.75 ]
 [ 0.   ]
 [ 0.   ]] [[ 0.655]
 [ 0.316]
 [ 0.47 ]
 [-0.   ]
 [-0.   ]]
siam score:  -0.87504125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.506407286414415
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8695006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.64580030248928
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.213]
 [21.857]
 [14.751]
 [37.934]
 [21.355]] [[0.701]
 [0.563]
 [0.38 ]
 [0.978]
 [0.55 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.871743
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.578]
 [54.35 ]
 [56.515]
 [55.839]
 [57.532]] [[0.688]
 [1.358]
 [1.444]
 [1.417]
 [1.485]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.15430625589549
printing an ep nov before normalisation:  54.427817438724496
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.52299976348877
siam score:  -0.8728566
siam score:  -0.8724005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.171442352014935
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87078553
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9981,     0.0000,     0.0003,     0.0008,     0.0008],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9962,     0.0004,     0.0002,     0.0032],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9632,     0.0001,     0.0366],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0000,     0.0003,     0.0213,     0.8228,     0.1556],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0002,     0.0182,     0.0781,     0.2981,     0.6054],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.66213004365829
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.68951538471613
siam score:  -0.8723987
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  58.87043945120896
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.9914,     0.0030,     0.0041,     0.0005,     0.0010],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9890,     0.0003,     0.0000,     0.0106],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0005,     0.9415,     0.0144,     0.0436],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0005,     0.0743,     0.7424,     0.1827],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0015, 0.0455, 0.0651, 0.2531, 0.6348], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.34025866580722
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.50728797688229
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.237]
 [24.237]
 [27.833]
 [27.548]
 [24.237]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.173]
 [57.459]
 [39.54 ]
 [42.578]
 [55.664]] [[0.554]
 [0.905]
 [0.623]
 [0.67 ]
 [0.876]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.438]
 [39.438]
 [39.438]
 [55.361]
 [39.438]] [[0.568]
 [0.568]
 [0.568]
 [1.004]
 [0.568]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  65.90340564726213
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.95897336747107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.4216604417674
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -3.43373403471106e-12
0.0 1.23683618764197e-12
0.0 0.0
0.0 0.0
0.0 -1.2299168251061872e-11
0.0 0.0
0.0 1.094124324089343e-12
0.0 -2.0758089943356215e-13
0.0 -3.5807705034482185e-12
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.802]
 [56.454]
 [51.802]
 [51.802]
 [51.802]] [[1.068]
 [1.208]
 [1.068]
 [1.068]
 [1.068]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  53.54670121045911
printing an ep nov before normalisation:  57.69783312626945
actions average: 
K:  0  action  0 :  tensor([    0.9837,     0.0062,     0.0000,     0.0012,     0.0090],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9947,     0.0003,     0.0001,     0.0045],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0006,     0.8938,     0.0455,     0.0601],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0025, 0.0016, 0.0607, 0.6469, 0.2883], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0151, 0.0230, 0.0620, 0.3150, 0.5848], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.887]
 [48.483]
 [48.483]
 [48.483]
 [48.483]] [[0.512]
 [0.455]
 [0.455]
 [0.455]
 [0.455]]
printing an ep nov before normalisation:  51.0881735603602
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.01710984233326
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  59.48326091985682
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.09695911286669
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[75.637]
 [75.637]
 [75.637]
 [75.637]
 [75.637]] [[1.27]
 [1.27]
 [1.27]
 [1.27]
 [1.27]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  116.03139209984744
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.094]
 [34.839]
 [33.413]
 [32.647]
 [37.357]] [[0.909]
 [0.732]
 [0.672]
 [0.64 ]
 [0.837]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.226788782957474
printing an ep nov before normalisation:  28.294651186533997
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.657]
 [21.657]
 [21.657]
 [21.657]
 [21.657]] [[0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]]
printing an ep nov before normalisation:  104.95175444613076
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.294]
 [77.594]
 [93.628]
 [77.594]
 [77.594]] [[0.304]
 [0.731]
 [1.038]
 [0.731]
 [0.731]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.869]
 [45.463]
 [38.641]
 [44.519]
 [38.641]] [[0.944]
 [0.891]
 [0.63 ]
 [0.855]
 [0.63 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.997]
 [47.249]
 [44.842]
 [42.313]
 [47.227]] [[0.821]
 [1.034]
 [0.936]
 [0.834]
 [1.033]]
printing an ep nov before normalisation:  52.08800338870561
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  56.63295290528862
siam score:  -0.8625039
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.905447288475024
printing an ep nov before normalisation:  34.916477370586705
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 49.39774823295806
printing an ep nov before normalisation:  39.533063201066035
printing an ep nov before normalisation:  24.444391020410826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  24.95935213405342
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8724336
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.562]
 [62.386]
 [64.562]
 [64.562]
 [61.944]] [[1.   ]
 [0.953]
 [1.   ]
 [1.   ]
 [0.943]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.379918505582154
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  79.07712596773925
printing an ep nov before normalisation:  45.356998443603516
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.037]
 [0.056]
 [0.062]
 [0.04 ]
 [0.044]] [[0.   ]
 [0.001]
 [0.001]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[13.642]
 [20.303]
 [25.544]
 [18.545]
 [18.224]] [[0.131]
 [0.195]
 [0.245]
 [0.178]
 [0.175]]
printing an ep nov before normalisation:  36.21485511584028
printing an ep nov before normalisation:  21.011741161346436
actions average: 
K:  2  action  0 :  tensor([    0.9918,     0.0024,     0.0000,     0.0005,     0.0053],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.8847,     0.0031,     0.0003,     0.1119],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0012,     0.0004,     0.9040,     0.0297,     0.0647],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0009,     0.0294,     0.8259,     0.1435],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0027, 0.0014, 0.1176, 0.1938, 0.6846], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.667420205754425
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.53 ]
 [40.53 ]
 [60.086]
 [40.53 ]
 [40.53 ]] [[0.402]
 [0.402]
 [0.761]
 [0.402]
 [0.402]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.393]
 [45.393]
 [45.393]
 [45.393]
 [45.393]] [[0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]]
printing an ep nov before normalisation:  43.519728027892036
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.475]
 [59.221]
 [52.496]
 [59.739]
 [59.247]] [[0.731]
 [0.617]
 [0.511]
 [0.625]
 [0.617]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.67230205856667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.451416366057025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9589,     0.0161,     0.0000,     0.0005,     0.0244],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.8967,     0.0384,     0.0003,     0.0640],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0043, 0.0016, 0.8410, 0.0422, 0.1109], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0137,     0.0005,     0.0516,     0.7859,     0.1483],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0034, 0.0017, 0.1118, 0.2626, 0.6206], grad_fn=<DivBackward0>)
siam score:  -0.8664223
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.789447224996586
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.562]
 [44.562]
 [43.11 ]
 [41.938]
 [44.562]] [[2.128]
 [2.128]
 [2.   ]
 [1.896]
 [2.128]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.398119324484384
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.89769618133539
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.395]
 [36.329]
 [38.395]
 [51.442]
 [38.395]] [[0.662]
 [0.594]
 [0.662]
 [1.09 ]
 [0.662]]
actions average: 
K:  2  action  0 :  tensor([0.9579, 0.0185, 0.0169, 0.0017, 0.0049], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0127,     0.9643,     0.0001,     0.0001,     0.0228],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0068,     0.8660,     0.0515,     0.0755],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0011,     0.0339,     0.8106,     0.1541],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0010, 0.0124, 0.1019, 0.2108, 0.6739], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.278]
 [48.278]
 [84.2  ]
 [48.278]
 [48.278]] [[0.753]
 [0.753]
 [1.509]
 [0.753]
 [0.753]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.975]
 [49.975]
 [49.975]
 [49.975]
 [49.975]] [[1.885]
 [1.885]
 [1.885]
 [1.885]
 [1.885]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.820932685820615
UNIT TEST: sample policy line 217 mcts : [0.026 0.077 0.564 0.128 0.205]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.9469, 0.0020, 0.0066, 0.0197, 0.0249], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0019, 0.9064, 0.0114, 0.0039, 0.0764], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0005,     0.8925,     0.0496,     0.0572],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0003,     0.0035,     0.8078,     0.1881],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0040, 0.0186, 0.0612, 0.2814, 0.6348], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.298]
 [69.298]
 [69.298]
 [69.298]
 [69.298]] [[1.661]
 [1.661]
 [1.661]
 [1.661]
 [1.661]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.967]
 [37.95 ]
 [37.95 ]
 [37.95 ]
 [37.95 ]] [[0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.62227511986766
printing an ep nov before normalisation:  47.07726488107274
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.10199002371996
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  53.64861488342285
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.50209094013677
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.183]
 [32.183]
 [50.694]
 [32.183]
 [32.183]] [[0.617]
 [0.617]
 [1.197]
 [0.617]
 [0.617]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 71.01633437684121
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.0781142305791
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.381370067596436
siam score:  -0.8794041
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 40.44573974609375
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.142500285746642
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.106]
 [53.892]
 [62.576]
 [53.892]
 [53.892]] [[0.333]
 [0.251]
 [0.328]
 [0.251]
 [0.251]]
printing an ep nov before normalisation:  42.95575401785647
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.139]
 [56.242]
 [49.139]
 [49.139]
 [55.651]] [[1.424]
 [1.8  ]
 [1.424]
 [1.424]
 [1.769]]
printing an ep nov before normalisation:  0.03612268389986184
printing an ep nov before normalisation:  28.74396913241047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  37.23194031793601
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.718]
 [45.06 ]
 [45.774]
 [52.654]
 [51.612]] [[0.25 ]
 [0.243]
 [0.251]
 [0.322]
 [0.312]]
printing an ep nov before normalisation:  0.002224249277560375
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9912,     0.0000,     0.0082,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9709,     0.0070,     0.0001,     0.0217],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0050,     0.9051,     0.0300,     0.0598],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0043, 0.0008, 0.0605, 0.7714, 0.1630], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0216, 0.0480, 0.0707, 0.3216, 0.5382], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  88.65731673586365
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  47.52432662005773
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.736961149958965
printing an ep nov before normalisation:  37.392284172231506
printing an ep nov before normalisation:  49.2152907086477
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.82390213012695
siam score:  -0.8760919
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  0.018350040687380442
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9996,     0.0000,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9808,     0.0003,     0.0009,     0.0179],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9701,     0.0157,     0.0140],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0003,     0.0231,     0.8617,     0.1145],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0028, 0.0250, 0.0918, 0.2785, 0.6019], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.537847903600337
using explorer policy with actor:  1
printing an ep nov before normalisation:  75.06974845319063
printing an ep nov before normalisation:  76.25151789317894
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.8631354
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.61302063840438
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.419]
 [61.419]
 [61.419]
 [61.419]
 [61.419]] [[1.789]
 [1.789]
 [1.789]
 [1.789]
 [1.789]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.6833332632707
printing an ep nov before normalisation:  52.53274256345569
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  68.38260545785735
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.279]
 [38.279]
 [38.279]
 [38.279]
 [38.279]] [[0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.82844058003783
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.999]
 [29.629]
 [29.629]
 [29.629]
 [29.629]] [[0.695]
 [0.434]
 [0.434]
 [0.434]
 [0.434]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.010658384879406
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.25439665596054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
printing an ep nov before normalisation:  32.62696032261476
printing an ep nov before normalisation:  22.950351456594003
printing an ep nov before normalisation:  31.566503047943115
siam score:  -0.87049013
actions average: 
K:  4  action  0 :  tensor([    0.9719,     0.0135,     0.0000,     0.0012,     0.0134],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9857,     0.0014,     0.0017,     0.0107],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0054,     0.7867,     0.0769,     0.1310],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0007,     0.0004,     0.1061,     0.7906,     0.1022],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0103, 0.0397, 0.1649, 0.1710, 0.6141], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9917,     0.0002,     0.0000,     0.0025,     0.0056],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9971,     0.0007,     0.0005,     0.0016],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.8977,     0.0549,     0.0473],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0005,     0.0606,     0.7804,     0.1585],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0004,     0.0045,     0.1064,     0.2489,     0.6398],
       grad_fn=<DivBackward0>)
siam score:  -0.8693865
printing an ep nov before normalisation:  47.624823856107874
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.767012323659664
printing an ep nov before normalisation:  82.83604650973096
line 256 mcts: sample exp_bonus 63.53937340194767
printing an ep nov before normalisation:  43.52513574157247
printing an ep nov before normalisation:  39.17148286223267
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.21014077106724
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.94040647286061
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.42808723449707
printing an ep nov before normalisation:  30.928239822387695
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.098544120788574
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  61.061172149412215
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.2443995742466
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.434657477925775
printing an ep nov before normalisation:  33.31301317005109
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9945,     0.0011,     0.0000,     0.0026,     0.0018],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9953,     0.0023,     0.0000,     0.0023],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0011,     0.0008,     0.8641,     0.0568,     0.0772],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0006,     0.0048,     0.0260,     0.8146,     0.1541],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0004,     0.0043,     0.1305,     0.2988,     0.5660],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.30246255431612
using explorer policy with actor:  1
siam score:  -0.86273485
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.03117399651662
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.831]
 [65.402]
 [58.225]
 [60.831]
 [60.831]] [[1.163]
 [1.296]
 [1.088]
 [1.163]
 [1.163]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.8569345
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.782]
 [50.02 ]
 [37.782]
 [37.782]
 [37.782]] [[0.525]
 [0.929]
 [0.525]
 [0.525]
 [0.525]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.339]
 [61.282]
 [44.339]
 [44.339]
 [55.277]] [[0.249]
 [0.475]
 [0.249]
 [0.249]
 [0.395]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.682473573193434
printing an ep nov before normalisation:  60.8684044198875
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.20215559005737
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  0.003236364545955439
printing an ep nov before normalisation:  46.70116816277336
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.722]
 [45.122]
 [42.737]
 [42.737]
 [42.737]] [[1.381]
 [1.406]
 [1.258]
 [1.258]
 [1.258]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.526397476214186
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.67658494698527
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.15468799941716
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.609]
 [55.563]
 [43.198]
 [44.42 ]
 [49.556]] [[0.677]
 [0.898]
 [0.554]
 [0.588]
 [0.731]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.83935313889011
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.2671459598437
siam score:  -0.87442154
printing an ep nov before normalisation:  44.17284965515137
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.02119376788107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.32]
 [40.32]
 [40.32]
 [40.32]
 [40.32]] [[0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]]
printing an ep nov before normalisation:  96.89898475079752
printing an ep nov before normalisation:  36.814092141255514
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.23085117340088
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.319]
 [37.791]
 [27.334]
 [28.157]
 [29.242]] [[0.557]
 [0.793]
 [0.411]
 [0.441]
 [0.481]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.301]
 [31.608]
 [24.872]
 [31.608]
 [23.676]] [[1.667]
 [2.459]
 [1.613]
 [2.459]
 [1.463]]
siam score:  -0.8651802
printing an ep nov before normalisation:  57.34246823071909
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8657739
printing an ep nov before normalisation:  47.72243022918701
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.77180678586352
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.87633315076358
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.535]
 [ 0.013]
 [43.535]
 [43.535]
 [43.535]] [[1.333]
 [0.   ]
 [1.333]
 [1.333]
 [1.333]]
printing an ep nov before normalisation:  58.00975440701068
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.82323992638927
printing an ep nov before normalisation:  39.431235236090146
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.20280170440674
printing an ep nov before normalisation:  36.778809494148476
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.302]
 [34.743]
 [32.91 ]
 [36.785]
 [34.473]] [[1.   ]
 [0.878]
 [0.79 ]
 [0.975]
 [0.865]]
printing an ep nov before normalisation:  40.51803770423604
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.97242267155938
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.015]
 [39.743]
 [40.015]
 [40.015]
 [40.015]] [[1.013]
 [1.   ]
 [1.013]
 [1.013]
 [1.013]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.94072751679789
printing an ep nov before normalisation:  53.65873177280205
siam score:  -0.86694247
printing an ep nov before normalisation:  31.491992473602295
printing an ep nov before normalisation:  43.09488621421087
printing an ep nov before normalisation:  32.99477577209473
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  26.547422409057617
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.87018716
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.959]
 [36.822]
 [28.959]
 [28.959]
 [28.959]] [[0.13 ]
 [0.194]
 [0.13 ]
 [0.13 ]
 [0.13 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.654]
 [47.965]
 [38.654]
 [38.654]
 [38.638]] [[1.145]
 [1.67 ]
 [1.145]
 [1.145]
 [1.144]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.775]
 [53.775]
 [80.191]
 [53.775]
 [53.775]] [[0.428]
 [0.428]
 [0.829]
 [0.428]
 [0.428]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.2696901160234688
printing an ep nov before normalisation:  20.61699911783891
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.431]
 [32.431]
 [53.075]
 [32.431]
 [32.431]] [[0.111]
 [0.111]
 [0.231]
 [0.111]
 [0.111]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.715]
 [21.472]
 [21.821]
 [31.559]
 [22.686]] [[0.69 ]
 [0.602]
 [0.616]
 [0.996]
 [0.649]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.73312927314878
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.37392262848487
printing an ep nov before normalisation:  27.311412367538924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9858,     0.0086,     0.0000,     0.0001,     0.0055],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0040,     0.9822,     0.0001,     0.0002,     0.0135],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0131,     0.8942,     0.0287,     0.0637],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0024, 0.0011, 0.0008, 0.7654, 0.2302], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0085, 0.0556, 0.1463, 0.2163, 0.5733], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.414]
 [67.377]
 [60.414]
 [60.414]
 [63.783]] [[1.26 ]
 [1.513]
 [1.26 ]
 [1.26 ]
 [1.382]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 56.339320972695134
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  37.48992586932811
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.3607680002816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8771817
printing an ep nov before normalisation:  33.32571744918823
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.418]
 [36.418]
 [36.418]
 [36.418]
 [36.418]] [[0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]]
printing an ep nov before normalisation:  28.826337390475803
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.758678760350904
printing an ep nov before normalisation:  38.95817502386229
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.20838231873178
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.25027423402973
actions average: 
K:  2  action  0 :  tensor([    0.9989,     0.0001,     0.0000,     0.0005,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9899,     0.0059,     0.0004,     0.0035],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0004,     0.8961,     0.0390,     0.0643],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0010,     0.0547,     0.7394,     0.2047],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0064, 0.0398, 0.1607, 0.2467, 0.5464], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  61.8591019424101
printing an ep nov before normalisation:  24.23326290592982
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  69.80612331301977
line 256 mcts: sample exp_bonus 56.36046236725072
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.224]
 [24.328]
 [28.216]
 [41.606]
 [35.532]] [[0.226]
 [0.08 ]
 [0.128]
 [0.292]
 [0.217]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86689234
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.01490200402942
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8623009
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.379838943481445
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  59.27548487237308
siam score:  -0.86018556
siam score:  -0.8627817
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.64 ]
 [56.057]
 [44.64 ]
 [44.64 ]
 [44.64 ]] [[0.422]
 [0.65 ]
 [0.422]
 [0.422]
 [0.422]]
printing an ep nov before normalisation:  69.80468482843112
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.54322112864633
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9987,     0.0001,     0.0000,     0.0005,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0051,     0.9693,     0.0006,     0.0005,     0.0245],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0035,     0.8889,     0.0489,     0.0586],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0060,     0.0007,     0.0325,     0.7706,     0.1902],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0076, 0.0149, 0.1130, 0.1326, 0.7318], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.24052143096924
printing an ep nov before normalisation:  49.2254233994568
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8676149
printing an ep nov before normalisation:  42.35332569910732
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.009348744520864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.41468201081668
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.043]
 [46.043]
 [76.315]
 [46.043]
 [46.043]] [[0.201]
 [0.201]
 [0.443]
 [0.201]
 [0.201]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.12210630282766033
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.02691873083879
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.87814469854183
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.019222734379411577
printing an ep nov before normalisation:  49.252658420138886
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.340649604797363
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.2048396419361
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8672064
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.8094822197387
printing an ep nov before normalisation:  52.03671276426068
printing an ep nov before normalisation:  45.05833630982
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9640,     0.0164,     0.0000,     0.0071,     0.0125],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9989,     0.0001,     0.0002,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0027,     0.0007,     0.8216,     0.0443,     0.1307],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0031,     0.0310,     0.8017,     0.1640],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0003,     0.0306,     0.0604,     0.2275,     0.6813],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.755]
 [37.083]
 [33.137]
 [40.743]
 [35.675]] [[0.712]
 [0.517]
 [0.417]
 [0.61 ]
 [0.481]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.183850136350486
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.78772883637886
actions average: 
K:  3  action  0 :  tensor([0.9501, 0.0249, 0.0022, 0.0023, 0.0204], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0117,     0.9832,     0.0005,     0.0005,     0.0041],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0006,     0.9242,     0.0128,     0.0623],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0003,     0.0488,     0.8311,     0.1197],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0010, 0.0010, 0.1124, 0.2624, 0.6232], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.21305483001191
printing an ep nov before normalisation:  28.964432523032173
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  68.75140406378104
printing an ep nov before normalisation:  50.51075536667534
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8713892
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.408]
 [52.093]
 [54.998]
 [55.258]
 [57.188]] [[0.523]
 [0.677]
 [0.755]
 [0.762]
 [0.814]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.226]
 [45.119]
 [47.463]
 [47.046]
 [46.624]] [[0.506]
 [0.794]
 [0.87 ]
 [0.857]
 [0.843]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.89944624314936
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.53977453880495
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  31.906983852386475
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.1080430271126
printing an ep nov before normalisation:  37.337894439697266
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9961,     0.0000,     0.0000,     0.0007,     0.0032],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0135,     0.9735,     0.0020,     0.0001,     0.0109],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0199,     0.8868,     0.0240,     0.0690],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0009, 0.0046, 0.0469, 0.6700, 0.2775], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0005,     0.0313,     0.0913,     0.3166,     0.5602],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.32435461769782
printing an ep nov before normalisation:  29.29947853088379
printing an ep nov before normalisation:  91.45088695674998
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.308 0.103 0.077 0.128 0.385]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.92911535395573
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.79178558711937
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.16377411170647
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.7921933574021
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 53.339372325807396
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86794645
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9980,     0.0008,     0.0005,     0.0002,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9997,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0027,     0.9182,     0.0318,     0.0471],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0019, 0.0019, 0.0244, 0.7845, 0.1873], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0021, 0.0321, 0.1508, 0.2637, 0.5513], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  66.27066797016526
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  70.89551615869219
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.299]
 [33.299]
 [33.299]
 [33.299]
 [33.299]] [[0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]]
siam score:  -0.8661943
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.033226050309167476
siam score:  -0.8675853
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.08753776550293
printing an ep nov before normalisation:  47.142123265967456
siam score:  -0.8698557
printing an ep nov before normalisation:  88.84170910355417
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.4136135058195123
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.26742706985354
printing an ep nov before normalisation:  61.68867700276064
siam score:  -0.8776299
siam score:  -0.87795675
printing an ep nov before normalisation:  52.808835205476875
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8782068
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 63.77559597231439
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.51887273896076
siam score:  -0.8773423
printing an ep nov before normalisation:  56.85090829412782
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [18.283]] [[-0.905]
 [-0.905]
 [-0.905]
 [-0.905]
 [ 1.   ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.832]
 [64.93 ]
 [55.832]
 [55.832]
 [55.832]] [[0.63 ]
 [0.812]
 [0.63 ]
 [0.63 ]
 [0.63 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.026 0.026 0.846 0.026 0.077]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.228259118951264
printing an ep nov before normalisation:  40.16307830810547
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.069]
 [39.318]
 [32.394]
 [37.202]
 [36.782]] [[1.135]
 [1.031]
 [0.62 ]
 [0.905]
 [0.88 ]]
siam score:  -0.87013716
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.12047306696574
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.85141261164021
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.26624769078556
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.36893081252423
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.33222850755746
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.87219375
siam score:  -0.87006944
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.69541301925398
printing an ep nov before normalisation:  49.07442341903357
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.86236817
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.54200252392038
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.005451299614378513
siam score:  -0.86523134
printing an ep nov before normalisation:  65.65278699513048
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0063888476597639965
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.296397214518095
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.51064393015295
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.1295071160543
siam score:  -0.8733861
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.204]
 [29.204]
 [29.204]
 [48.613]
 [29.204]] [[0.899]
 [0.899]
 [0.899]
 [1.498]
 [0.899]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.80468905653312
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 42.65482643145287
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.937]
 [40.341]
 [36.196]
 [35.925]
 [37.424]] [[1.279]
 [1.58 ]
 [1.297]
 [1.279]
 [1.381]]
printing an ep nov before normalisation:  36.92421585148196
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.76373767223337
siam score:  -0.8623365
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.57834988483338
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.607]
 [58.607]
 [58.607]
 [58.607]
 [58.607]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
line 256 mcts: sample exp_bonus 33.10326312977452
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  67.5572213927083
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.977]
 [34.977]
 [53.833]
 [34.977]
 [34.977]] [[0.503]
 [0.503]
 [1.019]
 [0.503]
 [0.503]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.348566276808054
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  27.791453937155264
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  25.2090229773229
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.445]
 [62.9  ]
 [39.029]
 [38.668]
 [59.829]] [[0.345]
 [0.646]
 [0.311]
 [0.306]
 [0.603]]
siam score:  -0.86610717
printing an ep nov before normalisation:  18.07279168700365
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.6961130846723
printing an ep nov before normalisation:  26.14458580860038
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  19.479150119834884
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8678868
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.40373039245545
printing an ep nov before normalisation:  43.14796507103366
printing an ep nov before normalisation:  59.59750499924051
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.77559951494301
printing an ep nov before normalisation:  49.26123551948527
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.0911069513745133
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.068316377130685
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9865,     0.0009,     0.0006,     0.0114],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0153,     0.9538,     0.0125,     0.0183],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0014,     0.0005,     0.0387,     0.7840,     0.1755],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0414, 0.0049, 0.1039, 0.2953, 0.5546], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.68654136342878
printing an ep nov before normalisation:  40.3780404538058
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.5290790382522
printing an ep nov before normalisation:  28.93119861383306
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.058]
 [33.807]
 [32.792]
 [33.807]
 [33.807]] [[1.331]
 [0.844]
 [0.796]
 [0.844]
 [0.844]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.47644873376306
siam score:  -0.8621885
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.5590750583002329
actions average: 
K:  1  action  0 :  tensor([    0.9894,     0.0004,     0.0000,     0.0013,     0.0090],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0020, 0.9694, 0.0025, 0.0015, 0.0246], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.8638,     0.0641,     0.0718],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0010, 0.0010, 0.0556, 0.7112, 0.2312], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0020, 0.1199, 0.0567, 0.2020, 0.6194], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  61.82306153095333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  17.374271154403687
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.02722229215606
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8720119
printing an ep nov before normalisation:  52.81408569958244
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.407]
 [42.407]
 [42.407]
 [42.37 ]
 [42.407]] [[0.726]
 [0.726]
 [0.726]
 [0.725]
 [0.726]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.80247688293457
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.645629108209498
printing an ep nov before normalisation:  22.774970531463623
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.937029732886536
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.433]
 [38.187]
 [27.743]
 [26.803]
 [33.49 ]] [[0.208]
 [0.228]
 [0.108]
 [0.097]
 [0.174]]
actions average: 
K:  1  action  0 :  tensor([    0.9984,     0.0001,     0.0000,     0.0000,     0.0014],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9701,     0.0043,     0.0000,     0.0255],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0013,     0.0007,     0.8823,     0.0361,     0.0796],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0026, 0.0050, 0.0345, 0.7524, 0.2055], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0361, 0.0040, 0.0777, 0.2122, 0.6700], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  31.773314476013184
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  14.606575595739056
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.387388982440925
printing an ep nov before normalisation:  56.45287299072209
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.582]
 [44.816]
 [40.18 ]
 [51.101]
 [44.619]] [[0.294]
 [0.18 ]
 [0.142]
 [0.232]
 [0.178]]
printing an ep nov before normalisation:  34.10240342534793
printing an ep nov before normalisation:  72.55318027352916
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[15.064]
 [15.064]
 [25.992]
 [15.064]
 [15.064]] [[0.936]
 [0.936]
 [2.   ]
 [0.936]
 [0.936]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9976,     0.0001,     0.0019,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0005,     0.9975,     0.0001,     0.0000,     0.0019],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0003,     0.8455,     0.0766,     0.0776],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0009, 0.0213, 0.0154, 0.8085, 0.1539], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0031, 0.0525, 0.1168, 0.3159, 0.5116], grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([    0.9720,     0.0007,     0.0236,     0.0014,     0.0022],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9656,     0.0035,     0.0002,     0.0305],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0004,     0.8626,     0.0490,     0.0879],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0042,     0.0003,     0.0877,     0.7932,     0.1147],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0010, 0.0346, 0.1251, 0.1785, 0.6607], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.75293004574039
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.536277870982545
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.419]
 [51.072]
 [46.236]
 [34.319]
 [44.736]] [[1.047]
 [1.344]
 [1.128]
 [0.596]
 [1.061]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.819363587585514
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.828546027064455
printing an ep nov before normalisation:  33.90775496564298
printing an ep nov before normalisation:  37.96426034496864
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.82578023114953
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.527]
 [29.302]
 [29.302]
 [29.302]
 [29.302]] [[0.333]
 [0.456]
 [0.456]
 [0.456]
 [0.456]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.488]
 [25.517]
 [24.569]
 [25.931]
 [23.748]] [[0.183]
 [0.163]
 [0.154]
 [0.167]
 [0.146]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8699153
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.02975088196064
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.71380713317425
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.425]
 [46.425]
 [46.425]
 [46.425]
 [46.425]] [[0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]]
printing an ep nov before normalisation:  30.699515342712402
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.26309999435547
printing an ep nov before normalisation:  37.158133549664484
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 6.751568734750173e-10
0.0 8.147204311391207e-10
0.0 0.0
0.0 8.238885874992873e-10
0.0 2.9785091238437574e-09
0.0 8.147204311391207e-10
0.0 0.0
0.0 2.2440187100729074e-09
0.0 1.6443866869794603e-09
0.0 1.1003517479251837e-09
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.60571472633925
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.337077604479845
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.50543210906975
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 3.8921418570289215e-12
0.0 5.466297004186285e-12
0.0 5.371155756984748e-12
0.0 0.0
0.0 2.361232732718154e-12
0.0 3.918089464574652e-12
0.0 0.0
0.0 6.292295999555322e-12
0.0 0.0
0.0 5.621982953947194e-14
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.343]
 [41.343]
 [41.343]
 [41.343]
 [41.343]] [[1.888]
 [1.888]
 [1.888]
 [1.888]
 [1.888]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.089008537586885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8685237
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.255]
 [34.494]
 [34.494]
 [24.221]
 [34.494]] [[1.   ]
 [1.559]
 [1.559]
 [0.862]
 [1.559]]
printing an ep nov before normalisation:  38.351505480842896
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.02609405581524
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.808]
 [51.699]
 [25.71 ]
 [21.105]
 [24.114]] [[0.174]
 [0.583]
 [0.216]
 [0.15 ]
 [0.193]]
printing an ep nov before normalisation:  54.23867340499512
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[72.859]
 [76.927]
 [72.859]
 [72.859]
 [72.859]] [[1.712]
 [1.827]
 [1.712]
 [1.712]
 [1.712]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.865220608737342
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.86994123
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.92926948638726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9863,     0.0000,     0.0127,     0.0002,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0012,     0.9248,     0.0007,     0.0018,     0.0715],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0002,     0.9262,     0.0181,     0.0556],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0003,     0.0426,     0.7912,     0.1657],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0159, 0.0582, 0.0726, 0.2008, 0.6524], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.922]
 [55.36 ]
 [53.922]
 [53.922]
 [53.922]] [[0.612]
 [0.635]
 [0.612]
 [0.612]
 [0.612]]
printing an ep nov before normalisation:  2.5823839462780995
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.132905733421893
printing an ep nov before normalisation:  0.042483188821864006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.128 0.103 0.051 0.641 0.077]
printing an ep nov before normalisation:  48.319454744890315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86436725
siam score:  -0.8655641
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.681]
 [43.242]
 [37.094]
 [39.687]
 [42.547]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9875,     0.0030,     0.0004,     0.0048,     0.0043],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.9704,     0.0001,     0.0001,     0.0289],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0115,     0.8746,     0.0489,     0.0648],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0066,     0.0758,     0.7866,     0.1308],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0015, 0.0217, 0.0878, 0.3187, 0.5703], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.803]
 [52.803]
 [52.803]
 [52.803]
 [52.803]] [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]]
printing an ep nov before normalisation:  52.48402285619704
printing an ep nov before normalisation:  29.810685620426515
actions average: 
K:  3  action  0 :  tensor([    0.9946,     0.0003,     0.0000,     0.0023,     0.0029],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0035,     0.9517,     0.0068,     0.0002,     0.0377],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0002,     0.9306,     0.0008,     0.0683],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0014,     0.0007,     0.0258,     0.8077,     0.1645],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0046, 0.0162, 0.0923, 0.3180, 0.5689], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 26.116666331690304
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.31 ]
 [44.515]
 [48.716]
 [48.459]
 [48.424]] [[0.722]
 [0.55 ]
 [0.656]
 [0.65 ]
 [0.649]]
printing an ep nov before normalisation:  41.468399276006835
printing an ep nov before normalisation:  45.85354828892476
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.741934804990954
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[15.288]
 [15.288]
 [15.288]
 [15.288]
 [15.288]] [[0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.29264557528296
printing an ep nov before normalisation:  46.549587353255575
printing an ep nov before normalisation:  79.7825179495981
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[72.539]
 [72.539]
 [72.539]
 [72.539]
 [72.539]] [[1.325]
 [1.325]
 [1.325]
 [1.325]
 [1.325]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.423]
 [30.423]
 [47.936]
 [30.423]
 [30.423]] [[0.532]
 [0.532]
 [1.009]
 [0.532]
 [0.532]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.944]
 [43.688]
 [ 0.   ]
 [ 0.   ]
 [22.87 ]] [[ 0.237]
 [ 0.601]
 [-0.205]
 [-0.205]
 [ 0.217]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.30815553665161
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.944]
 [41.995]
 [38.578]
 [40.294]
 [40.099]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[17.904]
 [25.183]
 [30.89 ]
 [26.54 ]
 [26.018]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.266]
 [37.166]
 [45.358]
 [41.572]
 [36.609]] [[0.61 ]
 [0.908]
 [1.322]
 [1.13 ]
 [0.88 ]]
printing an ep nov before normalisation:  46.608494014470374
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.753]
 [55.753]
 [55.753]
 [55.753]
 [55.753]] [[0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]]
printing an ep nov before normalisation:  62.43802337602491
printing an ep nov before normalisation:  48.47520112320011
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.23722031604336
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.395768150394424
printing an ep nov before normalisation:  76.23072628271018
printing an ep nov before normalisation:  36.40544462404971
printing an ep nov before normalisation:  59.413742089443
printing an ep nov before normalisation:  41.148173942713015
using explorer policy with actor:  1
siam score:  -0.86326116
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.854]
 [49.854]
 [49.854]
 [46.845]
 [49.854]] [[1.354]
 [1.354]
 [1.354]
 [1.216]
 [1.354]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.362]
 [37.362]
 [91.426]
 [37.362]
 [37.362]] [[0.476]
 [0.476]
 [1.54 ]
 [0.476]
 [0.476]]
printing an ep nov before normalisation:  64.20718602755636
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.952393646019445
siam score:  -0.86886626
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.82564353284474
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.36678224118623
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.34334011914251
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.427]
 [36.471]
 [36.471]
 [36.471]
 [36.471]] [[0.989]
 [0.579]
 [0.579]
 [0.579]
 [0.579]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.9767,     0.0050,     0.0006,     0.0005,     0.0173],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0005,     0.9643,     0.0002,     0.0004,     0.0347],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0029, 0.0053, 0.8980, 0.0330, 0.0608], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0014,     0.0499,     0.7706,     0.1779],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0004,     0.0110,     0.1652,     0.2321,     0.5913],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.525951519421945
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  53.735240036474536
actions average: 
K:  1  action  0 :  tensor([    0.9941,     0.0023,     0.0000,     0.0013,     0.0023],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0020,     0.9516,     0.0004,     0.0010,     0.0450],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0002,     0.9136,     0.0321,     0.0541],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0004,     0.0022,     0.8615,     0.1356],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0037, 0.0020, 0.1113, 0.1616, 0.7213], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.18594126270616
printing an ep nov before normalisation:  45.13561258093356
printing an ep nov before normalisation:  81.37130413122169
printing an ep nov before normalisation:  62.976955552834724
printing an ep nov before normalisation:  49.638298743499604
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.87457746
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 52.38546371459961
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.6254279519467
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.636]
 [51.355]
 [ 0.   ]
 [47.752]
 [ 0.   ]] [[ 0.36 ]
 [ 0.664]
 [-0.271]
 [ 0.599]
 [-0.271]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  47.84626447802494
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.9993,     0.0000,     0.0000,     0.0002,     0.0005],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.9576,     0.0029,     0.0001,     0.0387],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0271,     0.8792,     0.0328,     0.0608],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0003,     0.0731,     0.7562,     0.1703],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1001, 0.0435, 0.1087, 0.1801, 0.5676], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 73.56223569537107
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.71 ]
 [44.978]
 [48.169]
 [45.71 ]
 [45.71 ]] [[1.086]
 [1.053]
 [1.199]
 [1.086]
 [1.086]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 46.52787052084938
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.995476858956476
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0005533426497095206
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9981,     0.0001,     0.0000,     0.0009,     0.0009],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9967,     0.0002,     0.0005,     0.0026],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0002,     0.9188,     0.0127,     0.0683],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0004,     0.0014,     0.0298,     0.8457,     0.1227],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0024, 0.0241, 0.1125, 0.1846, 0.6763], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.868694
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.633]
 [47.633]
 [47.633]
 [47.633]
 [47.633]] [[1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.874433760248394
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.33371639251709
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.053]
 [41.012]
 [42.807]
 [38.733]
 [39.471]] [[0.734]
 [0.812]
 [0.884]
 [0.721]
 [0.75 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.983991622924805
siam score:  -0.8734883
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.528]
 [40.453]
 [40.347]
 [20.787]
 [41.328]] [[0.494]
 [0.973]
 [0.971]
 [0.5  ]
 [0.994]]
printing an ep nov before normalisation:  36.23659014701843
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.17367179933221
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.749496973514916
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.08 ]
 [30.312]
 [30.312]
 [30.312]
 [30.312]] [[1.089]
 [0.728]
 [0.728]
 [0.728]
 [0.728]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.647]
 [41.647]
 [41.647]
 [41.647]
 [41.647]] [[55.516]
 [55.516]
 [55.516]
 [55.516]
 [55.516]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.113]
 [4.29 ]
 [5.081]
 [5.816]
 [4.943]] [[0.206]
 [0.145]
 [0.171]
 [0.196]
 [0.167]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.31698747748705
printing an ep nov before normalisation:  62.8014994004503
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.02617400270026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 -1.6044273627069456e-12
0.0 0.0
0.0 9.970802507596605e-11
0.0 -1.6044273627069456e-12
0.0 1.3506597152649124e-10
0.0 -1.4054956682051868e-12
0.0 1.0904916552751903e-10
0.0 3.148310299067295e-11
0.0 4.9265866648535036e-11
0.0 8.718397749627453e-12
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.000951069887378253
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.62425001159393
printing an ep nov before normalisation:  36.26295566558838
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.9999,     0.0001,     0.0000,     0.0001,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0020, 0.9567, 0.0100, 0.0025, 0.0288], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0012,     0.9292,     0.0221,     0.0475],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0027, 0.0010, 0.0500, 0.7131, 0.2331], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0040, 0.0015, 0.0696, 0.2754, 0.6495], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.86616637238067
printing an ep nov before normalisation:  71.96245358225943
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.681529139564766
printing an ep nov before normalisation:  21.68210949542855
printing an ep nov before normalisation:  0.00014348275954034762
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.782]
 [76.313]
 [48.561]
 [69.313]
 [48.53 ]] [[0.36 ]
 [0.647]
 [0.322]
 [0.565]
 [0.321]]
using explorer policy with actor:  1
siam score:  -0.8719264
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.318689346313477
printing an ep nov before normalisation:  27.959085032122662
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.026]
 [37.026]
 [37.026]
 [37.026]
 [37.026]] [[0.98]
 [0.98]
 [0.98]
 [0.98]
 [0.98]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.59505551338997
printing an ep nov before normalisation:  69.40447725146787
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.06 ]
 [45.06 ]
 [45.06 ]
 [50.839]
 [45.06 ]] [[0.723]
 [0.723]
 [0.723]
 [0.9  ]
 [0.723]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.335538733552916
siam score:  -0.87117463
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.64464387557272
siam score:  -0.8709392
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.239]
 [36.239]
 [36.239]
 [36.239]
 [36.239]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8727585
printing an ep nov before normalisation:  54.136584073453164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.642911048440034
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 23.772374214221628
printing an ep nov before normalisation:  66.24278266535406
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.44056823889601
printing an ep nov before normalisation:  54.214141137467685
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9973,     0.0001,     0.0003,     0.0006,     0.0017],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9983,     0.0006,     0.0001,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9511,     0.0149,     0.0340],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0006,     0.0364,     0.7616,     0.2011],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0038, 0.0018, 0.0929, 0.2700, 0.6315], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8607944
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.542]
 [42.928]
 [42.928]
 [57.057]
 [42.928]] [[1.463]
 [0.951]
 [0.951]
 [1.483]
 [0.951]]
printing an ep nov before normalisation:  56.42286445913255
printing an ep nov before normalisation:  79.09391059697889
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.19350714071326
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 35.4969699382782
printing an ep nov before normalisation:  37.52330572747202
line 256 mcts: sample exp_bonus 42.41830074648277
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.46058686294167
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.03 ]
 [48.078]
 [48.078]
 [48.078]
 [48.078]] [[1.21 ]
 [1.296]
 [1.296]
 [1.296]
 [1.296]]
line 256 mcts: sample exp_bonus 30.521373748779297
printing an ep nov before normalisation:  35.06101264275154
printing an ep nov before normalisation:  50.87749498001588
printing an ep nov before normalisation:  43.99876594543457
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86371666
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.54 ]
 [33.54 ]
 [33.54 ]
 [39.808]
 [33.54 ]] [[1.377]
 [1.377]
 [1.377]
 [1.867]
 [1.377]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.71681421240862
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.26168115976681
actions average: 
K:  3  action  0 :  tensor([    0.9996,     0.0000,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9938,     0.0034,     0.0001,     0.0027],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0002,     0.8450,     0.0626,     0.0922],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0007,     0.0006,     0.0556,     0.7171,     0.2260],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0006,     0.0398,     0.1137,     0.1660,     0.6800],
       grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.46686532634306
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.008069201321063701
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.86484909057617
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9664,     0.0020,     0.0000,     0.0065,     0.0252],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9597,     0.0002,     0.0001,     0.0399],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0004,     0.8700,     0.0462,     0.0835],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0008,     0.0380,     0.8144,     0.1467],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0269, 0.0250, 0.0449, 0.1984, 0.7048], grad_fn=<DivBackward0>)
siam score:  -0.87821424
printing an ep nov before normalisation:  44.4397361494071
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  59.305514982625276
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.387]
 [46.434]
 [36.387]
 [46.91 ]
 [36.387]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9976,     0.0001,     0.0000,     0.0008,     0.0015],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0013,     0.9771,     0.0007,     0.0015,     0.0194],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0023,     0.0002,     0.9254,     0.0306,     0.0415],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0002,     0.0001,     0.8861,     0.1135],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0009, 0.0021, 0.1196, 0.1605, 0.7169], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0006682332332275109
printing an ep nov before normalisation:  37.308337688446045
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.166]
 [30.993]
 [35.467]
 [34.856]
 [32.117]] [[1.254]
 [0.788]
 [1.016]
 [0.985]
 [0.845]]
printing an ep nov before normalisation:  38.68967729239139
printing an ep nov before normalisation:  58.62318843736942
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8740082
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.22589650709206
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.78060570460358
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.154 0.128 0.103 0.513 0.103]
printing an ep nov before normalisation:  38.55824556860885
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.705]
 [46.77 ]
 [43.016]
 [44.364]
 [43.896]] [[0.201]
 [0.179]
 [0.151]
 [0.162]
 [0.158]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.260868879731476
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.424]
 [39.785]
 [39.657]
 [42.098]
 [39.678]] [[0.132]
 [0.32 ]
 [0.318]
 [0.353]
 [0.319]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9995,     0.0000,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0040, 0.9450, 0.0074, 0.0011, 0.0425], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0054,     0.0001,     0.9144,     0.0002,     0.0799],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0003,     0.0346,     0.8609,     0.1040],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0014, 0.0011, 0.0455, 0.2690, 0.6830], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.82 ]
 [36.701]
 [36.701]
 [41.358]
 [36.701]] [[1.347]
 [0.994]
 [0.994]
 [1.225]
 [0.994]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.83850267718429
siam score:  -0.8592761
siam score:  -0.8596202
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.221]
 [52.662]
 [61.652]
 [53.788]
 [55.207]] [[1.588]
 [1.502]
 [2.   ]
 [1.564]
 [1.643]]
printing an ep nov before normalisation:  49.70898628234863
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[103.208]
 [103.208]
 [100.757]
 [102.88 ]
 [103.693]] [[0.973]
 [0.973]
 [0.946]
 [0.97 ]
 [0.979]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.94713592529297
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.561475908793525
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.45797727124671
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.62540624542093
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.992]
 [49.771]
 [28.833]
 [44.076]
 [45.201]] [[0.997]
 [1.181]
 [0.684]
 [1.046]
 [1.073]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.45179396640975256
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.337]
 [23.337]
 [23.337]
 [33.753]
 [23.337]] [[0.483]
 [0.483]
 [0.483]
 [0.893]
 [0.483]]
printing an ep nov before normalisation:  36.0291404336705
printing an ep nov before normalisation:  53.249952046561674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.78340423841981
printing an ep nov before normalisation:  41.38716697692871
printing an ep nov before normalisation:  42.61982106277779
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.73480701541302
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.35601615905762
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9963,     0.0001,     0.0000,     0.0025,     0.0011],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0035,     0.9940,     0.0001,     0.0012,     0.0012],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9771,     0.0119,     0.0109],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0044, 0.0010, 0.0289, 0.7897, 0.1759], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0100, 0.0658, 0.1838, 0.1641, 0.5763], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.968]
 [28.968]
 [50.832]
 [28.968]
 [28.968]] [[0.512]
 [0.512]
 [1.15 ]
 [0.512]
 [0.512]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.8107288258311
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.86723635634393
printing an ep nov before normalisation:  21.301906449454172
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.9957755741339724
actions average: 
K:  3  action  0 :  tensor([    0.9873,     0.0005,     0.0001,     0.0077,     0.0044],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0066,     0.9860,     0.0006,     0.0005,     0.0063],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0047,     0.8765,     0.0615,     0.0571],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0004,     0.0547,     0.8355,     0.1092],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0736, 0.0501, 0.1224, 0.1191, 0.6348], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  90.56108902965273
printing an ep nov before normalisation:  67.84286124526736
printing an ep nov before normalisation:  27.996684098017553
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  84.43170656591317
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9967,     0.0002,     0.0000,     0.0014,     0.0017],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.9905,     0.0007,     0.0003,     0.0080],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0227,     0.9402,     0.0136,     0.0233],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0012,     0.1203,     0.7280,     0.1502],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0084, 0.1028, 0.0718, 0.2549, 0.5620], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.810726836238
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9422,     0.0164,     0.0031,     0.0007,     0.0377],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9234,     0.0183,     0.0010,     0.0569],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9966,     0.0013,     0.0021],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0013, 0.0008, 0.0385, 0.7915, 0.1679], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0010, 0.0092, 0.1019, 0.2243, 0.6635], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.887]
 [45.088]
 [39.887]
 [39.887]
 [39.887]] [[1.359]
 [1.728]
 [1.359]
 [1.359]
 [1.359]]
printing an ep nov before normalisation:  37.19488859176636
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.03215706191686
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.909784241294176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.868824
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.128 0.154 0.205 0.154 0.359]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.314]
 [31.266]
 [30.419]
 [25.057]
 [36.314]] [[1.261]
 [1.   ]
 [0.956]
 [0.679]
 [1.261]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  97.232423126297
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.898381233215332
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.178]
 [59.693]
 [36.253]
 [36.253]
 [36.253]] [[0.683]
 [0.969]
 [0.434]
 [0.434]
 [0.434]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.9973,     0.0003,     0.0008,     0.0005,     0.0010],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0073,     0.9696,     0.0012,     0.0003,     0.0216],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0025, 0.0026, 0.8727, 0.0508, 0.0715], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0170,     0.0005,     0.0615,     0.7880,     0.1330],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0161, 0.0020, 0.1650, 0.3021, 0.5147], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.62724686783838
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.429]
 [33.429]
 [33.429]
 [33.429]
 [33.429]] [[0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.384]
 [62.958]
 [58.384]
 [58.384]
 [64.182]] [[0.789]
 [0.882]
 [0.789]
 [0.789]
 [0.907]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.349]
 [65.349]
 [65.349]
 [65.349]
 [65.349]] [[0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.53745140892713
line 256 mcts: sample exp_bonus 46.398233208584365
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.918100953185586
printing an ep nov before normalisation:  29.040803909301758
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.25400831039378
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.84 ]
 [56.406]
 [58.884]
 [57.036]
 [56.367]] [[0.618]
 [1.02 ]
 [1.094]
 [1.039]
 [1.019]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.9986,     0.0000,     0.0002,     0.0001,     0.0011],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9963,     0.0000,     0.0004,     0.0032],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0004,     0.0002,     0.8083,     0.1038,     0.0872],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0004,     0.0467,     0.8587,     0.0940],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0767, 0.0492, 0.1234, 0.1561, 0.5946], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.462]
 [32.029]
 [50.593]
 [38.078]
 [37.287]] [[0.47 ]
 [0.432]
 [0.93 ]
 [0.594]
 [0.573]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87578833
printing an ep nov before normalisation:  40.64496040344238
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.69855785369873
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87469405
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9242,     0.0400,     0.0003,     0.0002,     0.0353],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9789,     0.0113,     0.0004,     0.0095],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0152,     0.8906,     0.0180,     0.0762],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0004,     0.0979,     0.7585,     0.1431],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0008, 0.0550, 0.1215, 0.1709, 0.6517], grad_fn=<DivBackward0>)
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  22.20953607309474
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.137]
 [31.353]
 [30.342]
 [31.488]
 [33.682]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  24.89823818206787
printing an ep nov before normalisation:  53.72969902226451
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.562]
 [57.562]
 [57.562]
 [57.562]
 [57.562]] [[0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]]
printing an ep nov before normalisation:  53.44779819063124
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.547233762679994
printing an ep nov before normalisation:  62.78542436540517
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.498]
 [42.498]
 [42.498]
 [ 0.587]
 [42.498]] [[1.333]
 [1.333]
 [1.333]
 [0.016]
 [1.333]]
printing an ep nov before normalisation:  41.60831129606363
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87331575
printing an ep nov before normalisation:  42.35988930130565
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.06 ]
 [31.868]
 [27.812]
 [31.384]
 [31.743]] [[1.051]
 [0.67 ]
 [0.518]
 [0.652]
 [0.665]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.249137289763556
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.39560465369946
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.25387733555873
siam score:  -0.86884826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.771]
 [54.168]
 [53.563]
 [51.124]
 [53.496]] [[1.3  ]
 [1.427]
 [1.395]
 [1.265]
 [1.391]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86913764
printing an ep nov before normalisation:  40.96641540527344
printing an ep nov before normalisation:  57.03021085428969
printing an ep nov before normalisation:  23.376196274763135
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8709172
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86967
UNIT TEST: sample policy line 217 mcts : [0.179 0.282 0.308 0.154 0.077]
actions average: 
K:  0  action  0 :  tensor([    0.9991,     0.0000,     0.0000,     0.0004,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0010,     0.9316,     0.0415,     0.0003,     0.0256],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0002,     0.9267,     0.0271,     0.0458],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0024, 0.0013, 0.0708, 0.7771, 0.1484], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0396, 0.0008, 0.0078, 0.2465, 0.7053], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.742806157968005
printing an ep nov before normalisation:  43.357812179791864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.770127078775495
printing an ep nov before normalisation:  64.31934364442631
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.71500674217498
printing an ep nov before normalisation:  55.92263190658605
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.288]
 [71.288]
 [71.288]
 [71.288]
 [71.288]] [[1.195]
 [1.195]
 [1.195]
 [1.195]
 [1.195]]
printing an ep nov before normalisation:  41.32616996765137
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.2467631048205
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.65447815805722
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.169]
 [35.265]
 [69.935]
 [31.108]
 [43.758]] [[0.49 ]
 [0.518]
 [1.402]
 [0.412]
 [0.734]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  79.85298744532939
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.31436055535237
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9597,     0.0262,     0.0005,     0.0012,     0.0124],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9768,     0.0002,     0.0002,     0.0224],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0002,     0.8777,     0.0543,     0.0678],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0007,     0.0004,     0.0353,     0.7836,     0.1800],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0010, 0.0022, 0.0606, 0.2922, 0.6441], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.218]
 [45.218]
 [45.218]
 [47.596]
 [45.218]] [[0.894]
 [0.894]
 [0.894]
 [0.942]
 [0.894]]
printing an ep nov before normalisation:  25.735790972382393
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.066009738148765
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.777916501383544
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 96.50890095590617
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 45.380893198561566
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  61.799679659146356
printing an ep nov before normalisation:  63.67527083143023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.751]
 [59.269]
 [26.064]
 [26.948]
 [22.342]] [[0.294]
 [0.973]
 [0.279]
 [0.298]
 [0.201]]
printing an ep nov before normalisation:  74.68826647584443
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.395]
 [47.163]
 [38.97 ]
 [40.395]
 [40.395]] [[1.012]
 [1.248]
 [0.963]
 [1.012]
 [1.012]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 1.0359670724695437e-09
0.0 1.2622648490902682e-10
0.0 1.9491846401109785e-10
0.0 5.849629729274913e-10
0.0 3.506041381279286e-10
0.0 3.506041381279286e-10
0.0 3.799768353174271e-10
0.0 0.0
0.0 0.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.97618257706709
siam score:  -0.86869705
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.317188441248376
printing an ep nov before normalisation:  33.237349253281685
printing an ep nov before normalisation:  84.18130652806501
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9993,     0.0002,     0.0000,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9614,     0.0004,     0.0003,     0.0375],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9403,     0.0301,     0.0296],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0017,     0.0007,     0.0429,     0.7653,     0.1895],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0183, 0.0349, 0.1165, 0.1845, 0.6457], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.948493003845215
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8588072
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  31.45604522459017
siam score:  -0.86022025
actions average: 
K:  2  action  0 :  tensor([    0.9681,     0.0119,     0.0009,     0.0002,     0.0189],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9738,     0.0027,     0.0001,     0.0232],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9337,     0.0325,     0.0338],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0002,     0.0172,     0.8083,     0.1742],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0041, 0.0294, 0.0412, 0.2031, 0.7221], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.005]
 [30.796]
 [32.005]
 [44.496]
 [40.993]] [[0.31 ]
 [0.288]
 [0.31 ]
 [0.54 ]
 [0.476]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.077 0.205 0.231 0.333 0.154]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.605]
 [52.628]
 [65.688]
 [53.603]
 [53.828]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.624]
 [55.787]
 [36.624]
 [43.258]
 [36.624]] [[0.291]
 [0.729]
 [0.291]
 [0.443]
 [0.291]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.54003984791654
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9950,     0.0006,     0.0000,     0.0035,     0.0009],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9760,     0.0048,     0.0002,     0.0189],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0001,     0.9349,     0.0371,     0.0275],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0002,     0.0017,     0.7871,     0.2108],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0010, 0.0007, 0.0446, 0.2801, 0.6736], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[96.925]
 [96.925]
 [96.925]
 [96.925]
 [96.925]] [[1.287]
 [1.287]
 [1.287]
 [1.287]
 [1.287]]
printing an ep nov before normalisation:  63.90096573267419
printing an ep nov before normalisation:  34.5197024456437
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.649]
 [54.216]
 [55.785]
 [54.571]
 [54.994]] [[0.682]
 [0.741]
 [0.776]
 [0.749]
 [0.758]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8763563
printing an ep nov before normalisation:  26.34290197730579
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8735278
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.431535289238575
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87344134
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[9.807]
 [8.977]
 [5.895]
 [9.106]
 [8.941]] [[0.334]
 [0.305]
 [0.2  ]
 [0.31 ]
 [0.304]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.91281096519842
actions average: 
K:  0  action  0 :  tensor([0.9855, 0.0024, 0.0043, 0.0045, 0.0033], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0015,     0.9922,     0.0031,     0.0008,     0.0024],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0018,     0.9036,     0.0264,     0.0680],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0036,     0.0007,     0.0530,     0.7562,     0.1865],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0090, 0.0332, 0.0770, 0.2292, 0.6515], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9125,     0.0505,     0.0003,     0.0101,     0.0267],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0005,     0.9594,     0.0004,     0.0006,     0.0391],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0005,     0.8633,     0.0381,     0.0980],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0005,     0.0484,     0.8140,     0.1368],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0007, 0.0799, 0.0746, 0.2383, 0.6065], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  80.38022650469907
printing an ep nov before normalisation:  80.64318794409627
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.58311153299636
printing an ep nov before normalisation:  29.80000921093211
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.426]
 [58.616]
 [52.426]
 [52.426]
 [52.426]] [[1.053]
 [1.269]
 [1.053]
 [1.053]
 [1.053]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.659]
 [58.26 ]
 [56.339]
 [51.368]
 [52.086]] [[0.291]
 [0.436]
 [0.414]
 [0.357]
 [0.365]]
actions average: 
K:  1  action  0 :  tensor([    0.9774,     0.0011,     0.0005,     0.0028,     0.0181],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9699,     0.0017,     0.0002,     0.0282],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0078,     0.9023,     0.0444,     0.0455],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0012,     0.0007,     0.0368,     0.7806,     0.1807],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0638, 0.0402, 0.0953, 0.2231, 0.5777], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  92.53626126179293
printing an ep nov before normalisation:  59.73641948779262
printing an ep nov before normalisation:  70.44498531283327
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.464]
 [27.464]
 [27.464]
 [40.185]
 [27.464]] [[0.377]
 [0.377]
 [0.377]
 [0.721]
 [0.377]]
siam score:  -0.8575384
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.894584585066944
printing an ep nov before normalisation:  50.95196438326587
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.530827045440674
printing an ep nov before normalisation:  73.9437438461731
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.038456400559646
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.45015545288978
printing an ep nov before normalisation:  46.54656198008082
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8575856
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.8604346
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  58.517772833305344
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.2399904258484
actions average: 
K:  3  action  0 :  tensor([    0.9995,     0.0001,     0.0001,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0127, 0.9272, 0.0139, 0.0011, 0.0452], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0023,     0.8839,     0.0402,     0.0735],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0029, 0.0009, 0.0366, 0.7739, 0.1856], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0125, 0.0211, 0.0619, 0.1895, 0.7148], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.671]
 [62.671]
 [62.671]
 [62.671]
 [62.671]] [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.44434864421851
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.851]
 [54.498]
 [50.865]
 [46.428]
 [48.661]] [[1.028]
 [1.654]
 [1.544]
 [1.409]
 [1.477]]
actions average: 
K:  1  action  0 :  tensor([    0.9987,     0.0001,     0.0000,     0.0007,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9686,     0.0032,     0.0008,     0.0267],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9410,     0.0245,     0.0345],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0000,     0.0001,     0.0032,     0.8445,     0.1522],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0016, 0.0694, 0.0516, 0.2040, 0.6733], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  88.43015716777023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.76763210427076
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.8735049
siam score:  -0.8745637
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.191888136102925
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.87181973
printing an ep nov before normalisation:  26.46477578392275
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86980057
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  70.6065498310624
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.9800,     0.0028,     0.0001,     0.0014,     0.0157],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9688,     0.0009,     0.0000,     0.0302],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0269,     0.8710,     0.0127,     0.0892],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0009,     0.0759,     0.7685,     0.1545],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0002,     0.0648,     0.0674,     0.2911,     0.5765],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.69762967441128
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.01639937226994
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9965,     0.0001,     0.0013,     0.0007,     0.0014],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0007,     0.9764,     0.0026,     0.0004,     0.0199],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0006,     0.0092,     0.8720,     0.0449,     0.0733],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0004,     0.0006,     0.0381,     0.6937,     0.2671],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0011, 0.0029, 0.0090, 0.2981, 0.6889], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  64.03704796107134
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.44289013575998
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.334]
 [47.562]
 [36.481]
 [39.158]
 [47.913]] [[0.448]
 [0.376]
 [0.237]
 [0.27 ]
 [0.38 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.69599319316393
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.360190344848206
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
siam score:  -0.8646873
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.488]
 [26.488]
 [26.488]
 [30.564]
 [26.488]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.86329865
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.75894411555569
actions average: 
K:  4  action  0 :  tensor([    0.9760,     0.0017,     0.0000,     0.0112,     0.0111],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.8795,     0.0011,     0.0009,     0.1182],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0020,     0.0002,     0.8956,     0.0144,     0.0878],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0008,     0.0605,     0.6709,     0.2677],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0004,     0.0290,     0.0950,     0.2622,     0.6134],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.47 ]
 [36.678]
 [38.951]
 [40.659]
 [41.062]] [[0.443]
 [0.252]
 [0.281]
 [0.303]
 [0.309]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.35]
 [31.35]
 [31.35]
 [31.35]
 [31.35]] [[0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  53.18782963573498
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.715]
 [53.17 ]
 [44.715]
 [45.03 ]
 [44.715]] [[0.442]
 [0.602]
 [0.442]
 [0.448]
 [0.442]]
line 256 mcts: sample exp_bonus 46.48075529171373
printing an ep nov before normalisation:  27.780921642788627
printing an ep nov before normalisation:  41.15975569464263
printing an ep nov before normalisation:  38.456382751464844
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.131]
 [68.56 ]
 [69.237]
 [68.131]
 [66.028]] [[1.216]
 [1.229]
 [1.248]
 [1.216]
 [1.156]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.95007699467576
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.823]
 [65.604]
 [42.391]
 [45.   ]
 [56.027]] [[0.47 ]
 [0.738]
 [0.477]
 [0.506]
 [0.63 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.29911407660591
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8675279
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.510545229234424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.91943302011107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.6264264918057
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[81.458]
 [81.458]
 [81.458]
 [81.458]
 [81.458]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
printing an ep nov before normalisation:  41.3216656645237
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.068]
 [33.501]
 [31.068]
 [31.068]
 [31.068]] [[1.055]
 [1.196]
 [1.055]
 [1.055]
 [1.055]]
printing an ep nov before normalisation:  43.75239847506653
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8696752
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.689]
 [32.689]
 [32.689]
 [42.255]
 [32.689]] [[0.719]
 [0.719]
 [0.719]
 [1.014]
 [0.719]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.345]
 [44.345]
 [44.345]
 [44.345]
 [44.345]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.087069562109487
line 256 mcts: sample exp_bonus 63.17028694882899
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9961,     0.0005,     0.0000,     0.0010,     0.0024],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0018,     0.9657,     0.0022,     0.0002,     0.0302],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9070,     0.0348,     0.0582],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0007,     0.0014,     0.0341,     0.7750,     0.1887],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0016, 0.0373, 0.0956, 0.1520, 0.7135], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  52.067427020100894
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.9928,     0.0002,     0.0001,     0.0020,     0.0050],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9644,     0.0100,     0.0009,     0.0245],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0004,     0.0001,     0.9068,     0.0135,     0.0792],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0007,     0.0003,     0.0134,     0.8307,     0.1548],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0093, 0.0295, 0.0537, 0.2227, 0.6848], grad_fn=<DivBackward0>)
siam score:  -0.8628137
siam score:  -0.86446106
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.93110275268555
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  66.0004806953142
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.458571821454335
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[77.617]
 [79.297]
 [77.617]
 [77.617]
 [77.617]] [[1.072]
 [1.105]
 [1.072]
 [1.072]
 [1.072]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.159]
 [44.159]
 [55.207]
 [52.844]
 [44.159]] [[0.742]
 [0.742]
 [1.059]
 [0.991]
 [0.742]]
printing an ep nov before normalisation:  56.51474558453939
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  38.30810138320609
printing an ep nov before normalisation:  23.301233651506084
printing an ep nov before normalisation:  52.20980700616152
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.81437494403506
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.323]
 [29.606]
 [52.991]
 [33.811]
 [26.252]] [[0.302]
 [0.352]
 [0.861]
 [0.444]
 [0.279]]
printing an ep nov before normalisation:  0.1075436270716068
printing an ep nov before normalisation:  35.46243923839381
printing an ep nov before normalisation:  40.257564276851184
printing an ep nov before normalisation:  33.74893069267273
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8655997
printing an ep nov before normalisation:  25.832673099588614
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.332]
 [43.332]
 [43.332]
 [43.332]
 [43.332]] [[28.902]
 [28.902]
 [28.902]
 [28.902]
 [28.902]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.973]
 [45.557]
 [37.93 ]
 [43.204]
 [36.399]] [[0.541]
 [1.045]
 [0.762]
 [0.957]
 [0.705]]
printing an ep nov before normalisation:  48.953752517700195
printing an ep nov before normalisation:  49.08404397684644
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.41649303911968
printing an ep nov before normalisation:  62.503375556872584
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.563]
 [50.179]
 [44.563]
 [44.563]
 [44.563]] [[1.53 ]
 [1.873]
 [1.53 ]
 [1.53 ]
 [1.53 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.44393062591553
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.753378544111726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8672452
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.761]
 [51.761]
 [51.761]
 [49.111]
 [51.761]] [[1.708]
 [1.708]
 [1.708]
 [1.587]
 [1.708]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.55831115020793
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.90499992726141
printing an ep nov before normalisation:  58.18726621793349
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.63899578673982
printing an ep nov before normalisation:  53.456100869258265
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86597353
printing an ep nov before normalisation:  40.957675380157966
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.3239900805641
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 60.65092712924863
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  50.04987716674805
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.96 ]
 [50.85 ]
 [35.815]
 [35.856]
 [38.184]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  47.257914543151855
printing an ep nov before normalisation:  40.72916802897032
printing an ep nov before normalisation:  44.62723731994629
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  70.232812164803
printing an ep nov before normalisation:  50.08424240190956
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.216]
 [34.847]
 [22.135]
 [26.628]
 [27.886]] [[0.562]
 [0.492]
 [0.228]
 [0.321]
 [0.347]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  109.98027779101227
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[74.258]
 [75.202]
 [75.202]
 [75.202]
 [75.202]] [[1.309]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.381]
 [39.834]
 [40.475]
 [40.475]
 [40.475]] [[0.975]
 [0.455]
 [0.469]
 [0.469]
 [0.469]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.9995,     0.0002,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0007,     0.9646,     0.0007,     0.0001,     0.0340],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0003,     0.9516,     0.0120,     0.0360],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0013, 0.0011, 0.0251, 0.7707, 0.2018], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0155, 0.0118, 0.0520, 0.1853, 0.7354], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.56332040430679
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.42231580946181
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.596]
 [50.189]
 [43.992]
 [39.061]
 [49.98 ]] [[0.251]
 [0.216]
 [0.167]
 [0.127]
 [0.214]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  4.723634753598844
actions average: 
K:  4  action  0 :  tensor([    0.9424,     0.0366,     0.0000,     0.0006,     0.0204],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0012,     0.9653,     0.0004,     0.0004,     0.0327],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0044,     0.7940,     0.0391,     0.1625],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0074,     0.0006,     0.0499,     0.7667,     0.1754],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0428, 0.1043, 0.0469, 0.1353, 0.6707], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.52587789473188
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8697043
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[104.099]
 [ 95.52 ]
 [ 95.52 ]
 [ 98.727]
 [ 95.52 ]] [[1.917]
 [1.729]
 [1.729]
 [1.799]
 [1.729]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[14.921]
 [10.994]
 [14.831]
 [11.231]
 [14.84 ]] [[0.664]
 [0.489]
 [0.66 ]
 [0.5  ]
 [0.661]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  61.91990006466027
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.484]
 [47.484]
 [47.484]
 [57.691]
 [47.484]] [[0.685]
 [0.685]
 [0.685]
 [0.89 ]
 [0.685]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [48.967]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.716]
 [ 0.816]
 [-0.716]
 [-0.716]
 [-0.716]]
siam score:  -0.8753041
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  17.49475111975869
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[14.192]
 [14.192]
 [14.192]
 [16.102]
 [14.192]] [[0.589]
 [0.589]
 [0.589]
 [0.674]
 [0.589]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.32065841566985
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.35252857208252
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.193598059605925
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.24023908356064
printing an ep nov before normalisation:  44.56173387192393
printing an ep nov before normalisation:  47.8688056842903
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.311]
 [61.311]
 [73.711]
 [61.311]
 [61.311]] [[1.424]
 [1.424]
 [1.774]
 [1.424]
 [1.424]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.181]
 [27.796]
 [28.219]
 [32.427]
 [27.052]] [[0.863]
 [0.841]
 [0.865]
 [1.106]
 [0.798]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.225]
 [58.326]
 [48.948]
 [48.948]
 [57.359]] [[1.069]
 [1.231]
 [0.933]
 [0.933]
 [1.201]]
printing an ep nov before normalisation:  50.963705555149254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.725]
 [37.214]
 [45.315]
 [45.999]
 [41.121]] [[0.508]
 [0.494]
 [0.72 ]
 [0.739]
 [0.603]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.787]
 [33.778]
 [32.572]
 [35.091]
 [31.463]] [[0.627]
 [0.825]
 [0.765]
 [0.89 ]
 [0.71 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.08572643953309
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  84.15913786050278
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.71791831331503
printing an ep nov before normalisation:  48.3389667240659
printing an ep nov before normalisation:  55.136937243308694
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.807]
 [57.187]
 [46.807]
 [46.807]
 [46.807]] [[0.849]
 [1.208]
 [0.849]
 [0.849]
 [0.849]]
printing an ep nov before normalisation:  26.539530754089355
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.852]
 [65.327]
 [51.852]
 [51.852]
 [51.852]] [[0.792]
 [1.152]
 [0.792]
 [0.792]
 [0.792]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 29.46734320172018
printing an ep nov before normalisation:  52.27746307184495
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.359165391456884
printing an ep nov before normalisation:  44.878825223316696
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[75.778]
 [75.778]
 [75.778]
 [76.768]
 [75.778]] [[1.967]
 [1.967]
 [1.967]
 [2.   ]
 [1.967]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  92.75440192234187
printing an ep nov before normalisation:  31.608848571777344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  91.30977848581254
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.26483675792006
printing an ep nov before normalisation:  60.858077050072296
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.45145153524289
printing an ep nov before normalisation:  3.0497491172553737
printing an ep nov before normalisation:  47.594104006660714
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.16577268448844
printing an ep nov before normalisation:  0.006067595026024719
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.07411011917009
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.95549821853638
siam score:  -0.86964226
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  88.88204610650051
printing an ep nov before normalisation:  71.47228325089235
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.21932315826416
printing an ep nov before normalisation:  47.3090738988477
printing an ep nov before normalisation:  36.46605876080539
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.93861870146234
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.813]
 [32.385]
 [25.813]
 [25.813]
 [25.813]] [[0.99]
 [1.47]
 [0.99]
 [0.99]
 [0.99]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9972,     0.0013,     0.0000,     0.0001,     0.0014],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0007,     0.9812,     0.0008,     0.0003,     0.0170],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0020,     0.8468,     0.0584,     0.0927],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0004,     0.0625,     0.7950,     0.1418],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0153, 0.0269, 0.0405, 0.1906, 0.7267], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  62.31057552242842
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.361506208968414
printing an ep nov before normalisation:  27.99595594406128
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.000248432159424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.026 0.026 0.538 0.282 0.128]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.419821341921455
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  27.11800593135104
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[79.927]
 [76.248]
 [79.927]
 [79.927]
 [80.834]] [[1.789]
 [1.676]
 [1.789]
 [1.789]
 [1.817]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[73.106]
 [76.979]
 [73.106]
 [73.106]
 [76.292]] [[1.85 ]
 [2.   ]
 [1.85 ]
 [1.85 ]
 [1.973]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.20300048472123
actions average: 
K:  4  action  0 :  tensor([    0.9993,     0.0001,     0.0003,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9881,     0.0030,     0.0000,     0.0088],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0016,     0.0004,     0.8621,     0.0609,     0.0751],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0011,     0.0287,     0.8307,     0.1391],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0006,     0.0356,     0.0020,     0.2682,     0.6935],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.819044647935634
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.179 0.077 0.436 0.154 0.154]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.014143393879066934
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.83708217115614
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  69.73238321588506
siam score:  -0.872572
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8727714
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.37929009928206
printing an ep nov before normalisation:  78.07338801214597
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.42139268802807
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9711,     0.0095,     0.0000,     0.0025,     0.0168],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9555,     0.0026,     0.0000,     0.0419],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0030,     0.9685,     0.0032,     0.0253],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0184,     0.0300,     0.6646,     0.2869],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0006,     0.0010,     0.0576,     0.2588,     0.6819],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  0.0013908758961633794
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.5  ]
 [35.5  ]
 [35.5  ]
 [36.554]
 [35.5  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.96950516284386
printing an ep nov before normalisation:  66.59671990111684
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  75.15878169184796
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.1409056331594
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  74.96993543119125
siam score:  -0.86560494
printing an ep nov before normalisation:  44.85318405053762
siam score:  -0.86851823
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0006840220891035642
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.18298788617493
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.747]
 [52.652]
 [54.037]
 [42.009]
 [52.253]] [[1.025]
 [0.959]
 [1.002]
 [0.627]
 [0.947]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.81582759345215
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.806]
 [48.709]
 [42.719]
 [48.205]
 [48.648]] [[0.584]
 [0.544]
 [0.428]
 [0.534]
 [0.542]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0025657655896793585
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.87236976623535
printing an ep nov before normalisation:  52.526827737619435
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87187034
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.604]
 [36.216]
 [36.216]
 [40.659]
 [36.216]] [[0.667]
 [0.766]
 [0.766]
 [0.934]
 [0.766]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.50658618345852
siam score:  -0.8724119
printing an ep nov before normalisation:  64.07678521394132
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  100.74570807931018
printing an ep nov before normalisation:  56.74725108676486
printing an ep nov before normalisation:  44.09946921059201
actions average: 
K:  0  action  0 :  tensor([    0.9996,     0.0001,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9619,     0.0118,     0.0000,     0.0261],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0001,     0.9612,     0.0141,     0.0245],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0005,     0.0008,     0.0257,     0.7850,     0.1881],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0014, 0.0340, 0.0229, 0.4745, 0.4672], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.512569610430518
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.566]
 [51.982]
 [42.566]
 [42.566]
 [42.566]] [[0.873]
 [1.333]
 [0.873]
 [0.873]
 [0.873]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  31.534346934493964
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 49.34997432183847
printing an ep nov before normalisation:  33.643248081207275
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.947606086730957
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.444277907665594
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.117971606419374
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.9882,     0.0001,     0.0042,     0.0038,     0.0038],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0021,     0.9636,     0.0001,     0.0020,     0.0322],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0006,     0.0011,     0.8496,     0.0409,     0.1077],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0005,     0.0003,     0.0360,     0.8335,     0.1297],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0010, 0.0835, 0.0623, 0.1890, 0.6642], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.18356973966163
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.345]
 [30.365]
 [27.638]
 [40.402]
 [28.71 ]] [[0.687]
 [0.647]
 [0.537]
 [1.055]
 [0.58 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8687886
printing an ep nov before normalisation:  37.14453295011274
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.733943297421604
siam score:  -0.8700975
printing an ep nov before normalisation:  78.54184537400022
printing an ep nov before normalisation:  67.70828097662148
printing an ep nov before normalisation:  50.63128405636337
actions average: 
K:  2  action  0 :  tensor([    0.9498,     0.0004,     0.0000,     0.0070,     0.0428],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0107,     0.9585,     0.0003,     0.0001,     0.0304],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0016, 0.0009, 0.9184, 0.0238, 0.0552], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0011,     0.0207,     0.8753,     0.1028],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0118, 0.0769, 0.0758, 0.2105, 0.6251], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 1.615671329307587e-10
0.0 6.722161442033806e-11
0.0 1.657187509258741e-10
0.0 7.316534747900411e-10
0.0 4.078964662491527e-11
0.0 2.618979007231496e-10
0.0 1.4139718893162584e-10
0.0 2.6369693517819453e-10
0.0 6.67718557965884e-10
0.0 6.178991423273746e-10
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.88148872204751
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.519095923440002
printing an ep nov before normalisation:  44.18118398417135
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.879]
 [46.879]
 [46.879]
 [57.507]
 [46.879]] [[0.989]
 [0.989]
 [0.989]
 [1.424]
 [0.989]]
siam score:  -0.8697203
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.261]
 [59.98 ]
 [51.261]
 [51.261]
 [51.261]] [[1.032]
 [1.404]
 [1.032]
 [1.032]
 [1.032]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.680561817768215
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.05596390581266
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9936,     0.0021,     0.0000,     0.0001,     0.0042],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9965,     0.0000,     0.0005,     0.0027],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0021,     0.8841,     0.0540,     0.0596],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0323, 0.0014, 0.0468, 0.7675, 0.1520], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0015, 0.0486, 0.0895, 0.3463, 0.5141], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9971,     0.0002,     0.0000,     0.0014,     0.0012],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0034,     0.9622,     0.0012,     0.0009,     0.0323],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0239,     0.8734,     0.0382,     0.0644],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0025, 0.0025, 0.0324, 0.8127, 0.1499], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0538, 0.1208, 0.0272, 0.0543, 0.7439], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.50799788156574
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.779]
 [66.779]
 [77.422]
 [66.779]
 [66.779]] [[0.806]
 [0.806]
 [1.   ]
 [0.806]
 [0.806]]
siam score:  -0.86354554
siam score:  -0.8626804
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  51.868567114580365
UNIT TEST: sample policy line 217 mcts : [0.051 0.077 0.59  0.154 0.128]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.104358486197896
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.156]
 [67.218]
 [65.156]
 [58.415]
 [65.974]] [[1.739]
 [1.845]
 [1.739]
 [1.392]
 [1.781]]
printing an ep nov before normalisation:  32.389095124686726
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.023185271145393926
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.669]
 [51.889]
 [51.889]
 [54.853]
 [51.889]] [[1.385]
 [0.942]
 [0.942]
 [1.045]
 [0.942]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.18894941775708
printing an ep nov before normalisation:  87.19804230209287
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.763]
 [22.902]
 [26.992]
 [41.238]
 [32.609]] [[0.514]
 [0.568]
 [0.762]
 [1.435]
 [1.027]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.909061922729386
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.80321727329721
printing an ep nov before normalisation:  69.70121575598868
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.9216094955193
printing an ep nov before normalisation:  64.73032981434412
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.67202750712968
printing an ep nov before normalisation:  23.078584840584945
siam score:  -0.8721527
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.45518510991497
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.78545230837025
printing an ep nov before normalisation:  38.15690040588379
printing an ep nov before normalisation:  32.37180522852421
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.642927591011365
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[83.589]
 [79.23 ]
 [83.589]
 [83.589]
 [82.408]] [[1.609]
 [1.486]
 [1.609]
 [1.609]
 [1.576]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.867961
printing an ep nov before normalisation:  65.65001587764215
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.36795949935913
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.69842139530601
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  29.547414779663086
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.15951891046976
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.293]
 [32.441]
 [33.184]
 [30.374]
 [32.509]] [[1.753]
 [1.768]
 [1.845]
 [1.556]
 [1.775]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.729568758866115
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.908559368896974
siam score:  -0.8645695
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[79.859]
 [84.405]
 [79.859]
 [79.859]
 [79.859]] [[1.783]
 [1.933]
 [1.783]
 [1.783]
 [1.783]]
printing an ep nov before normalisation:  44.80916596540633
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.37163224431565
printing an ep nov before normalisation:  51.74058085701001
printing an ep nov before normalisation:  49.16386316604716
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.509]
 [58.409]
 [56.778]
 [49.002]
 [54.372]] [[1.224]
 [1.388]
 [1.349]
 [1.164]
 [1.292]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.379]
 [46.531]
 [40.047]
 [38.903]
 [43.877]] [[0.919]
 [0.902]
 [0.777]
 [0.754]
 [0.851]]
actions average: 
K:  3  action  0 :  tensor([    0.9839,     0.0030,     0.0000,     0.0006,     0.0124],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0072,     0.9848,     0.0001,     0.0000,     0.0079],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0045,     0.8985,     0.0523,     0.0447],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0013,     0.0443,     0.7581,     0.1962],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0025, 0.0427, 0.0722, 0.2504, 0.6322], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.027985479399
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.557]
 [44.557]
 [44.557]
 [44.557]
 [44.557]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.42659616470337
printing an ep nov before normalisation:  41.45616548318424
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.642]
 [63.078]
 [54.642]
 [54.642]
 [54.642]] [[1.523]
 [1.82 ]
 [1.523]
 [1.523]
 [1.523]]
printing an ep nov before normalisation:  43.204483506526174
printing an ep nov before normalisation:  33.09990341518145
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.9882,     0.0008,     0.0002,     0.0039,     0.0068],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9807,     0.0001,     0.0002,     0.0189],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0002,     0.9549,     0.0239,     0.0207],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0003,     0.0778,     0.8271,     0.0947],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0081, 0.0274, 0.0543, 0.3421, 0.5681], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.72 ]
 [57.553]
 [60.72 ]
 [60.72 ]
 [62.543]] [[1.537]
 [1.419]
 [1.537]
 [1.537]
 [1.606]]
siam score:  -0.86883926
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.73904776721558
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  53.96766696596324
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8692515
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.073]
 [43.295]
 [61.073]
 [61.073]
 [61.073]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.00012546888683573343
printing an ep nov before normalisation:  45.36503973780136
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.272405013785814
printing an ep nov before normalisation:  38.401521110367995
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8690346
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  98.76557560148545
printing an ep nov before normalisation:  47.041310596403726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.458]
 [56.458]
 [56.458]
 [56.458]
 [56.458]] [[37.657]
 [37.657]
 [37.657]
 [37.657]
 [37.657]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  70.24243828333195
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.913]
 [46.079]
 [46.871]
 [41.051]
 [46.281]] [[1.405]
 [1.413]
 [1.449]
 [1.185]
 [1.422]]
printing an ep nov before normalisation:  34.285569190979004
line 256 mcts: sample exp_bonus 63.34034911027431
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.116]
 [59.696]
 [41.116]
 [41.116]
 [41.116]] [[0.661]
 [1.207]
 [0.661]
 [0.661]
 [0.661]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.566]
 [38.265]
 [28.392]
 [33.569]
 [30.582]] [[1.06 ]
 [0.512]
 [0.246]
 [0.386]
 [0.305]]
printing an ep nov before normalisation:  33.68284951844159
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  47.86513328552246
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.04553270492677
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.906]
 [35.119]
 [35.119]
 [35.119]
 [35.119]] [[0.451]
 [0.456]
 [0.456]
 [0.456]
 [0.456]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.59134945941066
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.14512066996669
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.154 0.154 0.128 0.103 0.462]
printing an ep nov before normalisation:  0.02768237718925093
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8739807
actions average: 
K:  4  action  0 :  tensor([    0.9489,     0.0250,     0.0003,     0.0025,     0.0233],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9762,     0.0030,     0.0001,     0.0206],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.8961,     0.0409,     0.0629],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0002,     0.0782,     0.8170,     0.1045],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0004,     0.0735,     0.0675,     0.1961,     0.6626],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.82481308616409
printing an ep nov before normalisation:  38.6470890045166
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -6.07347113165634e-11
0.0 -5.6924736904272116e-11
0.0 0.0
0.0 -1.354984317130702e-10
0.0 0.0
0.0 -6.372733594244504e-11
0.0 0.0
0.0 0.0
0.0 -4.876853742224895e-11
0.0 -6.973420820341644e-11
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  104.3260984083159
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.783]
 [50.691]
 [50.691]
 [50.691]
 [50.691]] [[1.674]
 [1.122]
 [1.122]
 [1.122]
 [1.122]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 63.009518904117115
printing an ep nov before normalisation:  40.831211089066045
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.20486172191547
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.3303949663632011
printing an ep nov before normalisation:  42.5936317631503
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87420225
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.26976776123047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.662258625030518
printing an ep nov before normalisation:  48.761824877771964
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.19768399346485
printing an ep nov before normalisation:  44.737632627947384
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.61339330251326
printing an ep nov before normalisation:  44.288320541381836
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.20075805029234
printing an ep nov before normalisation:  80.67430961181432
printing an ep nov before normalisation:  36.92680835723877
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
siam score:  -0.86252195
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9457,     0.0000,     0.0535,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0038,     0.9056,     0.0000,     0.0013,     0.0892],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0038,     0.0000,     0.9553,     0.0114,     0.0295],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0203, 0.0011, 0.0610, 0.6980, 0.2196], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0048, 0.0013, 0.0595, 0.1726, 0.7618], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.72397585282914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8653644
printing an ep nov before normalisation:  23.097898262956463
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.096536323715306
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.4351430710165
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87698436
actions average: 
K:  1  action  0 :  tensor([    1.0000,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9821,     0.0005,     0.0001,     0.0172],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0001,     0.9319,     0.0202,     0.0477],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0044, 0.0028, 0.0160, 0.8000, 0.1769], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0032, 0.0534, 0.1024, 0.2120, 0.6291], grad_fn=<DivBackward0>)
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.316]
 [54.838]
 [54.671]
 [54.166]
 [59.088]] [[0.186]
 [0.271]
 [0.27 ]
 [0.266]
 [0.297]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  1  action  0 :  tensor([    0.9454,     0.0391,     0.0003,     0.0044,     0.0108],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0086,     0.8722,     0.0256,     0.0003,     0.0934],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0120,     0.9414,     0.0010,     0.0456],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0089, 0.0031, 0.0432, 0.8316, 0.1132], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0009, 0.0137, 0.1042, 0.3246, 0.5566], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.569]
 [20.244]
 [73.611]
 [20.244]
 [20.244]] [[0.869]
 [0.231]
 [1.393]
 [0.231]
 [0.231]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.73992962282316
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.028197792368324
printing an ep nov before normalisation:  76.57461968069991
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.794]
 [55.422]
 [61.699]
 [49.915]
 [49.942]] [[0.1  ]
 [0.222]
 [0.265]
 [0.184]
 [0.184]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.27974858388052
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.21 ]
 [42.21 ]
 [52.486]
 [42.21 ]
 [42.21 ]] [[1.07 ]
 [1.07 ]
 [1.514]
 [1.07 ]
 [1.07 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.59238082881656
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.484]
 [44.352]
 [33.484]
 [33.484]
 [33.484]] [[0.593]
 [0.966]
 [0.593]
 [0.593]
 [0.593]]
printing an ep nov before normalisation:  39.00627269607652
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.121811851746955
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.152]
 [63.347]
 [51.152]
 [51.152]
 [51.152]] [[0.394]
 [0.562]
 [0.394]
 [0.394]
 [0.394]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.93278872911941
printing an ep nov before normalisation:  71.98309150680849
actions average: 
K:  2  action  0 :  tensor([    0.9521,     0.0001,     0.0003,     0.0028,     0.0447],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9607,     0.0015,     0.0004,     0.0370],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0001,     0.9347,     0.0196,     0.0456],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0006,     0.0191,     0.8938,     0.0862],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0108, 0.0929, 0.0287, 0.1783, 0.6892], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  69.89090249149532
printing an ep nov before normalisation:  59.55170851183799
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.53902585770269
printing an ep nov before normalisation:  37.14248694709214
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9471,     0.0129,     0.0003,     0.0392],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0007,     0.0003,     0.9188,     0.0256,     0.0547],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0010,     0.0006,     0.0734,     0.6477,     0.2774],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0057, 0.0232, 0.0573, 0.1745, 0.7394], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[88.072]
 [88.072]
 [85.77 ]
 [95.036]
 [91.894]] [[0.827]
 [0.827]
 [0.801]
 [0.905]
 [0.87 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.87216204
printing an ep nov before normalisation:  37.40746021270752
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.537]
 [33.537]
 [52.178]
 [33.537]
 [33.537]] [[0.378]
 [0.378]
 [0.778]
 [0.378]
 [0.378]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.791]
 [30.218]
 [22.906]
 [29.078]
 [28.287]] [[0.69 ]
 [0.435]
 [0.274]
 [0.41 ]
 [0.392]]
actions average: 
K:  0  action  0 :  tensor([    0.9917,     0.0002,     0.0000,     0.0050,     0.0031],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9556,     0.0001,     0.0029,     0.0409],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0005,     0.0001,     0.9727,     0.0108,     0.0160],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0006,     0.0007,     0.0395,     0.6719,     0.2872],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0010, 0.0313, 0.1430, 0.3002, 0.5245], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  69.06973492175649
siam score:  -0.86480254
printing an ep nov before normalisation:  78.91115666117012
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.9970,     0.0001,     0.0000,     0.0016,     0.0012],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9776,     0.0006,     0.0014,     0.0201],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.8818,     0.0526,     0.0655],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0005,     0.0006,     0.1041,     0.6435,     0.2513],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0047, 0.0317, 0.1645, 0.2702, 0.5289], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.566108226776123
printing an ep nov before normalisation:  62.169620639562005
printing an ep nov before normalisation:  0.07705473816130848
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.626149944759355
line 256 mcts: sample exp_bonus 19.530201960496406
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.939374291041737
printing an ep nov before normalisation:  63.475453663640074
printing an ep nov before normalisation:  51.11520695293467
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.47501222916239
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.409]
 [54.344]
 [48.909]
 [52.247]
 [50.009]] [[0.624]
 [0.807]
 [0.664]
 [0.752]
 [0.693]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[88.047]
 [88.047]
 [79.499]
 [93.613]
 [88.047]] [[1.861]
 [1.861]
 [1.647]
 [2.   ]
 [1.861]]
printing an ep nov before normalisation:  53.04152055663709
printing an ep nov before normalisation:  39.35474634170532
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.05060994374743
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.25448989868164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  17.722276005887817
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.968]
 [ 0.182]
 [42.968]
 [ 0.16 ]
 [ 0.165]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.477]
 [54.411]
 [68.716]
 [52.557]
 [51.658]] [[0.112]
 [0.202]
 [0.295]
 [0.19 ]
 [0.184]]
siam score:  -0.8682016
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.248]
 [75.808]
 [45.248]
 [45.248]
 [45.248]] [[0.441]
 [1.007]
 [0.441]
 [0.441]
 [0.441]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.25541506395119
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.797]
 [46.797]
 [46.797]
 [46.797]
 [46.797]] [[0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.010191841779771948
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9979,     0.0001,     0.0000,     0.0006,     0.0014],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0018,     0.9528,     0.0004,     0.0005,     0.0445],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0001,     0.8876,     0.0164,     0.0959],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0002,     0.0203,     0.8386,     0.1406],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0046, 0.0748, 0.0477, 0.1105, 0.7624], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.224]
 [47.114]
 [47.766]
 [46.273]
 [47.279]] [[0.65 ]
 [0.601]
 [0.616]
 [0.582]
 [0.605]]
printing an ep nov before normalisation:  61.09559911944679
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.9982,     0.0006,     0.0000,     0.0004,     0.0007],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9805,     0.0003,     0.0001,     0.0189],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9815,     0.0003,     0.0181],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0010,     0.0855,     0.8113,     0.1021],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0004,     0.0012,     0.1237,     0.2464,     0.6283],
       grad_fn=<DivBackward0>)
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.142330169677734
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.81775363165334
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.83945219377635
printing an ep nov before normalisation:  0.00024758144377301505
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.008959795121938896
printing an ep nov before normalisation:  61.05571239031097
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  16.3416588306427
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.11898415006685
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.986]
 [63.023]
 [63.888]
 [37.563]
 [37.211]] [[0.201]
 [0.463]
 [0.471]
 [0.225]
 [0.222]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.149]
 [59.397]
 [65.298]
 [65.298]
 [59.823]] [[1.632]
 [1.644]
 [1.944]
 [1.944]
 [1.666]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.303]
 [34.586]
 [40.978]
 [34.388]
 [37.513]] [[0.589]
 [0.894]
 [1.263]
 [0.882]
 [1.063]]
printing an ep nov before normalisation:  59.67389504645755
printing an ep nov before normalisation:  38.98979187011719
line 256 mcts: sample exp_bonus 48.622674881139154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.586]
 [28.085]
 [29.586]
 [29.586]
 [29.586]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.6088358502365
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  53.52953863906302
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.233]
 [67.839]
 [61.362]
 [52.99 ]
 [67.644]] [[0.796]
 [0.976]
 [0.841]
 [0.666]
 [0.971]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.418076007947693
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0000],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9961,     0.0000,     0.0001,     0.0037],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0018,     0.8248,     0.0643,     0.1087],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0005,     0.0004,     0.0251,     0.8179,     0.1561],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0435, 0.0547, 0.0452, 0.1926, 0.6641], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.056870701834846
printing an ep nov before normalisation:  39.53889730162868
siam score:  -0.8689596
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.149]
 [38.149]
 [55.117]
 [54.988]
 [38.149]] [[0.451]
 [0.451]
 [0.832]
 [0.83 ]
 [0.451]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.468]
 [43.468]
 [43.468]
 [42.776]
 [43.468]] [[1.157]
 [1.157]
 [1.157]
 [1.123]
 [1.157]]
siam score:  -0.8687668
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.05 ]
 [35.96 ]
 [48.454]
 [35.96 ]
 [35.96 ]] [[0.656]
 [0.563]
 [0.936]
 [0.563]
 [0.563]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.17155811873701
printing an ep nov before normalisation:  43.31497104022118
printing an ep nov before normalisation:  34.85124799802003
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.523]
 [43.394]
 [43.394]
 [43.394]
 [43.394]] [[1.31 ]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
printing an ep nov before normalisation:  69.78672848286902
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.66878295449458
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.193]
 [40.955]
 [32.738]
 [31.545]
 [41.164]] [[0.465]
 [0.744]
 [0.483]
 [0.445]
 [0.751]]
actions average: 
K:  1  action  0 :  tensor([    0.9995,     0.0001,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9850,     0.0003,     0.0002,     0.0142],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0004,     0.9735,     0.0132,     0.0128],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0006,     0.0004,     0.0296,     0.8732,     0.0961],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0088, 0.0268, 0.0234, 0.2327, 0.7083], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.9476, 0.0387, 0.0041, 0.0030, 0.0065], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0016,     0.9794,     0.0002,     0.0000,     0.0188],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0010,     0.9562,     0.0149,     0.0276],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0016, 0.0043, 0.0413, 0.7629, 0.1900], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0008, 0.0053, 0.0895, 0.2435, 0.6609], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9608,     0.0014,     0.0000,     0.0103,     0.0275],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9988,     0.0004,     0.0002,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0008,     0.8917,     0.0363,     0.0712],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0007,     0.0013,     0.0515,     0.7122,     0.2343],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0011, 0.0034, 0.0756, 0.2341, 0.6857], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.42975669505913
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 50.72306195281137
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.240763664245605
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.487]
 [30.001]
 [24.861]
 [24.495]
 [29.536]] [[0.252]
 [0.148]
 [0.105]
 [0.102]
 [0.144]]
printing an ep nov before normalisation:  0.0039010361660984927
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.13499229813419333
actions average: 
K:  2  action  0 :  tensor([    0.9955,     0.0006,     0.0000,     0.0008,     0.0030],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.9976,     0.0004,     0.0003,     0.0012],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0005,     0.0003,     0.8685,     0.0503,     0.0804],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0004,     0.0154,     0.7706,     0.2132],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0009, 0.0338, 0.1180, 0.1711, 0.6762], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  53.78666927487262
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.759]
 [60.569]
 [47.461]
 [43.137]
 [58.121]] [[0.721]
 [0.699]
 [0.452]
 [0.37 ]
 [0.653]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.524773814014885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.39706341180379
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.139]
 [48.487]
 [41.88 ]
 [42.848]
 [47.458]] [[1.797]
 [1.899]
 [1.4  ]
 [1.473]
 [1.821]]
printing an ep nov before normalisation:  42.98395762280246
printing an ep nov before normalisation:  23.664369583129883
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.81250858306885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.21782585338078
printing an ep nov before normalisation:  56.18261335658635
printing an ep nov before normalisation:  40.9161399642949
actions average: 
K:  3  action  0 :  tensor([    0.9942,     0.0002,     0.0003,     0.0006,     0.0047],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0021,     0.9769,     0.0003,     0.0006,     0.0201],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0004,     0.0103,     0.9409,     0.0167,     0.0317],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0003,     0.0425,     0.8281,     0.1289],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0023, 0.0548, 0.0632, 0.0862, 0.7935], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.62454795837402
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.162786918701094
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.296287059783936
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.294]
 [39.346]
 [39.837]
 [32.068]
 [39.221]] [[1.391]
 [1.458]
 [1.489]
 [0.999]
 [1.45 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.4791224148853
printing an ep nov before normalisation:  54.64468386231269
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9659,     0.0093,     0.0002,     0.0002,     0.0244],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9967,     0.0012,     0.0001,     0.0020],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0023,     0.0003,     0.8716,     0.0529,     0.0729],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0007,     0.0006,     0.0006,     0.8161,     0.1821],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0704, 0.0528, 0.0455, 0.0656, 0.7657], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  67.70410854536873
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9992,     0.0002,     0.0002,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9504,     0.0002,     0.0012,     0.0481],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0006,     0.8703,     0.0308,     0.0978],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0002,     0.1158,     0.8149,     0.0691],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0016, 0.0035, 0.0631, 0.1301, 0.8017], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.96345806121826
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.30968332523504
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.175]
 [27.305]
 [16.759]
 [33.078]
 [34.089]] [[0.541]
 [0.543]
 [0.333]
 [0.658]
 [0.678]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  91.68496200055077
siam score:  -0.8709906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8710333
printing an ep nov before normalisation:  51.82724293459801
printing an ep nov before normalisation:  37.17155216280901
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.07515291914464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9954,     0.0001,     0.0037,     0.0002,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9821,     0.0001,     0.0001,     0.0176],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0007,     0.9680,     0.0003,     0.0310],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0015, 0.0025, 0.0466, 0.7256, 0.2238], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0015, 0.1503, 0.0593, 0.2023, 0.5867], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.370822710784335
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.69636037691366
actions average: 
K:  2  action  0 :  tensor([    0.9982,     0.0006,     0.0000,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9799,     0.0001,     0.0001,     0.0198],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0004,     0.0157,     0.9119,     0.0261,     0.0460],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0007,     0.0007,     0.0263,     0.8238,     0.1484],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0047, 0.0931, 0.0520, 0.3194, 0.5307], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.014]
 [28.917]
 [23.813]
 [27.419]
 [27.598]] [[1.025]
 [0.713]
 [0.453]
 [0.637]
 [0.646]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8751396
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8739901
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  58.671680115401706
printing an ep nov before normalisation:  34.73027006520086
siam score:  -0.873154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  78.32676673622056
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.13671421528864
printing an ep nov before normalisation:  50.46823463270935
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8683959
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.956]
 [42.127]
 [34.311]
 [19.026]
 [27.925]] [[0.371]
 [0.453]
 [0.3  ]
 [0.   ]
 [0.175]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.870013
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.003]
 [50.424]
 [50.786]
 [52.219]
 [51.086]] [[1.282]
 [0.989]
 [1.003]
 [1.058]
 [1.014]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.94125286759277
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.915701293802776
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.99777335950393
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.502490042145894
siam score:  -0.86651176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.91 ]
 [22.113]
 [36.587]
 [29.701]
 [23.227]] [[0.125]
 [0.063]
 [0.145]
 [0.106]
 [0.07 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.301657427071714
siam score:  -0.8697771
siam score:  -0.8696294
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.89094131450966
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.982]
 [41.012]
 [34.343]
 [38.439]
 [36.935]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.07448172240679
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.025]
 [63.146]
 [52.025]
 [52.025]
 [52.025]] [[0.915]
 [1.197]
 [0.915]
 [0.915]
 [0.915]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8604014
printing an ep nov before normalisation:  47.87043563469771
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.023]
 [69.368]
 [58.023]
 [58.023]
 [58.023]] [[1.49 ]
 [1.881]
 [1.49 ]
 [1.49 ]
 [1.49 ]]
printing an ep nov before normalisation:  43.11108087280359
printing an ep nov before normalisation:  58.906867355986826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.314]
 [71.525]
 [37.314]
 [37.314]
 [37.314]] [[0.529]
 [1.364]
 [0.529]
 [0.529]
 [0.529]]
printing an ep nov before normalisation:  88.38402348263776
printing an ep nov before normalisation:  77.23081083942472
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.11108313172657
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.82450810648233
printing an ep nov before normalisation:  51.68600416921975
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.72 ]
 [33.517]
 [28.775]
 [41.293]
 [32.441]] [[0.42 ]
 [0.548]
 [0.388]
 [0.811]
 [0.512]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.123]
 [35.234]
 [25.386]
 [26.849]
 [25.056]] [[1.231]
 [1.54 ]
 [0.799]
 [0.909]
 [0.774]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.64805857340495
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.01666450500488
line 256 mcts: sample exp_bonus 39.41034818525448
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 57.57805977436496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.20516896003507
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0024609246575361494
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  99.17948563598068
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.251]
 [26.251]
 [26.251]
 [39.349]
 [26.251]] [[0.36 ]
 [0.36 ]
 [0.36 ]
 [0.702]
 [0.36 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.085173703952925
printing an ep nov before normalisation:  34.948471853550636
printing an ep nov before normalisation:  33.08652877807617
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.84285059895103
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8600601
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0006265087165502337
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.680910649379605
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.8900260925293
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  76.39908972001575
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8654839
printing an ep nov before normalisation:  70.29051832324556
printing an ep nov before normalisation:  59.84077309855937
printing an ep nov before normalisation:  63.60216902533113
printing an ep nov before normalisation:  61.63425955332109
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.277]
 [29.286]
 [29.286]
 [44.19 ]
 [29.286]] [[0.27 ]
 [0.159]
 [0.159]
 [0.367]
 [0.159]]
printing an ep nov before normalisation:  28.11979075148067
printing an ep nov before normalisation:  20.781137943267822
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[79.503]
 [79.503]
 [79.503]
 [79.503]
 [79.503]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  83.38881638099171
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[85.679]
 [85.679]
 [85.679]
 [85.679]
 [85.679]] [[0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.24241780173731
UNIT TEST: sample policy line 217 mcts : [0.231 0.179 0.128 0.308 0.154]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.065]
 [39.637]
 [52.218]
 [32.665]
 [32.665]] [[0.144]
 [0.188]
 [0.286]
 [0.134]
 [0.134]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.4315258842698
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.869146
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0008,     0.9781,     0.0001,     0.0001,     0.0210],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0199,     0.8850,     0.0396,     0.0555],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0221, 0.0016, 0.0786, 0.7706, 0.1272], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0041, 0.0844, 0.0473, 0.2248, 0.6394], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  105.82159678482378
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.34218349732586
printing an ep nov before normalisation:  55.85757599324643
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[17.381]
 [20.884]
 [27.343]
 [20.516]
 [20.688]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.9964,     0.0001,     0.0000,     0.0014,     0.0021],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9767,     0.0001,     0.0001,     0.0227],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0003,     0.8695,     0.0537,     0.0763],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0010,     0.0005,     0.0173,     0.8015,     0.1797],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0014, 0.0336, 0.1242, 0.1467, 0.6941], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.06349545651972
printing an ep nov before normalisation:  40.488819097630454
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.347]
 [53.347]
 [45.563]
 [52.477]
 [53.347]] [[0.824]
 [0.824]
 [0.595]
 [0.798]
 [0.824]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.52497736069066
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.21044108929279
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  76.47129352667079
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.81180551166502
printing an ep nov before normalisation:  32.481205463409424
actions average: 
K:  4  action  0 :  tensor([    0.9995,     0.0002,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0007,     0.9391,     0.0000,     0.0009,     0.0593],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0009,     0.0069,     0.9381,     0.0110,     0.0431],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0004,     0.0006,     0.0003,     0.9134,     0.0853],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0011, 0.0796, 0.1007, 0.2022, 0.6164], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.14719650306076
actions average: 
K:  4  action  0 :  tensor([    0.9991,     0.0000,     0.0000,     0.0003,     0.0005],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0017,     0.9702,     0.0008,     0.0001,     0.0272],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0012, 0.0029, 0.8195, 0.0836, 0.0928], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0026,     0.0007,     0.0445,     0.8348,     0.1174],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0098, 0.0159, 0.1072, 0.2212, 0.6459], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.232]
 [44.232]
 [50.058]
 [44.232]
 [44.232]] [[1.255]
 [1.255]
 [1.519]
 [1.255]
 [1.255]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.796]
 [54.796]
 [68.392]
 [54.796]
 [54.796]] [[0.248]
 [0.248]
 [0.333]
 [0.248]
 [0.248]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  88.51087054326224
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.75595998764038
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.02220344543457
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.933]
 [33.93 ]
 [33.93 ]
 [33.93 ]
 [33.93 ]] [[1.164]
 [0.573]
 [0.573]
 [0.573]
 [0.573]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  64.04146548731987
using explorer policy with actor:  1
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.077 0.205 0.333 0.282 0.103]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.559]
 [33.377]
 [33.377]
 [33.377]
 [33.377]] [[1.274]
 [0.659]
 [0.659]
 [0.659]
 [0.659]]
printing an ep nov before normalisation:  67.89662766795811
UNIT TEST: sample policy line 217 mcts : [0.103 0.128 0.179 0.564 0.026]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.21597612054811
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.701]
 [40.701]
 [39.67 ]
 [37.69 ]
 [42.728]] [[0.409]
 [0.409]
 [0.389]
 [0.351]
 [0.448]]
printing an ep nov before normalisation:  38.19541377264879
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.12351088851368
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87567437
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.33908387924646
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.273]
 [49.199]
 [42.33 ]
 [40.879]
 [40.556]] [[0.246]
 [0.262]
 [0.225]
 [0.218]
 [0.216]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.786]
 [61.643]
 [60.786]
 [60.786]
 [60.786]] [[1.234]
 [1.267]
 [1.234]
 [1.234]
 [1.234]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.011]
 [50.165]
 [50.311]
 [50.951]
 [50.651]] [[0.836]
 [0.994]
 [1.   ]
 [1.024]
 [1.013]]
printing an ep nov before normalisation:  43.971476554870605
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.013939863726705
actor:  1 policy actor:  1  step number:  106 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.44 ]
 [62.135]
 [57.037]
 [36.44 ]
 [36.44 ]] [[0.628]
 [1.373]
 [1.225]
 [0.628]
 [0.628]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  3.267403414497494e-06
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  0.007917623710795851
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.601]
 [48.601]
 [60.012]
 [59.485]
 [48.601]] [[0.79 ]
 [0.79 ]
 [1.083]
 [1.069]
 [0.79 ]]
printing an ep nov before normalisation:  37.671565480934476
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  47.79500639356145
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.331]
 [32.482]
 [25.907]
 [32.482]
 [28.118]] [[1.172]
 [0.875]
 [0.59 ]
 [0.875]
 [0.686]]
printing an ep nov before normalisation:  61.000685253164775
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  25.13191600542598
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.901]
 [65.901]
 [71.651]
 [65.901]
 [65.901]] [[1.74 ]
 [1.74 ]
 [1.971]
 [1.74 ]
 [1.74 ]]
printing an ep nov before normalisation:  56.33647886458127
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  62.29123792477683
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.515]
 [79.206]
 [86.739]
 [85.155]
 [85.547]] [[0.947]
 [1.303]
 [1.446]
 [1.416]
 [1.423]]
printing an ep nov before normalisation:  56.48408871879339
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
line 256 mcts: sample exp_bonus 61.780948481047226
printing an ep nov before normalisation:  44.072060849562284
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  52.42887943820445
printing an ep nov before normalisation:  36.395911255308924
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.44731807016693
printing an ep nov before normalisation:  58.358562979718215
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.156]
 [54.649]
 [36.697]
 [41.629]
 [48.402]] [[0.46 ]
 [0.502]
 [0.337]
 [0.382]
 [0.444]]
printing an ep nov before normalisation:  46.45477988900064
printing an ep nov before normalisation:  51.946015064474196
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.8196411
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  52.12848519165385
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  49.337649069827364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  40.126479570082445
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  47.788514908975095
siam score:  -0.8316658
line 256 mcts: sample exp_bonus 44.99892040952368
printing an ep nov before normalisation:  46.22661270798143
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.291]
 [62.77 ]
 [61.534]
 [61.534]
 [61.534]] [[1.246]
 [1.167]
 [1.129]
 [1.129]
 [1.129]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  0  action  0 :  tensor([    0.9991,     0.0001,     0.0005,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0015,     0.8060,     0.0004,     0.0014,     0.1907],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0008,     0.0004,     0.9288,     0.0209,     0.0491],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0006,     0.0009,     0.0142,     0.7939,     0.1905],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0041, 0.0949, 0.0865, 0.1622, 0.6524], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.8302539
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8287738
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  0.03492268590832737
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.8292009
printing an ep nov before normalisation:  53.93077227028933
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  54.49103004528384
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.275]
 [35.703]
 [35.703]
 [35.703]
 [35.703]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.9670672416687
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  65.0177313792929
printing an ep nov before normalisation:  96.46539450114074
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.56770812140571
printing an ep nov before normalisation:  39.33949540990129
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.608]
 [38.32 ]
 [40.909]
 [41.86 ]
 [40.052]] [[0.977]
 [0.835]
 [0.947]
 [0.988]
 [0.91 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  62.09211270133386
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  62.11278154841897
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.9492,     0.0464,     0.0000,     0.0013,     0.0031],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9772,     0.0001,     0.0000,     0.0225],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0054,     0.0001,     0.9378,     0.0164,     0.0403],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0003,     0.0151,     0.8751,     0.1094],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0004,     0.0741,     0.1019,     0.1351,     0.6885],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  37.161266334727486
printing an ep nov before normalisation:  49.08877864312018
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0004],
        [0.0012],
        [0.0003],
        [0.0004],
        [0.0003],
        [0.0000],
        [0.0004],
        [0.0004],
        [0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.00035929377375510524
0.0 0.0011816918369759962
0.0 0.00034184383533734084
0.0 0.0004102622000549366
0.0 0.0002651401499530188
0.0 0.0
0.0 0.00040580127699019783
0.0 0.0003974216766599897
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  48.48856858742584
printing an ep nov before normalisation:  35.07786035591176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.84387046
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  37.766618728637695
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.   ]
 [0.001]] [[55.673]
 [59.201]
 [55.38 ]
 [50.612]
 [58.503]] [[1.062]
 [1.158]
 [1.054]
 [0.924]
 [1.139]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.001]
 [0.   ]
 [0.   ]] [[29.481]
 [31.99 ]
 [36.767]
 [44.722]
 [29.558]] [[0.368]
 [0.427]
 [0.538]
 [0.723]
 [0.37 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[45.633]
 [45.633]
 [56.943]
 [45.633]
 [45.633]] [[0.801]
 [0.801]
 [1.127]
 [0.801]
 [0.801]]
printing an ep nov before normalisation:  83.64134307579316
line 256 mcts: sample exp_bonus 68.20941114670042
actions average: 
K:  0  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9202,     0.0128,     0.0012,     0.0656],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.8243,     0.0179,     0.1578],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0063,     0.0007,     0.0333,     0.7755,     0.1843],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0308, 0.0730, 0.0451, 0.1203, 0.7308], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.702452182769775
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  31.807508071263634
using explorer policy with actor:  0
printing an ep nov before normalisation:  48.35464656352997
printing an ep nov before normalisation:  52.10819671208178
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.733]
 [51.733]
 [51.733]
 [51.733]
 [51.733]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  59.31051002154963
printing an ep nov before normalisation:  38.01821734388794
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  0.0006821364422648912
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.79]
 [45.79]
 [45.79]
 [45.79]
 [45.79]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  29.803645408003725
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.11845293998544
printing an ep nov before normalisation:  38.80267443920973
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[55.509]
 [68.182]
 [55.509]
 [55.509]
 [55.509]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[46.635]
 [47.362]
 [42.383]
 [46.635]
 [44.535]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  31.322009563446045
actions average: 
K:  3  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9906,     0.0052,     0.0003,     0.0036],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0005,     0.0132,     0.8519,     0.0664,     0.0680],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0004,     0.0013,     0.9227,     0.0753],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0217, 0.0498, 0.0503, 0.1460, 0.7323], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  41.01146758599167
printing an ep nov before normalisation:  50.53454575920238
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  62.62335297868457
printing an ep nov before normalisation:  57.20567335226548
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  0
printing an ep nov before normalisation:  34.235905537206186
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  43.9713769376258
printing an ep nov before normalisation:  59.40861367348517
printing an ep nov before normalisation:  47.29056356937704
printing an ep nov before normalisation:  29.86702758309954
printing an ep nov before normalisation:  41.84224140346461
printing an ep nov before normalisation:  61.92730583619024
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.49 ]
 [40.144]
 [56.186]
 [44.983]
 [46.174]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[47.471]
 [47.471]
 [56.772]
 [47.471]
 [47.471]] [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.318]
 [57.388]
 [53.728]
 [49.279]
 [49.279]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[53.322]
 [53.322]
 [53.322]
 [53.322]
 [53.322]] [[1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]]
printing an ep nov before normalisation:  4.419380559284036e-06
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  33.91548156738281
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.001]
 [0.001]
 [0.002]] [[30.209]
 [32.363]
 [31.961]
 [36.905]
 [34.505]] [[0.002]
 [0.003]
 [0.001]
 [0.001]
 [0.002]]
printing an ep nov before normalisation:  47.045769707436605
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
printing an ep nov before normalisation:  55.018955775873955
printing an ep nov before normalisation:  33.429505825042725
printing an ep nov before normalisation:  34.83118860212749
printing an ep nov before normalisation:  62.40522893295157
printing an ep nov before normalisation:  41.66804624931982
printing an ep nov before normalisation:  51.002258269389685
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[38.395]
 [35.293]
 [46.839]
 [38.702]
 [38.657]] [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  30.690057248367246
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.06456201746335
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[57.438]
 [46.746]
 [61.879]
 [61.312]
 [46.746]] [[0.838]
 [0.552]
 [0.957]
 [0.942]
 [0.552]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[39.473]
 [58.198]
 [39.473]
 [39.473]
 [39.473]] [[0.473]
 [0.846]
 [0.473]
 [0.473]
 [0.473]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  33.42148755537108
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.001]
 [0.   ]] [[50.563]
 [54.855]
 [67.093]
 [50.79 ]
 [50.563]] [[0.   ]
 [0.   ]
 [0.001]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.86304903
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.866152
printing an ep nov before normalisation:  54.664984720781625
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  46.76572366347775
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  0.07975717535799731
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  4  action  0 :  tensor([    0.9426,     0.0375,     0.0067,     0.0001,     0.0131],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9755,     0.0000,     0.0001,     0.0243],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0003,     0.9467,     0.0153,     0.0376],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0008,     0.0181,     0.8580,     0.1229],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0137, 0.1226, 0.0433, 0.1990, 0.6214], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.08627443516384
printing an ep nov before normalisation:  39.76116418838501
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[56.576]
 [51.762]
 [56.576]
 [51.48 ]
 [55.635]] [[1.281]
 [1.12 ]
 [1.281]
 [1.11 ]
 [1.249]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  61.26742735964958
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[40.413]
 [38.756]
 [40.413]
 [47.275]
 [40.413]] [[0.756]
 [0.702]
 [0.756]
 [0.979]
 [0.756]]
printing an ep nov before normalisation:  60.415152900898974
printing an ep nov before normalisation:  41.88641647372432
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.001]
 [0.001]
 [0.001]] [[42.615]
 [38.306]
 [40.476]
 [40.88 ]
 [38.003]] [[1.097]
 [0.89 ]
 [0.994]
 [1.013]
 [0.875]]
printing an ep nov before normalisation:  42.99945854563969
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  44.37171532972024
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[37.386]
 [37.386]
 [37.386]
 [46.316]
 [38.78 ]] [[0.823]
 [0.823]
 [0.823]
 [1.123]
 [0.87 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  44.402823313784545
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  40.93822063587657
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  34.64867653927413
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  56.26489370185268
siam score:  -0.8705562
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[56.468]
 [57.796]
 [56.468]
 [56.468]
 [56.468]] [[1.276]
 [1.324]
 [1.276]
 [1.276]
 [1.276]]
siam score:  -0.87049043
printing an ep nov before normalisation:  47.79701429749639
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[41.101]
 [41.101]
 [62.276]
 [41.101]
 [41.101]] [[0.304]
 [0.304]
 [0.581]
 [0.304]
 [0.304]]
printing an ep nov before normalisation:  30.957958102226257
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  32.17954810116663
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  0
printing an ep nov before normalisation:  21.795624907240075
printing an ep nov before normalisation:  27.005818759255053
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  39.66757903599525
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
actions average: 
K:  0  action  0 :  tensor([    1.0000,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0045,     0.9245,     0.0001,     0.0006,     0.0703],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0024,     0.0004,     0.8991,     0.0571,     0.0410],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0052, 0.0015, 0.0437, 0.8067, 0.1428], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0091, 0.0600, 0.0820, 0.1676, 0.6814], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  1  action  0 :  tensor([    0.9997,     0.0001,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0005,     0.9844,     0.0003,     0.0002,     0.0146],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0018,     0.0004,     0.9199,     0.0329,     0.0449],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0012,     0.0006,     0.0294,     0.7987,     0.1701],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0042, 0.1304, 0.0525, 0.1822, 0.6308], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  53.47261867290845
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[47.208]
 [53.261]
 [66.199]
 [47.208]
 [47.208]] [[0.622]
 [0.758]
 [1.049]
 [0.622]
 [0.622]]
printing an ep nov before normalisation:  46.510862243028704
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.832037101527995
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  75.61797360510974
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  66.3823064691313
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  44.260956291412384
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  59.04584193885812
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9780,     0.0005,     0.0019,     0.0192],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0015,     0.0004,     0.9103,     0.0127,     0.0751],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0010, 0.0079, 0.0331, 0.7970, 0.1610], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0032, 0.0400, 0.0304, 0.1054, 0.8209], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.6225152015686
printing an ep nov before normalisation:  46.92893687104454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[54.744]
 [57.922]
 [54.744]
 [54.744]
 [59.506]] [[1.316]
 [1.431]
 [1.316]
 [1.316]
 [1.488]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  42.10521600604808
printing an ep nov before normalisation:  19.13330316543579
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  40.50546505969618
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.001]] [[48.863]
 [61.171]
 [48.863]
 [48.863]
 [48.616]] [[0.692]
 [0.966]
 [0.692]
 [0.692]
 [0.686]]
printing an ep nov before normalisation:  0.00020574296058839536
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.45873546600342
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[35.711]
 [14.995]
 [44.898]
 [35.328]
 [40.182]] [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.88968027
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  4  action  0 :  tensor([    0.9990,     0.0002,     0.0002,     0.0002,     0.0004],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0123,     0.9478,     0.0033,     0.0005,     0.0361],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0002,     0.9321,     0.0228,     0.0449],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0007,     0.0005,     0.0009,     0.8480,     0.1499],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0017, 0.1444, 0.1120, 0.0753, 0.6667], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  51.17916524562456
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.36547754919213
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.9070413
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.592448907327906
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  48.721357137173804
printing an ep nov before normalisation:  48.024997858640326
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.014]
 [0.014]
 [0.014]
 [0.012]] [[44.52 ]
 [47.055]
 [39.576]
 [46.804]
 [37.051]] [[0.745]
 [0.816]
 [0.619]
 [0.809]
 [0.55 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.005]
 [0.006]
 [0.006]
 [0.005]] [[28.168]
 [37.372]
 [28.168]
 [28.168]
 [32.144]] [[0.006]
 [0.005]
 [0.006]
 [0.006]
 [0.005]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.9260481
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
line 256 mcts: sample exp_bonus 35.292203024587245
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  74.05322462697438
printing an ep nov before normalisation:  35.71096074513896
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  45.382698525910634
printing an ep nov before normalisation:  48.319067866928656
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[31.479]
 [31.479]
 [31.479]
 [31.479]
 [31.479]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.93344843
printing an ep nov before normalisation:  83.67473107267084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.009]
 [0.012]
 [0.012]] [[34.082]
 [34.082]
 [51.954]
 [34.082]
 [34.082]] [[0.516]
 [0.516]
 [1.017]
 [0.516]
 [0.516]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  64.18336499046184
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.007]
 [0.009]
 [0.008]
 [0.008]] [[32.51 ]
 [31.626]
 [34.865]
 [35.777]
 [38.638]] [[0.673]
 [0.631]
 [0.771]
 [0.809]
 [0.931]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.20595980505026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  26.357810754026776
actions average: 
K:  2  action  0 :  tensor([    0.9969,     0.0004,     0.0000,     0.0010,     0.0017],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9409,     0.0006,     0.0000,     0.0583],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0087,     0.9183,     0.0225,     0.0505],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0009,     0.0008,     0.0215,     0.8629,     0.1139],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0009, 0.0680, 0.0613, 0.2125, 0.6573], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.01 ]
 [0.012]
 [0.012]
 [0.012]] [[35.005]
 [44.676]
 [45.647]
 [45.71 ]
 [45.73 ]] [[0.606]
 [1.011]
 [1.053]
 [1.056]
 [1.056]]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.9479871
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[38.726]
 [43.768]
 [38.726]
 [38.726]
 [38.726]] [[0.638]
 [0.813]
 [0.638]
 [0.638]
 [0.638]]
printing an ep nov before normalisation:  38.853046894073486
printing an ep nov before normalisation:  46.456159752032065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  62.58291568645697
printing an ep nov before normalisation:  55.068075166592514
line 256 mcts: sample exp_bonus 35.299254900245636
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.008]
 [0.009]
 [0.009]
 [0.009]] [[32.076]
 [48.402]
 [38.191]
 [36.491]
 [36.309]] [[0.617]
 [1.206]
 [0.838]
 [0.777]
 [0.77 ]]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.009]
 [0.01 ]
 [0.008]] [[38.723]
 [26.207]
 [26.334]
 [25.476]
 [38.723]] [[3.792]
 [1.99 ]
 [2.009]
 [1.886]
 [3.792]]
printing an ep nov before normalisation:  59.757391784884
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  59.06076852769563
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  37.59687709926727
printing an ep nov before normalisation:  31.299004891077704
printing an ep nov before normalisation:  31.85725284554028
printing an ep nov before normalisation:  75.30992844532754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  38.58350313043375
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.008]
 [0.001]
 [0.007]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.008]
 [0.001]
 [0.007]
 [0.007]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[40.26]
 [40.26]
 [40.26]
 [40.26]
 [40.26]] [[1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.111]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.94519454
siam score:  -0.9460864
printing an ep nov before normalisation:  39.05922138225882
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.006]
 [0.007]
 [0.007]
 [0.007]] [[43.658]
 [50.626]
 [52.518]
 [52.653]
 [53.43 ]] [[1.325]
 [1.653]
 [1.744]
 [1.75 ]
 [1.786]]
printing an ep nov before normalisation:  44.02676784447979
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  62.622395468408655
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  3  action  0 :  tensor([    0.9994,     0.0002,     0.0000,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9795,     0.0031,     0.0000,     0.0172],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0009,     0.9047,     0.0379,     0.0564],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0013,     0.0293,     0.8194,     0.1498],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0036, 0.0813, 0.0418, 0.2170, 0.6563], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
printing an ep nov before normalisation:  83.97675587343862
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.007]
 [0.006]
 [0.006]
 [0.006]] [[51.445]
 [60.671]
 [51.445]
 [51.445]
 [51.445]] [[0.921]
 [1.172]
 [0.921]
 [0.921]
 [0.921]]
printing an ep nov before normalisation:  38.585723671078014
printing an ep nov before normalisation:  58.421290135829366
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.9987,     0.0001,     0.0000,     0.0002,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9687,     0.0001,     0.0001,     0.0311],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0003,     0.8727,     0.0649,     0.0621],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0003,     0.0253,     0.8260,     0.1483],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0175, 0.0034, 0.0195, 0.2497, 0.7098], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.00857805372249
siam score:  -0.9407881
siam score:  -0.94146866
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[30.59]
 [30.59]
 [30.59]
 [30.59]
 [30.59]] [[0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  3  action  0 :  tensor([    0.9995,     0.0000,     0.0000,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9610,     0.0001,     0.0008,     0.0380],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9971,     0.0007,     0.0022],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0001,     0.0022,     0.9678,     0.0297],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0010, 0.0386, 0.1020, 0.1658, 0.6926], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.005]
 [0.005]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.005]
 [0.005]
 [0.005]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
line 256 mcts: sample exp_bonus 56.835046262043605
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  41.8556890153886
printing an ep nov before normalisation:  51.331711006649904
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[61.592]
 [61.592]
 [62.852]
 [69.635]
 [66.718]] [[1.011]
 [1.011]
 [1.046]
 [1.233]
 [1.152]]
printing an ep nov before normalisation:  37.44985103607178
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.005]] [[40.122]
 [40.122]
 [40.122]
 [40.122]
 [46.973]] [[1.211]
 [1.211]
 [1.211]
 [1.211]
 [1.672]]
printing an ep nov before normalisation:  46.615821397494784
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
printing an ep nov before normalisation:  42.99857789669257
printing an ep nov before normalisation:  38.681302602688504
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  78.28906270581341
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.004]
 [0.005]
 [0.005]
 [0.004]] [[56.312]
 [54.58 ]
 [54.559]
 [54.445]
 [58.091]] [[1.227]
 [1.175]
 [1.175]
 [1.171]
 [1.279]]
actions average: 
K:  3  action  0 :  tensor([    0.9218,     0.0144,     0.0003,     0.0259,     0.0376],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0015, 0.9374, 0.0028, 0.0020, 0.0563], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0156,     0.9219,     0.0041,     0.0583],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0002,     0.0169,     0.8493,     0.1334],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0014, 0.1155, 0.1068, 0.2034, 0.5729], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.004]
 [0.005]
 [0.005]
 [0.005]] [[68.637]
 [77.064]
 [68.637]
 [68.637]
 [68.637]] [[1.43 ]
 [1.762]
 [1.43 ]
 [1.43 ]
 [1.43 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  43.681905482421634
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.003]
 [0.001]
 [0.002]
 [0.003]] [[31.059]
 [46.733]
 [36.573]
 [36.346]
 [39.729]] [[0.   ]
 [0.003]
 [0.001]
 [0.002]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  33.15814676563399
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  26.64128336225178
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  36.23884078026319
actions average: 
K:  3  action  0 :  tensor([    0.9700,     0.0011,     0.0000,     0.0152,     0.0136],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0009,     0.9606,     0.0239,     0.0006,     0.0140],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0002,     0.9721,     0.0140,     0.0137],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0017,     0.0008,     0.0437,     0.8334,     0.1204],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0143, 0.1207, 0.1032, 0.1209, 0.6409], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.003]
 [0.   ]
 [0.003]
 [0.003]] [[34.545]
 [35.701]
 [34.895]
 [33.706]
 [35.405]] [[0.991]
 [1.061]
 [1.009]
 [0.939]
 [1.043]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.002]
 [0.003]
 [0.003]] [[28.451]
 [32.388]
 [29.252]
 [35.438]
 [30.219]] [[0.407]
 [0.53 ]
 [0.432]
 [0.625]
 [0.463]]
actions average: 
K:  3  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0214,     0.9514,     0.0034,     0.0008,     0.0230],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0011,     0.0008,     0.9357,     0.0272,     0.0353],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0033,     0.0005,     0.0131,     0.8124,     0.1707],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0083, 0.1115, 0.0641, 0.1964, 0.6197], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  78.25754526486503
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  38.78573179244995
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  41.38728913474181
printing an ep nov before normalisation:  43.95631690310111
printing an ep nov before normalisation:  38.63516330718994
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.598]
 [ 0.273]
 [ 0.275]
 [ 0.228]
 [31.598]] [[1.114]
 [0.01 ]
 [0.01 ]
 [0.008]
 [1.114]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[36.302]
 [36.302]
 [36.302]
 [36.302]
 [36.302]] [[0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[54.726]
 [54.726]
 [54.726]
 [58.302]
 [54.726]] [[1.112]
 [1.112]
 [1.112]
 [1.229]
 [1.112]]
actions average: 
K:  4  action  0 :  tensor([    0.9979,     0.0001,     0.0000,     0.0012,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9924,     0.0021,     0.0002,     0.0053],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0008,     0.0080,     0.8461,     0.0567,     0.0885],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0010,     0.0409,     0.8564,     0.1013],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0006,     0.0510,     0.1244,     0.2047,     0.6193],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.9331441
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[48.504]
 [57.985]
 [48.504]
 [48.504]
 [48.504]] [[0.852]
 [1.143]
 [0.852]
 [0.852]
 [0.852]]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  73.30517759087482
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  27.20299005508423
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  50.70529034101177
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.92782843
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  53.879558730175766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  27.567449483581118
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.92478514
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  35.538801380131765
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  50.245002717230065
printing an ep nov before normalisation:  48.94118056831099
printing an ep nov before normalisation:  49.80989749122801
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  49.522430753630275
printing an ep nov before normalisation:  53.115713957857615
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  1  action  0 :  tensor([    0.9935,     0.0013,     0.0014,     0.0005,     0.0033],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9834,     0.0008,     0.0000,     0.0157],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0004,     0.8926,     0.0447,     0.0623],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0005,     0.0011,     0.0488,     0.7511,     0.1985],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0015, 0.0579, 0.0778, 0.3604, 0.5023], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.002]
 [0.002]
 [0.002]] [[48.113]
 [44.804]
 [42.939]
 [49.518]
 [46.679]] [[0.798]
 [0.675]
 [0.604]
 [0.851]
 [0.744]]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  59.992843597343736
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  46.49681029565731
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  40.88738064647838
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.91317797
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.08089354397065
actions average: 
K:  3  action  0 :  tensor([    0.9997,     0.0001,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9577,     0.0000,     0.0000,     0.0421],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0050,     0.8075,     0.0629,     0.1245],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0009,     0.0003,     0.0535,     0.8509,     0.0943],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0257, 0.0120, 0.0565, 0.2286, 0.6772], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  49.99828730025616
siam score:  -0.90965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  50.71638550743184
printing an ep nov before normalisation:  26.592333876366098
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  36.448989978682505
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  46.80778205898332
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.001]
 [0.001]
 [0.001]] [[40.835]
 [48.008]
 [40.835]
 [40.835]
 [42.428]] [[0.562]
 [0.783]
 [0.562]
 [0.562]
 [0.611]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.002]]
printing an ep nov before normalisation:  20.459316470239198
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
line 256 mcts: sample exp_bonus 58.17111260058643
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.21122580368282
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  53.7000977698588
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.011]
 [0.009]
 [0.01 ]
 [0.01 ]] [[46.967]
 [46.1  ]
 [50.908]
 [50.278]
 [51.154]] [[1.289]
 [1.24 ]
 [1.499]
 [1.465]
 [1.513]]
printing an ep nov before normalisation:  48.26763631221878
printing an ep nov before normalisation:  53.93459332550903
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.83023917540376
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  46.619314920937995
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.002]
 [0.001]] [[43.273]
 [43.273]
 [51.65 ]
 [51.877]
 [43.273]] [[0.902]
 [0.902]
 [1.165]
 [1.172]
 [0.902]]
printing an ep nov before normalisation:  47.170937487136904
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  37.292411747111906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  60.65170305329731
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  35.52207614968181
printing an ep nov before normalisation:  51.56409311715513
printing an ep nov before normalisation:  44.901776471465084
siam score:  -0.90104955
printing an ep nov before normalisation:  29.109511375427246
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  47.967381923538014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  19.308807849884033
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.90339285
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  45.0865160733114
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
actions average: 
K:  4  action  0 :  tensor([    0.9834,     0.0025,     0.0008,     0.0036,     0.0096],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0091,     0.9407,     0.0009,     0.0008,     0.0485],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9353,     0.0171,     0.0476],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0007,     0.0023,     0.8889,     0.1081],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0232, 0.0556, 0.0385, 0.0944, 0.7883], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  78.25751883960797
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.52478313446045
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  45.237832327372345
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  29.63001823314353
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[31.21 ]
 [31.21 ]
 [30.484]
 [43.602]
 [31.21 ]] [[0.358]
 [0.358]
 [0.339]
 [0.675]
 [0.358]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.89393353
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.003]
 [0.003]
 [0.002]
 [0.002]] [[36.008]
 [39.21 ]
 [35.495]
 [32.068]
 [32.068]] [[0.884]
 [1.005]
 [0.867]
 [0.739]
 [0.739]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
siam score:  -0.89658636
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.144946863811995
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.002]
 [0.003]
 [0.002]] [[38.155]
 [38.456]
 [37.983]
 [30.336]
 [32.477]] [[0.25 ]
 [0.253]
 [0.248]
 [0.163]
 [0.186]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.003]
 [0.003]
 [0.002]] [[33.048]
 [32.775]
 [54.169]
 [50.064]
 [41.27 ]] [[0.293]
 [0.288]
 [0.726]
 [0.642]
 [0.462]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.003]
 [0.002]
 [0.004]] [[41.415]
 [41.415]
 [56.971]
 [43.41 ]
 [41.415]] [[0.507]
 [0.507]
 [0.866]
 [0.551]
 [0.507]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
siam score:  -0.9126309
printing an ep nov before normalisation:  42.64843643521411
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.287]
 [10.067]
 [29.638]
 [14.335]
 [29.672]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  47.432968744135756
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  34.44422535396509
printing an ep nov before normalisation:  27.533273784388033
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  47.6324776228691
printing an ep nov before normalisation:  49.626207654946086
printing an ep nov before normalisation:  40.77484130859375
printing an ep nov before normalisation:  41.15878826407011
printing an ep nov before normalisation:  46.369363732064805
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.007]
 [0.01 ]
 [0.007]
 [0.007]] [[36.191]
 [39.834]
 [36.744]
 [36.693]
 [39.382]] [[0.905]
 [1.108]
 [0.944]
 [0.938]
 [1.083]]
siam score:  -0.91486704
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  37.19903082899514
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.011]
 [0.012]
 [0.013]] [[ 99.973]
 [ 99.973]
 [ 95.01 ]
 [107.222]
 [ 99.973]] [[1.153]
 [1.153]
 [1.085]
 [1.248]
 [1.153]]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.9384763
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.025]
 [0.02 ]
 [0.024]
 [0.025]] [[32.574]
 [32.574]
 [51.603]
 [36.05 ]
 [32.574]] [[0.435]
 [0.435]
 [0.888]
 [0.518]
 [0.435]]
printing an ep nov before normalisation:  48.13446923945006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  1  action  0 :  tensor([    0.9988,     0.0006,     0.0000,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0007,     0.9253,     0.0170,     0.0006,     0.0564],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0031,     0.9370,     0.0241,     0.0355],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0027,     0.0008,     0.0354,     0.8484,     0.1128],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0039, 0.0970, 0.0682, 0.2358, 0.5951], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  65.14686566766304
printing an ep nov before normalisation:  33.645477294921875
printing an ep nov before normalisation:  70.93084291994025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.014]
 [0.017]
 [0.017]
 [0.016]] [[50.785]
 [70.146]
 [50.785]
 [50.785]
 [59.424]] [[0.615]
 [1.047]
 [0.615]
 [0.615]
 [0.808]]
printing an ep nov before normalisation:  25.96116304397583
printing an ep nov before normalisation:  0.743109460179312
printing an ep nov before normalisation:  72.44764670274697
actions average: 
K:  0  action  0 :  tensor([    0.9988,     0.0002,     0.0000,     0.0002,     0.0008],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9615,     0.0011,     0.0001,     0.0371],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0131,     0.9015,     0.0167,     0.0686],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0003,     0.0543,     0.7825,     0.1628],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0008, 0.0567, 0.0717, 0.2085, 0.6624], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.58793112855053
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.016]
 [0.019]
 [0.019]
 [0.019]] [[40.666]
 [40.544]
 [25.128]
 [25.128]
 [25.128]] [[0.904]
 [0.902]
 [0.428]
 [0.428]
 [0.428]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.882347088442614
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.003]
 [0.014]
 [0.015]
 [0.011]] [[25.308]
 [24.679]
 [46.821]
 [45.5  ]
 [20.906]] [[0.421]
 [0.393]
 [1.018]
 [0.983]
 [0.296]]
printing an ep nov before normalisation:  51.864791760639676
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.804247325472218
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  41.89747506889555
printing an ep nov before normalisation:  0.005759343352451651
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.26233739542759
printing an ep nov before normalisation:  40.34537032334756
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.015]
 [0.015]
 [0.014]
 [0.014]] [[33.84 ]
 [33.654]
 [34.698]
 [36.776]
 [37.491]] [[1.051]
 [1.041]
 [1.103]
 [1.226]
 [1.269]]
printing an ep nov before normalisation:  32.89098842136972
printing an ep nov before normalisation:  39.06401028214487
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  36.28708922330033
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  62.75807830444879
printing an ep nov before normalisation:  76.86222117520592
printing an ep nov before normalisation:  54.378255705361006
printing an ep nov before normalisation:  46.16073401905441
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Starting evaluation
printing an ep nov before normalisation:  71.96363409662116
printing an ep nov before normalisation:  80.86579458476507
printing an ep nov before normalisation:  68.49877885936247
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  34.567373690034536
printing an ep nov before normalisation:  45.133622130455294
printing an ep nov before normalisation:  38.98795112665408
printing an ep nov before normalisation:  44.02126669176227
printing an ep nov before normalisation:  46.51676706691948
printing an ep nov before normalisation:  55.448869317894385
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.018]
 [0.023]
 [0.014]
 [0.043]] [[39.16 ]
 [39.885]
 [33.609]
 [28.133]
 [33.849]] [[0.014]
 [0.018]
 [0.023]
 [0.014]
 [0.043]]
printing an ep nov before normalisation:  45.95621059000044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  36.306149467239145
printing an ep nov before normalisation:  55.13554254433007
printing an ep nov before normalisation:  34.168790762806374
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[42.006]
 [42.006]
 [42.006]
 [42.006]
 [42.006]] [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]]
printing an ep nov before normalisation:  37.0162034034729
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  0.016494692633841623
printing an ep nov before normalisation:  39.65329502747214
using explorer policy with actor:  0
printing an ep nov before normalisation:  37.961017360258
printing an ep nov before normalisation:  36.217426350490115
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.73301283353803
printing an ep nov before normalisation:  35.76307537616685
printing an ep nov before normalisation:  41.52101749940613
printing an ep nov before normalisation:  46.16869053760303
line 256 mcts: sample exp_bonus 44.546919289049995
actor:  0 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 39.854754491465805
actor:  0 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.446731661013963
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.08337672656629358, 0.08337672656629358, 0.08337672656629358, 0.5831163671685321, 0.08337672656629358, 0.08337672656629358]
printing an ep nov before normalisation:  41.8003162828687
printing an ep nov before normalisation:  73.84906382311853
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.08337672656629358, 0.08337672656629358, 0.08337672656629358, 0.5831163671685321, 0.08337672656629358, 0.08337672656629358]
printing an ep nov before normalisation:  69.21433220683703
actor:  0 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[23.756]
 [23.756]
 [23.756]
 [23.756]
 [23.756]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.08337676714236127, 0.08337676714236127, 0.08337676714236127, 0.5831161642881938, 0.08337676714236127, 0.08337676714236127]
printing an ep nov before normalisation:  38.43238196087513
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.08337676714236127, 0.08337676714236127, 0.08337676714236127, 0.5831161642881938, 0.08337676714236127, 0.08337676714236127]
printing an ep nov before normalisation:  28.805077075958252
printing an ep nov before normalisation:  35.34912428608184
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.08337676714236127, 0.08337676714236127, 0.08337676714236127, 0.5831161642881938, 0.08337676714236127, 0.08337676714236127]
line 256 mcts: sample exp_bonus 27.442928490291756
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.08337676714236127, 0.08337676714236127, 0.08337676714236127, 0.5831161642881938, 0.08337676714236127, 0.08337676714236127]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.08337676714236127, 0.08337676714236127, 0.08337676714236127, 0.5831161642881938, 0.08337676714236127, 0.08337676714236127]
printing an ep nov before normalisation:  0.005906309046963543
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
actor:  0 policy actor:  1  step number:  116 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0081 0.0 0.0081
printing an ep nov before normalisation:  47.72606882973022
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
printing an ep nov before normalisation:  38.31986400764251
printing an ep nov before normalisation:  31.261632442474365
printing an ep nov before normalisation:  61.91286073080727
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.013]
 [0.013]
 [0.012]
 [0.013]] [[28.121]
 [26.039]
 [25.915]
 [26.055]
 [25.746]] [[1.08 ]
 [0.919]
 [0.91 ]
 [0.92 ]
 [0.896]]
printing an ep nov before normalisation:  49.38119369502088
printing an ep nov before normalisation:  38.49835157394409
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
printing an ep nov before normalisation:  55.77028983078805
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
using another actor
printing an ep nov before normalisation:  27.145333342471414
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
using explorer policy with actor:  0
printing an ep nov before normalisation:  49.919420947198205
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.04163074493408
printing an ep nov before normalisation:  49.689403459359575
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.005]
 [0.007]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9789,     0.0001,     0.0000,     0.0206],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0072,     0.9228,     0.0156,     0.0544],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0005,     0.0004,     0.0012,     0.8947,     0.1032],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0031, 0.0094, 0.0754, 0.2021, 0.7099], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
printing an ep nov before normalisation:  48.109195964220916
maxi score, test score, baseline:  0.0081 0.2 0.2
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
printing an ep nov before normalisation:  46.50243712757197
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
printing an ep nov before normalisation:  48.53899045780068
printing an ep nov before normalisation:  49.978848372046066
siam score:  -0.93940705
printing an ep nov before normalisation:  45.49137129853433
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
printing an ep nov before normalisation:  34.09541130065918
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
printing an ep nov before normalisation:  50.868823473085385
printing an ep nov before normalisation:  99.24459203220047
printing an ep nov before normalisation:  42.50487930868055
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.016]
 [0.037]
 [0.018]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.016]
 [0.037]
 [0.018]
 [0.016]]
printing an ep nov before normalisation:  74.74866632322195
siam score:  -0.94561243
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
printing an ep nov before normalisation:  0.022246332962794213
maxi score, test score, baseline:  0.0081 0.2 0.2
printing an ep nov before normalisation:  64.7561499007699
printing an ep nov before normalisation:  59.16728234466633
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[28.733]
 [29.735]
 [28.733]
 [28.733]
 [28.733]] [[0.601]
 [0.64 ]
 [0.601]
 [0.601]
 [0.601]]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
siam score:  -0.94992125
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.045]
 [0.044]
 [0.036]
 [0.038]] [[39.944]
 [29.389]
 [38.883]
 [40.709]
 [40.063]] [[0.941]
 [0.491]
 [0.905]
 [0.976]
 [0.95 ]]
printing an ep nov before normalisation:  41.16523925936124
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
printing an ep nov before normalisation:  54.903113640558516
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
printing an ep nov before normalisation:  0.0003294970269962505
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[68.105]
 [68.105]
 [68.105]
 [68.105]
 [68.105]] [[1.892]
 [1.892]
 [1.892]
 [1.892]
 [1.892]]
printing an ep nov before normalisation:  41.20114872833471
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.054]
 [0.058]
 [0.051]
 [0.053]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.045]
 [0.054]
 [0.058]
 [0.051]
 [0.053]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
using explorer policy with actor:  1
siam score:  -0.9636015
printing an ep nov before normalisation:  51.299380288786615
siam score:  -0.9631157
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
printing an ep nov before normalisation:  27.066701157184536
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
siam score:  -0.9622247
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
using explorer policy with actor:  0
printing an ep nov before normalisation:  37.898135795942196
printing an ep nov before normalisation:  18.802338959902976
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
printing an ep nov before normalisation:  49.1328693906209
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
printing an ep nov before normalisation:  55.82219445730649
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
printing an ep nov before normalisation:  36.71744414154523
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]] [[54.058]
 [54.058]
 [54.058]
 [54.058]
 [54.058]] [[0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08338226292053419, 0.08338226292053419, 0.08338226292053419, 0.5830886853973292, 0.08338226292053419, 0.08338226292053419]
maxi score, test score, baseline:  0.0081 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.056]
 [0.058]
 [0.057]
 [0.056]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.052]
 [0.056]
 [0.058]
 [0.057]
 [0.056]]
maxi score, test score, baseline:  0.0081 0.2 0.2
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0081 0.2 0.2
actions average: 
K:  1  action  0 :  tensor([    0.9991,     0.0001,     0.0000,     0.0005,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0031,     0.9793,     0.0002,     0.0006,     0.0168],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0006,     0.0143,     0.9023,     0.0374,     0.0453],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0034,     0.0007,     0.0500,     0.8339,     0.1119],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0042, 0.1398, 0.1145, 0.2080, 0.5334], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
siam score:  -0.9681347
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
printing an ep nov before normalisation:  54.24144955350021
from probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
line 256 mcts: sample exp_bonus 31.766545017014842
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.031]
 [0.037]
 [0.033]
 [0.032]] [[26.737]
 [26.871]
 [43.874]
 [32.355]
 [29.687]] [[0.03 ]
 [0.031]
 [0.037]
 [0.033]
 [0.032]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
siam score:  -0.9682463
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
printing an ep nov before normalisation:  49.18637015578413
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
printing an ep nov before normalisation:  54.096532465791526
siam score:  -0.9676483
maxi score, test score, baseline:  0.0081 0.2 0.2
printing an ep nov before normalisation:  50.30848076351991
printing an ep nov before normalisation:  40.94464025716433
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
printing an ep nov before normalisation:  41.505912047122884
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.042]
 [0.042]
 [0.049]
 [0.042]] [[29.493]
 [29.493]
 [29.493]
 [46.05 ]
 [29.493]] [[0.528]
 [0.528]
 [0.528]
 [1.109]
 [0.528]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
printing an ep nov before normalisation:  30.968134049191676
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.054]
 [0.049]
 [0.047]
 [0.045]] [[48.157]
 [46.666]
 [39.702]
 [25.452]
 [35.993]] [[0.603]
 [0.595]
 [0.477]
 [0.241]
 [0.411]]
printing an ep nov before normalisation:  37.876487696019474
maxi score, test score, baseline:  0.0081 0.2 0.2
printing an ep nov before normalisation:  40.159376869375514
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
printing an ep nov before normalisation:  45.867472503056554
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.9989,     0.0000,     0.0000,     0.0002,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0415,     0.9403,     0.0003,     0.0004,     0.0175],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0009, 0.0202, 0.8371, 0.0300, 0.1117], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0001,     0.0515,     0.7423,     0.2059],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0012, 0.0468, 0.1386, 0.1129, 0.7005], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
printing an ep nov before normalisation:  51.875986554175604
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.045]
 [0.037]
 [0.037]
 [0.037]] [[55.62 ]
 [60.458]
 [55.62 ]
 [55.62 ]
 [55.62 ]] [[1.109]
 [1.276]
 [1.109]
 [1.109]
 [1.109]]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.032]
 [0.025]
 [0.025]
 [0.025]] [[48.387]
 [58.692]
 [48.387]
 [48.387]
 [48.387]] [[0.149]
 [0.208]
 [0.149]
 [0.149]
 [0.149]]
printing an ep nov before normalisation:  48.76651325028175
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[35.293]
 [35.293]
 [35.293]
 [35.293]
 [35.293]] [[0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]]
printing an ep nov before normalisation:  58.14591612492637
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
printing an ep nov before normalisation:  55.047897461941766
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
printing an ep nov before normalisation:  37.964932918548584
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
printing an ep nov before normalisation:  48.36281723498534
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.0833744588609374, 0.0833744588609374, 0.0833744588609374, 0.5831277056953129, 0.0833744588609374, 0.0833744588609374]
printing an ep nov before normalisation:  62.82130924777863
printing an ep nov before normalisation:  0.030935926102984013
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.037]
 [0.039]
 [0.039]
 [0.038]] [[19.688]
 [33.491]
 [51.64 ]
 [29.375]
 [23.36 ]] [[0.035]
 [0.037]
 [0.039]
 [0.039]
 [0.038]]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
using another actor
printing an ep nov before normalisation:  57.173510652152494
printing an ep nov before normalisation:  49.75836123660984
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
printing an ep nov before normalisation:  61.4846343641819
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
printing an ep nov before normalisation:  34.647850248548174
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
using another actor
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
printing an ep nov before normalisation:  33.5844522997153
printing an ep nov before normalisation:  35.907436342567365
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
printing an ep nov before normalisation:  40.4848403583758
siam score:  -0.9626392
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
printing an ep nov before normalisation:  36.37925344220365
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.12 ]
 [0.106]
 [0.106]
 [0.106]] [[51.01 ]
 [72.742]
 [51.01 ]
 [51.01 ]
 [51.01 ]] [[0.754]
 [1.155]
 [0.754]
 [0.754]
 [0.754]]
printing an ep nov before normalisation:  41.909183671225954
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
actions average: 
K:  0  action  0 :  tensor([    0.9844,     0.0006,     0.0000,     0.0092,     0.0058],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9723,     0.0137,     0.0002,     0.0136],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0203,     0.9166,     0.0004,     0.0627],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0016, 0.0029, 0.0454, 0.7789, 0.1712], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0446, 0.0603, 0.1428, 0.1156, 0.6368], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.005]
 [0.065]
 [0.06 ]
 [0.065]] [[50.609]
 [38.645]
 [40.91 ]
 [46.69 ]
 [40.91 ]] [[0.895]
 [0.519]
 [0.641]
 [0.795]
 [0.641]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.868]
 [48.868]
 [48.868]
 [48.868]
 [48.868]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
printing an ep nov before normalisation:  34.94079673778177
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
siam score:  -0.9640089
printing an ep nov before normalisation:  34.37561711590522
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.06 ]
 [0.073]
 [0.06 ]
 [0.056]] [[56.259]
 [51.213]
 [42.587]
 [51.213]
 [53.626]] [[1.172]
 [1.029]
 [0.801]
 [1.029]
 [1.093]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337185718271967, 0.08337185718271967, 0.08337185718271967, 0.5831407140864017, 0.08337185718271967, 0.08337185718271967]
actor:  1 policy actor:  1  step number:  101 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337055628266117, 0.08337055628266117, 0.08337055628266117, 0.5831472185866942, 0.08337055628266117, 0.08337055628266117]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337055628266117, 0.08337055628266117, 0.08337055628266117, 0.5831472185866942, 0.08337055628266117, 0.08337055628266117]
maxi score, test score, baseline:  0.0081 0.2 0.2
printing an ep nov before normalisation:  32.236843538054785
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337055628266117, 0.08337055628266117, 0.08337055628266117, 0.5831472185866942, 0.08337055628266117, 0.08337055628266117]
using explorer policy with actor:  1
siam score:  -0.9670266
siam score:  -0.9666422
printing an ep nov before normalisation:  40.90743001760658
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337055628266117, 0.08337055628266117, 0.08337055628266117, 0.5831472185866942, 0.08337055628266117, 0.08337055628266117]
printing an ep nov before normalisation:  28.07702373187409
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337055628266117, 0.08337055628266117, 0.08337055628266117, 0.5831472185866942, 0.08337055628266117, 0.08337055628266117]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337055628266117, 0.08337055628266117, 0.08337055628266117, 0.5831472185866942, 0.08337055628266117, 0.08337055628266117]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337055628266117, 0.08337055628266117, 0.08337055628266117, 0.5831472185866942, 0.08337055628266117, 0.08337055628266117]
printing an ep nov before normalisation:  62.92263916636774
printing an ep nov before normalisation:  23.927690024423605
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337055628266117, 0.08337055628266117, 0.08337055628266117, 0.5831472185866942, 0.08337055628266117, 0.08337055628266117]
from probs:  [0.08337055628266117, 0.08337055628266117, 0.08337055628266117, 0.5831472185866942, 0.08337055628266117, 0.08337055628266117]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337055628266117, 0.08337055628266117, 0.08337055628266117, 0.5831472185866942, 0.08337055628266117, 0.08337055628266117]
printing an ep nov before normalisation:  36.12416284249398
printing an ep nov before normalisation:  45.84231376647949
printing an ep nov before normalisation:  35.34928798675537
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08337055628266117, 0.08337055628266117, 0.08337055628266117, 0.5831472185866942, 0.08337055628266117, 0.08337055628266117]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.961929
Printing some Q and Qe and total Qs values:  [[1.055]
 [1.055]
 [1.055]
 [1.055]
 [1.055]] [[57.957]
 [57.957]
 [57.957]
 [57.957]
 [57.957]] [[2.295]
 [2.295]
 [2.295]
 [2.295]
 [2.295]]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0081 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.001]
 [0.032]
 [0.03 ]
 [0.024]] [[40.752]
 [24.152]
 [30.102]
 [35.057]
 [33.746]] [[0.021]
 [0.001]
 [0.032]
 [0.03 ]
 [0.024]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336925534196711, 0.08336925534196711, 0.08336925534196711, 0.5831537232901646, 0.08336925534196711, 0.08336925534196711]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336925534196711, 0.08336925534196711, 0.08336925534196711, 0.5831537232901646, 0.08336925534196711, 0.08336925534196711]
printing an ep nov before normalisation:  39.166237695484945
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336925534196711, 0.08336925534196711, 0.08336925534196711, 0.5831537232901646, 0.08336925534196711, 0.08336925534196711]
printing an ep nov before normalisation:  45.338204341939594
printing an ep nov before normalisation:  53.29091004836622
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0081 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.004]
 [0.039]
 [0.039]
 [0.034]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.032]
 [0.004]
 [0.039]
 [0.039]
 [0.034]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336925534196711, 0.08336925534196711, 0.08336925534196711, 0.5831537232901646, 0.08336925534196711, 0.08336925534196711]
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.039]
 [0.04 ]
 [0.04 ]
 [0.04 ]] [[37.556]
 [58.261]
 [37.556]
 [37.556]
 [37.556]] [[0.617]
 [1.094]
 [0.617]
 [0.617]
 [0.617]]
printing an ep nov before normalisation:  53.04271739111972
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.031]
 [0.035]
 [0.035]
 [0.035]] [[53.62 ]
 [64.175]
 [53.62 ]
 [53.62 ]
 [53.62 ]] [[0.803]
 [1.087]
 [0.803]
 [0.803]
 [0.803]]
siam score:  -0.9604441
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336925534196711, 0.08336925534196711, 0.08336925534196711, 0.5831537232901646, 0.08336925534196711, 0.08336925534196711]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336925534196711, 0.08336925534196711, 0.08336925534196711, 0.5831537232901646, 0.08336925534196711, 0.08336925534196711]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336925534196711, 0.08336925534196711, 0.08336925534196711, 0.5831537232901646, 0.08336925534196711, 0.08336925534196711]
printing an ep nov before normalisation:  41.98772327622665
siam score:  -0.9618783
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336925534196711, 0.08336925534196711, 0.08336925534196711, 0.5831537232901646, 0.08336925534196711, 0.08336925534196711]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336925534196711, 0.08336925534196711, 0.08336925534196711, 0.5831537232901646, 0.08336925534196711, 0.08336925534196711]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
printing an ep nov before normalisation:  41.828484041829384
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[66.392]
 [66.392]
 [66.392]
 [66.392]
 [66.392]] [[0.993]
 [0.993]
 [0.993]
 [0.993]
 [0.993]]
siam score:  -0.96168464
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.6353658997462
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
printing an ep nov before normalisation:  55.713820878429345
printing an ep nov before normalisation:  59.03699240024002
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
printing an ep nov before normalisation:  41.74045375112125
printing an ep nov before normalisation:  8.426094382230076e-05
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
printing an ep nov before normalisation:  43.0913627301355
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.022]
 [0.035]
 [0.033]
 [0.026]] [[31.813]
 [43.768]
 [27.522]
 [28.458]
 [28.759]] [[0.457]
 [0.787]
 [0.342]
 [0.365]
 [0.367]]
siam score:  -0.9644277
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.027]
 [0.027]
 [0.026]
 [0.027]] [[25.669]
 [25.669]
 [25.669]
 [36.342]
 [25.669]] [[0.625]
 [0.625]
 [0.625]
 [1.098]
 [0.625]]
maxi score, test score, baseline:  0.0081 0.2 0.2
printing an ep nov before normalisation:  58.20327084997987
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.149]
 [0.121]
 [0.121]
 [0.121]] [[31.718]
 [38.304]
 [31.718]
 [31.718]
 [31.718]] [[1.076]
 [1.482]
 [1.076]
 [1.076]
 [1.076]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
printing an ep nov before normalisation:  53.54097969444978
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
printing an ep nov before normalisation:  51.165640056601454
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0081 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.025]
 [0.041]
 [0.03 ]
 [0.029]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.023]
 [0.025]
 [0.041]
 [0.03 ]
 [0.029]]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.032]
 [0.041]
 [0.032]
 [0.033]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.032]
 [0.032]
 [0.041]
 [0.032]
 [0.033]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
printing an ep nov before normalisation:  65.43666825113678
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
maxi score, test score, baseline:  0.0081 0.2 0.2
printing an ep nov before normalisation:  69.95457684798393
Printing some Q and Qe and total Qs values:  [[0.762]
 [1.06 ]
 [0.762]
 [0.762]
 [0.762]] [[47.729]
 [61.212]
 [47.729]
 [47.729]
 [47.729]] [[1.432]
 [2.089]
 [1.432]
 [1.432]
 [1.432]]
printing an ep nov before normalisation:  46.259572232231754
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
siam score:  -0.94606817
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
Printing some Q and Qe and total Qs values:  [[0.05]
 [0.05]
 [0.05]
 [0.05]
 [0.05]] [[69.313]
 [69.313]
 [69.313]
 [69.313]
 [69.313]] [[1.356]
 [1.356]
 [1.356]
 [1.356]
 [1.356]]
actions average: 
K:  1  action  0 :  tensor([    0.9990,     0.0001,     0.0000,     0.0003,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9615,     0.0006,     0.0002,     0.0376],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9441,     0.0339,     0.0219],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0000,     0.0001,     0.0387,     0.9173,     0.0439],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0199, 0.0689, 0.0841, 0.1409, 0.6863], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
printing an ep nov before normalisation:  39.691108114814575
siam score:  -0.94927305
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
using another actor
actions average: 
K:  1  action  0 :  tensor([    0.9993,     0.0001,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9788,     0.0000,     0.0012,     0.0199],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0073,     0.0004,     0.8881,     0.0303,     0.0739],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0003,     0.0007,     0.9109,     0.0880],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0106, 0.0736, 0.1290, 0.1820, 0.6049], grad_fn=<DivBackward0>)
from probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.022]
 [0.02 ]
 [0.02 ]
 [0.02 ]] [[42.59 ]
 [57.955]
 [42.59 ]
 [42.59 ]
 [42.59 ]] [[0.02 ]
 [0.022]
 [0.02 ]
 [0.02 ]
 [0.02 ]]
printing an ep nov before normalisation:  50.99724877852504
printing an ep nov before normalisation:  36.66237831115723
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336888363716201, 0.08336888363716201, 0.08336888363716201, 0.5831555818141901, 0.08336888363716201, 0.08336888363716201]
printing an ep nov before normalisation:  39.350256411136165
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.038]
 [0.038]
 [0.043]
 [0.05 ]] [[58.083]
 [58.083]
 [58.083]
 [53.85 ]
 [58.187]] [[1.267]
 [1.267]
 [1.267]
 [1.139]
 [1.283]]
actor:  1 policy actor:  1  step number:  104 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.17186529210679
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
from probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.310118613861086
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
printing an ep nov before normalisation:  57.006419562951784
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
siam score:  -0.95106256
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
printing an ep nov before normalisation:  37.96870276703564
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.09 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]] [[55.586]
 [54.054]
 [43.386]
 [43.386]
 [43.386]] [[1.088]
 [1.058]
 [0.798]
 [0.798]
 [0.798]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
printing an ep nov before normalisation:  40.555193184245994
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
printing an ep nov before normalisation:  27.930648557421694
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
printing an ep nov before normalisation:  43.34397795907557
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
printing an ep nov before normalisation:  65.98749582726995
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
printing an ep nov before normalisation:  48.51872461985318
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.066]
 [0.094]
 [0.066]
 [0.066]] [[36.36 ]
 [36.36 ]
 [38.412]
 [36.36 ]
 [36.36 ]] [[1.284]
 [1.284]
 [1.427]
 [1.284]
 [1.284]]
actions average: 
K:  4  action  0 :  tensor([    0.9847,     0.0018,     0.0015,     0.0009,     0.0111],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9974,     0.0007,     0.0001,     0.0016],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0001,     0.8808,     0.0683,     0.0508],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0005,     0.0001,     0.0316,     0.9395,     0.0282],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0008, 0.0129, 0.1215, 0.1005, 0.7643], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.002]
 [0.055]
 [0.055]
 [0.055]] [[20.414]
 [21.363]
 [30.575]
 [32.62 ]
 [30.663]] [[0.236]
 [0.196]
 [0.433]
 [0.473]
 [0.435]]
printing an ep nov before normalisation:  49.95009790321024
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
printing an ep nov before normalisation:  50.62940029504472
actions average: 
K:  3  action  0 :  tensor([    0.9995,     0.0000,     0.0000,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0219,     0.9376,     0.0002,     0.0002,     0.0401],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0035,     0.9130,     0.0227,     0.0607],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0009,     0.0002,     0.0487,     0.8625,     0.0877],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0353, 0.0266, 0.1622, 0.1598, 0.6161], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0081 0.2 0.2
printing an ep nov before normalisation:  59.09526263738531
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
printing an ep nov before normalisation:  38.236534797025
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.058]
 [0.062]
 [0.058]
 [0.058]] [[30.932]
 [30.932]
 [38.944]
 [30.932]
 [30.932]] [[0.633]
 [0.633]
 [0.909]
 [0.633]
 [0.633]]
printing an ep nov before normalisation:  49.45600086109977
printing an ep nov before normalisation:  32.98415228920728
printing an ep nov before normalisation:  50.078622172379305
maxi score, test score, baseline:  0.0081 0.2 0.2
printing an ep nov before normalisation:  35.63891921978196
printing an ep nov before normalisation:  40.88602543501882
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.054]
 [0.057]
 [0.054]
 [0.054]] [[38.115]
 [37.303]
 [40.359]
 [37.303]
 [37.303]] [[1.06 ]
 [1.014]
 [1.186]
 [1.014]
 [1.014]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
UNIT TEST: sample policy line 217 mcts : [0.256 0.026 0.154 0.436 0.128]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
printing an ep nov before normalisation:  34.914653301239014
printing an ep nov before normalisation:  35.4872670101206
printing an ep nov before normalisation:  53.68821945375122
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336860485638112, 0.08336860485638112, 0.08336860485638112, 0.5831569757180943, 0.08336860485638112, 0.08336860485638112]
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  51.54455232206413
printing an ep nov before normalisation:  39.65291713521838
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.08336838802559478, 0.08336838802559478, 0.08336838802559478, 0.5831580598720261, 0.08336838802559478, 0.08336838802559478]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[60.245]
 [60.245]
 [60.245]
 [60.245]
 [60.245]] [[1.364]
 [1.364]
 [1.364]
 [1.364]
 [1.364]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
printing an ep nov before normalisation:  47.66655445098877
printing an ep nov before normalisation:  41.25014781951904
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.17 ]
 [0.144]
 [0.1  ]
 [0.166]] [[37.003]
 [31.995]
 [33.082]
 [35.761]
 [31.812]] [[1.133]
 [0.98 ]
 [1.002]
 [1.075]
 [0.968]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[40.186]
 [40.676]
 [40.676]
 [40.676]
 [40.676]] [[1.565]
 [1.646]
 [1.646]
 [1.646]
 [1.646]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
printing an ep nov before normalisation:  50.87942600250244
printing an ep nov before normalisation:  45.73832209414447
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  50.06348816868679
printing an ep nov before normalisation:  61.77304782322937
printing an ep nov before normalisation:  47.4705862445562
printing an ep nov before normalisation:  56.464352961864094
printing an ep nov before normalisation:  39.448317266752106
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
printing an ep nov before normalisation:  63.174010205741126
printing an ep nov before normalisation:  55.25528182480556
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  55.80484147020213
printing an ep nov before normalisation:  57.995799271684824
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[32.905]
 [32.905]
 [32.905]
 [32.905]
 [32.905]] [[1.379]
 [1.379]
 [1.379]
 [1.379]
 [1.379]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.62139940261841
maxi score, test score, baseline:  0.0081 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.096]
 [0.092]
 [0.081]
 [0.084]] [[29.044]
 [29.865]
 [32.873]
 [34.048]
 [34.134]] [[1.067]
 [1.149]
 [1.342]
 [1.408]
 [1.417]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.838986423815236
siam score:  -0.95416564
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
printing an ep nov before normalisation:  36.33516388053334
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
actions average: 
K:  3  action  0 :  tensor([    0.9940,     0.0004,     0.0000,     0.0027,     0.0030],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9711,     0.0002,     0.0002,     0.0283],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0114,     0.9281,     0.0165,     0.0439],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0001,     0.0418,     0.9134,     0.0446],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0048, 0.1034, 0.0422, 0.1799, 0.6697], grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([    0.9497,     0.0253,     0.0004,     0.0009,     0.0236],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9771,     0.0001,     0.0001,     0.0227],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0267,     0.8438,     0.0562,     0.0731],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0004,     0.0036,     0.0239,     0.8530,     0.1191],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0016, 0.1184, 0.0457, 0.1411, 0.6932], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 50.222832956087494
printing an ep nov before normalisation:  35.93735084941487
printing an ep nov before normalisation:  48.138051620695585
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
printing an ep nov before normalisation:  39.48719168854685
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07898601712841487, 0.07898601712841487, 0.13159440685136595, 0.5524615246349746, 0.07898601712841487, 0.07898601712841487]
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07940320518694015, 0.07940320518694015, 0.12700145690315462, 0.5553857223490848, 0.07940320518694015, 0.07940320518694015]
printing an ep nov before normalisation:  71.07467169361126
printing an ep nov before normalisation:  35.10055214068945
printing an ep nov before normalisation:  22.51903670174735
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07940320518694015, 0.07940320518694015, 0.12700145690315462, 0.5553857223490848, 0.07940320518694015, 0.07940320518694015]
using explorer policy with actor:  1
from probs:  [0.07940320518694015, 0.07940320518694015, 0.12700145690315462, 0.5553857223490848, 0.07940320518694015, 0.07940320518694015]
printing an ep nov before normalisation:  29.298991600260617
printing an ep nov before normalisation:  29.044948930550827
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07940320518694015, 0.07940320518694015, 0.12700145690315462, 0.5553857223490848, 0.07940320518694015, 0.07940320518694015]
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]] [[36.899]
 [36.899]
 [36.899]
 [36.899]
 [36.899]] [[0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07940320518694015, 0.07940320518694015, 0.12700145690315462, 0.5553857223490848, 0.07940320518694015, 0.07940320518694015]
printing an ep nov before normalisation:  31.760659217834473
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07940320518694015, 0.07940320518694015, 0.12700145690315462, 0.5553857223490848, 0.07940320518694015, 0.07940320518694015]
printing an ep nov before normalisation:  36.410369873046875
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
printing an ep nov before normalisation:  65.04745195539955
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07579877608523711, 0.07579877608523711, 0.16666666666666666, 0.5301382289923849, 0.07579877608523711, 0.07579877608523711]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.9966,     0.0003,     0.0000,     0.0005,     0.0027],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0433,     0.9012,     0.0005,     0.0002,     0.0548],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0012,     0.0006,     0.8491,     0.0744,     0.0747],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0018,     0.0005,     0.0261,     0.8973,     0.0744],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0727, 0.0362, 0.1124, 0.1987, 0.5800], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07579877608523711, 0.07579877608523711, 0.16666666666666666, 0.5301382289923849, 0.07579877608523711, 0.07579877608523711]
printing an ep nov before normalisation:  0.001000057912392549
actions average: 
K:  2  action  0 :  tensor([    0.9981,     0.0003,     0.0000,     0.0008,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0019,     0.9823,     0.0044,     0.0002,     0.0112],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0017, 0.0256, 0.8160, 0.0252, 0.1315], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0009,     0.0004,     0.0361,     0.8266,     0.1360],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0013, 0.0567, 0.0582, 0.1189, 0.7649], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.582]
 [0.554]
 [0.59 ]
 [0.575]] [[57.792]
 [58.651]
 [54.908]
 [54.032]
 [57.905]] [[1.481]
 [1.486]
 [1.363]
 [1.376]
 [1.46 ]]
printing an ep nov before normalisation:  28.201943363988278
printing an ep nov before normalisation:  48.63685689229079
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.028]
 [0.032]
 [0.031]
 [0.035]] [[20.092]
 [20.049]
 [30.298]
 [19.206]
 [20.479]] [[0.112]
 [0.139]
 [0.253]
 [0.132]
 [0.151]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07579877608523711, 0.07579877608523711, 0.16666666666666666, 0.5301382289923849, 0.07579877608523711, 0.07579877608523711]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.075]
 [0.075]
 [0.074]
 [0.075]] [[52.487]
 [52.487]
 [54.967]
 [55.498]
 [55.542]] [[1.317]
 [1.317]
 [1.391]
 [1.406]
 [1.408]]
actions average: 
K:  3  action  0 :  tensor([    0.9979,     0.0001,     0.0000,     0.0003,     0.0017],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9844,     0.0001,     0.0001,     0.0153],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0022,     0.0005,     0.9242,     0.0210,     0.0521],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0001,     0.0451,     0.9210,     0.0334],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0214, 0.0533, 0.1460, 0.1440, 0.6353], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07579877608523711, 0.07579877608523711, 0.16666666666666666, 0.5301382289923849, 0.07579877608523711, 0.07579877608523711]
printing an ep nov before normalisation:  52.51473110211046
printing an ep nov before normalisation:  47.19265309499228
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07579877608523711, 0.07579877608523711, 0.16666666666666666, 0.5301382289923849, 0.07579877608523711, 0.07579877608523711]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07579877608523711, 0.07579877608523711, 0.16666666666666666, 0.5301382289923849, 0.07579877608523711, 0.07579877608523711]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07579877608523711, 0.07579877608523711, 0.16666666666666666, 0.5301382289923849, 0.07579877608523711, 0.07579877608523711]
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.034]
 [0.034]
 [0.034]
 [0.034]] [[76.668]
 [51.532]
 [51.532]
 [51.532]
 [51.532]] [[0.028]
 [0.034]
 [0.034]
 [0.034]
 [0.034]]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07579877608523711, 0.07579877608523711, 0.16666666666666666, 0.5301382289923849, 0.07579877608523711, 0.07579877608523711]
printing an ep nov before normalisation:  57.75145276687353
printing an ep nov before normalisation:  32.23912709132576
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.083]
 [0.077]
 [0.077]
 [0.077]] [[40.47 ]
 [44.436]
 [40.47 ]
 [40.47 ]
 [40.47 ]] [[0.934]
 [1.113]
 [0.934]
 [0.934]
 [0.934]]
printing an ep nov before normalisation:  46.91276394921696
printing an ep nov before normalisation:  48.19872268419732
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.08964763368879
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07642940166099038, 0.07642940166099038, 0.15972533858930696, 0.5345570547667315, 0.07642940166099038, 0.07642940166099038]
printing an ep nov before normalisation:  32.55914661370854
printing an ep nov before normalisation:  64.56904505528234
printing an ep nov before normalisation:  36.87705921059215
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[60.838]
 [48.258]
 [48.258]
 [48.258]
 [48.258]] [[2.133]
 [1.703]
 [1.703]
 [1.703]
 [1.703]]
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.006]
 [0.085]
 [0.081]
 [0.057]] [[31.143]
 [18.587]
 [30.133]
 [34.961]
 [20.481]] [[0.774]
 [0.19 ]
 [0.742]
 [0.937]
 [0.319]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07642940166099038, 0.07642940166099038, 0.15972533858930696, 0.5345570547667315, 0.07642940166099038, 0.07642940166099038]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07642940166099038, 0.07642940166099038, 0.15972533858930696, 0.5345570547667315, 0.07642940166099038, 0.07642940166099038]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07642940166099038, 0.07642940166099038, 0.15972533858930696, 0.5345570547667315, 0.07642940166099038, 0.07642940166099038]
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[54.712]
 [54.712]
 [54.712]
 [54.712]
 [54.712]] [[1.86]
 [1.86]
 [1.86]
 [1.86]
 [1.86]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.77113467348271
siam score:  -0.9423874
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07642940166099038, 0.07642940166099038, 0.15972533858930696, 0.5345570547667315, 0.07642940166099038, 0.07642940166099038]
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  61.85925848205038
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07696301229997285, 0.07696301229997285, 0.15385185889999609, 0.5382960919001123, 0.07696301229997285, 0.07696301229997285]
UNIT TEST: sample policy line 217 mcts : [0.667 0.179 0.077 0.051 0.026]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07696301229997285, 0.07696301229997285, 0.15385185889999609, 0.5382960919001123, 0.07696301229997285, 0.07696301229997285]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07696301229997285, 0.07696301229997285, 0.15385185889999609, 0.5382960919001123, 0.07696301229997285, 0.07696301229997285]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
siam score:  -0.9413153
printing an ep nov before normalisation:  63.293393401947654
printing an ep nov before normalisation:  66.299339747899
printing an ep nov before normalisation:  54.38895030107904
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
printing an ep nov before normalisation:  60.12381553526013
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.058]
 [0.058]
 [0.072]
 [0.058]] [[38.571]
 [38.571]
 [38.571]
 [43.281]
 [38.571]] [[0.631]
 [0.631]
 [0.631]
 [0.758]
 [0.631]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
printing an ep nov before normalisation:  70.15216873642032
printing an ep nov before normalisation:  57.38544793190926
printing an ep nov before normalisation:  39.13482207929088
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
printing an ep nov before normalisation:  0.019099131659743307
printing an ep nov before normalisation:  0.5088084076805899
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]] [[68.489]
 [68.489]
 [68.489]
 [68.489]
 [68.489]] [[1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]]
printing an ep nov before normalisation:  75.18591883731025
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.075]
 [0.068]
 [0.064]
 [0.063]] [[47.285]
 [25.151]
 [26.338]
 [24.108]
 [23.062]] [[0.803]
 [0.366]
 [0.384]
 [0.332]
 [0.31 ]]
siam score:  -0.94480634
printing an ep nov before normalisation:  74.08692308587905
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.114]
 [0.016]
 [0.056]
 [0.065]] [[36.963]
 [41.436]
 [43.236]
 [53.227]
 [46.192]] [[0.547]
 [0.783]
 [0.735]
 [1.058]
 [0.868]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.091]
 [0.091]
 [0.091]
 [0.   ]] [[38.429]
 [38.017]
 [38.017]
 [38.017]
 [25.366]] [[0.667]
 [0.744]
 [0.744]
 [0.744]
 [0.235]]
printing an ep nov before normalisation:  45.41465433215328
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
printing an ep nov before normalisation:  71.33522271986217
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.102]
 [0.082]
 [0.082]
 [0.07 ]] [[43.305]
 [50.473]
 [52.643]
 [52.643]
 [36.371]] [[1.255]
 [1.769]
 [1.885]
 [1.885]
 [0.85 ]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
printing an ep nov before normalisation:  41.11455371992655
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
printing an ep nov before normalisation:  64.11149064334367
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.067]
 [0.063]
 [0.062]
 [0.063]] [[42.025]
 [36.455]
 [40.638]
 [44.971]
 [42.254]] [[0.922]
 [0.692]
 [0.825]
 [0.965]
 [0.877]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.46659596436967
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]] [[21.356]
 [21.356]
 [21.356]
 [21.356]
 [21.356]] [[0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]]
printing an ep nov before normalisation:  47.86191188360439
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
printing an ep nov before normalisation:  29.999225148705385
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
printing an ep nov before normalisation:  27.67021657514117
printing an ep nov before normalisation:  26.354622840881348
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
printing an ep nov before normalisation:  26.07534923849711
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
printing an ep nov before normalisation:  0.011015369272229236
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07411649290371541, 0.07411649290371541, 0.1481566319140764, 0.5183573269658813, 0.11113656240889591, 0.07411649290371541]
printing an ep nov before normalisation:  46.80988197194822
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
from probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
line 256 mcts: sample exp_bonus 33.869640592030244
siam score:  -0.9357576
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.143]
 [0.015]
 [0.081]
 [0.109]] [[25.592]
 [30.22 ]
 [28.883]
 [37.617]
 [29.553]] [[0.61 ]
 [0.963]
 [0.767]
 [1.277]
 [0.895]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.106]
 [0.106]
 [0.106]
 [0.106]] [[37.792]
 [31.777]
 [31.777]
 [31.777]
 [31.777]] [[1.232]
 [0.918]
 [0.918]
 [0.918]
 [0.918]]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.547]
 [0.547]
 [0.612]
 [0.547]] [[40.301]
 [40.301]
 [40.301]
 [47.967]
 [40.301]] [[1.695]
 [1.695]
 [1.695]
 [2.212]
 [1.695]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
printing an ep nov before normalisation:  64.68836453967788
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
printing an ep nov before normalisation:  31.449054597379106
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
maxi score, test score, baseline:  0.0081 0.2 0.2
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
printing an ep nov before normalisation:  37.62940223500896
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
printing an ep nov before normalisation:  30.98219469881348
Starting evaluation
printing an ep nov before normalisation:  49.406981773546235
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  39.68633866300089
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.001]
 [0.055]
 [0.047]
 [0.05 ]] [[32.868]
 [27.4  ]
 [45.406]
 [41.069]
 [40.006]] [[0.036]
 [0.001]
 [0.055]
 [0.047]
 [0.05 ]]
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
printing an ep nov before normalisation:  37.80609575668407
printing an ep nov before normalisation:  33.944168386014326
maxi score, test score, baseline:  0.0081 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
Printing some Q and Qe and total Qs values:  [[1.493]
 [1.493]
 [1.494]
 [1.488]
 [1.495]] [[23.52 ]
 [23.805]
 [44.415]
 [23.674]
 [46.681]] [[2.139]
 [2.147]
 [2.715]
 [2.138]
 [2.778]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0101 0.2 0.2
probs:  [0.07147336327757683, 0.10717085204848555, 0.14286834081939423, 0.4998432285284811, 0.10717085204848555, 0.07147336327757683]
line 256 mcts: sample exp_bonus 31.94221731374068
printing an ep nov before normalisation:  35.1450968158331
printing an ep nov before normalisation:  57.329403733209666
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.27581685075312
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.166]
 [0.088]
 [0.073]
 [0.089]] [[41.445]
 [49.759]
 [40.794]
 [34.317]
 [41.445]] [[0.089]
 [0.166]
 [0.088]
 [0.073]
 [0.089]]
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.39726781115818
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 54.673164325042464
printing an ep nov before normalisation:  30.506386756896973
printing an ep nov before normalisation:  56.06009899825332
actor:  0 policy actor:  0  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.07226619710762515, 0.10558400989316921, 0.1389018226787133, 0.5053977633196981, 0.10558400989316921, 0.07226619710762515]
printing an ep nov before normalisation:  44.71250057220459
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.094]
 [0.086]
 [0.081]
 [0.082]] [[41.144]
 [51.651]
 [59.734]
 [54.435]
 [55.901]] [[0.405]
 [0.651]
 [0.821]
 [0.7  ]
 [0.733]]
actor:  0 policy actor:  0  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
printing an ep nov before normalisation:  91.46589832506257
actions average: 
K:  1  action  0 :  tensor([    0.9933,     0.0055,     0.0000,     0.0002,     0.0010],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9744,     0.0059,     0.0002,     0.0194],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0333,     0.8499,     0.0269,     0.0897],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0017,     0.0001,     0.0415,     0.8148,     0.1419],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0008, 0.0020, 0.0576, 0.1708, 0.7687], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.07226680427952, 0.10558440276910117, 0.13890200125868235, 0.5053955846440752, 0.10558440276910117, 0.07226680427952]
printing an ep nov before normalisation:  64.12822868204768
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.07226680427952, 0.10558440276910117, 0.13890200125868235, 0.5053955846440752, 0.10558440276910117, 0.07226680427952]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 5.759]
 [ 8.197]
 [ 8.466]
 [12.208]
 [11.599]] [[0.209]
 [0.298]
 [0.308]
 [0.445]
 [0.422]]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.07226680427952, 0.10558440276910117, 0.13890200125868235, 0.5053955846440752, 0.10558440276910117, 0.07226680427952]
printing an ep nov before normalisation:  37.50634228858374
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.07226680427952, 0.10558440276910117, 0.13890200125868235, 0.5053955846440752, 0.10558440276910117, 0.07226680427952]
printing an ep nov before normalisation:  46.94128833026806
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.07226680427952, 0.10558440276910117, 0.13890200125868235, 0.5053955846440752, 0.10558440276910117, 0.07226680427952]
printing an ep nov before normalisation:  55.845350835559294
printing an ep nov before normalisation:  50.900630950927734
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
UNIT TEST: sample policy line 217 mcts : [0.103 0.154 0.128 0.179 0.436]
printing an ep nov before normalisation:  53.27420055412769
using explorer policy with actor:  1
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.0699391770422639, 0.1021816735837315, 0.16666666666666666, 0.4890916320813426, 0.1021816735837315, 0.0699391770422639]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.0699391770422639, 0.1021816735837315, 0.16666666666666666, 0.4890916320813426, 0.1021816735837315, 0.0699391770422639]
printing an ep nov before normalisation:  30.245047807693485
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.0699391770422639, 0.1021816735837315, 0.16666666666666666, 0.4890916320813426, 0.1021816735837315, 0.0699391770422639]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.0699391770422639, 0.1021816735837315, 0.16666666666666666, 0.4890916320813426, 0.1021816735837315, 0.0699391770422639]
printing an ep nov before normalisation:  61.07441537272822
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.0699391770422639, 0.1021816735837315, 0.16666666666666666, 0.4890916320813426, 0.1021816735837315, 0.0699391770422639]
siam score:  -0.9351396
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.0707529001861665, 0.10104145802211392, 0.16161857369400875, 0.4947927098894304, 0.10104145802211392, 0.0707529001861665]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.0707529001861665, 0.10104145802211392, 0.16161857369400875, 0.4947927098894304, 0.10104145802211392, 0.0707529001861665]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.0707529001861665, 0.10104145802211392, 0.16161857369400875, 0.4947927098894304, 0.10104145802211392, 0.0707529001861665]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  74.31706628948596
siam score:  -0.9372501
printing an ep nov before normalisation:  47.50258721592032
printing an ep nov before normalisation:  34.48335715994289
printing an ep nov before normalisation:  1.2171048629227244
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.13 ]
 [0.106]
 [0.106]
 [0.106]] [[44.734]
 [41.971]
 [44.734]
 [44.734]
 [44.734]] [[1.339]
 [1.248]
 [1.339]
 [1.339]
 [1.339]]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06867520178124131, 0.09807264124686893, 0.1862649596437518, 0.48023935430002795, 0.09807264124686893, 0.06867520178124131]
printing an ep nov before normalisation:  41.11453487998295
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06867520178124131, 0.09807264124686893, 0.1862649596437518, 0.48023935430002795, 0.09807264124686893, 0.06867520178124131]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06867520178124131, 0.09807264124686893, 0.1862649596437518, 0.48023935430002795, 0.09807264124686893, 0.06867520178124131]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06867520178124131, 0.09807264124686893, 0.1862649596437518, 0.48023935430002795, 0.09807264124686893, 0.06867520178124131]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
printing an ep nov before normalisation:  43.14181055503541
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
printing an ep nov before normalisation:  63.17524874915391
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06867520178124131, 0.09807264124686892, 0.18626495964375175, 0.48023935430002795, 0.09807264124686892, 0.06867520178124131]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06867520178124131, 0.09807264124686892, 0.18626495964375175, 0.48023935430002795, 0.09807264124686892, 0.06867520178124131]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.081]
 [0.092]
 [0.081]
 [0.082]] [[43.471]
 [31.636]
 [34.227]
 [31.636]
 [37.132]] [[1.008]
 [0.557]
 [0.669]
 [0.557]
 [0.773]]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06867520178124131, 0.09807264124686892, 0.18626495964375175, 0.48023935430002795, 0.09807264124686892, 0.06867520178124131]
line 256 mcts: sample exp_bonus 0.0
actions average: 
K:  3  action  0 :  tensor([    0.9988,     0.0003,     0.0000,     0.0003,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9547,     0.0005,     0.0004,     0.0442],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0002,     0.9208,     0.0157,     0.0632],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0000,     0.0001,     0.0161,     0.9423,     0.0415],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0238, 0.0275, 0.0963, 0.2176, 0.6348], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  54.060739435023116
printing an ep nov before normalisation:  65.13576741253453
using explorer policy with actor:  1
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06867520178124131, 0.09807264124686892, 0.18626495964375175, 0.48023935430002795, 0.09807264124686892, 0.06867520178124131]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06867520178124131, 0.09807264124686892, 0.18626495964375175, 0.48023935430002795, 0.09807264124686892, 0.06867520178124131]
printing an ep nov before normalisation:  49.01031455262247
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06867520178124131, 0.09807264124686892, 0.18626495964375175, 0.48023935430002795, 0.09807264124686892, 0.06867520178124131]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06671626504685847, 0.12383078025817745, 0.18094529546949642, 0.4665178715260913, 0.09527352265251796, 0.06671626504685847]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06671626504685847, 0.12383078025817745, 0.18094529546949642, 0.4665178715260913, 0.09527352265251796, 0.06671626504685847]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06671626504685847, 0.12383078025817745, 0.18094529546949642, 0.4665178715260913, 0.09527352265251796, 0.06671626504685847]
line 256 mcts: sample exp_bonus 46.4581359687689
from probs:  [0.06671626504685847, 0.12383078025817745, 0.18094529546949642, 0.4665178715260913, 0.09527352265251796, 0.06671626504685847]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
printing an ep nov before normalisation:  45.73418661411897
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06671626504685847, 0.12383078025817745, 0.18094529546949642, 0.4665178715260913, 0.09527352265251796, 0.06671626504685847]
printing an ep nov before normalisation:  49.352209426693506
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06671626504685847, 0.12383078025817745, 0.18094529546949642, 0.4665178715260913, 0.09527352265251796, 0.06671626504685847]
siam score:  -0.9444608
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.06671626504685847, 0.12383078025817745, 0.18094529546949642, 0.4665178715260913, 0.09527352265251796, 0.06671626504685847]
actor:  0 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.172024390340766
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06671626504685847, 0.12383078025817745, 0.18094529546949642, 0.4665178715260913, 0.09527352265251796, 0.06671626504685847]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.9192399088847
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06671626504685847, 0.12383078025817745, 0.18094529546949642, 0.4665178715260913, 0.09527352265251796, 0.06671626504685847]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06671626504685847, 0.12383078025817745, 0.18094529546949642, 0.4665178715260913, 0.09527352265251796, 0.06671626504685847]
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]] [[49.39]
 [49.39]
 [49.39]
 [49.39]
 [49.39]] [[0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]]
from probs:  [0.06671626504685847, 0.12383078025817745, 0.18094529546949642, 0.4665178715260913, 0.09527352265251796, 0.06671626504685847]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06671626504685847, 0.12383078025817745, 0.18094529546949642, 0.4665178715260913, 0.09527352265251796, 0.06671626504685847]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.0540341037012
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06671626504685847, 0.12383078025817745, 0.18094529546949642, 0.4665178715260913, 0.09527352265251796, 0.06671626504685847]
printing an ep nov before normalisation:  62.78007721542339
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.096]
 [0.102]
 [0.096]
 [0.096]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.096]
 [0.096]
 [0.102]
 [0.096]
 [0.096]]
from probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
printing an ep nov before normalisation:  28.098667260580402
printing an ep nov before normalisation:  56.666302015511704
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
using explorer policy with actor:  1
using another actor
from probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
printing an ep nov before normalisation:  29.414011034587798
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
siam score:  -0.94261694
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
actions average: 
K:  4  action  0 :  tensor([    0.9400,     0.0007,     0.0008,     0.0006,     0.0579],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9467,     0.0002,     0.0004,     0.0526],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0006,     0.0133,     0.8715,     0.0357,     0.0789],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0019,     0.0008,     0.0347,     0.9129,     0.0497],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0007,     0.0816,     0.0760,     0.0030,     0.8388],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
printing an ep nov before normalisation:  74.48182347277859
printing an ep nov before normalisation:  25.241245175229512
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.088]
 [0.101]
 [0.108]
 [0.112]] [[28.291]
 [31.291]
 [32.379]
 [36.29 ]
 [37.29 ]] [[0.278]
 [0.329]
 [0.359]
 [0.426]
 [0.445]]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.076]
 [0.076]
 [0.076]
 [0.076]] [[52.235]
 [29.792]
 [29.792]
 [29.792]
 [29.792]] [[0.337]
 [0.205]
 [0.205]
 [0.205]
 [0.205]]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
Printing some Q and Qe and total Qs values:  [[1.046]
 [1.046]
 [1.046]
 [1.046]
 [1.046]] [[45.787]
 [45.787]
 [45.787]
 [45.787]
 [45.787]] [[1.266]
 [1.266]
 [1.266]
 [1.266]
 [1.266]]
printing an ep nov before normalisation:  52.08750021462892
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.06486619025577012, 0.12039372284353189, 0.1759212554312936, 0.4535589183701024, 0.12039372284353189, 0.06486619025577012]
printing an ep nov before normalisation:  34.04293447066077
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0241 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]] [[46.455]
 [46.455]
 [46.455]
 [46.455]
 [46.455]] [[1.754]
 [1.754]
 [1.754]
 [1.754]
 [1.754]]
printing an ep nov before normalisation:  35.12369737359508
line 256 mcts: sample exp_bonus 23.438105922974746
printing an ep nov before normalisation:  30.724751949310303
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.09012932695178688, 0.11714250567468562, 0.1711688631204831, 0.44130065034947064, 0.11714250567468562, 0.06311614822888813]
printing an ep nov before normalisation:  37.99065552822278
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.09012932695178688, 0.11714250567468562, 0.1711688631204831, 0.44130065034947064, 0.11714250567468562, 0.06311614822888813]
printing an ep nov before normalisation:  54.25284308844618
siam score:  -0.95270056
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.09012932695178688, 0.11714250567468562, 0.1711688631204831, 0.44130065034947064, 0.11714250567468562, 0.06311614822888813]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.09012932695178688, 0.11714250567468562, 0.1711688631204831, 0.44130065034947064, 0.11714250567468562, 0.06311614822888813]
maxi score, test score, baseline:  0.0241 0.35 0.35
siam score:  -0.9478757
actor:  1 policy actor:  1  step number:  97 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0877603462423866, 0.11406245305047998, 0.16666666666666669, 0.4296877347476003, 0.1403645598585733, 0.061458239434293245]
printing an ep nov before normalisation:  28.707879139190073
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0877603462423866, 0.11406245305047998, 0.16666666666666669, 0.4296877347476003, 0.1403645598585733, 0.061458239434293245]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0877603462423866, 0.11406245305047998, 0.16666666666666669, 0.4296877347476003, 0.1403645598585733, 0.061458239434293245]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.523]
 [0.464]
 [0.456]
 [0.472]
 [0.432]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  68.10580694428342
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0877603462423866, 0.11406245305047998, 0.16666666666666669, 0.4296877347476003, 0.1403645598585733, 0.061458239434293245]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0877603462423866, 0.11406245305047998, 0.16666666666666669, 0.4296877347476003, 0.1403645598585733, 0.061458239434293245]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0877603462423866, 0.11406245305047998, 0.16666666666666669, 0.4296877347476003, 0.1403645598585733, 0.061458239434293245]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0877603462423866, 0.11406245305047998, 0.16666666666666669, 0.4296877347476003, 0.1403645598585733, 0.061458239434293245]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.0877603462423866, 0.11406245305047998, 0.16666666666666669, 0.4296877347476003, 0.1403645598585733, 0.061458239434293245]
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.029]
 [0.044]
 [0.046]
 [0.036]] [[22.147]
 [43.289]
 [33.388]
 [22.784]
 [25.102]] [[0.033]
 [0.029]
 [0.044]
 [0.046]
 [0.036]]
using another actor
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08551288482461179, 0.11114039487999754, 0.1880229250461548, 0.4186705155446266, 0.13676790493538327, 0.05988537476922604]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.107]
 [0.125]
 [0.106]
 [0.085]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.089]
 [0.107]
 [0.125]
 [0.106]
 [0.085]]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08551288482461179, 0.11114039487999754, 0.1880229250461548, 0.4186705155446266, 0.13676790493538327, 0.05988537476922604]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08551288482461179, 0.11114039487999754, 0.1880229250461548, 0.4186705155446266, 0.13676790493538327, 0.05988537476922604]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08551288482461179, 0.11114039487999754, 0.1880229250461548, 0.4186705155446266, 0.13676790493538327, 0.05988537476922604]
printing an ep nov before normalisation:  45.639965629224946
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.61777686408842
printing an ep nov before normalisation:  50.83031193299731
printing an ep nov before normalisation:  30.805336955982096
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08540820328517293, 0.10978574229962104, 0.1829183593429654, 0.42669374948744654, 0.13416328131406913, 0.0610306642707248]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08540820328517293, 0.10978574229962104, 0.1829183593429654, 0.42669374948744654, 0.13416328131406913, 0.0610306642707248]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08540820328517293, 0.10978574229962104, 0.1829183593429654, 0.42669374948744654, 0.13416328131406913, 0.0610306642707248]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  85.23769170136516
siam score:  -0.9423878
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.735]
 [0.735]
 [0.696]
 [0.735]] [[57.781]
 [57.781]
 [57.781]
 [53.924]
 [57.781]] [[1.934]
 [1.934]
 [1.934]
 [1.77 ]
 [1.934]]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08531325843886765, 0.10855708936109594, 0.17828858212778084, 0.433970722272292, 0.1318009202833242, 0.06206942751663937]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08531325843886765, 0.10855708936109594, 0.17828858212778084, 0.433970722272292, 0.1318009202833242, 0.06206942751663937]
printing an ep nov before normalisation:  62.94895420432462
siam score:  -0.94241387
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08531325843886765, 0.10855708936109594, 0.17828858212778084, 0.433970722272292, 0.1318009202833242, 0.06206942751663937]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.842]
 [0.842]
 [0.842]
 [0.842]] [[64.275]
 [50.177]
 [50.177]
 [50.177]
 [50.177]] [[1.849]
 [1.611]
 [1.611]
 [1.611]
 [1.611]]
printing an ep nov before normalisation:  65.6771562503887
Printing some Q and Qe and total Qs values:  [[0.819]
 [0.819]
 [0.822]
 [0.819]
 [0.857]] [[35.376]
 [35.376]
 [31.824]
 [35.376]
 [35.763]] [[1.964]
 [1.964]
 [1.748]
 [1.964]
 [2.026]]
printing an ep nov before normalisation:  60.307381844956325
actor:  1 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0852267522031972, 0.1074376379659616, 0.1740702952542548, 0.44060092440742765, 0.12964852372872598, 0.06301586644043282]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  19.998092651367188
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0852267522031972, 0.1074376379659616, 0.1740702952542548, 0.44060092440742765, 0.12964852372872598, 0.06301586644043282]
line 256 mcts: sample exp_bonus 40.260289514015426
printing an ep nov before normalisation:  35.757256338462916
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.35 0.35
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.209]
 [0.151]
 [0.151]
 [0.151]] [[28.022]
 [37.701]
 [23.508]
 [23.508]
 [23.508]] [[1.078]
 [1.514]
 [0.965]
 [0.965]
 [0.965]]
line 256 mcts: sample exp_bonus 38.39360821394366
printing an ep nov before normalisation:  61.536087048846376
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08514760742253202, 0.1064134489644802, 0.1702109735903247, 0.4466669136356509, 0.12767929050642834, 0.06388176588058385]
printing an ep nov before normalisation:  40.270270226415384
maxi score, test score, baseline:  0.0241 0.35 0.35
printing an ep nov before normalisation:  27.695038169514792
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08514760742253202, 0.1064134489644802, 0.1702109735903247, 0.4466669136356509, 0.12767929050642834, 0.06388176588058385]
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.138]
 [0.135]
 [0.138]
 [0.138]] [[38.048]
 [38.048]
 [34.202]
 [38.048]
 [38.048]] [[2.162]
 [2.162]
 [1.802]
 [2.162]
 [2.162]]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08514760742253202, 0.1064134489644802, 0.1702109735903247, 0.4466669136356509, 0.12767929050642834, 0.06388176588058385]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08514760742253202, 0.1064134489644802, 0.1702109735903247, 0.4466669136356509, 0.12767929050642834, 0.06388176588058385]
printing an ep nov before normalisation:  57.82925591296098
printing an ep nov before normalisation:  48.53423622624478
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08514760742253202, 0.1064134489644802, 0.1702109735903247, 0.4466669136356509, 0.12767929050642834, 0.06388176588058385]
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]]
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[39.68]
 [39.68]
 [39.68]
 [39.68]
 [39.68]] [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]]
printing an ep nov before normalisation:  28.40891599506213
printing an ep nov before normalisation:  56.91580207579411
printing an ep nov before normalisation:  65.14385644084226
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.95406646
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.675]
 [0.719]
 [0.741]
 [0.727]] [[46.025]
 [48.273]
 [47.509]
 [39.81 ]
 [47.86 ]] [[0.591]
 [1.063]
 [1.094]
 [0.993]
 [1.108]]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
actions average: 
K:  2  action  0 :  tensor([    0.9819,     0.0013,     0.0000,     0.0040,     0.0128],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9504,     0.0001,     0.0002,     0.0492],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0022, 0.0245, 0.8781, 0.0330, 0.0622], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0007,     0.0011,     0.8732,     0.1249],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0424, 0.0833, 0.1766, 0.1285, 0.5691], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
printing an ep nov before normalisation:  73.69319891288843
Printing some Q and Qe and total Qs values:  [[0.902]
 [0.817]
 [0.902]
 [0.902]
 [0.902]] [[55.032]
 [62.321]
 [55.032]
 [55.032]
 [55.032]] [[1.446]
 [1.484]
 [1.446]
 [1.446]
 [1.446]]
printing an ep nov before normalisation:  0.045965116293587016
printing an ep nov before normalisation:  38.57590980743914
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.05244875195318
printing an ep nov before normalisation:  52.77034399862302
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9630,     0.0113,     0.0002,     0.0253],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0011, 0.0012, 0.8617, 0.0569, 0.0791], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0011,     0.0005,     0.0437,     0.8578,     0.0969],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0053, 0.0565, 0.0873, 0.1641, 0.6867], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.58808238408761
maxi score, test score, baseline:  0.0241 0.35 0.35
printing an ep nov before normalisation:  33.74138395575271
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
maxi score, test score, baseline:  0.0241 0.35 0.35
from probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
printing an ep nov before normalisation:  38.50432872772217
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.24 ]
 [0.265]
 [0.24 ]
 [0.24 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.24 ]
 [0.24 ]
 [0.265]
 [0.24 ]
 [0.24 ]]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
siam score:  -0.95603323
printing an ep nov before normalisation:  62.77506800959171
maxi score, test score, baseline:  0.0241 0.35 0.35
printing an ep nov before normalisation:  47.18461513519287
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08167665368719303, 0.12247185991734037, 0.1632670661474877, 0.42843590664344544, 0.14286946303241402, 0.06127905057211937]
main train batch thing paused
add a thread
using another actor
Adding thread: now have 2 threads
printing an ep nov before normalisation:  32.265026573580215
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  81.39979925836495
printing an ep nov before normalisation:  35.48197836754692
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08004533422255632, 0.12002441073522264, 0.1799930255042221, 0.41986748458022005, 0.1400139489915558, 0.06005579596622316]
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]] [[53.256]
 [53.256]
 [53.256]
 [53.256]
 [53.256]] [[1.711]
 [1.711]
 [1.711]
 [1.711]
 [1.711]]
printing an ep nov before normalisation:  40.24061086122095
printing an ep nov before normalisation:  56.67081168056759
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08004533422255632, 0.12002441073522264, 0.1799930255042221, 0.41986748458022005, 0.1400139489915558, 0.06005579596622316]
printing an ep nov before normalisation:  46.87868915769144
printing an ep nov before normalisation:  30.204425108352417
actor:  1 policy actor:  1  step number:  102 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07847800541159486, 0.11767296596940455, 0.19606288708502395, 0.4116351701529773, 0.1372704462483094, 0.05888052513269001]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]] [[53.332]
 [53.332]
 [53.332]
 [53.332]
 [53.332]] [[2.134]
 [2.134]
 [2.134]
 [2.134]
 [2.134]]
printing an ep nov before normalisation:  57.76378438594178
printing an ep nov before normalisation:  42.90093991048551
siam score:  -0.95816725
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07847800541159486, 0.11767296596940455, 0.19606288708502395, 0.4116351701529773, 0.1372704462483094, 0.05888052513269001]
printing an ep nov before normalisation:  62.80133235073734
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07847800541159486, 0.11767296596940455, 0.19606288708502395, 0.4116351701529773, 0.1372704462483094, 0.05888052513269001]
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07866245984240834, 0.11637854848137617, 0.1918107257593119, 0.418107257593119, 0.13523659280086012, 0.05980441552292442]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.69075298309326
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.031]
 [0.187]
 [0.18 ]
 [0.181]] [[35.021]
 [35.878]
 [41.921]
 [44.153]
 [40.134]] [[1.062]
 [0.948]
 [1.392]
 [1.492]
 [1.302]]
printing an ep nov before normalisation:  35.40830795884536
printing an ep nov before normalisation:  34.16277933782141
printing an ep nov before normalisation:  30.370623079852553
siam score:  -0.96072686
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07866245984240834, 0.11637854848137617, 0.1918107257593119, 0.418107257593119, 0.13523659280086012, 0.05980441552292442]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.09571654764861777, 0.11422527434897836, 0.18826018115042067, 0.4103649015547477, 0.13273400104933894, 0.05869909424789661]
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9631,     0.0000,     0.0003,     0.0360],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0168,     0.8854,     0.0336,     0.0640],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0013,     0.0228,     0.8896,     0.0862],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0016, 0.1525, 0.0542, 0.2702, 0.5214], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.09571654764861777, 0.11422527434897836, 0.18826018115042067, 0.4103649015547477, 0.13273400104933894, 0.05869909424789661]
printing an ep nov before normalisation:  38.92726736004609
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.09571654764861777, 0.11422527434897836, 0.18826018115042067, 0.4103649015547477, 0.13273400104933894, 0.05869909424789661]
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.09571654764861777, 0.11422527434897836, 0.18826018115042067, 0.4103649015547477, 0.13273400104933894, 0.05869909424789661]
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.09571654764861777, 0.11422527434897836, 0.18826018115042067, 0.4103649015547477, 0.13273400104933894, 0.05869909424789661]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.602]
 [0.252]
 [0.157]
 [0.235]] [[51.633]
 [52.958]
 [41.522]
 [37.283]
 [46.021]] [[0.915]
 [1.344]
 [0.701]
 [0.497]
 [0.8  ]]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08168497411680911, 0.10208058032877493, 0.20405861138860404, 0.428410279720228, 0.12247618654074073, 0.061289367904843295]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08168497411680911, 0.10208058032877493, 0.20405861138860404, 0.428410279720228, 0.12247618654074073, 0.061289367904843295]
printing an ep nov before normalisation:  66.65388031925053
printing an ep nov before normalisation:  56.083722349831234
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08175090866431696, 0.10134685281870534, 0.19932657359064734, 0.4344779034433082, 0.12094279697309375, 0.06215496450992855]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08175090866431696, 0.10134685281870534, 0.19932657359064734, 0.4344779034433082, 0.12094279697309375, 0.06215496450992855]
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.183]
 [0.164]
 [0.132]
 [0.132]] [[37.085]
 [49.507]
 [31.44 ]
 [37.085]
 [37.085]] [[0.737]
 [1.174]
 [0.594]
 [0.737]
 [0.737]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  52.81473104212349
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.93698348238281
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.35 0.35
from probs:  [0.08175090866431696, 0.10134685281870534, 0.19932657359064734, 0.4344779034433082, 0.12094279697309375, 0.06215496450992855]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08175090866431696, 0.10134685281870534, 0.19932657359064734, 0.4344779034433082, 0.12094279697309375, 0.06215496450992855]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08175090866431696, 0.10134685281870534, 0.19932657359064734, 0.4344779034433082, 0.12094279697309375, 0.06215496450992855]
maxi score, test score, baseline:  0.0241 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.15]
 [0.15]
 [0.15]
 [0.15]
 [0.15]] [[43.407]
 [56.428]
 [43.407]
 [43.407]
 [43.407]] [[0.959]
 [1.422]
 [0.959]
 [0.959]
 [0.959]]
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.138]
 [0.136]
 [0.13 ]
 [0.13 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.118]
 [0.138]
 [0.136]
 [0.13 ]
 [0.13 ]]
printing an ep nov before normalisation:  62.44411845586149
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.146]
 [0.241]
 [0.336]
 [0.723]] [[63.987]
 [57.377]
 [45.523]
 [49.933]
 [60.236]] [[0.848]
 [0.738]
 [0.638]
 [0.806]
 [1.363]]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08175090866431695, 0.10134685281870531, 0.19932657359064734, 0.4344779034433081, 0.12094279697309374, 0.062154964509928544]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08175090866431695, 0.10134685281870531, 0.19932657359064734, 0.4344779034433081, 0.12094279697309374, 0.062154964509928544]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[55.122]
 [55.122]
 [55.122]
 [55.122]
 [55.122]] [[1.125]
 [1.125]
 [1.125]
 [1.125]
 [1.125]]
from probs:  [0.08175090866431693, 0.10134685281870533, 0.19932657359064734, 0.43447790344330806, 0.12094279697309371, 0.06215496450992853]
printing an ep nov before normalisation:  62.1977771836341
printing an ep nov before normalisation:  34.482848629915246
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08175090866431693, 0.10134685281870533, 0.19932657359064734, 0.43447790344330806, 0.12094279697309371, 0.06215496450992853]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08175090866431693, 0.10134685281870533, 0.19932657359064734, 0.43447790344330806, 0.12094279697309371, 0.06215496450992853]
siam score:  -0.94583917
actor:  1 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.085335565506654
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08018104127131193, 0.09940006913694631, 0.21471423633075265, 0.42612354285273096, 0.11861909700258072, 0.06096201340567754]
printing an ep nov before normalisation:  88.71476413543728
printing an ep nov before normalisation:  40.66839049866832
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08018104127131193, 0.09940006913694631, 0.21471423633075265, 0.42612354285273096, 0.11861909700258072, 0.06096201340567754]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08029900295358428, 0.09880635946353049, 0.2098504985232079, 0.43193877664256264, 0.11731371597347673, 0.06179164644363805]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08029900295358428, 0.09880635946353049, 0.2098504985232079, 0.43193877664256264, 0.11731371597347673, 0.06179164644363805]
using explorer policy with actor:  1
siam score:  -0.94511676
printing an ep nov before normalisation:  85.39658127780936
Printing some Q and Qe and total Qs values:  [[0.935]
 [0.814]
 [0.778]
 [0.833]
 [0.902]] [[40.491]
 [38.339]
 [57.448]
 [35.781]
 [34.743]] [[1.383]
 [1.218]
 [1.562]
 [1.187]
 [1.235]]
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.132]
 [0.132]
 [0.132]
 [0.132]] [[40.756]
 [50.29 ]
 [50.29 ]
 [50.29 ]
 [50.29 ]] [[0.74 ]
 [1.121]
 [1.121]
 [1.121]
 [1.121]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[44.186]
 [44.186]
 [44.186]
 [44.186]
 [44.186]] [[44.803]
 [44.803]
 [44.803]
 [44.803]
 [44.803]]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08029900295358428, 0.09880635946353049, 0.2098504985232079, 0.43193877664256264, 0.11731371597347673, 0.06179164644363805]
printing an ep nov before normalisation:  53.18653319072673
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.08029900295358428, 0.09880635946353049, 0.2098504985232079, 0.43193877664256264, 0.11731371597347673, 0.06179164644363805]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08040854045069284, 0.09825504932296328, 0.205334102556586, 0.4373387178961018, 0.11610155819523371, 0.0625620315784224]
printing an ep nov before normalisation:  74.34332000348012
actions average: 
K:  3  action  0 :  tensor([    0.9982,     0.0002,     0.0000,     0.0009,     0.0007],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0012,     0.9643,     0.0009,     0.0015,     0.0322],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.8797,     0.0588,     0.0614],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0003,     0.0315,     0.9118,     0.0562],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0056, 0.2102, 0.0389, 0.0093, 0.7360], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07899991932584292, 0.09653326879400766, 0.2192667150711609, 0.4296669086891379, 0.1140666182621724, 0.06146656985767817]
printing an ep nov before normalisation:  45.83283657204775
maxi score, test score, baseline:  0.0241 0.35 0.35
siam score:  -0.94471174
maxi score, test score, baseline:  0.0241 0.35 0.35
printing an ep nov before normalisation:  28.278606695340955
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07899991932584292, 0.09653326879400766, 0.2192667150711609, 0.4296669086891379, 0.1140666182621724, 0.06146656985767817]
maxi score, test score, baseline:  0.0241 0.35 0.35
printing an ep nov before normalisation:  43.40656280517578
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07899991932584292, 0.09653326879400766, 0.2192667150711609, 0.4296669086891379, 0.1140666182621724, 0.06146656985767817]
line 256 mcts: sample exp_bonus 55.04544746900346
UNIT TEST: sample policy line 217 mcts : [0.128 0.154 0.154 0.41  0.154]
maxi score, test score, baseline:  0.0241 0.35 0.35
line 256 mcts: sample exp_bonus 60.90121397748418
printing an ep nov before normalisation:  53.05165561176903
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]] [[126.148]
 [126.148]
 [126.148]
 [126.148]
 [126.148]] [[1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.447]]
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.149]
 [0.107]
 [0.106]
 [0.12 ]] [[33.997]
 [50.766]
 [48.094]
 [43.732]
 [37.963]] [[0.421]
 [0.836]
 [0.741]
 [0.656]
 [0.556]]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07899991932584292, 0.09653326879400766, 0.2192667150711609, 0.4296669086891379, 0.1140666182621724, 0.06146656985767817]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07763988091135478, 0.11210186249405615, 0.2154878072421602, 0.4222596967383685, 0.11210186249405615, 0.06040889012000408]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.03436756134033
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07763988091135478, 0.11210186249405615, 0.21548780724216027, 0.4222596967383685, 0.11210186249405615, 0.06040889012000408]
printing an ep nov before normalisation:  34.49602415014377
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.498]
 [0.484]
 [0.498]
 [0.494]] [[36.761]
 [49.022]
 [41.48 ]
 [41.72 ]
 [44.881]] [[0.505]
 [0.498]
 [0.484]
 [0.498]
 [0.494]]
printing an ep nov before normalisation:  65.7941787988702
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0778307359073886, 0.11114420994211789, 0.21108463204630573, 0.4276222132720461, 0.11114420994211789, 0.06117399889002396]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0778307359073886, 0.11114420994211789, 0.21108463204630573, 0.4276222132720461, 0.11114420994211789, 0.06117399889002396]
printing an ep nov before normalisation:  72.55804325008988
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0778307359073886, 0.11114420994211789, 0.21108463204630573, 0.4276222132720461, 0.11114420994211789, 0.06117399889002396]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07817666716391085, 0.10940843169429525, 0.20310372528544846, 0.4373419592633316, 0.10940843169429525, 0.06256078489871865]
printing an ep nov before normalisation:  34.215490263719815
siam score:  -0.93549365
printing an ep nov before normalisation:  43.547637876801645
printing an ep nov before normalisation:  0.0034909235296254337
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.14 ]
 [0.162]
 [0.133]
 [0.133]] [[47.775]
 [61.222]
 [69.575]
 [66.885]
 [66.405]] [[0.686]
 [0.922]
 [1.082]
 [1.009]
 [1.001]]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07817666716391085, 0.10940843169429525, 0.20310372528544846, 0.4373419592633316, 0.10940843169429525, 0.06256078489871865]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[59.232]
 [59.232]
 [59.232]
 [59.232]
 [59.232]] [[1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.56394470982285
printing an ep nov before normalisation:  70.7143728027714
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07817666716391085, 0.10940843169429525, 0.20310372528544846, 0.4373419592633316, 0.10940843169429525, 0.06256078489871865]
printing an ep nov before normalisation:  46.275806284850056
maxi score, test score, baseline:  0.0241 0.35 0.35
printing an ep nov before normalisation:  38.41653319832604
actor:  1 policy actor:  1  step number:  110 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0783339114462023, 0.10861942752179009, 0.19947597574855339, 0.4417601043532555, 0.10861942752179009, 0.06319115340840843]
printing an ep nov before normalisation:  35.05016202623175
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.103]
 [0.144]
 [0.1  ]
 [0.103]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.073]
 [0.103]
 [0.144]
 [0.1  ]
 [0.103]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.37378726175092
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 60.28563570127134
printing an ep nov before normalisation:  39.74958896636963
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07716648229981747, 0.12191657448324207, 0.1965000614556164, 0.43516721976721434, 0.1069998770887672, 0.06224978490534258]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using another actor
siam score:  -0.9319997
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07603339668878646, 0.12012525775910658, 0.1936116928763067, 0.42876828525134725, 0.12012525775910658, 0.06133610966534642]
maxi score, test score, baseline:  0.0241 0.35 0.35
printing an ep nov before normalisation:  31.202852584942075
printing an ep nov before normalisation:  85.13839855925035
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.06 ]
 [0.097]
 [0.072]
 [0.07 ]] [[37.834]
 [35.795]
 [55.952]
 [41.284]
 [43.545]] [[0.393]
 [0.327]
 [0.651]
 [0.416]
 [0.447]]
printing an ep nov before normalisation:  70.4127629512972
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]] [[25.033]
 [25.033]
 [25.033]
 [25.033]
 [25.033]] [[0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]]
maxi score, test score, baseline:  0.0241 0.35 0.35
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07493316109946277, 0.11838587426287513, 0.20529130058969983, 0.4225548664067617, 0.11838587426287513, 0.06044892337832531]
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.222]
 [0.124]
 [0.109]
 [0.109]] [[58.312]
 [50.248]
 [67.434]
 [49.517]
 [45.659]] [[0.829]
 [0.731]
 [1.073]
 [0.601]
 [0.502]]
line 256 mcts: sample exp_bonus 22.15318165719509
printing an ep nov before normalisation:  69.11181489881503
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07493316109946277, 0.11838587426287513, 0.20529130058969983, 0.4225548664067617, 0.11838587426287513, 0.06044892337832531]
printing an ep nov before normalisation:  28.673137769533515
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07493316109946277, 0.11838587426287513, 0.20529130058969983, 0.4225548664067617, 0.11838587426287513, 0.06044892337832531]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0881416441930502, 0.11669619781981984, 0.20235985870012868, 0.41651901090090077, 0.11669619781981984, 0.0595870905662806]
Printing some Q and Qe and total Qs values:  [[1.499]
 [1.498]
 [1.444]
 [1.498]
 [1.499]] [[44.111]
 [40.746]
 [42.342]
 [44.657]
 [47.434]] [[2.703]
 [2.61 ]
 [2.6  ]
 [2.717]
 [2.794]]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  78.08824753865693
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.088009030170452, 0.1157705489338219, 0.19905510522393155, 0.42114725533089065, 0.1157705489338219, 0.06024751140708212]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.088009030170452, 0.1157705489338219, 0.19905510522393155, 0.42114725533089065, 0.1157705489338219, 0.06024751140708212]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.088009030170452, 0.1157705489338219, 0.19905510522393155, 0.42114725533089065, 0.1157705489338219, 0.06024751140708212]
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.205]
 [0.241]
 [0.241]
 [0.241]] [[41.997]
 [50.879]
 [41.997]
 [41.997]
 [41.997]] [[1.237]
 [1.538]
 [1.237]
 [1.237]
 [1.237]]
printing an ep nov before normalisation:  67.23214518044857
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.088009030170452, 0.1157705489338219, 0.19905510522393155, 0.42114725533089065, 0.1157705489338219, 0.06024751140708212]
printing an ep nov before normalisation:  69.37324420875508
printing an ep nov before normalisation:  51.8434131386933
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.721]
 [0.62 ]
 [0.625]
 [0.659]] [[46.712]
 [46.712]
 [57.481]
 [58.02 ]
 [58.379]] [[1.763]
 [1.763]
 [2.119]
 [2.147]
 [2.197]]
printing an ep nov before normalisation:  54.552252914871666
actor:  1 policy actor:  1  step number:  111 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  78.50360655766237
using another actor
printing an ep nov before normalisation:  65.0678917315501
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08788358317919819, 0.11489492608918739, 0.19592895481915495, 0.4255253695540631, 0.11489492608918739, 0.06087224026920901]
printing an ep nov before normalisation:  43.00555179726883
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  90.93147458160308
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.478]
 [0.055]
 [0.166]
 [0.248]] [[50.247]
 [58.362]
 [48.936]
 [52.994]
 [54.847]] [[0.761]
 [1.117]
 [0.516]
 [0.704]
 [0.821]]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08776473750049228, 0.11406538055588374, 0.19296730972205808, 0.4296730972205812, 0.11406538055588374, 0.061464094445100835]
printing an ep nov before normalisation:  59.95253123249611
printing an ep nov before normalisation:  72.88254240134775
printing an ep nov before normalisation:  49.7372883567589
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08765198542803994, 0.11327836853245943, 0.19015751784571785, 0.43360815733770286, 0.11327836853245943, 0.06202560232362048]
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08765198542803994, 0.11327836853245943, 0.19015751784571785, 0.43360815733770286, 0.11327836853245943, 0.06202560232362048]
printing an ep nov before normalisation:  52.29607582092285
printing an ep nov before normalisation:  47.70303619191534
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08765198542803994, 0.11327836853245943, 0.19015751784571785, 0.43360815733770286, 0.11327836853245943, 0.06202560232362048]
maxi score, test score, baseline:  0.0241 0.35 0.35
printing an ep nov before normalisation:  60.357021778984105
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08765198542803994, 0.11327836853245943, 0.19015751784571785, 0.43360815733770286, 0.11327836853245943, 0.06202560232362048]
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.498]
 [0.225]
 [0.162]
 [0.236]] [[58.962]
 [50.564]
 [43.12 ]
 [40.769]
 [52.381]] [[0.937]
 [1.056]
 [0.635]
 [0.526]
 [0.83 ]]
Printing some Q and Qe and total Qs values:  [[1.001]
 [1.001]
 [1.001]
 [0.866]
 [1.001]] [[45.519]
 [45.519]
 [45.519]
 [46.653]
 [45.519]] [[1.496]
 [1.496]
 [1.496]
 [1.384]
 [1.496]]
Printing some Q and Qe and total Qs values:  [[0.172]
 [0.177]
 [0.178]
 [0.172]
 [0.172]] [[61.811]
 [62.768]
 [66.369]
 [61.811]
 [61.811]] [[1.352]
 [1.384]
 [1.491]
 [1.352]
 [1.352]]
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.293]
 [0.162]
 [0.162]
 [0.162]] [[30.332]
 [48.022]
 [30.332]
 [30.332]
 [30.332]] [[0.684]
 [1.366]
 [0.684]
 [0.684]
 [0.684]]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08765198542803994, 0.11327836853245943, 0.19015751784571785, 0.43360815733770286, 0.11327836853245943, 0.06202560232362048]
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
printing an ep nov before normalisation:  51.28306865692139
printing an ep nov before normalisation:  48.66480198479435
siam score:  -0.93581176
Printing some Q and Qe and total Qs values:  [[1.121]
 [1.121]
 [1.121]
 [1.121]
 [1.121]] [[68.296]
 [68.296]
 [68.296]
 [68.296]
 [68.754]] [[2.424]
 [2.424]
 [2.424]
 [2.424]
 [2.437]]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.302]
 [1.233]
 [1.135]
 [1.135]
 [1.166]] [[52.483]
 [55.854]
 [50.642]
 [50.642]
 [57.974]] [[2.371]
 [2.422]
 [2.138]
 [2.138]
 [2.431]]
printing an ep nov before normalisation:  99.38728212677587
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]] [[26.977]
 [26.977]
 [26.977]
 [26.977]
 [26.977]] [[1.254]
 [1.254]
 [1.254]
 [1.254]
 [1.254]]
maxi score, test score, baseline:  0.0241 0.35 0.35
printing an ep nov before normalisation:  52.77195872748844
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08646546583748627, 0.11114275840031101, 0.19751328237019763, 0.4319475617170326, 0.11114275840031101, 0.06178817327466153]
Printing some Q and Qe and total Qs values:  [[1.335]
 [1.203]
 [1.335]
 [1.335]
 [1.121]] [[72.917]
 [71.946]
 [72.917]
 [72.917]
 [71.984]] [[2.61 ]
 [2.454]
 [2.61 ]
 [2.61 ]
 [2.373]]
maxi score, test score, baseline:  0.0241 0.35 0.35
printing an ep nov before normalisation:  55.60791429634012
siam score:  -0.9363332
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0853636946533626, 0.10915968646213452, 0.2043436536972222, 0.43040557588055534, 0.10915968646213452, 0.06156770284459068]
siam score:  -0.93747354
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0853636946533626, 0.10915968646213452, 0.2043436536972222, 0.43040557588055534, 0.10915968646213452, 0.06156770284459068]
printing an ep nov before normalisation:  61.30102946831766
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  67.06696916817344
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08531725947898235, 0.10855994724689215, 0.20153069831853135, 0.4339575759976293, 0.10855994724689215, 0.06207457171107257]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08531725947898235, 0.10855994724689215, 0.20153069831853135, 0.4339575759976293, 0.10855994724689215, 0.06207457171107257]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08531725947898235, 0.10855994724689215, 0.20153069831853135, 0.4339575759976293, 0.10855994724689215, 0.06207457171107257]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08531725947898235, 0.10855994724689215, 0.20153069831853135, 0.4339575759976293, 0.10855994724689215, 0.06207457171107257]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  75.56889559758923
printing an ep nov before normalisation:  27.56175994873047
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08431608278640852, 0.10677533293556986, 0.2078419586067958, 0.432434460098409, 0.10677533293556986, 0.06185683263724719]
maxi score, test score, baseline:  0.0241 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.786]
 [0.797]
 [0.788]
 [0.787]] [[52.474]
 [24.607]
 [29.331]
 [37.519]
 [29.704]] [[1.641]
 [1.008]
 [1.118]
 [1.282]
 [1.116]]
printing an ep nov before normalisation:  51.962355456508405
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08431608278640852, 0.10677533293556986, 0.2078419586067958, 0.432434460098409, 0.10677533293556986, 0.06185683263724719]
printing an ep nov before normalisation:  50.371219212418055
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.803]
 [0.803]
 [0.837]
 [0.803]] [[24.195]
 [37.535]
 [37.535]
 [26.355]
 [37.535]] [[1.449]
 [1.753]
 [1.753]
 [1.504]
 [1.753]]
printing an ep nov before normalisation:  49.49879669828633
printing an ep nov before normalisation:  55.77860269628552
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.08431608278640852, 0.10677533293556986, 0.2078419586067958, 0.432434460098409, 0.10677533293556986, 0.06185683263724719]
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.073]
 [0.064]
 [0.064]
 [0.064]] [[35.655]
 [64.473]
 [35.655]
 [35.655]
 [35.655]] [[0.881]
 [1.625]
 [0.881]
 [0.881]
 [0.881]]
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.192]
 [0.187]
 [0.179]
 [0.182]] [[66.649]
 [66.649]
 [74.423]
 [76.528]
 [75.769]] [[1.418]
 [1.418]
 [1.594]
 [1.635]
 [1.62 ]]
printing an ep nov before normalisation:  72.06218627980364
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0843160827864085, 0.10677533293556982, 0.20784195860679577, 0.43243446009840886, 0.10677533293556982, 0.06185683263724717]
printing an ep nov before normalisation:  54.61856946187327
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  55.12921814362839
printing an ep nov before normalisation:  51.234556071803816
printing an ep nov before normalisation:  58.78959572126299
printing an ep nov before normalisation:  87.4621797733458
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0843160827864085, 0.10677533293556982, 0.20784195860679577, 0.43243446009840886, 0.10677533293556982, 0.06185683263724717]
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[58.452]
 [58.452]
 [58.452]
 [58.452]
 [58.452]] [[2.614]
 [2.614]
 [2.614]
 [2.614]
 [2.614]]
printing an ep nov before normalisation:  40.33615589141846
printing an ep nov before normalisation:  35.681644858712794
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.0843160827864085, 0.10677533293556982, 0.20784195860679577, 0.43243446009840886, 0.10677533293556982, 0.06185683263724717]
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  86 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07652548744509387, 0.10004057767680852, 0.20585848371952442, 0.4527669311525281, 0.10004057767680852, 0.06476794232923656]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  44.65105171351431
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07652548744509387, 0.10004057767680852, 0.20585848371952442, 0.4527669311525281, 0.10004057767680852, 0.06476794232923656]
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.314]
 [0.008]
 [0.137]
 [0.239]] [[36.147]
 [46.561]
 [29.516]
 [26.355]
 [36.515]] [[0.535]
 [0.825]
 [0.197]
 [0.266]
 [0.561]]
printing an ep nov before normalisation:  46.5009693463887
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07563703778883656, 0.09887864516189956, 0.21508668202721457, 0.44750275575784465, 0.09887864516189956, 0.06401623410230506]
printing an ep nov before normalisation:  44.915641634215866
line 256 mcts: sample exp_bonus 35.91343606713867
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[57.693]
 [58.558]
 [58.558]
 [58.558]
 [58.558]] [[1.799]
 [1.854]
 [1.854]
 [1.854]
 [1.854]]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07496218111888606, 0.0974204224775262, 0.220940749950047, 0.44552316353644844, 0.0974204224775262, 0.06373306043956599]
maxi score, test score, baseline:  0.0241 0.35 0.35
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07496218111888606, 0.0974204224775262, 0.220940749950047, 0.44552316353644844, 0.0974204224775262, 0.06373306043956599]
printing an ep nov before normalisation:  40.55362984409247
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07496218111888606, 0.0974204224775262, 0.220940749950047, 0.44552316353644844, 0.0974204224775262, 0.06373306043956599]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07496218111888606, 0.0974204224775262, 0.220940749950047, 0.44552316353644844, 0.0974204224775262, 0.06373306043956599]
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[59.431]
 [59.431]
 [59.431]
 [59.431]
 [59.431]] [[1.915]
 [1.915]
 [1.915]
 [1.915]
 [1.915]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07496218111888606, 0.0974204224775262, 0.220940749950047, 0.44552316353644844, 0.0974204224775262, 0.06373306043956599]
printing an ep nov before normalisation:  73.815028077033
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07496218111888606, 0.0974204224775262, 0.220940749950047, 0.44552316353644844, 0.0974204224775262, 0.06373306043956599]
printing an ep nov before normalisation:  46.37929190133695
Printing some Q and Qe and total Qs values:  [[0.993]
 [1.312]
 [1.274]
 [0.934]
 [1.023]] [[43.863]
 [40.674]
 [46.039]
 [41.222]
 [44.679]] [[2.079]
 [2.319]
 [2.414]
 [1.954]
 [2.13 ]]
actor:  1 policy actor:  1  step number:  102 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.930121833623282
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07532359166592015, 0.09681607990138993, 0.21502476519647368, 0.45144213578664116, 0.09681607990138993, 0.06457734754818527]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07532359166592015, 0.09681607990138993, 0.21502476519647368, 0.45144213578664116, 0.09681607990138993, 0.06457734754818527]
printing an ep nov before normalisation:  31.67392146450345
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07532359166592015, 0.09681607990138993, 0.21502476519647368, 0.45144213578664116, 0.09681607990138993, 0.06457734754818527]
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.593]
 [0.611]
 [0.611]
 [0.611]] [[60.052]
 [63.651]
 [60.052]
 [60.052]
 [60.052]] [[2.034]
 [2.134]
 [2.034]
 [2.034]
 [2.034]]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07452353571269796, 0.09578733516361382, 0.22337013186910898, 0.44664002610372544, 0.09578733516361382, 0.06389163598724004]
printing an ep nov before normalisation:  58.83880505547954
printing an ep nov before normalisation:  61.593804705112795
printing an ep nov before normalisation:  52.64088539404289
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07374032634411055, 0.09478025245487796, 0.22101980911948252, 0.44193903328254036, 0.10530021551026167, 0.06322036328872685]
maxi score, test score, baseline:  0.0241 0.35 0.35
using another actor
from probs:  [0.07374032634411055, 0.09478025245487796, 0.22101980911948252, 0.44193903328254036, 0.10530021551026167, 0.06322036328872685]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07374032634411055, 0.09478025245487796, 0.22101980911948252, 0.44193903328254036, 0.10530021551026167, 0.06322036328872685]
using explorer policy with actor:  0
printing an ep nov before normalisation:  28.53166983967552
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07374032634411055, 0.09478025245487796, 0.22101980911948252, 0.44193903328254036, 0.10530021551026167, 0.06322036328872685]
printing an ep nov before normalisation:  63.61116515615991
printing an ep nov before normalisation:  27.671718204336095
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.190184667424276
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.07374032634411055, 0.09478025245487796, 0.22101980911948252, 0.44193903328254036, 0.10530021551026167, 0.06322036328872685]
printing an ep nov before normalisation:  43.279935260095854
using explorer policy with actor:  1
printing an ep nov before normalisation:  91.98872528716927
actor:  0 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  63.658396509563104
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
siam score:  -0.936561
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.9359038
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.07297343700227396, 0.09379415470547234, 0.22912881977626184, 0.43733599680824553, 0.10420451355707153, 0.06256307815067477]
printing an ep nov before normalisation:  42.60235094455406
printing an ep nov before normalisation:  45.96577654632041
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.044]
 [0.055]
 [0.045]
 [0.047]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.044]
 [0.044]
 [0.055]
 [0.045]
 [0.047]]
printing an ep nov before normalisation:  33.831658363342285
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.07297343700227396, 0.09379415470547234, 0.22912881977626184, 0.43733599680824553, 0.10420451355707153, 0.06256307815067477]
printing an ep nov before normalisation:  33.07933568954468
printing an ep nov before normalisation:  48.089835022654114
printing an ep nov before normalisation:  32.74501323063859
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.731098651885986
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.07318550069884286, 0.09358139145545898, 0.22615468137346365, 0.44031153431793274, 0.10377933683376701, 0.06298755532053482]
line 256 mcts: sample exp_bonus 37.54959748971264
printing an ep nov before normalisation:  54.66221914137828
printing an ep nov before normalisation:  21.572466324597336
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.07318550069884286, 0.09358139145545898, 0.22615468137346365, 0.44031153431793274, 0.10377933683376701, 0.06298755532053482]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.93830763205394
printing an ep nov before normalisation:  38.36303419462391
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.399918294330757
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.07318550069884286, 0.09358139145545898, 0.22615468137346365, 0.44031153431793274, 0.10377933683376701, 0.06298755532053482]
printing an ep nov before normalisation:  56.88654726714821
printing an ep nov before normalisation:  53.20125001828795
printing an ep nov before normalisation:  52.623605918784335
printing an ep nov before normalisation:  43.73673837396039
printing an ep nov before normalisation:  38.1575225881759
printing an ep nov before normalisation:  22.07458450934446
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.07318550069884286, 0.09358139145545898, 0.22615468137346365, 0.44031153431793274, 0.10377933683376701, 0.06298755532053482]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.457]
 [0.445]
 [0.469]
 [0.444]] [[29.577]
 [38.757]
 [34.773]
 [28.166]
 [33.31 ]] [[0.474]
 [0.457]
 [0.445]
 [0.469]
 [0.444]]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.090387338319005
actor:  0 policy actor:  0  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.007529026479460299
actor:  0 policy actor:  0  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.07318550069884286, 0.09358139145545898, 0.22615468137346365, 0.44031153431793274, 0.10377933683376701, 0.06298755532053482]
actor:  0 policy actor:  0  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.47 ]
 [0.451]
 [0.483]
 [0.451]] [[37.918]
 [38.587]
 [42.996]
 [38.133]
 [42.996]] [[0.469]
 [0.47 ]
 [0.451]
 [0.483]
 [0.451]]
maxi score, test score, baseline:  0.0601 0.35 0.35
maxi score, test score, baseline:  0.0601 0.35 0.35
probs:  [0.07318550069884287, 0.09358139145545899, 0.22615468137346365, 0.4403115343179328, 0.10377933683376703, 0.06298755532053482]
printing an ep nov before normalisation:  64.58443492996372
maxi score, test score, baseline:  0.0601 0.35 0.35
probs:  [0.07318550069884287, 0.09358139145545899, 0.22615468137346365, 0.4403115343179328, 0.10377933683376703, 0.06298755532053482]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  97 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0621 0.85 0.85
probs:  [0.07319512688110058, 0.09358891737976954, 0.22614855562111788, 0.44028335585714196, 0.10378581262910404, 0.06299823163176609]
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.601]
 [0.468]
 [0.468]
 [0.468]] [[45.526]
 [38.551]
 [37.618]
 [37.618]
 [37.618]] [[1.607]
 [1.613]
 [1.437]
 [1.437]
 [1.437]]
printing an ep nov before normalisation:  24.886455535888672
maxi score, test score, baseline:  0.0621 0.85 0.85
probs:  [0.07319512688110058, 0.09358891737976954, 0.22614855562111788, 0.44028335585714196, 0.10378581262910404, 0.06299823163176609]
printing an ep nov before normalisation:  43.95065665054721
maxi score, test score, baseline:  0.0621 0.85 0.85
probs:  [0.07319512688110058, 0.09358891737976954, 0.22614855562111788, 0.44028335585714196, 0.10378581262910404, 0.06299823163176609]
printing an ep nov before normalisation:  52.72519097133755
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.48 ]
 [0.431]
 [0.431]
 [0.41 ]] [[39.479]
 [49.672]
 [39.479]
 [39.479]
 [44.233]] [[0.919]
 [1.168]
 [0.919]
 [0.919]
 [0.991]]
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.687]
 [0.587]
 [0.587]
 [0.587]] [[47.595]
 [37.094]
 [41.146]
 [41.146]
 [41.146]] [[1.945]
 [1.455]
 [1.543]
 [1.543]
 [1.543]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.244456100739036
maxi score, test score, baseline:  0.0621 0.85 0.85
probs:  [0.07319512688110058, 0.09358891737976954, 0.22614855562111788, 0.44028335585714196, 0.10378581262910404, 0.06299823163176609]
printing an ep nov before normalisation:  29.89374423313924
maxi score, test score, baseline:  0.0621 0.85 0.85
probs:  [0.07319512688110058, 0.09358891737976954, 0.22614855562111788, 0.44028335585714196, 0.10378581262910404, 0.06299823163176609]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  81.26878556557394
maxi score, test score, baseline:  0.0621 0.85 0.85
probs:  [0.07245705836330096, 0.09264483157116506, 0.22386535742228164, 0.43583697610485445, 0.11283260477902912, 0.06236317175936893]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.0621 0.85 0.85
probs:  [0.07245705836330096, 0.09264483157116506, 0.22386535742228164, 0.43583697610485445, 0.11283260477902912, 0.06236317175936893]
printing an ep nov before normalisation:  47.909026141028775
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  56.34079692526299
maxi score, test score, baseline:  0.0621 0.85 0.85
printing an ep nov before normalisation:  71.00159166611562
printing an ep nov before normalisation:  48.981728663536884
line 256 mcts: sample exp_bonus 39.287339052585494
printing an ep nov before normalisation:  58.62516882527529
maxi score, test score, baseline:  0.0621 0.85 0.85
probs:  [0.07267302525405765, 0.09246116028829113, 0.22108403801080873, 0.4387535233873769, 0.11224929532252459, 0.06277895773694092]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0621 0.85 0.85
probs:  [0.07196178015577756, 0.10135295183157074, 0.21891763853474344, 0.43445289749056, 0.11115000905683513, 0.062164722930513165]
maxi score, test score, baseline:  0.0621 0.85 0.85
probs:  [0.07196178015577756, 0.10135295183157074, 0.21891763853474344, 0.43445289749056, 0.11115000905683513, 0.062164722930513165]
maxi score, test score, baseline:  0.0621 0.85 0.85
probs:  [0.07196178015577756, 0.10135295183157074, 0.21891763853474344, 0.43445289749056, 0.11115000905683513, 0.062164722930513165]
printing an ep nov before normalisation:  58.74086351447628
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.14 ]
 [0.127]
 [0.127]
 [0.137]] [[66.664]
 [64.614]
 [66.664]
 [66.664]
 [65.944]] [[1.977]
 [1.918]
 [1.977]
 [1.977]
 [1.961]]
maxi score, test score, baseline:  0.0621 0.85 0.85
probs:  [0.07126434699968869, 0.10037013944046164, 0.22649524001714447, 0.4302357871025551, 0.11007207025405262, 0.0615624161860977]
line 256 mcts: sample exp_bonus 36.35257087886225
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.532]
 [0.532]
 [0.601]
 [0.574]] [[41.009]
 [44.379]
 [44.379]
 [41.742]
 [43.532]] [[0.602]
 [0.532]
 [0.532]
 [0.601]
 [0.574]]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[51.951]
 [51.951]
 [51.951]
 [51.951]
 [51.951]] [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]]
maxi score, test score, baseline:  0.0621 0.85 0.85
probs:  [0.07126434699968869, 0.10037013944046164, 0.22649524001714447, 0.4302357871025551, 0.11007207025405262, 0.0615624161860977]
maxi score, test score, baseline:  0.0621 0.85 0.85
probs:  [0.07126434699968869, 0.10037013944046164, 0.22649524001714447, 0.4302357871025551, 0.11007207025405262, 0.0615624161860977]
from probs:  [0.07126434699968869, 0.10037013944046164, 0.22649524001714447, 0.4302357871025551, 0.11007207025405262, 0.0615624161860977]
actor:  0 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0641 0.85 0.85
probs:  [0.07126434699968869, 0.10037013944046164, 0.22649524001714447, 0.4302357871025551, 0.11007207025405262, 0.0615624161860977]
maxi score, test score, baseline:  0.0641 0.85 0.85
siam score:  -0.9321048
printing an ep nov before normalisation:  75.68748586543579
maxi score, test score, baseline:  0.0641 0.85 0.85
probs:  [0.07126434699968869, 0.10037013944046164, 0.22649524001714447, 0.4302357871025551, 0.11007207025405262, 0.0615624161860977]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.   ]
 [0.195]
 [0.182]
 [0.18 ]] [[23.356]
 [28.596]
 [56.021]
 [30.075]
 [43.447]] [[0.196]
 [0.105]
 [0.508]
 [0.298]
 [0.397]]
line 256 mcts: sample exp_bonus 68.2245168335078
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.493632892929234
printing an ep nov before normalisation:  80.25661078521165
line 256 mcts: sample exp_bonus 57.53276016325662
printing an ep nov before normalisation:  82.13626830229647
Printing some Q and Qe and total Qs values:  [[0.92 ]
 [0.92 ]
 [0.954]
 [0.926]
 [0.95 ]] [[50.955]
 [45.263]
 [29.623]
 [47.708]
 [44.618]] [[1.334]
 [1.255]
 [1.073]
 [1.295]
 [1.277]]
printing an ep nov before normalisation:  56.18868022299298
printing an ep nov before normalisation:  60.44936637358032
printing an ep nov before normalisation:  64.38333062288851
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07149479896317652, 0.10004635927422358, 0.2237697872887608, 0.4331478962364391, 0.10956354604457258, 0.061977612192827505]
printing an ep nov before normalisation:  41.38193689613703
printing an ep nov before normalisation:  65.63507815219963
printing an ep nov before normalisation:  53.217315678767875
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0641 0.85 0.85
probs:  [0.07082150719289115, 0.10853107813339301, 0.22165979095489854, 0.4290624311276587, 0.10853107813339301, 0.0613941144577657]
maxi score, test score, baseline:  0.0641 0.85 0.85
probs:  [0.07082150719289115, 0.10853107813339301, 0.22165979095489854, 0.4290624311276587, 0.10853107813339301, 0.0613941144577657]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0641 0.85 0.85
probs:  [0.07082150719289115, 0.10853107813339301, 0.22165979095489854, 0.4290624311276587, 0.10853107813339301, 0.0613941144577657]
printing an ep nov before normalisation:  63.28857870838005
printing an ep nov before normalisation:  33.59470162665346
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0641 0.85 0.85
probs:  [0.07105375707329911, 0.10806520594815106, 0.2190995525727069, 0.4319153836031056, 0.10806520594815106, 0.06180089485458611]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.692]
 [0.266]
 [0.193]
 [0.303]] [[42.63 ]
 [44.504]
 [32.209]
 [30.064]
 [36.24 ]] [[1.558]
 [1.891]
 [0.843]
 [0.662]
 [1.084]]
maxi score, test score, baseline:  0.0641 0.85 0.85
probs:  [0.07105375707329911, 0.10806520594815106, 0.2190995525727069, 0.4319153836031056, 0.10806520594815106, 0.06180089485458611]
maxi score, test score, baseline:  0.0641 0.85 0.85
probs:  [0.07105375707329911, 0.10806520594815106, 0.2190995525727069, 0.4319153836031056, 0.10806520594815106, 0.06180089485458611]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0641 0.85 0.85
probs:  [0.06318342060420416, 0.10198963787762759, 0.2281098440162538, 0.4415440390200826, 0.10198963787762759, 0.06318342060420416]
printing an ep nov before normalisation:  55.51849059173285
maxi score, test score, baseline:  0.0641 0.85 0.85
probs:  [0.06318342060420418, 0.1019896378776276, 0.2281098440162538, 0.44154403902008266, 0.1019896378776276, 0.06318342060420418]
actor:  0 policy actor:  1  step number:  105 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0661 0.85 0.85
probs:  [0.06318342060420418, 0.1019896378776276, 0.2281098440162538, 0.44154403902008266, 0.1019896378776276, 0.06318342060420418]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.793]
 [0.819]
 [0.885]
 [0.83 ]] [[54.745]
 [57.386]
 [48.963]
 [42.505]
 [49.227]] [[1.595]
 [1.655]
 [1.442]
 [1.324]
 [1.46 ]]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.801]
 [0.803]
 [0.801]
 [0.801]] [[43.104]
 [43.104]
 [52.951]
 [43.104]
 [43.104]] [[1.267]
 [1.267]
 [1.484]
 [1.267]
 [1.267]]
printing an ep nov before normalisation:  90.99639270228934
maxi score, test score, baseline:  0.0661 0.85 0.85
probs:  [0.06257713301339946, 0.10101019159306733, 0.22591763197698797, 0.4372994541651613, 0.11061845623798433, 0.06257713301339946]
printing an ep nov before normalisation:  78.43360598996247
printing an ep nov before normalisation:  98.88330928892637
maxi score, test score, baseline:  0.0661 0.85 0.85
probs:  [0.06257713301339946, 0.10101019159306733, 0.22591763197698797, 0.4372994541651613, 0.11061845623798433, 0.06257713301339946]
printing an ep nov before normalisation:  64.4562432807226
maxi score, test score, baseline:  0.0661 0.85 0.85
probs:  [0.06257713301339946, 0.10101019159306733, 0.22591763197698797, 0.4372994541651613, 0.11061845623798433, 0.06257713301339946]
printing an ep nov before normalisation:  39.73919431833851
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  62.188860864431696
maxi score, test score, baseline:  0.0661 0.85 0.85
printing an ep nov before normalisation:  56.47892475128174
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0661 0.85 0.85
maxi score, test score, baseline:  0.0661 0.85 0.85
probs:  [0.0629692569651141, 0.10067740594749686, 0.2232288901402408, 0.4400507467889416, 0.11010444319309254, 0.0629692569651141]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0661 0.85 0.85
probs:  [0.06334686257335193, 0.10035694165155423, 0.22063969865571173, 0.4427001731249256, 0.10960946142110481, 0.06334686257335193]
printing an ep nov before normalisation:  40.510306605410996
printing an ep nov before normalisation:  38.28537939133285
Printing some Q and Qe and total Qs values:  [[1.186]
 [1.186]
 [1.186]
 [1.186]
 [1.186]] [[41.39]
 [41.39]
 [41.39]
 [41.39]
 [41.39]] [[1.706]
 [1.706]
 [1.706]
 [1.706]
 [1.706]]
actor:  0 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using another actor
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.605]
 [0.596]
 [0.616]
 [0.634]] [[48.48 ]
 [50.19 ]
 [43.745]
 [31.788]
 [39.037]] [[0.603]
 [0.605]
 [0.596]
 [0.616]
 [0.634]]
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.06334686257335193, 0.10035694165155423, 0.22063969865571173, 0.4427001731249256, 0.10960946142110481, 0.06334686257335193]
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.06334686257335193, 0.10035694165155423, 0.22063969865571173, 0.4427001731249256, 0.10960946142110481, 0.06334686257335193]
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.23294685322263
printing an ep nov before normalisation:  27.42197036743164
printing an ep nov before normalisation:  63.289019528761315
printing an ep nov before normalisation:  77.85925396027082
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  96.5956354027232
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.06257534229192838, 0.10718590988110192, 0.22317338561295313, 0.4373041100409862, 0.10718590988110192, 0.06257534229192838]
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.06257534229192838, 0.10718590988110192, 0.22317338561295313, 0.4373041100409862, 0.10718590988110192, 0.06257534229192838]
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.06257534229192838, 0.10718590988110192, 0.22317338561295313, 0.4373041100409862, 0.10718590988110192, 0.06257534229192838]
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.06257534229192838, 0.10718590988110192, 0.22317338561295313, 0.4373041100409862, 0.10718590988110192, 0.06257534229192838]
printing an ep nov before normalisation:  66.74934708219756
Printing some Q and Qe and total Qs values:  [[0.976]
 [0.932]
 [0.885]
 [0.895]
 [0.891]] [[40.461]
 [42.767]
 [44.775]
 [37.629]
 [45.   ]] [[2.178]
 [2.23 ]
 [2.266]
 [1.98 ]
 [2.281]]
Printing some Q and Qe and total Qs values:  [[1.5  ]
 [1.499]
 [1.5  ]
 [1.501]
 [1.501]] [[11.823]
 [11.426]
 [22.959]
 [13.26 ]
 [12.049]] [[2.096]
 [2.074]
 [2.662]
 [2.17 ]
 [2.108]]
actor:  1 policy actor:  1  step number:  101 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.0629399861775676, 0.1067681610321165, 0.22072141565394368, 0.4398622899266881, 0.1067681610321165, 0.0629399861775676]
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.128]
 [0.134]
 [0.134]
 [0.134]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.134]
 [0.128]
 [0.134]
 [0.134]
 [0.134]]
printing an ep nov before normalisation:  34.53534543514252
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
printing an ep nov before normalisation:  30.418144497733763
printing an ep nov before normalisation:  47.718746154559334
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.0629399861775676, 0.1067681610321165, 0.22072141565394368, 0.4398622899266881, 0.1067681610321165, 0.0629399861775676]
Printing some Q and Qe and total Qs values:  [[0.845]
 [0.831]
 [0.846]
 [0.831]
 [0.859]] [[50.738]
 [44.006]
 [49.332]
 [44.006]
 [47.452]] [[1.834]
 [1.54 ]
 [1.777]
 [1.54 ]
 [1.712]]
Printing some Q and Qe and total Qs values:  [[0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]] [[39.095]
 [39.095]
 [39.095]
 [39.095]
 [39.095]] [[52.937]
 [52.937]
 [52.937]
 [52.937]
 [52.937]]
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
Printing some Q and Qe and total Qs values:  [[0.89]
 [0.89]
 [0.89]
 [0.93]
 [0.89]] [[41.671]
 [41.671]
 [41.671]
 [54.54 ]
 [41.671]] [[1.623]
 [1.623]
 [1.623]
 [2.03 ]
 [1.623]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.39699383528905
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.0629399861775676, 0.1067681610321165, 0.22072141565394368, 0.4398622899266881, 0.1067681610321165, 0.0629399861775676]
printing an ep nov before normalisation:  33.98041248321533
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.90405633452478
printing an ep nov before normalisation:  31.455764770507812
printing an ep nov before normalisation:  67.06891598615923
printing an ep nov before normalisation:  49.432278382552134
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.0629399861775676, 0.1067681610321165, 0.22072141565394368, 0.4398622899266881, 0.1067681610321165, 0.0629399861775676]
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.0629399861775676, 0.1067681610321165, 0.22072141565394368, 0.4398622899266881, 0.1067681610321165, 0.0629399861775676]
actor:  1 policy actor:  1  step number:  115 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.9418147
siam score:  -0.9414737
printing an ep nov before normalisation:  77.35503132288862
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.06329206006447513, 0.10636481281538829, 0.21835396996776252, 0.44233228427251076, 0.10636481281538829, 0.06329206006447513]
actor:  1 policy actor:  1  step number:  117 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.74723148345947
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]] [[66.511]
 [66.511]
 [66.511]
 [66.511]
 [66.511]] [[2.768]
 [2.768]
 [2.768]
 [2.768]
 [2.768]]
printing an ep nov before normalisation:  53.81307708099488
actions average: 
K:  1  action  0 :  tensor([    0.9993,     0.0003,     0.0000,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9668,     0.0001,     0.0006,     0.0322],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0012,     0.0002,     0.9139,     0.0423,     0.0425],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0002,     0.0010,     0.9146,     0.0842],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0035, 0.1642, 0.0560, 0.0990, 0.6774], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.17837911313088
printing an ep nov before normalisation:  50.763750748167595
printing an ep nov before normalisation:  47.69567095456509
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.06275219425610143, 0.10545677195907344, 0.21648867398680074, 0.43855247804225506, 0.11399768749966784, 0.06275219425610143]
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.794]
 [0.804]
 [0.954]
 [0.868]] [[58.605]
 [61.418]
 [61.334]
 [49.897]
 [52.859]] [[1.864]
 [1.898]
 [1.907]
 [1.851]
 [1.819]]
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.062221479500891255, 0.10456412294647588, 0.21465499590499598, 0.4348367418220359, 0.11303265163559281, 0.07069000819000817]
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.062221479500891255, 0.10456412294647588, 0.21465499590499598, 0.4348367418220359, 0.11303265163559281, 0.07069000819000817]
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.062221479500891255, 0.10456412294647588, 0.21465499590499598, 0.4348367418220359, 0.11303265163559281, 0.07069000819000817]
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
siam score:  -0.94266486
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.06185702394981696, 0.10492947986085109, 0.21691786522953987, 0.4322801447847105, 0.11354397104305793, 0.07047151513202378]
printing an ep nov before normalisation:  41.540321608145454
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.06185702394981696, 0.10492947986085109, 0.21691786522953987, 0.4322801447847105, 0.11354397104305793, 0.07047151513202378]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.061329428073835526, 0.11257457117305068, 0.215064857371481, 0.42858628695154416, 0.11257457117305068, 0.06987028525703806]
printing an ep nov before normalisation:  43.34274509200654
actor:  1 policy actor:  1  step number:  107 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.060300839879496845, 0.11068465256815627, 0.22824688217502817, 0.42138483081488914, 0.11068465256815627, 0.06869814199427342]
printing an ep nov before normalisation:  54.47069412372594
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.060300839879496845, 0.11068465256815627, 0.22824688217502817, 0.42138483081488914, 0.11068465256815627, 0.06869814199427342]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.157]
 [0.149]
 [0.149]
 [0.149]] [[59.454]
 [75.683]
 [59.454]
 [59.454]
 [59.454]] [[1.116]
 [1.426]
 [1.116]
 [1.116]
 [1.116]]
siam score:  -0.9438292
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.402]
 [0.255]
 [0.255]
 [0.255]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.255]
 [0.402]
 [0.255]
 [0.255]
 [0.255]]
printing an ep nov before normalisation:  70.22008895341533
from probs:  [0.060300839879496845, 0.11068465256815627, 0.22824688217502817, 0.42138483081488914, 0.11068465256815627, 0.06869814199427342]
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.06030083987949684, 0.11068465256815625, 0.22824688217502817, 0.4213848308148891, 0.11068465256815625, 0.0686981419942734]
printing an ep nov before normalisation:  28.118211282435034
printing an ep nov before normalisation:  69.23164723528753
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.114]
 [0.142]
 [0.111]
 [0.105]] [[43.932]
 [37.112]
 [34.924]
 [28.174]
 [33.515]] [[0.103]
 [0.114]
 [0.142]
 [0.111]
 [0.105]]
line 256 mcts: sample exp_bonus 86.46974844862726
Printing some Q and Qe and total Qs values:  [[0.9  ]
 [0.9  ]
 [0.884]
 [0.93 ]
 [0.977]] [[36.951]
 [36.455]
 [40.952]
 [43.062]
 [35.57 ]] [[1.76 ]
 [1.735]
 [1.94 ]
 [2.09 ]
 [1.77 ]]
Printing some Q and Qe and total Qs values:  [[0.998]
 [0.997]
 [0.942]
 [0.938]
 [0.972]] [[45.518]
 [44.725]
 [47.323]
 [47.335]
 [46.483]] [[2.436]
 [2.395]
 [2.469]
 [2.466]
 [2.457]]
printing an ep nov before normalisation:  64.28483519938375
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.06068197250821266, 0.11023325808878857, 0.2258529244434657, 0.4240580667657692, 0.11023325808878857, 0.0689405201049753]
printing an ep nov before normalisation:  59.60506269593032
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.06068197250821266, 0.11023325808878857, 0.2258529244434657, 0.4240580667657692, 0.11023325808878857, 0.0689405201049753]
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.06068197250821266, 0.11023325808878857, 0.2258529244434657, 0.4240580667657692, 0.11023325808878857, 0.0689405201049753]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.916]
 [0.9  ]
 [0.882]
 [0.889]
 [0.911]] [[28.312]
 [37.589]
 [36.352]
 [31.123]
 [30.853]] [[1.019]
 [1.119]
 [1.085]
 [1.026]
 [1.046]]
Printing some Q and Qe and total Qs values:  [[1.087]
 [1.087]
 [1.087]
 [1.087]
 [1.087]] [[51.846]
 [51.846]
 [51.846]
 [51.846]
 [51.846]] [[2.449]
 [2.449]
 [2.449]
 [2.449]
 [2.449]]
printing an ep nov before normalisation:  35.34981727600098
printing an ep nov before normalisation:  56.63433769963047
printing an ep nov before normalisation:  56.75425788617501
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.349598043368864
actions average: 
K:  0  action  0 :  tensor([    1.0000,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9852,     0.0000,     0.0018,     0.0129],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0004,     0.9050,     0.0279,     0.0665],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0020, 0.0016, 0.0160, 0.7563, 0.2241], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0044, 0.0394, 0.0570, 0.2546, 0.6446], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.05969735900452285, 0.10844286629360106, 0.2221823833014502, 0.41716441245776287, 0.10844286629360106, 0.08407011264906195]
printing an ep nov before normalisation:  73.09720428554952
printing an ep nov before normalisation:  49.40305233001709
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.05969735900452285, 0.10844286629360106, 0.2221823833014502, 0.41716441245776287, 0.10844286629360106, 0.08407011264906195]
printing an ep nov before normalisation:  48.738326275568106
printing an ep nov before normalisation:  60.600894441591535
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.05969735900452285, 0.10844286629360106, 0.2221823833014502, 0.41716441245776287, 0.10844286629360106, 0.08407011264906195]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.05921696387254458, 0.10756933012989951, 0.22039151806372767, 0.4138009830931474, 0.11562805783945868, 0.08339314700122204]
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.04 ]
 [0.087]
 [0.04 ]
 [0.04 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.045]
 [0.04 ]
 [0.087]
 [0.04 ]
 [0.04 ]]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.232]
 [0.251]
 [0.179]
 [0.554]] [[55.719]
 [63.276]
 [60.233]
 [55.988]
 [57.291]] [[0.222]
 [0.232]
 [0.251]
 [0.179]
 [0.554]]
siam score:  -0.94092256
printing an ep nov before normalisation:  30.774998664855957
printing an ep nov before normalisation:  21.023335456848145
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.05874425568533484, 0.10670977167703788, 0.21862930899101166, 0.41049137295782373, 0.11470402434232174, 0.0907212663464702]
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.045]
 [0.087]
 [0.065]
 [0.058]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.044]
 [0.045]
 [0.087]
 [0.065]
 [0.058]]
using explorer policy with actor:  1
from probs:  [0.05874425568533484, 0.10670977167703788, 0.21862930899101166, 0.41049137295782373, 0.11470402434232174, 0.0907212663464702]
printing an ep nov before normalisation:  66.29908153613574
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.05874425568533484, 0.10670977167703788, 0.21862930899101166, 0.41049137295782373, 0.11470402434232174, 0.0907212663464702]
using another actor
printing an ep nov before normalisation:  31.225750912232733
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
printing an ep nov before normalisation:  56.76740635715243
Printing some Q and Qe and total Qs values:  [[1.082]
 [1.082]
 [1.082]
 [1.082]
 [1.082]] [[59.749]
 [59.749]
 [59.749]
 [59.749]
 [59.749]] [[1.948]
 [1.948]
 [1.948]
 [1.948]
 [1.948]]
printing an ep nov before normalisation:  75.89840818037959
maxi score, test score, baseline:  0.06810000000000001 0.85 0.85
probs:  [0.05874425568533484, 0.10670977167703789, 0.21862930899101168, 0.41049137295782373, 0.11470402434232174, 0.0907212663464702]
printing an ep nov before normalisation:  68.16260088370977
printing an ep nov before normalisation:  74.1016557429675
line 256 mcts: sample exp_bonus 55.24477301434571
actor:  0 policy actor:  0  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.361]
 [1.131]
 [1.131]
 [1.131]
 [1.131]] [[42.575]
 [31.264]
 [31.264]
 [31.264]
 [31.264]] [[2.487]
 [1.957]
 [1.957]
 [1.957]
 [1.957]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07010000000000001 0.85 0.85
probs:  [0.05874425568533484, 0.10670977167703789, 0.21862930899101168, 0.41049137295782373, 0.11470402434232174, 0.0907212663464702]
maxi score, test score, baseline:  0.07010000000000001 0.85 0.85
probs:  [0.05874425568533484, 0.10670977167703789, 0.21862930899101168, 0.41049137295782373, 0.11470402434232174, 0.0907212663464702]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.07010000000000001 0.85 0.85
probs:  [0.05913188834567796, 0.10634227882806324, 0.2164998566202956, 0.41320981696356746, 0.11421067724179415, 0.09060548200060148]
using explorer policy with actor:  1
from probs:  [0.05913188834567796, 0.10634227882806324, 0.2164998566202956, 0.41320981696356746, 0.11421067724179415, 0.09060548200060148]
maxi score, test score, baseline:  0.07010000000000001 0.85 0.85
probs:  [0.05913188834567796, 0.10634227882806324, 0.2164998566202956, 0.41320981696356746, 0.11421067724179415, 0.09060548200060148]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.54 ]
 [0.564]
 [0.617]
 [0.579]] [[29.883]
 [33.878]
 [37.108]
 [51.732]
 [37.918]] [[0.558]
 [0.54 ]
 [0.564]
 [0.617]
 [0.579]]
printing an ep nov before normalisation:  28.237909003721313
siam score:  -0.9379295
from probs:  [0.05913188834567796, 0.10634227882806324, 0.2164998566202956, 0.41320981696356746, 0.11421067724179415, 0.09060548200060148]
printing an ep nov before normalisation:  52.38369214852864
using explorer policy with actor:  0
maxi score, test score, baseline:  0.07010000000000001 0.85 0.85
probs:  [0.05913188834567796, 0.10634227882806324, 0.2164998566202956, 0.41320981696356746, 0.11421067724179415, 0.09060548200060148]
printing an ep nov before normalisation:  37.076829348965575
actor:  0 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0721 0.85 0.85
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0721 0.85 0.85
probs:  [0.059507505100603944, 0.10598617758708899, 0.21443641338888741, 0.4158439941636558, 0.11373262300150318, 0.09049328675826063]
from probs:  [0.059507505100603944, 0.10598617758708899, 0.21443641338888741, 0.4158439941636558, 0.11373262300150318, 0.09049328675826063]
printing an ep nov before normalisation:  31.471594883715387
siam score:  -0.93871856
maxi score, test score, baseline:  0.0721 0.85 0.85
using explorer policy with actor:  0
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0741 0.85 0.85
probs:  [0.059507505100603944, 0.10598617758708899, 0.21443641338888741, 0.4158439941636558, 0.11373262300150318, 0.09049328675826063]
actor:  0 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.059507505100603944, 0.10598617758708899, 0.21443641338888741, 0.4158439941636558, 0.11373262300150318, 0.09049328675826063]
siam score:  -0.9374025
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.059507505100603944, 0.10598617758708899, 0.21443641338888741, 0.4158439941636558, 0.11373262300150318, 0.09049328675826063]
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.059507505100603944, 0.10598617758708899, 0.21443641338888741, 0.4158439941636558, 0.11373262300150318, 0.09049328675826063]
printing an ep nov before normalisation:  41.08362699499234
printing an ep nov before normalisation:  49.57247798475697
printing an ep nov before normalisation:  54.03151649749242
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  53.817173604320516
Printing some Q and Qe and total Qs values:  [[1.336]
 [1.34 ]
 [1.252]
 [1.252]
 [1.319]] [[53.696]
 [53.12 ]
 [45.828]
 [45.828]
 [53.407]] [[1.59 ]
 [1.589]
 [1.443]
 [1.443]
 [1.571]]
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.05987165612955895, 0.10564094635974797, 0.21243595689685568, 0.4183977629327062, 0.11326916139811281, 0.0903845162830183]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.219]
 [0.332]
 [0.253]
 [0.219]] [[37.281]
 [37.281]
 [46.532]
 [34.025]
 [37.281]] [[0.877]
 [0.877]
 [1.301]
 [0.802]
 [0.877]]
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.06141169916513713, 0.1013819399885028, 0.21329861429392666, 0.42913791474010105, 0.10937598815317595, 0.08539384365915653]
printing an ep nov before normalisation:  56.923666913697666
printing an ep nov before normalisation:  66.84062724537493
UNIT TEST: sample policy line 217 mcts : [0.128 0.641 0.051 0.026 0.154]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  54.04415835001925
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.0617573293160051, 0.10109833082250319, 0.21125313504069784, 0.43156274347708706, 0.10896653112380282, 0.08536193021990394]
printing an ep nov before normalisation:  74.23460341740787
siam score:  -0.94323653
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.8600459573715
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.062092245828212184, 0.10082351280541754, 0.20927106034159254, 0.4339124088093836, 0.10856976620085863, 0.08533100601453539]
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.062092245828212184, 0.10082351280541754, 0.20927106034159254, 0.4339124088093836, 0.10856976620085863, 0.08533100601453539]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.283]
 [0.296]
 [0.259]
 [0.843]] [[48.486]
 [49.858]
 [43.808]
 [43.187]
 [44.71 ]] [[1.624]
 [1.656]
 [1.346]
 [1.276]
 [1.941]]
printing an ep nov before normalisation:  52.98116499053002
printing an ep nov before normalisation:  47.06495761871338
Printing some Q and Qe and total Qs values:  [[1.177]
 [1.297]
 [1.224]
 [1.224]
 [1.306]] [[41.133]
 [56.182]
 [49.296]
 [49.296]
 [52.476]] [[1.821]
 [2.52 ]
 [2.182]
 [2.182]
 [2.387]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.93670388994325
printing an ep nov before normalisation:  44.2654323747057
maxi score, test score, baseline:  0.0761 0.85 0.85
printing an ep nov before normalisation:  67.17234739580772
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.062092245828212184, 0.10082351280541754, 0.20927106034159249, 0.4339124088093836, 0.10856976620085863, 0.08533100601453539]
printing an ep nov before normalisation:  53.32680863494046
maxi score, test score, baseline:  0.0761 0.85 0.85
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.062092245828212184, 0.10082351280541754, 0.20927106034159254, 0.4339124088093836, 0.10856976620085863, 0.08533100601453539]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.06161560672709407, 0.10773558426154059, 0.20766220225284135, 0.430575427002666, 0.10773558426154059, 0.08467559549431732]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.577]
 [0.586]
 [0.605]
 [0.586]] [[36.064]
 [47.557]
 [36.064]
 [38.154]
 [36.064]] [[0.586]
 [0.577]
 [0.586]
 [0.605]
 [0.586]]
printing an ep nov before normalisation:  57.02930749087787
printing an ep nov before normalisation:  52.98243299859772
printing an ep nov before normalisation:  59.125198046179584
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.06161560672709407, 0.10773558426154059, 0.20766220225284135, 0.430575427002666, 0.10773558426154059, 0.08467559549431732]
printing an ep nov before normalisation:  67.83326415711956
printing an ep nov before normalisation:  42.30357189231764
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.0619450595962675, 0.10736647953041656, 0.20577955605440615, 0.4328866557251513, 0.10736647953041656, 0.08465576956334202]
printing an ep nov before normalisation:  65.73312847433304
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.06226468123919522, 0.10700838927954014, 0.20395309003362075, 0.43512891490873606, 0.10700838927954014, 0.08463653525936768]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.06257490524792289, 0.10666082773115557, 0.20218032644482634, 0.4373052463554004, 0.10666082773115557, 0.0846178664895392]
Printing some Q and Qe and total Qs values:  [[1.329]
 [1.339]
 [1.329]
 [1.329]
 [1.329]] [[42.483]
 [51.158]
 [42.483]
 [42.483]
 [42.483]] [[1.709]
 [1.881]
 [1.709]
 [1.709]
 [1.709]]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  75.278962816807
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.0624247141454013, 0.11274841536256391, 0.19901761744912835, 0.4362579231871805, 0.10555931518868354, 0.08399201466704241]
printing an ep nov before normalisation:  28.90658479071411
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.0624247141454013, 0.11274841536256391, 0.19901761744912835, 0.4362579231871805, 0.10555931518868354, 0.08399201466704241]
Printing some Q and Qe and total Qs values:  [[1.097]
 [1.101]
 [1.097]
 [1.097]
 [1.097]] [[46.466]
 [51.548]
 [46.466]
 [46.466]
 [46.466]] [[2.533]
 [2.768]
 [2.533]
 [2.533]
 [2.533]]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.90553379058838
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.06272167057877832, 0.11233178234799779, 0.1973776882380882, 0.4383410882600111, 0.10524462352382358, 0.08398314705130093]
printing an ep nov before normalisation:  74.3801839498959
line 256 mcts: sample exp_bonus 73.89525320981838
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.06272167057877832, 0.11233178234799779, 0.1973776882380882, 0.4383410882600111, 0.10524462352382358, 0.08398314705130093]
maxi score, test score, baseline:  0.0761 0.85 0.85
probs:  [0.06272167057877832, 0.11233178234799779, 0.1973776882380882, 0.4383410882600111, 0.10524462352382358, 0.08398314705130093]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  0
printing an ep nov before normalisation:  40.964836924387384
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0781 0.85 0.85
probs:  [0.06301032308478792, 0.11192679983129253, 0.19578361711101466, 0.44036600084353744, 0.10493873172464899, 0.08397452740471846]
printing an ep nov before normalisation:  70.81820387880138
maxi score, test score, baseline:  0.0781 0.85 0.85
probs:  [0.06301032308478792, 0.11192679983129253, 0.19578361711101466, 0.44036600084353744, 0.10493873172464899, 0.08397452740471846]
actor:  0 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.281]
 [0.409]
 [0.409]
 [0.769]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.409]
 [0.281]
 [0.409]
 [0.409]
 [0.769]]
printing an ep nov before normalisation:  51.8516150866231
maxi score, test score, baseline:  0.0801 0.85 0.85
probs:  [0.06301032308478792, 0.11192679983129253, 0.19578361711101466, 0.44036600084353744, 0.10493873172464899, 0.08397452740471846]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0801 0.85 0.85
probs:  [0.06257363618082518, 0.11808991910660732, 0.19442480812955779, 0.4373085459298546, 0.10421084837516177, 0.08339224227799347]
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.276]
 [0.57 ]
 [0.57 ]
 [0.725]] [[49.378]
 [57.912]
 [49.378]
 [49.378]
 [48.834]] [[1.623]
 [1.609]
 [1.623]
 [1.623]
 [1.76 ]]
printing an ep nov before normalisation:  34.461941719055176
maxi score, test score, baseline:  0.0801 0.85 0.85
probs:  [0.06257363618082518, 0.11808991910660732, 0.19442480812955779, 0.4373085459298546, 0.10421084837516177, 0.08339224227799347]
printing an ep nov before normalisation:  29.31681275367737
maxi score, test score, baseline:  0.0801 0.85 0.85
probs:  [0.06257363618082518, 0.11808991910660732, 0.19442480812955779, 0.4373085459298546, 0.10421084837516177, 0.08339224227799347]
printing an ep nov before normalisation:  50.96204163670979
maxi score, test score, baseline:  0.0801 0.85 0.85
probs:  [0.06257363618082518, 0.11808991910660732, 0.19442480812955779, 0.4373085459298546, 0.10421084837516177, 0.08339224227799347]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.632]
 [ 0.041]
 [ 0.032]
 [ 0.029]
 [ 0.035]] [[0.277]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.9960,     0.0004,     0.0000,     0.0001,     0.0035],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9447,     0.0000,     0.0003,     0.0549],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0031, 0.0053, 0.8902, 0.0353, 0.0661], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0006,     0.0286,     0.9172,     0.0534],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0019, 0.0557, 0.0403, 0.1306, 0.7714], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0801 0.85 0.85
printing an ep nov before normalisation:  66.39123492611179
printing an ep nov before normalisation:  63.56016005428169
maxi score, test score, baseline:  0.0801 0.85 0.85
probs:  [0.06257363618082518, 0.11808991910660732, 0.19442480812955779, 0.4373085459298546, 0.10421084837516177, 0.08339224227799347]
maxi score, test score, baseline:  0.0801 0.85 0.85
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.606]
 [0.606]
 [0.649]
 [0.606]] [[36.791]
 [36.791]
 [36.791]
 [47.146]
 [36.791]] [[0.606]
 [0.606]
 [0.606]
 [0.649]
 [0.606]]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  57.51053474016439
printing an ep nov before normalisation:  46.32572677831159
using explorer policy with actor:  0
printing an ep nov before normalisation:  55.882253572379334
printing an ep nov before normalisation:  65.58602796668644
printing an ep nov before normalisation:  35.23203839704758
Printing some Q and Qe and total Qs values:  [[1.454]
 [1.448]
 [1.454]
 [1.454]
 [1.458]] [[30.037]
 [37.668]
 [30.037]
 [30.037]
 [36.843]] [[2.156]
 [2.328]
 [2.156]
 [2.156]
 [2.319]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
actor:  0 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.806657710345384
printing an ep nov before normalisation:  74.69404413403451
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.062295328546167955, 0.12225333129624172, 0.18887333435187917, 0.4353673456577378, 0.10892933068511419, 0.0822813294628592]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.899]
 [0.899]
 [0.878]
 [0.899]
 [0.899]] [[68.791]
 [68.791]
 [77.543]
 [68.791]
 [68.791]] [[2.341]
 [2.341]
 [2.586]
 [2.341]
 [2.341]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06216435890737031, 0.12094690702197453, 0.19279224360649075, 0.4344538302998635, 0.10788411855206248, 0.08175854161223838]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06216435890737031, 0.12094690702197453, 0.19279224360649075, 0.4344538302998635, 0.10788411855206248, 0.08175854161223838]
printing an ep nov before normalisation:  20.20348310470581
siam score:  -0.93667334
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06216435890737031, 0.12094690702197453, 0.19279224360649075, 0.4344538302998635, 0.10788411855206248, 0.08175854161223838]
siam score:  -0.935993
printing an ep nov before normalisation:  70.54833200275368
Printing some Q and Qe and total Qs values:  [[1.049]
 [0.992]
 [0.944]
 [0.938]
 [0.992]] [[57.498]
 [46.664]
 [54.435]
 [55.059]
 [46.664]] [[2.368]
 [1.956]
 [2.163]
 [2.177]
 [1.956]]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06216435890737031, 0.12094690702197453, 0.19279224360649075, 0.4344538302998635, 0.10788411855206248, 0.08175854161223838]
printing an ep nov before normalisation:  31.603415545177686
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  54.68360931700627
printing an ep nov before normalisation:  57.03654042766658
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.362]
 [0.322]
 [0.322]
 [0.322]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.322]
 [0.362]
 [0.322]
 [0.322]
 [0.322]]
line 256 mcts: sample exp_bonus 49.24490289706065
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06136387601377217, 0.119387862700061, 0.19030606864996955, 0.42884912502693456, 0.11294075306825112, 0.08715231454101165]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.755]
 [0.458]
 [0.458]
 [0.375]] [[40.301]
 [41.902]
 [40.301]
 [40.301]
 [47.001]] [[1.231]
 [1.573]
 [1.231]
 [1.231]
 [1.336]]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06136387601377217, 0.119387862700061, 0.19030606864996955, 0.42884912502693456, 0.11294075306825112, 0.08715231454101165]
siam score:  -0.9348063
printing an ep nov before normalisation:  74.09225577308042
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.334]
 [0.334]
 [0.328]
 [0.334]] [[86.91 ]
 [86.91 ]
 [86.91 ]
 [88.447]
 [86.91 ]] [[1.962]
 [1.962]
 [1.962]
 [1.991]
 [1.962]]
printing an ep nov before normalisation:  83.61956608674133
printing an ep nov before normalisation:  60.523085058679534
UNIT TEST: sample policy line 217 mcts : [0.026 0.436 0.026 0.128 0.385]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06136387601377217, 0.119387862700061, 0.19030606864996955, 0.42884912502693456, 0.11294075306825112, 0.08715231454101165]
printing an ep nov before normalisation:  74.59386825337972
printing an ep nov before normalisation:  59.00485131940387
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06136387601377217, 0.119387862700061, 0.19030606864996955, 0.42884912502693456, 0.11294075306825112, 0.08715231454101165]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06136387601377217, 0.119387862700061, 0.19030606864996955, 0.42884912502693456, 0.11294075306825112, 0.08715231454101165]
line 256 mcts: sample exp_bonus 0.165159178864738
Printing some Q and Qe and total Qs values:  [[1.502]
 [1.5  ]
 [1.501]
 [1.501]
 [1.501]] [[11.121]
 [ 4.799]
 [ 9.939]
 [10.484]
 [11.105]] [[1.754]
 [1.608]
 [1.726]
 [1.738]
 [1.753]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06097133229360441, 0.1186233328607293, 0.1954926669502291, 0.4261006692187286, 0.11221755501993762, 0.08659444365677102]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06097133229360441, 0.1186233328607293, 0.1954926669502291, 0.4261006692187286, 0.11221755501993762, 0.08659444365677102]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.060583789631148424, 0.11786854323032832, 0.20061318731803254, 0.42338722909262083, 0.11150357060819721, 0.08604368011967281]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.060583789631148424, 0.11786854323032832, 0.20061318731803254, 0.42338722909262083, 0.11150357060819721, 0.08604368011967281]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.060583789631148424, 0.11786854323032832, 0.20061318731803254, 0.42338722909262083, 0.11150357060819721, 0.08604368011967281]
printing an ep nov before normalisation:  46.271133332753955
Printing some Q and Qe and total Qs values:  [[0.939]
 [0.885]
 [0.885]
 [0.885]
 [0.885]] [[69.145]
 [66.427]
 [66.427]
 [66.427]
 [66.427]] [[2.272]
 [2.149]
 [2.149]
 [2.149]
 [2.149]]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.0602011530601632, 0.11712330884977894, 0.20566888452251447, 0.42070813972772936, 0.11079862487315496, 0.08549988896665908]
printing an ep nov before normalisation:  56.5763030443125
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.0602011530601632, 0.11712330884977894, 0.20566888452251447, 0.42070813972772936, 0.11079862487315496, 0.08549988896665908]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.953]
 [0.973]
 [0.962]
 [0.986]
 [0.953]] [[53.26 ]
 [57.932]
 [57.46 ]
 [54.406]
 [53.26 ]] [[2.525]
 [2.75 ]
 [2.719]
 [2.609]
 [2.525]]
maxi score, test score, baseline:  0.0821 0.85 0.85
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06049063596805336, 0.1167014757496722, 0.20414055985441257, 0.42273827011626336, 0.11045582688504786, 0.0854732314265506]
printing an ep nov before normalisation:  55.5018348730514
printing an ep nov before normalisation:  58.19903835977673
printing an ep nov before normalisation:  57.00019836425781
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06049063596805336, 0.1167014757496722, 0.20414055985441257, 0.42273827011626336, 0.11045582688504786, 0.0854732314265506]
siam score:  -0.93020326
UNIT TEST: sample policy line 217 mcts : [0.026 0.744 0.026 0.026 0.179]
printing an ep nov before normalisation:  68.55147671985367
Printing some Q and Qe and total Qs values:  [[1.004]
 [1.063]
 [1.064]
 [1.028]
 [1.033]] [[63.067]
 [51.154]
 [50.027]
 [61.66 ]
 [62.47 ]] [[2.089]
 [1.757]
 [1.721]
 [2.066]
 [2.099]]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06049063596805336, 0.1167014757496722, 0.20414055985441257, 0.42273827011626336, 0.11045582688504786, 0.0854732314265506]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  64.02636114279875
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.134]
 [0.161]
 [0.148]
 [0.147]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.121]
 [0.134]
 [0.161]
 [0.148]
 [0.147]]
printing an ep nov before normalisation:  26.60691738128662
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.060772973242685574, 0.11629005523195723, 0.20264996054860204, 0.4247182885056885, 0.1101214905664826, 0.08544723190458407]
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06040091072477603, 0.1155773609253731, 0.2014073945707463, 0.42211319537313435, 0.1155773609253731, 0.08492377748059694]
Printing some Q and Qe and total Qs values:  [[1.198]
 [1.109]
 [1.109]
 [1.109]
 [1.109]] [[49.825]
 [35.239]
 [35.239]
 [35.239]
 [35.239]] [[1.824]
 [1.459]
 [1.459]
 [1.459]
 [1.459]]
siam score:  -0.93118316
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06040091072477603, 0.1155773609253731, 0.2014073945707463, 0.42211319537313435, 0.1155773609253731, 0.08492377748059694]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06040091072477603, 0.1155773609253731, 0.2014073945707463, 0.42211319537313435, 0.1155773609253731, 0.08492377748059694]
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06003338603387024, 0.1209666892526111, 0.20017998343697416, 0.41953987502444096, 0.114873358930737, 0.08440670732136657]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0821 0.85 0.85
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06003338603387024, 0.1209666892526111, 0.20017998343697416, 0.41953987502444096, 0.114873358930737, 0.08440670732136657]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.603]
 [0.603]
 [0.522]
 [0.568]] [[31.424]
 [39.823]
 [45.336]
 [39.703]
 [32.435]] [[0.603]
 [0.603]
 [0.603]
 [0.522]
 [0.568]]
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]] [[28.245]
 [28.245]
 [28.245]
 [28.245]
 [28.245]] [[0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]]
printing an ep nov before normalisation:  52.060347769167144
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 44.582716244449756
printing an ep nov before normalisation:  59.319888043166884
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06003338603387024, 0.1209666892526111, 0.20017998343697416, 0.41953987502444096, 0.114873358930737, 0.08440670732136657]
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.363]
 [0.672]
 [0.548]
 [0.569]] [[30.891]
 [28.701]
 [28.532]
 [25.07 ]
 [25.006]] [[0.6  ]
 [0.363]
 [0.672]
 [0.548]
 [0.569]]
printing an ep nov before normalisation:  33.739230630453484
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.562]
 [0.643]
 [0.692]
 [0.696]] [[17.537]
 [36.75 ]
 [44.74 ]
 [20.176]
 [18.749]] [[0.622]
 [0.562]
 [0.643]
 [0.692]
 [0.696]]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06003338603387024, 0.12096668925261107, 0.20017998343697413, 0.41953987502444096, 0.114873358930737, 0.08440670732136656]
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06003338603387024, 0.12096668925261107, 0.20017998343697413, 0.41953987502444096, 0.114873358930737, 0.08440670732136656]
printing an ep nov before normalisation:  62.50729701734554
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.001]
 [0.347]
 [0.263]
 [0.293]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.182]
 [0.001]
 [0.347]
 [0.263]
 [0.293]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 34.25745523359512
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.06003338603387024, 0.12096668925261107, 0.20017998343697413, 0.41953987502444096, 0.114873358930737, 0.08440670732136656]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05967031665536139, 0.12023428835987383, 0.19896745157574, 0.4169977497119847, 0.12023428835987383, 0.08389590533716636]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.137]
 [1.137]
 [1.137]
 [1.137]
 [1.137]] [[56.956]
 [56.956]
 [56.956]
 [56.956]
 [56.956]] [[1.544]
 [1.544]
 [1.544]
 [1.544]
 [1.544]]
maxi score, test score, baseline:  0.0821 0.85 0.85
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.059311622063153914, 0.12553062153821787, 0.1977695300564694, 0.414486255611224, 0.11951071249503024, 0.08339125823590443]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.059597907902890476, 0.12502881603630925, 0.19640798854549335, 0.416493770448811, 0.11908055166054392, 0.08339096540595184]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.059597907902890476, 0.12502881603630925, 0.19640798854549335, 0.416493770448811, 0.11908055166054392, 0.08339096540595184]
printing an ep nov before normalisation:  35.289788515624615
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.059597907902890476, 0.12502881603630925, 0.19640798854549335, 0.416493770448811, 0.11908055166054392, 0.08339096540595184]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 51.27546114178453
actions average: 
K:  2  action  0 :  tensor([    0.9667,     0.0019,     0.0000,     0.0007,     0.0307],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9770,     0.0001,     0.0014,     0.0214],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0063,     0.9183,     0.0362,     0.0391],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0010,     0.0573,     0.7570,     0.1843],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0011, 0.0307, 0.1209, 0.1462, 0.7011], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.598]
 [0.373]
 [0.373]
 [0.373]] [[40.636]
 [52.727]
 [40.636]
 [40.636]
 [40.636]] [[1.27]
 [1.89]
 [1.27]
 [1.27]
 [1.27]]
printing an ep nov before normalisation:  28.883254528045654
printing an ep nov before normalisation:  49.16258258297023
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.059597907902890476, 0.12502881603630925, 0.19640798854549335, 0.416493770448811, 0.11908055166054392, 0.08339096540595184]
printing an ep nov before normalisation:  73.68039167985505
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[29.224]
 [29.224]
 [29.224]
 [29.224]
 [29.224]] [[0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05924600815634877, 0.12428970963966053, 0.20115953866539263, 0.41402983442895835, 0.11837664586845041, 0.0828982632411894]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05924600815634877, 0.12428970963966053, 0.20115953866539263, 0.41402983442895835, 0.11837664586845041, 0.0828982632411894]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05924600815634877, 0.12428970963966053, 0.20115953866539263, 0.41402983442895835, 0.11837664586845041, 0.0828982632411894]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05924600815634877, 0.12428970963966053, 0.20115953866539263, 0.41402983442895835, 0.11837664586845041, 0.0828982632411894]
printing an ep nov before normalisation:  52.61379886389171
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  60.1667249544888
maxi score, test score, baseline:  0.0821 0.85 0.85
printing an ep nov before normalisation:  48.543214865700165
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05889824884811889, 0.12355929953924756, 0.2058551822370476, 0.4115948889815479, 0.11768102220369041, 0.08241135819034748]
printing an ep nov before normalisation:  53.304695147742535
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05889824884811889, 0.12355929953924756, 0.2058551822370476, 0.4115948889815479, 0.11768102220369041, 0.08241135819034748]
printing an ep nov before normalisation:  40.69808157135932
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05889824884811889, 0.12355929953924756, 0.2058551822370476, 0.4115948889815479, 0.11768102220369041, 0.08241135819034748]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.059673427024419745, 0.12023563814267273, 0.20502273370822688, 0.41699047262211225, 0.11417941703084745, 0.08389831147172094]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.059673427024419745, 0.12023563814267273, 0.20502273370822688, 0.41699047262211225, 0.11417941703084745, 0.08389831147172094]
printing an ep nov before normalisation:  24.90292174814883
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 4.092]
 [ 4.733]
 [ 3.326]
 [11.855]
 [ 4.671]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  57.43222415682732
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.059673427024419745, 0.12023563814267275, 0.2050227337082269, 0.41699047262211225, 0.11417941703084745, 0.08389831147172094]
printing an ep nov before normalisation:  27.629332824183177
printing an ep nov before normalisation:  58.26058709322692
maxi score, test score, baseline:  0.0821 0.85 0.85
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.059673427024419745, 0.12023563814267275, 0.2050227337082269, 0.41699047262211225, 0.11417941703084745, 0.08389831147172094]
Printing some Q and Qe and total Qs values:  [[1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.173]] [[25.59 ]
 [25.59 ]
 [25.59 ]
 [25.59 ]
 [27.931]] [[1.974]
 [1.974]
 [1.974]
 [1.974]
 [2.064]]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.509]
 [0.465]
 [0.465]
 [0.465]] [[39.511]
 [50.82 ]
 [39.511]
 [39.511]
 [39.511]] [[1.474]
 [1.99 ]
 [1.474]
 [1.474]
 [1.474]]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05931472405801846, 0.11951207505352214, 0.2098081015467776, 0.41447909493149, 0.11349233995397179, 0.08339366445621993]
siam score:  -0.93265444
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.726]
 [0.776]
 [0.778]
 [0.774]] [[38.674]
 [38.405]
 [44.096]
 [41.783]
 [41.203]] [[1.551]
 [1.474]
 [1.75 ]
 [1.66 ]
 [1.633]]
printing an ep nov before normalisation:  46.32824948821656
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05931472405801846, 0.11951207505352214, 0.2098081015467776, 0.41447909493149, 0.11349233995397179, 0.08339366445621993]
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05896031725769527, 0.1187971780404572, 0.2085524692146, 0.41199779587599034, 0.112813491962181, 0.08887874764907625]
siam score:  -0.93294096
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05896031725769527, 0.1187971780404572, 0.2085524692146, 0.41199779587599034, 0.112813491962181, 0.08887874764907625]
printing an ep nov before normalisation:  41.05654716491699
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05896031725769527, 0.11879717804045715, 0.2085524692145999, 0.41199779587599034, 0.11281349196218099, 0.08887874764907625]
printing an ep nov before normalisation:  33.63599426361818
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.21028234847162
siam score:  -0.93374306
maxi score, test score, baseline:  0.0821 0.85 0.85
siam score:  -0.93375885
maxi score, test score, baseline:  0.0821 0.85 0.85
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05924905695879527, 0.11837801643101808, 0.20707145563935223, 0.414022813792132, 0.1124651204837958, 0.08881353669490667]
from probs:  [0.05924905695879527, 0.11837801643101808, 0.20707145563935223, 0.414022813792132, 0.1124651204837958, 0.08881353669490667]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05924905695879527, 0.11837801643101808, 0.20707145563935223, 0.414022813792132, 0.1124651204837958, 0.08881353669490667]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
Printing some Q and Qe and total Qs values:  [[1.121]
 [1.019]
 [1.019]
 [1.019]
 [1.019]] [[48.354]
 [47.692]
 [47.692]
 [47.692]
 [47.692]] [[2.146]
 [2.021]
 [2.021]
 [2.021]
 [2.021]]
siam score:  -0.9372777
printing an ep nov before normalisation:  29.877845069947952
printing an ep nov before normalisation:  49.73012997281646
printing an ep nov before normalisation:  25.251869075269905
printing an ep nov before normalisation:  44.1592109002204
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05924905695879527, 0.11837801643101808, 0.20707145563935223, 0.414022813792132, 0.1124651204837958, 0.08881353669490667]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.506]
 [0.506]
 [0.514]
 [0.506]] [[43.27 ]
 [43.27 ]
 [43.27 ]
 [48.044]
 [43.27 ]] [[0.506]
 [0.506]
 [0.506]
 [0.514]
 [0.506]]
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.05924905695879527, 0.11837801643101808, 0.20707145563935223, 0.414022813792132, 0.1124651204837958, 0.08881353669490667]
printing an ep nov before normalisation:  36.20232270423873
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.362]
 [0.526]
 [0.574]
 [0.573]] [[43.603]
 [39.449]
 [57.339]
 [51.388]
 [42.032]] [[0.639]
 [0.362]
 [0.526]
 [0.574]
 [0.573]]
printing an ep nov before normalisation:  55.80417624866857
printing an ep nov before normalisation:  55.57431955684278
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0821 0.85 0.85
probs:  [0.059531044656724486, 0.11796865666214751, 0.205625074670282, 0.4160004778898049, 0.11212489546160521, 0.08874985065943601]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.24696049434701
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  55.06501059159939
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]] [[33.665]
 [33.665]
 [33.665]
 [33.665]
 [33.665]] [[1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.112]]
printing an ep nov before normalisation:  43.11822829367021
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.9398858
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.854956543582674
Printing some Q and Qe and total Qs values:  [[0.988]
 [0.899]
 [0.899]
 [0.899]
 [0.899]] [[50.206]
 [39.291]
 [39.291]
 [39.291]
 [39.291]] [[0.988]
 [0.899]
 [0.899]
 [0.899]
 [0.899]]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.46798227208416
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1101 0.85 0.85
probs:  [0.059531044656724486, 0.11796865666214751, 0.205625074670282, 0.4160004778898049, 0.11212489546160521, 0.08874985065943601]
Printing some Q and Qe and total Qs values:  [[0.901]
 [0.901]
 [1.004]
 [1.002]
 [0.901]] [[26.057]
 [26.057]
 [39.267]
 [43.968]
 [26.057]] [[0.901]
 [0.901]
 [1.004]
 [1.002]
 [0.901]]
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1201 0.85 0.85
probs:  [0.059531044656724486, 0.11796865666214751, 0.20562507467028202, 0.4160004778898049, 0.11212489546160521, 0.08874985065943601]
actor:  0 policy actor:  0  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  62.0555871678953
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.465083889812526
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1221 1.0 1.0
printing an ep nov before normalisation:  66.99455948531178
printing an ep nov before normalisation:  57.53973108138893
printing an ep nov before normalisation:  22.040247917175293
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.562709936406826
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.454]
 [0.394]
 [0.394]
 [0.394]] [[60.587]
 [61.025]
 [60.587]
 [60.587]
 [60.587]] [[1.441]
 [1.513]
 [1.441]
 [1.441]
 [1.441]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9854,     0.0010,     0.0000,     0.0032,     0.0105],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9788,     0.0002,     0.0003,     0.0207],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0013, 0.0154, 0.8367, 0.0162, 0.1304], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0003,     0.0052,     0.8740,     0.1202],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0126, 0.0654, 0.1172, 0.1916, 0.6132], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.18285232038318
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.19390428963578
printing an ep nov before normalisation:  45.8093531987918
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.03180752509706508
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.0297677516757
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.238]
 [0.238]
 [0.227]
 [0.238]] [[41.555]
 [41.555]
 [41.555]
 [44.81 ]
 [41.555]] [[1.395]
 [1.395]
 [1.395]
 [1.533]
 [1.395]]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.056650968341195
printing an ep nov before normalisation:  45.06347732909572
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.598]
 [0.538]
 [0.538]
 [0.538]] [[38.481]
 [48.987]
 [38.481]
 [38.481]
 [38.481]] [[1.116]
 [1.475]
 [1.116]
 [1.116]
 [1.116]]
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1221 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.902]
 [0.882]
 [0.882]
 [0.883]
 [0.882]] [[58.755]
 [69.147]
 [69.147]
 [76.076]
 [69.147]] [[2.005]
 [2.323]
 [2.323]
 [2.55 ]
 [2.323]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1221 1.0 1.0
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.091055489258274
maxi score, test score, baseline:  0.1221 1.0 1.0
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.002]
 [0.391]
 [0.386]
 [0.34 ]] [[46.735]
 [50.235]
 [48.688]
 [33.807]
 [48.599]] [[1.017]
 [0.624]
 [0.982]
 [0.667]
 [0.928]]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.07726717092065
printing an ep nov before normalisation:  28.332225309406905
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.765]
 [0.767]
 [0.767]
 [0.767]] [[47.719]
 [56.232]
 [47.719]
 [47.719]
 [47.719]] [[2.078]
 [2.634]
 [2.078]
 [2.078]
 [2.078]]
Printing some Q and Qe and total Qs values:  [[1.06]
 [1.  ]
 [1.  ]
 [1.  ]
 [1.  ]] [[54.823]
 [49.273]
 [49.273]
 [49.273]
 [49.273]] [[2.66 ]
 [2.341]
 [2.341]
 [2.341]
 [2.341]]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 49.890902378151296
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.301]
 [0.328]
 [0.301]
 [0.301]] [[61.956]
 [61.956]
 [68.055]
 [61.956]
 [61.956]] [[1.794]
 [1.794]
 [1.995]
 [1.794]
 [1.794]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  85.62276837075449
printing an ep nov before normalisation:  32.99448036688203
printing an ep nov before normalisation:  51.42156425667905
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.222]
 [0.276]
 [0.211]
 [0.229]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.21 ]
 [0.222]
 [0.276]
 [0.211]
 [0.229]]
printing an ep nov before normalisation:  19.18064246646863
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1221 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  65.11073318667925
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.319]
 [0.298]
 [0.319]
 [0.319]] [[39.96 ]
 [39.96 ]
 [58.339]
 [39.96 ]
 [39.96 ]] [[0.643]
 [0.643]
 [0.846]
 [0.643]
 [0.643]]
printing an ep nov before normalisation:  66.69001363739392
printing an ep nov before normalisation:  59.771389984710076
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.4917547359763
Printing some Q and Qe and total Qs values:  [[0.953]
 [0.948]
 [0.949]
 [0.949]
 [0.948]] [[58.513]
 [64.614]
 [62.878]
 [56.423]
 [62.273]] [[2.027]
 [2.205]
 [2.155]
 [1.96 ]
 [2.135]]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.17847549918253
Printing some Q and Qe and total Qs values:  [[1.031]
 [1.005]
 [0.989]
 [0.977]
 [0.987]] [[47.305]
 [46.248]
 [53.679]
 [54.786]
 [46.914]] [[1.8  ]
 [1.742]
 [1.951]
 [1.973]
 [1.744]]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.68931891258205
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.8640457885860542
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.71353656269093
line 256 mcts: sample exp_bonus 42.821561889696945
printing an ep nov before normalisation:  50.00043146007876
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  46.460549763477715
UNIT TEST: sample policy line 217 mcts : [0.026 0.077 0.051 0.026 0.821]
maxi score, test score, baseline:  0.1241 1.0 1.0
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.10366005921766
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1261 1.0 1.0
printing an ep nov before normalisation:  68.4866991699903
printing an ep nov before normalisation:  42.79023334956712
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.904]
 [0.913]
 [0.913]
 [0.913]] [[37.657]
 [44.605]
 [37.657]
 [37.657]
 [37.657]] [[1.676]
 [1.935]
 [1.676]
 [1.676]
 [1.676]]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.75251557790383
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.381]
 [0.403]
 [0.428]
 [0.428]] [[81.337]
 [76.716]
 [71.638]
 [81.337]
 [81.337]] [[2.324]
 [2.126]
 [1.983]
 [2.324]
 [2.324]]
siam score:  -0.93464965
maxi score, test score, baseline:  0.1261 1.0 1.0
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  32.28093623518765
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.199]
 [0.201]
 [0.222]
 [0.199]] [[33.074]
 [33.074]
 [40.403]
 [27.056]
 [33.074]] [[0.896]
 [0.896]
 [1.164]
 [0.702]
 [0.896]]
printing an ep nov before normalisation:  59.994521981290056
maxi score, test score, baseline:  0.1261 1.0 1.0
actions average: 
K:  1  action  0 :  tensor([    0.9990,     0.0001,     0.0000,     0.0001,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9521,     0.0004,     0.0033,     0.0441],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0012,     0.0002,     0.9506,     0.0165,     0.0315],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0005,     0.0004,     0.0359,     0.8319,     0.1313],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0009, 0.0955, 0.1200, 0.1080, 0.6757], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.584]
 [0.593]
 [0.659]
 [0.591]] [[39.613]
 [30.62 ]
 [39.613]
 [51.693]
 [41.885]] [[0.593]
 [0.584]
 [0.593]
 [0.659]
 [0.591]]
maxi score, test score, baseline:  0.1261 1.0 1.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.973]
 [0.992]
 [0.997]
 [0.995]
 [0.996]] [[ 6.74 ]
 [10.783]
 [10.941]
 [10.372]
 [11.014]] [[0.973]
 [0.992]
 [0.997]
 [0.995]
 [0.996]]
actor:  0 policy actor:  0  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.60125956185246
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1281 1.0 1.0
printing an ep nov before normalisation:  53.58113740088961
printing an ep nov before normalisation:  45.82155377376563
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.288]
 [0.269]
 [0.259]
 [0.593]] [[41.492]
 [39.7  ]
 [36.   ]
 [31.328]
 [35.43 ]] [[0.294]
 [0.288]
 [0.269]
 [0.259]
 [0.593]]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  76.11925087859372
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.386]
 [1.249]
 [1.249]
 [1.249]
 [1.249]] [[73.303]
 [76.521]
 [76.521]
 [76.521]
 [76.521]] [[1.698]
 [1.58 ]
 [1.58 ]
 [1.58 ]
 [1.58 ]]
printing an ep nov before normalisation:  65.79280556061576
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.29151549315941
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.27887535095215
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  59.43568997554489
maxi score, test score, baseline:  0.1301 1.0 1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.199588855573325
maxi score, test score, baseline:  0.1301 1.0 1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.424]
 [1.384]
 [1.424]
 [1.424]
 [1.424]] [[62.901]
 [67.344]
 [62.901]
 [62.901]
 [62.901]] [[1.703]
 [1.696]
 [1.703]
 [1.703]
 [1.703]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.05079582042202
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.81849848361543
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.92319065199035
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.934260366508745
Printing some Q and Qe and total Qs values:  [[1.038]
 [1.038]
 [1.038]
 [1.038]
 [1.038]] [[51.978]
 [51.978]
 [51.978]
 [51.978]
 [51.978]] [[2.371]
 [2.371]
 [2.371]
 [2.371]
 [2.371]]
printing an ep nov before normalisation:  70.55195154774546
line 256 mcts: sample exp_bonus 48.98186121387184
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.279]
 [1.273]
 [1.279]
 [1.279]
 [1.291]] [[48.986]
 [52.877]
 [48.986]
 [48.986]
 [53.987]] [[2.213]
 [2.327]
 [2.213]
 [2.213]
 [2.38 ]]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  66.3338680395827
maxi score, test score, baseline:  0.1301 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.68822066101091
Printing some Q and Qe and total Qs values:  [[1.222]
 [1.175]
 [1.175]
 [1.175]
 [1.175]] [[50.712]
 [44.109]
 [44.109]
 [44.109]
 [44.109]] [[2.169]
 [1.919]
 [1.919]
 [1.919]
 [1.919]]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.93230414
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  56.56774813819859
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 39.74789478429125
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.49043846130371
maxi score, test score, baseline:  0.1301 1.0 1.0
printing an ep nov before normalisation:  41.86984990498489
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.443]
 [0.422]
 [0.499]
 [0.427]] [[43.189]
 [47.325]
 [46.961]
 [39.988]
 [42.227]] [[1.562]
 [1.794]
 [1.755]
 [1.48 ]
 [1.521]]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  57.7238130569458
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  68.11754015875192
printing an ep nov before normalisation:  30.411348342895508
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]] [[20.088]
 [20.088]
 [20.088]
 [20.088]
 [20.088]] [[0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]]
printing an ep nov before normalisation:  64.20403338141729
printing an ep nov before normalisation:  49.80608522891998
maxi score, test score, baseline:  0.1301 1.0 1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.77210940264462
printing an ep nov before normalisation:  20.826084083980984
actions average: 
K:  3  action  0 :  tensor([    0.9886,     0.0005,     0.0001,     0.0005,     0.0103],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0018,     0.9521,     0.0001,     0.0001,     0.0459],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0153,     0.9188,     0.0162,     0.0494],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0026, 0.0066, 0.0318, 0.8766, 0.0823], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0014, 0.0505, 0.0629, 0.1338, 0.7514], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.139]
 [1.331]
 [1.139]
 [1.139]
 [1.139]] [[47.444]
 [53.896]
 [47.444]
 [47.444]
 [47.444]] [[2.44 ]
 [2.809]
 [2.44 ]
 [2.44 ]
 [2.44 ]]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  61.80412997466385
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.784]
 [0.825]
 [0.83 ]
 [0.816]] [[72.07 ]
 [71.263]
 [62.094]
 [65.316]
 [66.606]] [[2.116]
 [2.08 ]
 [1.874]
 [1.967]
 [1.986]]
printing an ep nov before normalisation:  75.91417661547273
maxi score, test score, baseline:  0.1321 1.0 1.0
line 256 mcts: sample exp_bonus 55.911080512927164
maxi score, test score, baseline:  0.1321 1.0 1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
printing an ep nov before normalisation:  56.58579775645546
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.82529028852568
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.534505256785145
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.97049397931747
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.06756365903386
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.91724391602303
printing an ep nov before normalisation:  36.36092206483897
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.332]
 [1.175]
 [1.241]
 [1.241]
 [1.243]] [[25.414]
 [26.934]
 [27.294]
 [27.294]
 [28.352]] [[2.505]
 [2.494]
 [2.595]
 [2.595]
 [2.698]]
printing an ep nov before normalisation:  41.889767085221635
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.94557762145996
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9668,     0.0002,     0.0000,     0.0031,     0.0299],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9281,     0.0186,     0.0004,     0.0527],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0078,     0.8355,     0.0486,     0.1080],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0003,     0.0408,     0.8410,     0.1177],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0011, 0.0864, 0.0918, 0.1567, 0.6640], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  116 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.376]
 [0.321]
 [0.46 ]
 [0.348]] [[34.216]
 [47.066]
 [32.407]
 [34.216]
 [39.699]] [[0.645]
 [0.688]
 [0.488]
 [0.645]
 [0.588]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.28436871123604
Printing some Q and Qe and total Qs values:  [[1.025]
 [0.927]
 [0.927]
 [0.927]
 [0.927]] [[57.917]
 [48.247]
 [48.247]
 [48.247]
 [48.247]] [[1.511]
 [1.298]
 [1.298]
 [1.298]
 [1.298]]
printing an ep nov before normalisation:  54.047859329955706
Printing some Q and Qe and total Qs values:  [[1.118]
 [0.047]
 [0.923]
 [0.99 ]
 [0.904]] [[41.677]
 [37.063]
 [47.223]
 [50.867]
 [37.943]] [[1.293]
 [0.185]
 [1.143]
 [1.24 ]
 [1.05 ]]
printing an ep nov before normalisation:  51.58112934153671
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.1321 1.0 1.0
line 256 mcts: sample exp_bonus 0.0388092481613711
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1321 1.0 1.0
printing an ep nov before normalisation:  47.11967310377812
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.469]
 [0.404]
 [0.229]
 [0.259]] [[40.551]
 [43.188]
 [40.551]
 [48.077]
 [38.94 ]] [[0.404]
 [0.469]
 [0.404]
 [0.229]
 [0.259]]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9868,     0.0080,     0.0038,     0.0001,     0.0012],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9301,     0.0016,     0.0002,     0.0681],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0002,     0.9090,     0.0444,     0.0463],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0004,     0.0135,     0.9446,     0.0415],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0055, 0.0042, 0.0774, 0.1514, 0.7615], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.081844182750594
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.986303210258484
maxi score, test score, baseline:  0.1321 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]] [[0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1321 1.0 1.0
printing an ep nov before normalisation:  59.343541840391126
printing an ep nov before normalisation:  62.53916666517131
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.9176567
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.313]
 [0.36 ]
 [0.313]
 [0.313]] [[21.954]
 [21.954]
 [21.528]
 [21.954]
 [21.954]] [[0.403]
 [0.403]
 [0.446]
 [0.403]
 [0.403]]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  51.40142571384194
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.39240125411378
printing an ep nov before normalisation:  33.80324951088884
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  110 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  16.410398151266005
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1321 1.0 1.0
printing an ep nov before normalisation:  48.2494911153439
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.78230454743109
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 42.635399174653294
UNIT TEST: sample policy line 217 mcts : [0.    0.077 0.769 0.077 0.077]
siam score:  -0.93054587
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
printing an ep nov before normalisation:  55.3506390348282
printing an ep nov before normalisation:  45.83737750534653
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.893]
 [0.905]
 [0.886]
 [0.979]
 [0.917]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.893]
 [0.905]
 [0.886]
 [0.979]
 [0.917]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.193]
 [1.21 ]
 [1.193]
 [1.024]
 [1.17 ]] [[40.96 ]
 [28.879]
 [40.96 ]
 [44.963]
 [45.553]] [[2.347]
 [1.89 ]
 [2.347]
 [2.334]
 [2.503]]
printing an ep nov before normalisation:  43.32984683072442
printing an ep nov before normalisation:  46.74556744146767
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.089]
 [1.049]
 [0.895]
 [1.019]
 [1.049]] [[56.47 ]
 [54.572]
 [54.689]
 [57.133]
 [54.572]] [[2.672]
 [2.555]
 [2.406]
 [2.629]
 [2.555]]
printing an ep nov before normalisation:  48.19304559823879
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.6  ]
 [0.604]
 [0.651]
 [0.604]] [[33.701]
 [39.428]
 [33.701]
 [44.172]
 [33.701]] [[0.604]
 [0.6  ]
 [0.604]
 [0.651]
 [0.604]]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9246123
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.796552658081055
printing an ep nov before normalisation:  63.82021356217139
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.92315507
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 49.43967183233843
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.03232507064780066
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  104 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  36.10991045390813
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.86907786608313
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.876]
 [0.864]
 [0.864]
 [0.864]
 [0.864]] [[67.438]
 [56.415]
 [56.415]
 [56.415]
 [56.415]] [[1.876]
 [1.654]
 [1.654]
 [1.654]
 [1.654]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.22501635072008
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0009462253649417107
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.291]
 [1.349]
 [1.291]
 [1.291]
 [1.189]] [[57.534]
 [59.212]
 [57.534]
 [57.534]
 [57.058]] [[2.13 ]
 [2.227]
 [2.13 ]
 [2.13 ]
 [2.017]]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.85059760999473
printing an ep nov before normalisation:  51.5179345800392
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.00393894779703
printing an ep nov before normalisation:  36.27663991886818
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.37732807868317
Printing some Q and Qe and total Qs values:  [[0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]] [[28.723]
 [28.723]
 [28.723]
 [28.723]
 [28.723]] [[1.478]
 [1.478]
 [1.478]
 [1.478]
 [1.478]]
printing an ep nov before normalisation:  35.39926477876833
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.221]
 [1.221]
 [1.221]
 [1.221]
 [1.221]] [[47.434]
 [47.434]
 [47.434]
 [47.434]
 [47.434]] [[32.859]
 [32.859]
 [32.859]
 [32.859]
 [32.859]]
printing an ep nov before normalisation:  56.85387060309797
printing an ep nov before normalisation:  45.99763972638413
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.92146456
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  38.190940028388745
printing an ep nov before normalisation:  39.07255911925454
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.747]
 [0.591]
 [0.591]
 [0.591]] [[43.647]
 [50.255]
 [43.647]
 [43.647]
 [43.647]] [[1.284]
 [1.639]
 [1.284]
 [1.284]
 [1.284]]
maxi score, test score, baseline:  0.1361 1.0 1.0
printing an ep nov before normalisation:  36.341300413662516
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.033]
 [0.033]
 [0.032]
 [0.023]
 [0.019]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  36.454689502716064
actor:  1 policy actor:  1  step number:  97 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.366074358326806
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.442]
 [1.317]
 [1.308]
 [1.308]
 [1.309]] [[37.504]
 [28.868]
 [25.362]
 [25.362]
 [28.971]] [[2.196]
 [1.814]
 [1.702]
 [1.702]
 [1.81 ]]
printing an ep nov before normalisation:  53.29104934417399
actor:  0 policy actor:  0  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  70.26930441886255
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[39.915]
 [39.915]
 [39.915]
 [39.915]
 [39.915]] [[0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1381 1.0 1.0
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.994721092247453
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.47287414456317
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1401 1.0 1.0
printing an ep nov before normalisation:  65.64233353441186
actions average: 
K:  0  action  0 :  tensor([    0.9777,     0.0015,     0.0000,     0.0089,     0.0119],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9781,     0.0095,     0.0017,     0.0107],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0007,     0.0001,     0.9108,     0.0347,     0.0536],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0003,     0.0002,     0.0343,     0.8700,     0.0952],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0015, 0.0009, 0.1573, 0.1400, 0.7004], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.031]
 [1.006]
 [1.001]
 [1.003]
 [1.005]] [[64.944]
 [67.358]
 [71.965]
 [73.184]
 [72.151]] [[2.104]
 [2.139]
 [2.248]
 [2.281]
 [2.257]]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  67.21014896267249
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.06869844206389
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.452777212348394
printing an ep nov before normalisation:  51.28061804487577
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.349]
 [0.392]
 [0.351]
 [0.353]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.353]
 [0.349]
 [0.392]
 [0.351]
 [0.353]]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.17860003580341
UNIT TEST: sample policy line 217 mcts : [0.179 0.128 0.051 0.231 0.41 ]
Printing some Q and Qe and total Qs values:  [[0.889]
 [0.889]
 [0.889]
 [0.935]
 [0.899]] [[54.16 ]
 [54.16 ]
 [54.16 ]
 [57.681]
 [59.542]] [[1.386]
 [1.386]
 [1.386]
 [1.482]
 [1.472]]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  62.528957219481306
printing an ep nov before normalisation:  52.55928099393044
actions average: 
K:  1  action  0 :  tensor([    0.9598,     0.0198,     0.0001,     0.0001,     0.0203],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9825,     0.0000,     0.0001,     0.0173],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0013, 0.0107, 0.8657, 0.0290, 0.0933], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0006,     0.0238,     0.9011,     0.0744],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0009, 0.0039, 0.1113, 0.1354, 0.7485], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  64.9288708538712
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.003]
 [0.492]
 [0.309]
 [0.274]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.204]
 [0.003]
 [0.492]
 [0.309]
 [0.274]]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.851]
 [0.851]
 [0.851]
 [0.89 ]
 [0.851]] [[39.399]
 [39.399]
 [39.399]
 [52.582]
 [39.399]] [[1.512]
 [1.512]
 [1.512]
 [1.887]
 [1.512]]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.05622386932373
siam score:  -0.92188776
printing an ep nov before normalisation:  59.94372674130415
Printing some Q and Qe and total Qs values:  [[1.38 ]
 [1.218]
 [1.218]
 [1.218]
 [1.218]] [[65.389]
 [65.846]
 [65.846]
 [65.846]
 [65.846]] [[2.013]
 [1.857]
 [1.857]
 [1.857]
 [1.857]]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.930397987365723
line 256 mcts: sample exp_bonus 34.387040445217266
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.288]
 [1.369]
 [1.212]
 [1.159]
 [1.248]] [[30.976]
 [26.734]
 [25.411]
 [29.479]
 [26.143]] [[1.763]
 [1.778]
 [1.602]
 [1.611]
 [1.649]]
printing an ep nov before normalisation:  0.07462319385354022
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.489]
 [0.714]
 [0.714]
 [0.785]] [[67.93 ]
 [68.731]
 [67.93 ]
 [67.93 ]
 [66.432]] [[2.461]
 [2.261]
 [2.461]
 [2.461]
 [2.485]]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 14.737117228826424
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.88372033495881
printing an ep nov before normalisation:  36.232404708862305
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1401 1.0 1.0
printing an ep nov before normalisation:  46.52186862835345
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.12049121645835
printing an ep nov before normalisation:  37.34490158005045
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1401 1.0 1.0
printing an ep nov before normalisation:  69.73629037061609
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.594]
 [0.441]
 [0.441]
 [0.441]] [[31.832]
 [28.069]
 [31.832]
 [31.832]
 [31.832]] [[1.457]
 [1.399]
 [1.457]
 [1.457]
 [1.457]]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.235]
 [1.207]
 [1.154]
 [1.073]
 [1.239]] [[40.339]
 [49.187]
 [42.399]
 [37.175]
 [48.858]] [[1.811]
 [2.168]
 [1.82 ]
 [1.511]
 [2.185]]
maxi score, test score, baseline:  0.1401 1.0 1.0
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.291258595943106
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.661]
 [0.019]
 [0.331]
 [0.419]] [[26.476]
 [28.099]
 [21.802]
 [23.459]
 [19.066]] [[0.69 ]
 [1.406]
 [0.501]
 [0.882]
 [0.786]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1401 1.0 1.0
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9997,     0.0002,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0012, 0.9558, 0.0113, 0.0013, 0.0304], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9514,     0.0300,     0.0185],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0028,     0.0002,     0.0126,     0.9034,     0.0810],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0155, 0.1056, 0.0897, 0.1230, 0.6662], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.003]
 [1.086]
 [1.003]
 [1.003]
 [1.003]] [[66.081]
 [63.586]
 [66.081]
 [66.081]
 [66.081]] [[2.519]
 [2.519]
 [2.519]
 [2.519]
 [2.519]]
printing an ep nov before normalisation:  59.136085510253906
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  113 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.773]
 [0.722]
 [0.773]
 [0.82 ]
 [0.773]] [[36.401]
 [43.743]
 [36.401]
 [30.441]
 [36.401]] [[0.951]
 [0.964]
 [0.951]
 [0.947]
 [0.951]]
Printing some Q and Qe and total Qs values:  [[0.938]
 [0.921]
 [0.921]
 [0.921]
 [0.921]] [[63.572]
 [64.939]
 [64.939]
 [64.939]
 [64.939]] [[2.232]
 [2.254]
 [2.254]
 [2.254]
 [2.254]]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.9109784
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  61.922503850294625
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1401 1.0 1.0
Starting evaluation
printing an ep nov before normalisation:  66.37118121288813
printing an ep nov before normalisation:  52.604385128451305
printing an ep nov before normalisation:  59.18752202178349
printing an ep nov before normalisation:  36.53610651834499
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.26 ]
 [0.314]
 [0.299]
 [0.557]] [[37.397]
 [40.401]
 [36.559]
 [27.267]
 [32.282]] [[0.228]
 [0.26 ]
 [0.314]
 [0.299]
 [0.557]]
printing an ep nov before normalisation:  48.4485854612458
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.73255634307861
siam score:  -0.9133748
printing an ep nov before normalisation:  41.25157789520291
printing an ep nov before normalisation:  50.46710809916927
printing an ep nov before normalisation:  26.825809478759766
printing an ep nov before normalisation:  65.96056481060798
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.855]
 [0.719]
 [0.719]
 [0.719]] [[34.953]
 [49.968]
 [34.953]
 [34.953]
 [34.953]] [[0.719]
 [0.855]
 [0.719]
 [0.719]
 [0.719]]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.803]
 [0.867]
 [0.803]
 [0.651]
 [0.762]] [[45.267]
 [53.334]
 [45.267]
 [32.988]
 [39.276]] [[0.803]
 [0.867]
 [0.803]
 [0.651]
 [0.762]]
printing an ep nov before normalisation:  53.63878255796523
printing an ep nov before normalisation:  31.627497042165395
printing an ep nov before normalisation:  0.05391539406218726
printing an ep nov before normalisation:  53.18665695066406
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  23.565806357844657
printing an ep nov before normalisation:  51.089682015411185
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  55.00189116426121
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.12818929805234802
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  69.76390337608407
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  39.74194687672026
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.91836196
printing an ep nov before normalisation:  54.21926501066344
printing an ep nov before normalisation:  71.4177400796814
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.383]
 [1.345]
 [1.307]
 [1.307]
 [1.243]] [[54.16 ]
 [54.594]
 [51.67 ]
 [51.67 ]
 [55.33 ]] [[1.668]
 [1.634]
 [1.57 ]
 [1.57 ]
 [1.538]]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.354]
 [0.496]
 [0.354]
 [0.354]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.354]
 [0.354]
 [0.496]
 [0.354]
 [0.354]]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.709192813987535
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.394]
 [0.381]
 [0.394]
 [0.394]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.394]
 [0.394]
 [0.381]
 [0.394]
 [0.394]]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.448]
 [0.469]
 [0.448]
 [0.448]] [[31.038]
 [31.038]
 [40.463]
 [31.038]
 [31.038]] [[1.155]
 [1.155]
 [1.582]
 [1.155]
 [1.155]]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  45.21468510059867
using explorer policy with actor:  1
siam score:  -0.90473574
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.20116096915673
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.10920875859029
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.19836233470917
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.601]
 [0.49 ]
 [0.49 ]
 [0.49 ]] [[36.707]
 [44.053]
 [36.707]
 [36.707]
 [36.707]] [[1.321]
 [1.771]
 [1.321]
 [1.321]
 [1.321]]
printing an ep nov before normalisation:  55.24792095514425
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.29646110555191
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.57112915477948
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.01373070624539
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.72 ]
 [0.756]
 [0.797]
 [0.762]] [[34.186]
 [34.106]
 [29.015]
 [27.643]
 [32.637]] [[1.517]
 [1.55 ]
 [1.335]
 [1.308]
 [1.519]]
siam score:  -0.9041576
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.417]
 [1.366]
 [1.242]
 [1.242]
 [1.343]] [[28.57 ]
 [30.818]
 [31.829]
 [31.829]
 [29.962]] [[2.498]
 [2.533]
 [2.447]
 [2.447]
 [2.477]]
printing an ep nov before normalisation:  27.13181000652475
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
printing an ep nov before normalisation:  60.6964237166456
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.647346045526916
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.512637754135255
Printing some Q and Qe and total Qs values:  [[1.246]
 [1.246]
 [1.246]
 [1.246]
 [1.246]] [[71.777]
 [71.777]
 [71.777]
 [71.777]
 [71.777]] [[1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.576]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  57.496490778480236
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.967]
 [0.921]
 [0.921]
 [0.921]
 [0.921]] [[70.33 ]
 [73.513]
 [73.513]
 [73.513]
 [73.513]] [[1.708]
 [1.721]
 [1.721]
 [1.721]
 [1.721]]
siam score:  -0.9139658
printing an ep nov before normalisation:  76.85495107111609
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.995]
 [1.004]
 [1.004]
 [0.974]
 [1.004]] [[52.049]
 [34.003]
 [34.003]
 [38.462]
 [34.003]] [[2.32 ]
 [1.639]
 [1.639]
 [1.779]
 [1.639]]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  108 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.74457853815173
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.19270403202451
printing an ep nov before normalisation:  21.881098747253418
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.40427815059294
printing an ep nov before normalisation:  36.53889507712564
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.89690736056979
Printing some Q and Qe and total Qs values:  [[1.33 ]
 [1.381]
 [1.424]
 [1.225]
 [1.386]] [[68.498]
 [70.788]
 [75.435]
 [69.573]
 [72.234]] [[2.197]
 [2.292]
 [2.424]
 [2.113]
 [2.325]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  49.58607245582187
siam score:  -0.90749085
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.28854944640104
printing an ep nov before normalisation:  34.51485872268677
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
actor:  0 policy actor:  0  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.1626557371589854
Printing some Q and Qe and total Qs values:  [[1.111]
 [1.035]
 [1.111]
 [1.111]
 [1.111]] [[42.877]
 [46.122]
 [42.877]
 [42.877]
 [42.877]] [[2.124]
 [2.169]
 [2.124]
 [2.124]
 [2.124]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 38.683528894038446
maxi score, test score, baseline:  0.1861 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.65025127479776
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.34400036460003
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.29 ]
 [0.387]
 [0.267]
 [0.261]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.24 ]
 [0.29 ]
 [0.387]
 [0.267]
 [0.261]]
maxi score, test score, baseline:  0.1861 1.0 1.0
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.744]
 [0.765]
 [0.765]
 [0.765]] [[64.607]
 [60.05 ]
 [50.165]
 [50.165]
 [50.165]] [[2.006]
 [1.978]
 [1.662]
 [1.662]
 [1.662]]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.717]
 [0.54 ]
 [0.449]
 [0.612]] [[36.711]
 [43.3  ]
 [36.711]
 [30.601]
 [42.219]] [[0.699]
 [0.923]
 [0.699]
 [0.565]
 [0.81 ]]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.293697816734948
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  54.65348525778765
printing an ep nov before normalisation:  52.64553541683
actions average: 
K:  3  action  0 :  tensor([    0.9965,     0.0001,     0.0000,     0.0015,     0.0019],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9805,     0.0001,     0.0001,     0.0192],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0019,     0.0002,     0.9270,     0.0333,     0.0375],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0010, 0.0012, 0.0009, 0.8661, 0.1307], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0009, 0.0868, 0.0532, 0.0835, 0.7756], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.75976357459951
printing an ep nov before normalisation:  50.29101276997961
printing an ep nov before normalisation:  0.24831816632286063
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.721005380918275
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1861 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.39862155063935
maxi score, test score, baseline:  0.1861 1.0 1.0
line 256 mcts: sample exp_bonus 45.92713038896771
printing an ep nov before normalisation:  61.23512506559301
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1861 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.106]
 [1.106]
 [1.106]
 [1.106]
 [1.106]] [[47.19]
 [47.19]
 [47.19]
 [47.19]
 [47.19]] [[1.785]
 [1.785]
 [1.785]
 [1.785]
 [1.785]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.67284154891968
printing an ep nov before normalisation:  46.34150901615448
printing an ep nov before normalisation:  72.71473181337237
Printing some Q and Qe and total Qs values:  [[1.084]
 [0.925]
 [0.925]
 [0.925]
 [0.925]] [[50.914]
 [38.765]
 [38.765]
 [38.765]
 [38.765]] [[1.863]
 [1.413]
 [1.413]
 [1.413]
 [1.413]]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.026 0.641 0.    0.051 0.282]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.64034281609497
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.042071926576945
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.75533833236908
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  22.560362021128338
Printing some Q and Qe and total Qs values:  [[0.989]
 [0.989]
 [0.989]
 [0.989]
 [0.989]] [[76.484]
 [76.484]
 [76.484]
 [76.484]
 [76.484]] [[1.989]
 [1.989]
 [1.989]
 [1.989]
 [1.989]]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.759]
 [0.759]
 [0.759]
 [0.759]] [[34.91 ]
 [32.088]
 [32.088]
 [32.088]
 [32.088]] [[0.971]
 [0.885]
 [0.885]
 [0.885]
 [0.885]]
printing an ep nov before normalisation:  38.60241312003965
printing an ep nov before normalisation:  48.478143057091266
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9849,     0.0005,     0.0141,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9378,     0.0098,     0.0002,     0.0522],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0161,     0.9650,     0.0005,     0.0182],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0005,     0.0762,     0.7638,     0.1592],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0014, 0.0459, 0.0873, 0.1445, 0.7209], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  51.43258579895197
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  40.757100888832774
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.33492270669422
printing an ep nov before normalisation:  24.87495565948615
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.12372531760511
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9955,     0.0001,     0.0001,     0.0001,     0.0043],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.8952,     0.0002,     0.0204,     0.0836],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0020,     0.0004,     0.9632,     0.0149,     0.0196],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0002,     0.0361,     0.8634,     0.1001],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0046, 0.0672, 0.0771, 0.1076, 0.7436], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.235553398451856
printing an ep nov before normalisation:  50.963223350147615
printing an ep nov before normalisation:  36.104360789368485
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.40942263250393
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.986]
 [0.798]
 [0.85 ]
 [0.855]] [[39.422]
 [37.956]
 [35.546]
 [39.06 ]
 [39.045]] [[1.953]
 [2.162]
 [1.825]
 [2.094]
 [2.099]]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.484725364557185
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.94 ]
 [0.711]
 [0.834]
 [0.638]] [[39.476]
 [51.721]
 [37.404]
 [38.049]
 [39.283]] [[0.659]
 [0.94 ]
 [0.711]
 [0.834]
 [0.638]]
actor:  0 policy actor:  0  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  35.801629804339896
printing an ep nov before normalisation:  30.595097147947634
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.709]
 [0.771]
 [0.738]
 [0.733]] [[30.099]
 [30.099]
 [41.01 ]
 [47.225]
 [46.099]] [[0.932]
 [0.932]
 [1.319]
 [1.471]
 [1.433]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.758]
 [0.765]
 [0.739]
 [0.737]] [[38.235]
 [37.453]
 [40.389]
 [42.484]
 [36.976]] [[0.52 ]
 [1.244]
 [1.322]
 [1.346]
 [1.211]]
printing an ep nov before normalisation:  44.30206918095096
printing an ep nov before normalisation:  41.1590766042103
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.638]
 [0.765]
 [0.765]
 [0.768]] [[37.16 ]
 [47.553]
 [37.16 ]
 [37.16 ]
 [43.646]] [[1.642]
 [1.914]
 [1.642]
 [1.642]
 [1.894]]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.50135811477835
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.048823853047544
siam score:  -0.89498055
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8963474
printing an ep nov before normalisation:  47.11848252254709
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1881 1.0 1.0
printing an ep nov before normalisation:  52.998349777747094
Printing some Q and Qe and total Qs values:  [[1.288]
 [1.288]
 [1.288]
 [1.288]
 [1.288]] [[37.456]
 [37.456]
 [37.456]
 [37.456]
 [37.456]] [[26.271]
 [26.271]
 [26.271]
 [26.271]
 [26.271]]
printing an ep nov before normalisation:  39.68389307367316
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  48.53314161543802
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  47.8390553788127
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  59.90341974870995
printing an ep nov before normalisation:  37.92056534972777
maxi score, test score, baseline:  0.1881 1.0 1.0
line 256 mcts: sample exp_bonus 38.5475666775588
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.12776456366863
actions average: 
K:  4  action  0 :  tensor([    0.9095,     0.0033,     0.0002,     0.0461,     0.0409],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0043,     0.9159,     0.0169,     0.0002,     0.0627],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0001,     0.9308,     0.0208,     0.0481],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0221,     0.0005,     0.0486,     0.8989,     0.0299],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0018, 0.0519, 0.1212, 0.1960, 0.6291], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.027]
 [1.027]
 [1.027]
 [1.027]
 [1.027]] [[60.461]
 [60.461]
 [60.461]
 [60.461]
 [60.461]] [[2.36]
 [2.36]
 [2.36]
 [2.36]
 [2.36]]
printing an ep nov before normalisation:  54.74650504956829
siam score:  -0.8991115
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  24.782362827521222
printing an ep nov before normalisation:  69.87557429697036
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.76697789542176
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  61.43603971737173
printing an ep nov before normalisation:  49.394402503967285
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1881 1.0 1.0
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.485725909720955
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1881 1.0 1.0
siam score:  -0.89978164
maxi score, test score, baseline:  0.1881 1.0 1.0
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1881 1.0 1.0
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.96888903897656
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1881 1.0 1.0
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.23769568324282
maxi score, test score, baseline:  0.1881 1.0 1.0
printing an ep nov before normalisation:  43.042359352111816
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  60.433833847879285
line 256 mcts: sample exp_bonus 56.769704519198044
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.914]
 [1.162]
 [0.914]
 [0.914]] [[32.715]
 [32.715]
 [37.336]
 [32.715]
 [32.715]] [[2.064]
 [2.064]
 [2.543]
 [2.064]
 [2.064]]
printing an ep nov before normalisation:  66.55022879148216
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.94329201560646
printing an ep nov before normalisation:  42.147972000326256
actions average: 
K:  1  action  0 :  tensor([    0.9662,     0.0116,     0.0001,     0.0033,     0.0189],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0011,     0.9516,     0.0007,     0.0026,     0.0441],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0008,     0.0001,     0.9080,     0.0363,     0.0548],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0060, 0.0117, 0.0016, 0.9528, 0.0279], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0021, 0.0303, 0.0672, 0.0684, 0.8320], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  59.0231300630863
UNIT TEST: sample policy line 217 mcts : [0.385 0.487 0.    0.026 0.103]
printing an ep nov before normalisation:  57.033981068785025
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.14877810294141
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.73657851481392
printing an ep nov before normalisation:  30.71057162257818
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.35018630039564
printing an ep nov before normalisation:  47.812076415040806
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.168427961643815
printing an ep nov before normalisation:  0.0024972583344151644
printing an ep nov before normalisation:  61.25722087579718
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [1.289]
 [1.172]
 [0.936]
 [1.112]] [[38.273]
 [45.309]
 [39.833]
 [40.485]
 [38.543]] [[0.784]
 [2.306]
 [1.984]
 [1.773]
 [1.876]]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.584310177954215
printing an ep nov before normalisation:  72.75350092965863
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.722]
 [0.597]
 [0.597]
 [0.597]] [[51.668]
 [53.968]
 [51.668]
 [51.668]
 [51.668]] [[1.866]
 [2.094]
 [1.866]
 [1.866]
 [1.866]]
Printing some Q and Qe and total Qs values:  [[0.88 ]
 [0.88 ]
 [0.842]
 [0.928]
 [0.88 ]] [[41.979]
 [41.979]
 [39.603]
 [45.723]
 [41.979]] [[1.606]
 [1.606]
 [1.488]
 [1.782]
 [1.606]]
printing an ep nov before normalisation:  67.13477449933029
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  26.413404941558838
printing an ep nov before normalisation:  35.326109121376525
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.285]
 [0.324]
 [0.265]
 [0.261]] [[26.038]
 [28.768]
 [32.908]
 [26.035]
 [26.677]] [[0.226]
 [0.285]
 [0.324]
 [0.265]
 [0.261]]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.018604871297825
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  59.32475731438913
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.4181782959916
printing an ep nov before normalisation:  65.98359157374084
siam score:  -0.9060591
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.753]
 [0.672]
 [0.672]
 [0.672]] [[39.462]
 [40.87 ]
 [38.803]
 [38.803]
 [38.803]] [[0.916]
 [1.167]
 [1.048]
 [1.048]
 [1.048]]
printing an ep nov before normalisation:  46.77386552117207
printing an ep nov before normalisation:  36.30507854172336
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.29014573848839
printing an ep nov before normalisation:  28.965882249269015
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  112 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.788774546905312
Printing some Q and Qe and total Qs values:  [[0.949]
 [0.942]
 [0.942]
 [0.942]
 [0.942]] [[46.114]
 [47.11 ]
 [47.11 ]
 [47.11 ]
 [47.11 ]] [[1.149]
 [1.148]
 [1.148]
 [1.148]
 [1.148]]
printing an ep nov before normalisation:  47.490863105824815
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1921 1.0 1.0
printing an ep nov before normalisation:  40.59792196081398
siam score:  -0.9093454
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.196]
 [1.232]
 [1.196]
 [1.196]
 [1.051]] [[31.487]
 [46.076]
 [31.487]
 [31.487]
 [32.067]] [[1.424]
 [1.661]
 [1.424]
 [1.424]
 [1.288]]
printing an ep nov before normalisation:  49.565468712004424
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.9153176370472
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1921 1.0 1.0
printing an ep nov before normalisation:  50.21229867721962
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.238436102905766
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.22683048044763
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.556]
 [0.54 ]
 [0.627]
 [0.544]] [[27.608]
 [28.537]
 [38.249]
 [26.348]
 [27.338]] [[1.72 ]
 [1.784]
 [2.54 ]
 [1.68 ]
 [1.676]]
printing an ep nov before normalisation:  54.61029324405142
printing an ep nov before normalisation:  43.98901219962265
printing an ep nov before normalisation:  29.53806816957084
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  60.490796956414925
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.149087218435202
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  26.196842586745596
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.008058036413558511
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.145587493298414
printing an ep nov before normalisation:  51.5115573331131
printing an ep nov before normalisation:  49.531564414636044
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.67861946311623
printing an ep nov before normalisation:  55.82786891444999
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.077 0.077 0.128 0.692 0.026]
maxi score, test score, baseline:  0.1921 1.0 1.0
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.630697808315404
printing an ep nov before normalisation:  27.97481765208625
actions average: 
K:  0  action  0 :  tensor([    0.9993,     0.0001,     0.0000,     0.0001,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9884,     0.0000,     0.0002,     0.0113],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0033,     0.0002,     0.8179,     0.0475,     0.1310],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0004,     0.0006,     0.0316,     0.8379,     0.1296],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0023, 0.1069, 0.0908, 0.1132, 0.6868], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 40.85517726062958
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.662614979039404
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.542994525616066
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]]
printing an ep nov before normalisation:  66.98689566654109
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.636]
 [0.577]
 [0.577]
 [0.577]] [[42.945]
 [55.209]
 [42.945]
 [42.945]
 [42.945]] [[1.304]
 [1.813]
 [1.304]
 [1.304]
 [1.304]]
maxi score, test score, baseline:  0.1941 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.    0.282 0.41  0.103 0.205]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  56.67586520309691
Printing some Q and Qe and total Qs values:  [[0.954]
 [0.837]
 [0.894]
 [0.863]
 [0.871]] [[48.968]
 [33.875]
 [43.912]
 [40.226]
 [39.389]] [[2.1  ]
 [1.457]
 [1.864]
 [1.704]
 [1.683]]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.825]
 [0.9  ]
 [0.963]
 [0.899]] [[33.886]
 [26.934]
 [33.692]
 [39.037]
 [37.207]] [[0.708]
 [1.212]
 [1.554]
 [1.828]
 [1.691]]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  53.659712693368746
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.29148372763765
maxi score, test score, baseline:  0.1941 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.534]
 [0.487]
 [0.444]
 [0.478]] [[46.992]
 [41.121]
 [45.727]
 [44.776]
 [45.997]] [[1.147]
 [0.941]
 [0.981]
 [0.919]
 [0.977]]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.772]
 [0.715]
 [0.715]
 [0.715]] [[50.761]
 [53.7  ]
 [50.761]
 [50.761]
 [50.761]] [[1.284]
 [1.402]
 [1.284]
 [1.284]
 [1.284]]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9022832
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  41.16126017294038
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1941 1.0 1.0
printing an ep nov before normalisation:  37.25122928619385
printing an ep nov before normalisation:  54.30417279688668
maxi score, test score, baseline:  0.1941 1.0 1.0
printing an ep nov before normalisation:  51.94226264552278
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.59 ]
 [0.59 ]
 [0.591]
 [0.59 ]] [[50.875]
 [50.875]
 [50.875]
 [47.347]
 [50.875]] [[1.585]
 [1.585]
 [1.585]
 [1.491]
 [1.585]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.311936241206176
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.157523499020094
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.603]
 [0.583]
 [0.583]
 [0.583]] [[45.78 ]
 [43.153]
 [45.78 ]
 [45.78 ]
 [45.78 ]] [[2.469]
 [2.331]
 [2.469]
 [2.469]
 [2.469]]
printing an ep nov before normalisation:  30.641605852574326
printing an ep nov before normalisation:  42.19788770622765
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.991]
 [0.96 ]
 [0.96 ]
 [0.96 ]
 [0.96 ]] [[47.763]
 [46.319]
 [46.319]
 [46.319]
 [46.319]] [[2.575]
 [2.458]
 [2.458]
 [2.458]
 [2.458]]
actions average: 
K:  3  action  0 :  tensor([    0.9980,     0.0006,     0.0001,     0.0001,     0.0010],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9593,     0.0001,     0.0003,     0.0401],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0013,     0.9481,     0.0248,     0.0257],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0004,     0.0006,     0.8785,     0.1204],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0112, 0.0557, 0.0344, 0.1168, 0.7819], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.1531602764871
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.495]
 [0.508]
 [0.514]
 [0.499]] [[27.401]
 [20.373]
 [31.755]
 [20.837]
 [21.497]] [[0.462]
 [0.495]
 [0.508]
 [0.514]
 [0.499]]
printing an ep nov before normalisation:  34.54547166824341
actor:  0 policy actor:  0  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.41753708822879
printing an ep nov before normalisation:  36.93688953835026
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.30362183841262
printing an ep nov before normalisation:  45.82736736239232
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.15808409993261
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.78 ]
 [0.643]
 [0.643]
 [0.643]] [[ 0.   ]
 [37.757]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.173]
 [1.548]
 [0.173]
 [0.173]
 [0.173]]
actor:  0 policy actor:  0  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.82 ]] [[53.58]
 [53.58]
 [53.58]
 [53.58]
 [48.26]] [[2.074]
 [2.074]
 [2.074]
 [2.074]
 [1.924]]
printing an ep nov before normalisation:  76.02287092889931
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1981 1.0 1.0
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.484]] [[43.067]
 [43.067]
 [43.067]
 [43.067]
 [43.067]] [[3.298]
 [3.298]
 [3.298]
 [3.298]
 [3.298]]
maxi score, test score, baseline:  0.1981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.90586045123639
printing an ep nov before normalisation:  45.1411484622513
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  33.74292850494385
actor:  1 policy actor:  1  step number:  108 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 42.15739431256217
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.90093946
printing an ep nov before normalisation:  33.274644116906316
maxi score, test score, baseline:  0.1981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.27799721978437
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  65.10432951101383
actor:  0 policy actor:  0  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  57.21181424626399
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.029]
 [0.996]
 [0.996]
 [0.996]
 [0.993]] [[60.127]
 [65.723]
 [65.723]
 [65.723]
 [69.013]] [[2.407]
 [2.556]
 [2.556]
 [2.556]
 [2.66 ]]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
printing an ep nov before normalisation:  50.369669615410885
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.233748609092586
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.81064713952978
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.38128185272217
Printing some Q and Qe and total Qs values:  [[1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]] [[55.54 ]
 [43.531]
 [43.531]
 [43.531]
 [43.531]] [[1.605]
 [1.415]
 [1.415]
 [1.415]
 [1.415]]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.80158805847168
line 256 mcts: sample exp_bonus 55.01461174260645
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.60473210767848
printing an ep nov before normalisation:  56.32805605312704
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.86648790262366
printing an ep nov before normalisation:  57.33329776527568
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.31027366676926
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.90048254
printing an ep nov before normalisation:  64.46143044789511
printing an ep nov before normalisation:  60.00569746355215
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]]
printing an ep nov before normalisation:  47.956397784981654
printing an ep nov before normalisation:  40.82103639433273
printing an ep nov before normalisation:  44.098078508427236
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[1.292]
 [1.292]
 [1.292]
 [1.292]
 [1.292]] [[58.898]
 [58.898]
 [58.898]
 [58.898]
 [58.898]] [[2.625]
 [2.625]
 [2.625]
 [2.625]
 [2.625]]
printing an ep nov before normalisation:  33.47825384309255
printing an ep nov before normalisation:  41.61161982938349
printing an ep nov before normalisation:  62.88474236095881
printing an ep nov before normalisation:  55.99962080505364
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.543953955009094
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]] [[52.982]
 [52.982]
 [52.982]
 [52.982]
 [52.982]] [[1.789]
 [1.789]
 [1.789]
 [1.789]
 [1.789]]
printing an ep nov before normalisation:  25.051447578853278
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.69357736608035
printing an ep nov before normalisation:  42.958574295043945
printing an ep nov before normalisation:  50.29322055773447
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.289330005645752
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
printing an ep nov before normalisation:  33.96583320987703
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.420936584472656
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.12 ]
 [1.052]
 [1.052]
 [1.052]
 [1.052]] [[54.714]
 [47.992]
 [47.992]
 [47.992]
 [47.992]] [[2.367]
 [2.047]
 [2.047]
 [2.047]
 [2.047]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.35617406377994
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.489358491687995
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([    0.9446,     0.0006,     0.0002,     0.0217,     0.0329],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9436,     0.0003,     0.0004,     0.0553],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0052,     0.0003,     0.8160,     0.0723,     0.1062],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0007,     0.0005,     0.0029,     0.8399,     0.1560],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0014, 0.1013, 0.0722, 0.0699, 0.7551], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
printing an ep nov before normalisation:  57.95436993529542
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
printing an ep nov before normalisation:  47.28495585394083
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.868]
 [0.829]
 [0.868]
 [0.932]
 [0.868]] [[27.561]
 [35.085]
 [27.561]
 [23.373]
 [27.561]] [[1.063]
 [1.113]
 [1.063]
 [1.078]
 [1.063]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.1211674913641
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.517986201711125
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9979,     0.0001,     0.0000,     0.0004,     0.0016],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9757,     0.0002,     0.0002,     0.0238],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0139,     0.8873,     0.0290,     0.0694],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0002,     0.0200,     0.8415,     0.1381],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0008, 0.1019, 0.0442, 0.0828, 0.7703], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.31014316080653
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.90293944
Printing some Q and Qe and total Qs values:  [[0.884]
 [0.904]
 [0.884]
 [0.884]
 [0.817]] [[40.3  ]
 [49.169]
 [40.3  ]
 [40.3  ]
 [40.011]] [[0.884]
 [0.904]
 [0.884]
 [0.884]
 [0.817]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  32.59430010057727
actor:  0 policy actor:  0  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  60.42692238825436
siam score:  -0.90138376
printing an ep nov before normalisation:  48.02614361857954
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2481 1.0 1.0
printing an ep nov before normalisation:  67.30599298345868
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 50.432424543689336
printing an ep nov before normalisation:  49.822875527090346
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.16338065622541
Printing some Q and Qe and total Qs values:  [[1.241]
 [1.396]
 [1.241]
 [1.241]
 [1.241]] [[37.653]
 [37.644]
 [37.653]
 [37.653]
 [37.653]] [[2.322]
 [2.476]
 [2.322]
 [2.322]
 [2.322]]
line 256 mcts: sample exp_bonus 30.468837626794055
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.04 ]
 [1.242]
 [1.04 ]
 [0.927]
 [1.154]] [[37.446]
 [29.137]
 [37.446]
 [43.259]
 [34.863]] [[1.869]
 [1.698]
 [1.869]
 [2.016]
 [1.867]]
printing an ep nov before normalisation:  42.50999029550311
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.274134857000654
printing an ep nov before normalisation:  55.544100439343865
printing an ep nov before normalisation:  38.850683535484265
actions average: 
K:  1  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9467,     0.0058,     0.0062,     0.0408],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0005,     0.0001,     0.9640,     0.0183,     0.0172],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0006,     0.0004,     0.0193,     0.9057,     0.0741],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0143, 0.0802, 0.0218, 0.0409, 0.8428], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2501 1.0 1.0
printing an ep nov before normalisation:  32.78420686721802
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  32.46947526931763
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.045]
 [1.196]
 [0.997]
 [0.997]
 [1.01 ]] [[54.24 ]
 [41.715]
 [46.256]
 [46.256]
 [44.876]] [[2.606]
 [2.151]
 [2.171]
 [2.171]
 [2.117]]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.3521307491997
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.132386284311565
printing an ep nov before normalisation:  37.89165623123714
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9762,     0.0071,     0.0007,     0.0015,     0.0145],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9773,     0.0006,     0.0002,     0.0218],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0006,     0.0001,     0.9005,     0.0460,     0.0528],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0003,     0.0009,     0.0440,     0.8622,     0.0925],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0119, 0.0759, 0.0721, 0.0680, 0.7721], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2501 1.0 1.0
printing an ep nov before normalisation:  29.69421997945991
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.89146066
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[55.788]
 [55.788]
 [55.788]
 [55.788]
 [55.788]] [[1.94]
 [1.94]
 [1.94]
 [1.94]
 [1.94]]
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.906]
 [0.906]
 [0.892]
 [0.906]] [[56.251]
 [56.251]
 [56.251]
 [61.83 ]
 [56.251]] [[2.042]
 [2.042]
 [2.042]
 [2.225]
 [2.042]]
siam score:  -0.89122754
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.938211116196435
maxi score, test score, baseline:  0.2501 1.0 1.0
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.12973299953585
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2501 1.0 1.0
line 256 mcts: sample exp_bonus 59.787389638822695
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
printing an ep nov before normalisation:  45.96276066794843
Printing some Q and Qe and total Qs values:  [[1.023]
 [0.938]
 [0.938]
 [0.938]
 [0.913]] [[54.254]
 [48.965]
 [48.965]
 [48.965]
 [44.433]] [[2.443]
 [2.131]
 [2.131]
 [2.131]
 [1.912]]
printing an ep nov before normalisation:  59.03451794743656
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
printing an ep nov before normalisation:  41.38319158824594
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.813]
 [0.813]
 [0.892]
 [0.813]] [[41.095]
 [41.095]
 [41.095]
 [47.295]
 [41.095]] [[1.873]
 [1.873]
 [1.873]
 [2.182]
 [1.873]]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.34861723506254
printing an ep nov before normalisation:  39.7436599252157
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  39.8623413089375
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.075410871653432
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  27.248083994620593
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  51.02136749196926
printing an ep nov before normalisation:  49.10277517594701
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.890122
printing an ep nov before normalisation:  20.78742027282715
printing an ep nov before normalisation:  53.83217010855581
printing an ep nov before normalisation:  50.23939441545697
printing an ep nov before normalisation:  29.40472102869784
printing an ep nov before normalisation:  23.411227038746652
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.8100394538097
actor:  0 policy actor:  0  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.3223315566323
actor:  0 policy actor:  0  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2541 1.0 1.0
printing an ep nov before normalisation:  30.749149322509766
printing an ep nov before normalisation:  36.96215672798733
siam score:  -0.8856931
printing an ep nov before normalisation:  27.999853122537704
printing an ep nov before normalisation:  52.68230740163113
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.604535367075908
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.75 ]] [[57.386]
 [57.386]
 [57.386]
 [57.386]
 [57.099]] [[2.002]
 [2.002]
 [2.002]
 [2.002]
 [1.99 ]]
printing an ep nov before normalisation:  34.24926229281362
printing an ep nov before normalisation:  32.443482875823975
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.699]
 [0.639]
 [0.639]
 [0.626]] [[66.373]
 [62.942]
 [66.373]
 [66.373]
 [67.816]] [[1.879]
 [1.847]
 [1.879]
 [1.879]
 [1.905]]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 64.51290735615937
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  55.20158301637854
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.54889532895135
printing an ep nov before normalisation:  42.06918301554502
siam score:  -0.87042147
maxi score, test score, baseline:  0.2541 1.0 1.0
printing an ep nov before normalisation:  56.01922259707094
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  42.80596566742329
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.711]
 [0.562]
 [0.562]
 [0.562]] [[36.711]
 [50.156]
 [36.711]
 [36.711]
 [36.711]] [[0.942]
 [1.329]
 [0.942]
 [0.942]
 [0.942]]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.943]
 [0.85 ]
 [0.088]
 [0.822]
 [0.795]] [[44.322]
 [43.751]
 [36.627]
 [37.484]
 [39.021]] [[0.943]
 [0.85 ]
 [0.088]
 [0.822]
 [0.795]]
actor:  0 policy actor:  0  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.543795396352095
printing an ep nov before normalisation:  36.39254770352882
printing an ep nov before normalisation:  62.11716649315068
printing an ep nov before normalisation:  37.6257855480823
printing an ep nov before normalisation:  50.8230866240536
printing an ep nov before normalisation:  51.7675676255104
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.2293768013412
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  28.256050069260176
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.82391357421875
UNIT TEST: sample policy line 217 mcts : [0.026 0.897 0.026 0.026 0.026]
siam score:  -0.8827635
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.7625791769816
printing an ep nov before normalisation:  45.48412210945253
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8855371
printing an ep nov before normalisation:  27.26363182067871
printing an ep nov before normalisation:  0.25696917504063777
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.799]
 [0.81 ]
 [0.868]
 [0.848]] [[36.624]
 [34.393]
 [41.047]
 [44.048]
 [42.134]] [[1.319]
 [1.231]
 [1.457]
 [1.612]
 [1.53 ]]
printing an ep nov before normalisation:  45.1133247442567
printing an ep nov before normalisation:  35.911192893981934
printing an ep nov before normalisation:  41.70096691349743
printing an ep nov before normalisation:  49.84154407449828
maxi score, test score, baseline:  0.2561 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.2561 1.0 1.0
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.78381830631813
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.58856380377448
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8928339
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9995,     0.0000,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9506,     0.0077,     0.0003,     0.0412],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0006,     0.0138,     0.8950,     0.0281,     0.0625],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0006,     0.0005,     0.8871,     0.1116],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0005,     0.1259,     0.0045,     0.0430,     0.8262],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.91458693963499
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.77763361850234
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.97641721416082
siam score:  -0.8856157
printing an ep nov before normalisation:  46.4624980814748
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.077 0.179 0.231 0.385 0.128]
printing an ep nov before normalisation:  29.18027400970459
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.098564210936686
printing an ep nov before normalisation:  36.53966349740985
printing an ep nov before normalisation:  54.95365221275732
maxi score, test score, baseline:  0.2581 1.0 1.0
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  43.55263834440748
maxi score, test score, baseline:  0.2581 1.0 1.0
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.896]
 [0.117]
 [0.974]
 [0.896]] [[35.056]
 [31.926]
 [35.204]
 [40.575]
 [31.926]] [[0.456]
 [1.234]
 [0.539]
 [1.534]
 [1.234]]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.938059996921496
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  60.150166396932974
Printing some Q and Qe and total Qs values:  [[0.803]
 [0.793]
 [0.803]
 [0.803]
 [0.803]] [[43.596]
 [53.328]
 [43.596]
 [43.596]
 [43.596]] [[1.702]
 [2.042]
 [1.702]
 [1.702]
 [1.702]]
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.563]
 [0.681]
 [0.681]
 [0.681]] [[58.732]
 [55.844]
 [55.761]
 [55.761]
 [55.761]] [[1.948]
 [1.8  ]
 [1.915]
 [1.915]
 [1.915]]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.907142639160156
Printing some Q and Qe and total Qs values:  [[1.264]
 [1.249]
 [1.207]
 [1.146]
 [1.22 ]] [[32.375]
 [39.933]
 [38.938]
 [42.228]
 [37.327]] [[1.747]
 [2.034]
 [1.952]
 [2.022]
 [1.9  ]]
maxi score, test score, baseline:  0.2581 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.745]
 [0.553]
 [0.553]
 [0.524]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.553]
 [0.745]
 [0.553]
 [0.553]
 [0.524]]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2581 1.0 1.0
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
printing an ep nov before normalisation:  48.30777995007277
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9636,     0.0001,     0.0002,     0.0359],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0002,     0.9920,     0.0044,     0.0033],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0007,     0.0005,     0.0003,     0.9296,     0.0689],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0014, 0.0736, 0.0233, 0.0880, 0.8136], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  30.147458742985894
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.85905170440674
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.489]
 [0.499]
 [0.502]
 [0.5  ]] [[61.038]
 [79.223]
 [77.255]
 [85.383]
 [75.783]] [[0.987]
 [1.073]
 [1.066]
 [1.138]
 [1.055]]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.83347384174545
printing an ep nov before normalisation:  52.07203040112051
maxi score, test score, baseline:  0.2581 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.841]
 [0.841]
 [0.881]
 [0.875]
 [0.841]] [[63.342]
 [63.342]
 [65.187]
 [60.793]
 [63.342]] [[2.441]
 [2.441]
 [2.548]
 [2.383]
 [2.441]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.067]
 [0.054]
 [0.864]
 [0.864]
 [0.864]] [[49.956]
 [46.032]
 [47.9  ]
 [47.9  ]
 [47.9  ]] [[2.409]
 [1.19 ]
 [2.098]
 [2.098]
 [2.098]]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.01698939247147
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  117 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  80.62825226689574
maxi score, test score, baseline:  0.2581 1.0 1.0
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.14705056300393
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  2  action  0 :  tensor([    0.9954,     0.0002,     0.0037,     0.0001,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0036,     0.9374,     0.0205,     0.0002,     0.0383],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0035, 0.0156, 0.9110, 0.0248, 0.0450], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0006,     0.0333,     0.9079,     0.0578],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0102, 0.1068, 0.0037, 0.0460, 0.8333], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.736]
 [0.753]
 [0.75 ]
 [0.732]] [[53.5  ]
 [54.882]
 [49.183]
 [51.589]
 [54.996]] [[2.139]
 [2.222]
 [1.991]
 [2.092]
 [2.223]]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.291903898731434
maxi score, test score, baseline:  0.2581 1.0 1.0
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.710989475250244
printing an ep nov before normalisation:  0.06204300941547558
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.683]
 [0.619]
 [0.619]
 [0.619]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.619]
 [0.683]
 [0.619]
 [0.619]
 [0.619]]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]] [[51.106]
 [51.106]
 [51.106]
 [51.106]
 [51.106]] [[1.843]
 [1.843]
 [1.843]
 [1.843]
 [1.843]]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.16130799106498
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2581 1.0 1.0
maxi score, test score, baseline:  0.2581 1.0 1.0
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.629]
 [0.695]
 [0.729]
 [0.711]] [[55.688]
 [60.414]
 [44.643]
 [61.717]
 [53.21 ]] [[1.653]
 [1.749]
 [1.442]
 [1.881]
 [1.661]]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  56.41839753642056
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.059458255767822
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.876987
printing an ep nov before normalisation:  55.840259549282734
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.366]
 [1.423]
 [1.366]
 [1.366]
 [1.366]] [[58.432]
 [58.542]
 [58.432]
 [58.432]
 [58.432]] [[3.36 ]
 [3.423]
 [3.36 ]
 [3.36 ]
 [3.36 ]]
actions average: 
K:  1  action  0 :  tensor([    1.0000,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0026,     0.9140,     0.0171,     0.0004,     0.0659],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9409,     0.0346,     0.0243],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0023, 0.0012, 0.0136, 0.8711, 0.1119], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0043, 0.0708, 0.0310, 0.1085, 0.7853], grad_fn=<DivBackward0>)
siam score:  -0.87668455
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9871,     0.0005,     0.0000,     0.0002,     0.0123],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9983,     0.0001,     0.0001,     0.0013],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0001,     0.9744,     0.0002,     0.0250],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0004,     0.0057,     0.9342,     0.0595],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0066, 0.1233, 0.0014, 0.1235, 0.7452], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
printing an ep nov before normalisation:  31.94071452557624
maxi score, test score, baseline:  0.2581 1.0 1.0
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.0226343052616
printing an ep nov before normalisation:  47.62775041179534
printing an ep nov before normalisation:  55.66465795302446
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.9942193031311
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.84173110053442
siam score:  -0.86909384
printing an ep nov before normalisation:  47.75773689592007
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2601 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.619]
 [0.559]
 [0.559]
 [0.559]] [[39.348]
 [43.249]
 [39.348]
 [39.348]
 [39.348]] [[1.216]
 [1.408]
 [1.216]
 [1.216]
 [1.216]]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.36105195393934
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0020008642366065033
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
printing an ep nov before normalisation:  43.329505596632906
maxi score, test score, baseline:  0.2621 1.0 1.0
printing an ep nov before normalisation:  0.04380095506462567
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  26.56227531792473
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.68812099988631
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.82875849246812
printing an ep nov before normalisation:  53.22343235833776
printing an ep nov before normalisation:  58.25973267808569
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.94]
 [0.94]
 [0.94]
 [0.94]
 [0.94]] [[37.543]
 [37.543]
 [37.543]
 [37.543]
 [37.543]] [[2.687]
 [2.687]
 [2.687]
 [2.687]
 [2.687]]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.00106327237087
maxi score, test score, baseline:  0.2621 1.0 1.0
printing an ep nov before normalisation:  54.15463630321938
maxi score, test score, baseline:  0.2621 1.0 1.0
printing an ep nov before normalisation:  46.28543365819939
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.9700, 0.0069, 0.0014, 0.0030, 0.0186], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9372,     0.0013,     0.0021,     0.0588],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0147,     0.8635,     0.0428,     0.0787],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0003,     0.0033,     0.9194,     0.0767],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0009, 0.0816, 0.0781, 0.0476, 0.7918], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.53706741333008
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.94058209024898
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.499]
 [0.499]
 [0.528]
 [0.499]] [[52.019]
 [52.019]
 [52.019]
 [55.894]
 [52.019]] [[2.302]
 [2.302]
 [2.302]
 [2.528]
 [2.302]]
printing an ep nov before normalisation:  49.30772696166011
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.813438515324926
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.864]
 [0.864]
 [0.864]
 [0.991]
 [0.864]] [[45.542]
 [45.542]
 [45.542]
 [42.855]
 [45.542]] [[1.923]
 [1.923]
 [1.923]
 [1.959]
 [1.923]]
Printing some Q and Qe and total Qs values:  [[0.874]
 [0.889]
 [0.071]
 [0.959]
 [0.874]] [[41.444]
 [44.367]
 [45.917]
 [47.558]
 [41.444]] [[1.471]
 [1.584]
 [0.818]
 [1.762]
 [1.471]]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9783,     0.0001,     0.0211,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9400,     0.0095,     0.0004,     0.0495],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0039,     0.9920,     0.0002,     0.0039],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0006,     0.0158,     0.8971,     0.0863],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0051, 0.0985, 0.0539, 0.1315, 0.7109], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.589]
 [0.51 ]
 [0.5  ]
 [0.502]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.48 ]
 [0.589]
 [0.51 ]
 [0.5  ]
 [0.502]]
printing an ep nov before normalisation:  52.38583564758301
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87529874
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.2621 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.016]
 [0.654]
 [0.611]
 [0.668]] [[37.297]
 [29.814]
 [36.054]
 [36.671]
 [36.23 ]] [[0.692]
 [0.016]
 [0.654]
 [0.611]
 [0.668]]
printing an ep nov before normalisation:  63.045969009399414
actor:  0 policy actor:  0  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.284]
 [1.294]
 [1.267]
 [1.27 ]
 [1.313]] [[37.823]
 [41.001]
 [36.573]
 [41.315]
 [37.909]] [[1.559]
 [1.617]
 [1.523]
 [1.598]
 [1.589]]
printing an ep nov before normalisation:  96.76181414495518
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2641 1.0 1.0
printing an ep nov before normalisation:  48.769017947326596
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.93465982128208
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  48.8021861006802
printing an ep nov before normalisation:  58.3121213166132
using explorer policy with actor:  1
siam score:  -0.8726706
maxi score, test score, baseline:  0.2641 1.0 1.0
actions average: 
K:  4  action  0 :  tensor([    0.9984,     0.0001,     0.0000,     0.0002,     0.0012],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9711,     0.0068,     0.0002,     0.0218],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0062, 0.0299, 0.7837, 0.0549, 0.1253], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0001,     0.0214,     0.9568,     0.0217],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0010, 0.1448, 0.0350, 0.1156, 0.7036], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.13039203153698
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.890020421245175
actions average: 
K:  3  action  0 :  tensor([    0.9975,     0.0002,     0.0001,     0.0004,     0.0017],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9654,     0.0089,     0.0000,     0.0256],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0004,     0.0013,     0.9594,     0.0188,     0.0202],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0006,     0.0005,     0.9464,     0.0524],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0011, 0.0675, 0.0555, 0.1093, 0.7666], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.511]
 [0.511]
 [0.511]
 [0.511]] [[45.631]
 [41.856]
 [41.856]
 [41.856]
 [41.856]] [[1.265]
 [1.107]
 [1.107]
 [1.107]
 [1.107]]
maxi score, test score, baseline:  0.2641 1.0 1.0
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.4392763030755
printing an ep nov before normalisation:  54.95622392558962
printing an ep nov before normalisation:  60.72755309705242
printing an ep nov before normalisation:  63.90232828718665
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.482]
 [1.484]
 [1.482]
 [1.484]
 [1.487]] [[15.406]
 [15.145]
 [10.092]
 [10.39 ]
 [16.258]] [[1.879]
 [1.874]
 [1.742]
 [1.752]
 [1.906]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.709461182305382
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.666]
 [0.666]
 [0.733]
 [0.666]] [[32.799]
 [32.799]
 [32.799]
 [53.877]
 [32.799]] [[0.666]
 [0.666]
 [0.666]
 [0.733]
 [0.666]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.34723490030077
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.91910542496734
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  86 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8816144
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.501]
 [0.739]
 [0.501]
 [0.501]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.501]
 [0.501]
 [0.739]
 [0.501]
 [0.501]]
printing an ep nov before normalisation:  62.986844079783026
actor:  0 policy actor:  0  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.026468800037954
printing an ep nov before normalisation:  55.883940639297776
printing an ep nov before normalisation:  58.19713266226284
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.56220120708436
maxi score, test score, baseline:  0.2681 1.0 1.0
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.090158294786775
printing an ep nov before normalisation:  40.83042409397996
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2681 1.0 1.0
printing an ep nov before normalisation:  64.31207656860352
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]] [[43.578]
 [43.578]
 [43.578]
 [43.578]
 [43.578]] [[2.071]
 [2.071]
 [2.071]
 [2.071]
 [2.071]]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.490603008011675
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
deleting a thread, now have 1 threads
Frames:  268579 train batches done:  31473 episodes:  9628
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8828174
printing an ep nov before normalisation:  36.18540521760015
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.71683760381019
printing an ep nov before normalisation:  31.26961804049509
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8823774
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.026 0.564 0.231 0.026 0.154]
Printing some Q and Qe and total Qs values:  [[0.941]
 [0.931]
 [0.924]
 [0.799]
 [0.936]] [[47.39 ]
 [52.952]
 [49.115]
 [44.741]
 [52.5  ]] [[0.941]
 [0.931]
 [0.924]
 [0.799]
 [0.936]]
actor:  0 policy actor:  0  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.76 ]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.738]] [[46.495]
 [46.495]
 [46.495]
 [46.495]
 [51.212]] [[1.886]
 [1.886]
 [1.886]
 [1.886]
 [2.025]]
line 256 mcts: sample exp_bonus 46.40871600477155
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.973]
 [0.974]
 [0.974]
 [0.974]
 [0.974]] [[44.568]
 [30.577]
 [30.577]
 [30.577]
 [30.577]] [[1.903]
 [1.471]
 [1.471]
 [1.471]
 [1.471]]
printing an ep nov before normalisation:  51.1904682402141
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2701 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.051612629376734
printing an ep nov before normalisation:  61.78793960153387
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([    0.9951,     0.0001,     0.0000,     0.0025,     0.0024],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.8663,     0.0014,     0.0005,     0.1314],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0044,     0.9659,     0.0002,     0.0293],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0001,     0.0003,     0.9955,     0.0041],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0008,     0.0692,     0.0175,     0.0171,     0.8955],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]] [[34.309]
 [34.309]
 [34.309]
 [34.309]
 [34.309]] [[0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.06134370129784
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.25080459356399
siam score:  -0.8784314
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  0  action  0 :  tensor([    0.9992,     0.0001,     0.0000,     0.0001,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9723,     0.0003,     0.0004,     0.0270],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0027,     0.0002,     0.9544,     0.0012,     0.0415],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0010, 0.0021, 0.0013, 0.9308, 0.0649], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0020, 0.0791, 0.0320, 0.1830, 0.7038], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.539458575875116
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.23076299961747
printing an ep nov before normalisation:  44.15241487851001
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.00001617624988
siam score:  -0.8800076
line 256 mcts: sample exp_bonus 46.69425709707183
maxi score, test score, baseline:  0.2721 1.0 1.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.644101854706577
printing an ep nov before normalisation:  36.38267520480888
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2721 1.0 1.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.54058415701843
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.24913482330481
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.78693672404455
printing an ep nov before normalisation:  40.36517292835107
actor:  0 policy actor:  0  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  84.41820608445737
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.611]
 [0.605]
 [0.611]
 [0.611]] [[51.069]
 [51.069]
 [75.802]
 [51.069]
 [51.069]] [[1.458]
 [1.458]
 [2.081]
 [1.458]
 [1.458]]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8849461
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.86753028001512
printing an ep nov before normalisation:  39.736010139419705
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.824393415892075
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.001798041019626
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.93459069174252
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.5160851951448
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
Starting evaluation
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.18]
 [0.18]
 [0.18]
 [0.18]
 [0.18]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.18]
 [0.18]
 [0.18]
 [0.18]
 [0.18]]
printing an ep nov before normalisation:  54.22942664968113
printing an ep nov before normalisation:  32.713490062289765
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.542]
 [0.438]
 [0.438]
 [0.498]] [[41.835]
 [43.351]
 [41.835]
 [41.835]
 [45.831]] [[0.438]
 [0.542]
 [0.438]
 [0.438]
 [0.498]]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.725516276946408
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  39.14029814093148
printing an ep nov before normalisation:  28.784501552581787
printing an ep nov before normalisation:  46.544035809871694
printing an ep nov before normalisation:  39.8192560108531
printing an ep nov before normalisation:  35.76668872529741
printing an ep nov before normalisation:  38.44324408545684
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  38.25783318123121
Printing some Q and Qe and total Qs values:  [[0.891]
 [0.916]
 [0.891]
 [0.891]
 [0.885]] [[48.756]
 [54.486]
 [48.756]
 [48.756]
 [52.541]] [[0.891]
 [0.916]
 [0.891]
 [0.891]
 [0.885]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.923]
 [0.923]
 [0.923]
 [0.923]] [[51.811]
 [33.812]
 [33.812]
 [33.812]
 [33.812]] [[0.994]
 [0.923]
 [0.923]
 [0.923]
 [0.923]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.91555347964674
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.76789079582942
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.227585669175674
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.44107185294658
siam score:  -0.88137054
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9990,     0.0002,     0.0000,     0.0003,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9991,     0.0000,     0.0003,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0039, 0.0016, 0.9072, 0.0243, 0.0630], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0001,     0.0000,     0.9903,     0.0096],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0023, 0.1897, 0.0084, 0.0246, 0.7750], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  25.68901777267456
printing an ep nov before normalisation:  28.597291543541164
printing an ep nov before normalisation:  24.021946342502215
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.88015056
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
printing an ep nov before normalisation:  34.15251250580215
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.845]
 [0.856]
 [0.867]
 [0.846]] [[32.229]
 [36.617]
 [35.605]
 [33.1  ]
 [35.28 ]] [[1.529]
 [1.704]
 [1.676]
 [1.591]
 [1.654]]
printing an ep nov before normalisation:  33.77622951227652
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.92]
 [0.92]
 [0.92]
 [0.92]
 [0.92]] [[47.459]
 [47.459]
 [47.459]
 [47.459]
 [47.459]] [[1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.46050544042916
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
printing an ep nov before normalisation:  35.48813619517703
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.797]
 [0.719]
 [0.719]
 [0.719]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.719]
 [0.797]
 [0.719]
 [0.719]
 [0.719]]
printing an ep nov before normalisation:  50.408901103657094
printing an ep nov before normalisation:  39.79381799697876
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.665]
 [0.668]
 [0.633]
 [0.796]] [[38.868]
 [37.985]
 [35.193]
 [21.812]
 [35.497]] [[1.845]
 [1.804]
 [1.671]
 [0.981]
 [1.814]]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.772]
 [0.676]
 [0.712]
 [0.733]] [[30.212]
 [32.281]
 [30.212]
 [25.817]
 [23.442]] [[0.855]
 [0.971]
 [0.855]
 [0.851]
 [0.85 ]]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.11383190647347
actions average: 
K:  2  action  0 :  tensor([    0.9311,     0.0003,     0.0017,     0.0021,     0.0648],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9425,     0.0080,     0.0097,     0.0397],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0006,     0.9772,     0.0002,     0.0218],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0003,     0.0005,     0.9603,     0.0388],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0024, 0.0955, 0.0630, 0.1088, 0.7303], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.735]
 [0.844]
 [0.735]
 [0.735]] [[35.235]
 [35.235]
 [46.361]
 [35.235]
 [35.235]] [[1.48 ]
 [1.48 ]
 [2.165]
 [1.48 ]
 [1.48 ]]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.45555919357749
maxi score, test score, baseline:  0.3141 1.0 1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.84328541401994
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9953,     0.0004,     0.0000,     0.0019,     0.0024],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9689,     0.0002,     0.0007,     0.0300],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0003,     0.8871,     0.0571,     0.0553],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0027,     0.0008,     0.0189,     0.8757,     0.1019],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0091, 0.0559, 0.0237, 0.1415, 0.7697], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.87854385
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.04949173576689
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.147496219483806
printing an ep nov before normalisation:  56.50094566270618
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.817]
 [0.817]
 [0.817]
 [0.875]
 [0.817]] [[60.648]
 [60.648]
 [60.648]
 [61.97 ]
 [60.648]] [[1.787]
 [1.787]
 [1.787]
 [1.875]
 [1.787]]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.31189155578613
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8793826
printing an ep nov before normalisation:  58.65882500688605
printing an ep nov before normalisation:  41.85389912314854
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  55.11585870172384
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.28156228998976
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.56281461708497
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3161 1.0 1.0
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.475694719848327
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.373]
 [0.355]
 [0.355]
 [0.355]] [[45.743]
 [44.651]
 [45.743]
 [45.743]
 [45.743]] [[0.355]
 [0.373]
 [0.355]
 [0.355]
 [0.355]]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.69350391178631
printing an ep nov before normalisation:  20.030907322690936
actions average: 
K:  4  action  0 :  tensor([    0.9981,     0.0002,     0.0000,     0.0008,     0.0009],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9486,     0.0003,     0.0003,     0.0506],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0015,     0.0001,     0.7679,     0.0813,     0.1492],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0006,     0.0727,     0.7844,     0.1419],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0001,     0.1069,     0.0054,     0.0701,     0.8175],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8683371
maxi score, test score, baseline:  0.3161 1.0 1.0
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.1168678727248107
actor:  0 policy actor:  0  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  59.36267235340318
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9981,     0.0008,     0.0000,     0.0003,     0.0007],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9487,     0.0074,     0.0009,     0.0428],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0002,     0.9801,     0.0005,     0.0189],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0000,     0.0001,     0.0090,     0.9057,     0.0851],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0005,     0.1251,     0.0330,     0.1470,     0.6944],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87078494
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.168922768739485
maxi score, test score, baseline:  0.3181 1.0 1.0
printing an ep nov before normalisation:  35.614322423427
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.78861651459867
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.88263148113081
Printing some Q and Qe and total Qs values:  [[0.913]
 [1.331]
 [0.887]
 [0.913]
 [0.913]] [[35.022]
 [43.51 ]
 [27.821]
 [35.022]
 [35.022]] [[1.673]
 [2.407]
 [1.38 ]
 [1.673]
 [1.673]]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.754]
 [0.754]
 [0.754]
 [0.754]] [[70.581]
 [62.153]
 [62.153]
 [62.153]
 [62.153]] [[1.627]
 [1.501]
 [1.501]
 [1.501]
 [1.501]]
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0029,     0.9380,     0.0001,     0.0001,     0.0588],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0204,     0.9605,     0.0027,     0.0162],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0011, 0.0050, 0.0654, 0.8528, 0.0756], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0013, 0.0750, 0.0543, 0.0833, 0.7861], grad_fn=<DivBackward0>)
siam score:  -0.86755013
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.629694292038934
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
printing an ep nov before normalisation:  41.93474331796851
printing an ep nov before normalisation:  57.83294948961197
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.46967708622991
actor:  1 policy actor:  1  step number:  110 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.518176078796387
printing an ep nov before normalisation:  54.25101409018072
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.575207233428955
printing an ep nov before normalisation:  49.52816963195801
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.64407944584427
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.89852097451609
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.824278133394245
actor:  0 policy actor:  0  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  4  action  0 :  tensor([    0.9928,     0.0001,     0.0000,     0.0000,     0.0070],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0080,     0.8719,     0.0013,     0.0003,     0.1186],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0009, 0.0253, 0.8625, 0.0450, 0.0662], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0015,     0.0057,     0.9864,     0.0064],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0027, 0.0830, 0.0137, 0.0993, 0.8012], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3201 1.0 1.0
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86237013
maxi score, test score, baseline:  0.3201 1.0 1.0
maxi score, test score, baseline:  0.3201 1.0 1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.457]
 [1.324]
 [1.247]
 [1.387]
 [1.296]] [[56.038]
 [59.539]
 [52.491]
 [56.955]
 [57.062]] [[3.282]
 [3.324]
 [2.894]
 [3.258]
 [3.172]]
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.79346466230213
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.00982550169513
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.6120088320232
Printing some Q and Qe and total Qs values:  [[1.302]
 [1.341]
 [1.302]
 [1.302]
 [1.346]] [[30.829]
 [42.538]
 [30.829]
 [30.829]
 [43.219]] [[1.932]
 [2.399]
 [1.932]
 [1.932]
 [2.429]]
actor:  1 policy actor:  1  step number:  104 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3201 1.0 1.0
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.9061390708983
Printing some Q and Qe and total Qs values:  [[1.058]
 [1.038]
 [1.028]
 [1.035]
 [1.038]] [[55.935]
 [55.213]
 [58.813]
 [58.278]
 [55.213]] [[1.749]
 [1.717]
 [1.769]
 [1.767]
 [1.717]]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  55.103997708899016
siam score:  -0.87057894
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.601]
 [0.637]
 [0.642]
 [0.599]] [[40.517]
 [46.937]
 [32.791]
 [31.828]
 [40.104]] [[0.085]
 [0.601]
 [0.637]
 [0.642]
 [0.599]]
siam score:  -0.87596565
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
printing an ep nov before normalisation:  48.11528013206897
maxi score, test score, baseline:  0.3241 1.0 1.0
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.630726003089684
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.263]
 [1.258]
 [1.263]
 [1.263]
 [1.253]] [[36.753]
 [38.287]
 [36.753]
 [36.753]
 [37.8  ]] [[1.881]
 [1.925]
 [1.881]
 [1.881]
 [1.905]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
actions average: 
K:  2  action  0 :  tensor([    0.9966,     0.0003,     0.0000,     0.0008,     0.0023],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9523,     0.0062,     0.0003,     0.0409],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0018, 0.0043, 0.9510, 0.0272, 0.0158], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0564, 0.0012, 0.0172, 0.9090, 0.0161], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0021, 0.2043, 0.0438, 0.1161, 0.6337], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.5413501214744
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9844,     0.0038,     0.0000,     0.0057,     0.0062],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9249,     0.0006,     0.0011,     0.0732],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0017,     0.9631,     0.0015,     0.0336],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0145,     0.0374,     0.7829,     0.1648],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0043, 0.0844, 0.0394, 0.0779, 0.7940], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.16584300994873
printing an ep nov before normalisation:  47.34197919118108
siam score:  -0.8700308
maxi score, test score, baseline:  0.3241 1.0 1.0
printing an ep nov before normalisation:  51.890556387419416
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.312604904174805
printing an ep nov before normalisation:  50.15474762253224
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.11109048334341
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.28574275970459
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.467]
 [1.298]
 [1.259]
 [1.196]
 [1.358]] [[36.332]
 [37.351]
 [38.276]
 [34.955]
 [37.272]] [[2.458]
 [2.346]
 [2.361]
 [2.108]
 [2.402]]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.60510003078549
printing an ep nov before normalisation:  51.814959412382386
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.92075477563115
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.35904359817505
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.839]
 [0.857]
 [0.986]
 [0.998]] [[35.561]
 [31.83 ]
 [32.907]
 [35.299]
 [35.148]] [[0.968]
 [0.976]
 [1.003]
 [1.151]
 [1.162]]
printing an ep nov before normalisation:  2.0567573244534287e-05
actions average: 
K:  1  action  0 :  tensor([    0.9959,     0.0005,     0.0033,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9644,     0.0002,     0.0005,     0.0348],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0004,     0.9863,     0.0021,     0.0109],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0003,     0.0183,     0.8872,     0.0939],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0111, 0.1174, 0.0238, 0.0441, 0.8037], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  48.15838583367534
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.085293116987195
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.78981641846756
Printing some Q and Qe and total Qs values:  [[0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]] [[49.458]
 [49.458]
 [49.458]
 [49.458]
 [49.458]] [[0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]]
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.67955304457047
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.9412222271399
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  29.79488521978192
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.87047297
printing an ep nov before normalisation:  43.19269953010323
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.811]
 [0.673]
 [0.673]
 [0.673]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.673]
 [0.811]
 [0.673]
 [0.673]
 [0.673]]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.009326457977295
printing an ep nov before normalisation:  41.831942375224656
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.507504796395583
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.836]
 [0.84 ]
 [0.843]
 [0.838]] [[45.215]
 [48.624]
 [48.571]
 [46.574]
 [46.321]] [[1.411]
 [1.496]
 [1.499]
 [1.453]
 [1.441]]
maxi score, test score, baseline:  0.3261 1.0 1.0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]] [[28.16]
 [28.16]
 [28.16]
 [28.16]
 [28.16]] [[0.839]
 [0.839]
 [0.839]
 [0.839]
 [0.839]]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.536]
 [0.576]
 [0.536]
 [0.536]] [[42.644]
 [42.644]
 [45.262]
 [42.644]
 [42.644]] [[1.103]
 [1.103]
 [1.195]
 [1.103]
 [1.103]]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.15734486106348
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.21793583539871
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.42850923538208
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.62791996048101
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.93244603245078
printing an ep nov before normalisation:  43.81161009346791
siam score:  -0.86994994
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.3742],
        [0.0000],
        [0.5509],
        [0.2394],
        [0.6285],
        [0.0000],
        [0.3887],
        [0.7738],
        [0.7060],
        [0.6877]], dtype=torch.float64)
0.0 0.3741768986693512
0.0 0.0
0.0 0.5509387643777561
0.0 0.23940246015875338
0.0 0.6285082764333568
0.0 0.0
0.0 0.38865001385085257
0.0 0.7738395155730223
0.0 0.7060228407265172
0.0 0.6876743291395645
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.9994,     0.0000,     0.0003,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9338,     0.0006,     0.0004,     0.0649],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0005,     0.0003,     0.9601,     0.0018,     0.0373],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0006,     0.0004,     0.0001,     0.9238,     0.0751],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0019, 0.0777, 0.1199, 0.1322, 0.6683], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86874276
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.380305228728076
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.04005314474881061
printing an ep nov before normalisation:  42.439030404211245
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.35930824279785
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.01339928154453
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.87181458542254
Printing some Q and Qe and total Qs values:  [[1.05 ]
 [1.075]
 [1.075]
 [1.075]
 [1.075]] [[61.317]
 [49.41 ]
 [49.41 ]
 [49.41 ]
 [49.41 ]] [[1.933]
 [1.734]
 [1.734]
 [1.734]
 [1.734]]
printing an ep nov before normalisation:  45.45910646361509
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.801]
 [0.793]
 [0.827]
 [0.799]] [[37.882]
 [37.786]
 [37.882]
 [29.864]
 [36.239]] [[1.073]
 [1.08 ]
 [1.073]
 [1.019]
 [1.061]]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.940865854380768
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8673129
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.831185721994274
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.62423650107879
printing an ep nov before normalisation:  50.540120033856624
siam score:  -0.86906594
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.274991035461426
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  58.14629413394607
printing an ep nov before normalisation:  30.668776862375395
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.10578576380921
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8614494
Printing some Q and Qe and total Qs values:  [[1.344]
 [1.348]
 [1.344]
 [1.344]
 [1.255]] [[52.515]
 [54.296]
 [52.515]
 [52.515]
 [48.477]] [[1.643]
 [1.661]
 [1.643]
 [1.643]
 [1.52 ]]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.860981
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.40989547729339
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.331]
 [1.304]
 [1.331]
 [1.331]
 [1.222]] [[42.439]
 [52.228]
 [42.439]
 [42.439]
 [43.521]] [[1.853]
 [2.057]
 [1.853]
 [1.853]
 [1.77 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  63.983983471498384
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.17828795477189
printing an ep nov before normalisation:  67.45599763984042
using explorer policy with actor:  1
siam score:  -0.86137307
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.10762188947136
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.17485769092879
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8632995
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8644279
printing an ep nov before normalisation:  57.42005332992674
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
siam score:  -0.86471057
Printing some Q and Qe and total Qs values:  [[1.484]
 [1.309]
 [1.36 ]
 [1.27 ]
 [1.341]] [[30.355]
 [27.416]
 [28.007]
 [29.705]
 [27.346]] [[2.622]
 [2.337]
 [2.41 ]
 [2.383]
 [2.366]]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.298293113708496
maxi score, test score, baseline:  0.3261 1.0 1.0
printing an ep nov before normalisation:  31.380581582260575
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.138512344022814
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.686]
 [0.692]
 [0.924]
 [0.686]] [[35.623]
 [28.799]
 [25.716]
 [26.259]
 [30.162]] [[1.421]
 [1.228]
 [1.132]
 [1.382]
 [1.274]]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.880798537427005
printing an ep nov before normalisation:  44.03631499121711
printing an ep nov before normalisation:  42.709834074153775
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.14 ]
 [0.952]
 [0.967]
 [1.003]
 [0.978]] [[34.404]
 [30.795]
 [36.218]
 [30.334]
 [31.583]] [[1.525]
 [1.263]
 [1.39 ]
 [1.303]
 [1.305]]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.485950469970703
Printing some Q and Qe and total Qs values:  [[1.355]
 [1.353]
 [1.355]
 [1.355]
 [1.313]] [[43.075]
 [46.703]
 [43.075]
 [43.075]
 [45.462]] [[2.184]
 [2.293]
 [2.184]
 [2.184]
 [2.215]]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.867]] [[46.111]
 [46.111]
 [46.111]
 [46.111]
 [47.91 ]] [[2.144]
 [2.144]
 [2.144]
 [2.144]
 [2.224]]
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.54110296973923
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  86 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.004]
 [0.693]
 [0.556]
 [0.684]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.761]
 [0.004]
 [0.693]
 [0.556]
 [0.684]]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.73158779007994
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  28.663021326065063
actions average: 
K:  4  action  0 :  tensor([    0.9651,     0.0006,     0.0000,     0.0085,     0.0257],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9834,     0.0001,     0.0001,     0.0162],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0007,     0.0191,     0.8824,     0.0333,     0.0645],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0007,     0.0012,     0.8544,     0.1436],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0003,     0.0380,     0.0222,     0.0861,     0.8533],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.496]
 [0.467]
 [0.467]
 [0.47 ]] [[41.448]
 [40.009]
 [41.448]
 [41.448]
 [42.157]] [[0.467]
 [0.496]
 [0.467]
 [0.467]
 [0.47 ]]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.83268928527832
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.124]
 [1.245]
 [1.124]
 [1.124]
 [1.168]] [[42.83 ]
 [45.775]
 [42.83 ]
 [42.83 ]
 [45.722]] [[1.716]
 [1.908]
 [1.716]
 [1.716]
 [1.829]]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.723]
 [0.712]
 [0.705]
 [0.839]] [[30.937]
 [29.758]
 [27.475]
 [31.69 ]
 [32.997]] [[0.86 ]
 [0.867]
 [0.834]
 [0.867]
 [1.014]]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.016]
 [0.666]
 [0.594]
 [0.602]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.509]
 [0.016]
 [0.666]
 [0.594]
 [0.602]]
siam score:  -0.8663826
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([    0.9989,     0.0004,     0.0000,     0.0002,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9540,     0.0003,     0.0002,     0.0454],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0006,     0.0002,     0.9398,     0.0181,     0.0413],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0005,     0.0007,     0.0001,     0.9695,     0.0292],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0007,     0.0889,     0.0230,     0.0647,     0.8228],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.238508341218218
printing an ep nov before normalisation:  44.890615944183054
printing an ep nov before normalisation:  52.032736111200975
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.48611908823294
line 256 mcts: sample exp_bonus 47.34463182600452
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.773]
 [0.675]
 [0.675]
 [0.667]] [[43.935]
 [46.858]
 [43.935]
 [43.935]
 [44.772]] [[1.153]
 [1.314]
 [1.153]
 [1.153]
 [1.163]]
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.84474872133872
actions average: 
K:  1  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0007,     0.9735,     0.0048,     0.0010,     0.0201],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0000,     0.9097,     0.0180,     0.0719],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0015,     0.0006,     0.0207,     0.7828,     0.1945],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0007,     0.0836,     0.0762,     0.0916,     0.7478],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.20890808105469
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.042]
 [1.264]
 [1.102]
 [1.122]
 [1.163]] [[45.581]
 [36.247]
 [35.092]
 [45.576]
 [39.637]] [[1.476]
 [1.564]
 [1.385]
 [1.556]
 [1.511]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  64.60654932080189
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.04031259202971
printing an ep nov before normalisation:  36.84063834325647
printing an ep nov before normalisation:  29.206272121835635
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.159914091642044
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.115993881657324
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.71 ]
 [0.635]
 [0.635]
 [0.635]] [[30.939]
 [35.687]
 [30.939]
 [30.939]
 [30.939]] [[1.044]
 [1.22 ]
 [1.044]
 [1.044]
 [1.044]]
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.40157413482666
printing an ep nov before normalisation:  42.62366660078685
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.931874739041525
printing an ep nov before normalisation:  53.55511671564242
printing an ep nov before normalisation:  42.96782989723836
printing an ep nov before normalisation:  41.80594935658199
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.498527759416014
actions average: 
K:  0  action  0 :  tensor([    0.9996,     0.0000,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9556,     0.0005,     0.0001,     0.0432],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0006,     0.0002,     0.9672,     0.0144,     0.0176],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0007,     0.0005,     0.0315,     0.8857,     0.0816],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0017, 0.0943, 0.0162, 0.0840, 0.8037], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.87685984203844
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
printing an ep nov before normalisation:  52.21066131122074
Printing some Q and Qe and total Qs values:  [[1.386]
 [1.345]
 [1.283]
 [1.223]
 [1.317]] [[46.834]
 [46.125]
 [41.805]
 [41.971]
 [45.414]] [[2.916]
 [2.831]
 [2.505]
 [2.454]
 [2.759]]
actions average: 
K:  1  action  0 :  tensor([    0.9851,     0.0064,     0.0000,     0.0008,     0.0077],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0033,     0.9604,     0.0001,     0.0001,     0.0361],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9263,     0.0408,     0.0327],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0004,     0.0005,     0.0018,     0.9573,     0.0401],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0370, 0.0394, 0.0687, 0.0969, 0.7580], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.75856900829806
actions average: 
K:  0  action  0 :  tensor([    0.9871,     0.0062,     0.0005,     0.0003,     0.0060],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0101,     0.9545,     0.0001,     0.0024,     0.0329],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9536,     0.0189,     0.0274],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0010,     0.0003,     0.0009,     0.9096,     0.0882],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0067, 0.0916, 0.0459, 0.1281, 0.7276], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  97 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.20176153479839
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.6629957916436
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.474]
 [0.474]
 [0.551]
 [0.474]] [[34.825]
 [34.825]
 [34.825]
 [34.538]
 [34.825]] [[0.873]
 [0.873]
 [0.873]
 [0.944]
 [0.873]]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.29413025935222
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.25838292031719
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
printing an ep nov before normalisation:  44.54904372110801
maxi score, test score, baseline:  0.3281 1.0 1.0
printing an ep nov before normalisation:  63.42333428042451
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
printing an ep nov before normalisation:  0.10656807351779207
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  41.233301067297106
maxi score, test score, baseline:  0.3281 1.0 1.0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 53.757642022234805
Printing some Q and Qe and total Qs values:  [[1.397]
 [1.385]
 [1.399]
 [1.399]
 [1.386]] [[63.528]
 [66.778]
 [58.612]
 [58.612]
 [64.712]] [[1.919]
 [1.956]
 [1.849]
 [1.849]
 [1.926]]
maxi score, test score, baseline:  0.3281 1.0 1.0
actions average: 
K:  3  action  0 :  tensor([    0.9742,     0.0005,     0.0001,     0.0015,     0.0237],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9935,     0.0022,     0.0002,     0.0038],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0017, 0.0034, 0.8966, 0.0231, 0.0752], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0002,     0.0208,     0.9264,     0.0525],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0007,     0.1755,     0.0175,     0.0298,     0.7765],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.394176376886115
siam score:  -0.8572218
UNIT TEST: sample policy line 217 mcts : [0.077 0.    0.    0.897 0.026]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9989,     0.0003,     0.0000,     0.0003,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9596,     0.0026,     0.0011,     0.0365],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0006,     0.0001,     0.8683,     0.0603,     0.0707],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0011,     0.0004,     0.0144,     0.8475,     0.1366],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0016, 0.1058, 0.0565, 0.1042, 0.7320], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.89638140877988
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.59244808443041
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.839779518819924
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.603884733112835
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3301 1.0 1.0
maxi score, test score, baseline:  0.3301 1.0 1.0
printing an ep nov before normalisation:  56.731844211430875
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.14273166656494
printing an ep nov before normalisation:  56.68199017223969
printing an ep nov before normalisation:  53.035753859453074
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.727]
 [0.608]
 [0.608]
 [0.608]] [[28.734]
 [33.615]
 [28.734]
 [28.734]
 [28.734]] [[1.165]
 [1.464]
 [1.165]
 [1.165]
 [1.165]]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.17622757919742
Printing some Q and Qe and total Qs values:  [[1.35 ]
 [1.341]
 [1.225]
 [1.35 ]
 [1.35 ]] [[39.551]
 [42.012]
 [42.752]
 [39.551]
 [39.551]] [[2.436]
 [2.57 ]
 [2.497]
 [2.436]
 [2.436]]
maxi score, test score, baseline:  0.3301 1.0 1.0
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86164474
printing an ep nov before normalisation:  48.914554668828075
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.88823833260291
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.481]
 [0.481]
 [0.481]
 [0.572]] [[51.539]
 [32.183]
 [32.183]
 [32.183]
 [32.88 ]] [[0.463]
 [0.481]
 [0.481]
 [0.481]
 [0.572]]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.69 ]
 [0.711]
 [0.739]
 [0.697]] [[61.917]
 [63.725]
 [51.429]
 [60.856]
 [60.757]] [[2.423]
 [2.515]
 [2.016]
 [2.443]
 [2.397]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9958,     0.0025,     0.0003,     0.0004,     0.0011],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0016,     0.9115,     0.0011,     0.0006,     0.0853],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9976,     0.0002,     0.0022],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0001,     0.0041,     0.9122,     0.0834],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0018, 0.0816, 0.0532, 0.0023, 0.8611], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3341 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.92 ]
 [0.921]
 [0.921]
 [0.921]
 [0.921]] [[47.506]
 [47.938]
 [47.938]
 [47.938]
 [47.938]] [[2.477]
 [2.5  ]
 [2.5  ]
 [2.5  ]
 [2.5  ]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.19246491448988
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3341 1.0 1.0
printing an ep nov before normalisation:  58.70549080513557
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.30864423841835
printing an ep nov before normalisation:  37.092022216926054
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8521304
printing an ep nov before normalisation:  44.370569390142705
printing an ep nov before normalisation:  36.30258831244785
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.95513147729838
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.43316528183828
printing an ep nov before normalisation:  60.966743793110346
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.96701246911829
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  105 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
printing an ep nov before normalisation:  62.04726682640195
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.846864
printing an ep nov before normalisation:  37.25373463222861
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.497]
 [0.472]
 [0.02 ]
 [0.489]] [[31.285]
 [40.399]
 [34.873]
 [33.731]
 [33.206]] [[0.53 ]
 [0.497]
 [0.472]
 [0.02 ]
 [0.489]]
printing an ep nov before normalisation:  42.29307073422181
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.558]
 [0.583]
 [0.558]
 [0.558]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.558]
 [0.558]
 [0.583]
 [0.558]
 [0.558]]
siam score:  -0.84713453
printing an ep nov before normalisation:  40.63052286500709
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.976]
 [0.946]
 [0.946]
 [0.946]
 [0.951]] [[40.271]
 [43.465]
 [43.465]
 [43.465]
 [42.18 ]] [[1.512]
 [1.561]
 [1.561]
 [1.561]
 [1.534]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10073
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.607060096945137
printing an ep nov before normalisation:  33.875720500946045
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.052]
 [1.032]
 [1.032]
 [0.968]
 [1.032]] [[55.404]
 [51.598]
 [51.598]
 [42.472]
 [51.598]] [[2.521]
 [2.362]
 [2.362]
 [1.961]
 [2.362]]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.0906316297085
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.328]
 [0.293]
 [0.293]
 [0.296]] [[38.217]
 [34.883]
 [38.217]
 [38.217]
 [38.327]] [[0.293]
 [0.328]
 [0.293]
 [0.293]
 [0.296]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3341 1.0 1.0
siam score:  -0.8530815
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.85331485993751
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.41866054243611
printing an ep nov before normalisation:  41.95753020885814
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.85411996
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.38088969670417
Printing some Q and Qe and total Qs values:  [[1.306]
 [1.334]
 [1.306]
 [1.306]
 [1.325]] [[41.686]
 [48.36 ]
 [41.686]
 [41.686]
 [45.173]] [[2.04 ]
 [2.259]
 [2.04 ]
 [2.04 ]
 [2.159]]
printing an ep nov before normalisation:  49.59024472049032
printing an ep nov before normalisation:  51.19712916426503
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.146139993432016
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.140209270404
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3341 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.689]
 [0.686]
 [0.716]
 [0.707]] [[43.893]
 [41.066]
 [35.538]
 [43.893]
 [39.447]] [[1.848]
 [1.681]
 [1.404]
 [1.848]
 [1.619]]
printing an ep nov before normalisation:  53.020933588133474
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.23467615655784
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.53967888028715
maxi score, test score, baseline:  0.3361 1.0 1.0
actions average: 
K:  4  action  0 :  tensor([    0.9916,     0.0008,     0.0000,     0.0001,     0.0075],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0281,     0.9344,     0.0001,     0.0000,     0.0374],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0008,     0.9568,     0.0007,     0.0413],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0006,     0.0002,     0.0012,     0.9341,     0.0639],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0040, 0.0763, 0.0865, 0.0520, 0.7813], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.9767,     0.0047,     0.0045,     0.0005,     0.0136],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9859,     0.0003,     0.0002,     0.0136],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0009,     0.0058,     0.9087,     0.0008,     0.0838],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0004,     0.0051,     0.9275,     0.0670],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0011, 0.1574, 0.0036, 0.0680, 0.7698], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.205210767130275
line 256 mcts: sample exp_bonus 33.93395451157408
Printing some Q and Qe and total Qs values:  [[0.853]
 [0.805]
 [0.787]
 [0.908]
 [0.775]] [[42.228]
 [43.125]
 [41.047]
 [47.835]
 [41.066]] [[1.619]
 [1.608]
 [1.505]
 [1.903]
 [1.494]]
Printing some Q and Qe and total Qs values:  [[1.028]
 [1.102]
 [1.028]
 [1.051]
 [1.05 ]] [[55.602]
 [53.043]
 [55.602]
 [56.02 ]
 [55.144]] [[2.474]
 [2.429]
 [2.474]
 [2.517]
 [2.475]]
printing an ep nov before normalisation:  39.06497912011198
printing an ep nov before normalisation:  37.32269502299905
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.307]
 [1.331]
 [1.307]
 [1.307]
 [1.307]] [[45.141]
 [43.54 ]
 [45.141]
 [45.141]
 [45.141]] [[2.959]
 [2.875]
 [2.959]
 [2.959]
 [2.959]]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.852]
 [0.852]
 [0.903]
 [0.852]] [[53.118]
 [53.118]
 [53.118]
 [52.757]
 [53.118]] [[2.418]
 [2.418]
 [2.418]
 [2.453]
 [2.418]]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.9818,     0.0050,     0.0002,     0.0003,     0.0127],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9402,     0.0003,     0.0002,     0.0586],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9378,     0.0243,     0.0377],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0023,     0.0004,     0.0010,     0.9148,     0.0815],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0034, 0.1157, 0.0149, 0.0944, 0.7716], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.846575
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.489]
 [0.42 ]
 [0.441]
 [0.453]] [[36.433]
 [39.407]
 [31.635]
 [34.952]
 [38.276]] [[0.015]
 [0.489]
 [0.42 ]
 [0.441]
 [0.453]]
printing an ep nov before normalisation:  22.775001525878906
actor:  0 policy actor:  0  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.4419944743465
line 256 mcts: sample exp_bonus 44.72748679831042
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.182]
 [1.008]
 [0.905]
 [1.084]
 [1.008]] [[49.742]
 [50.015]
 [50.764]
 [45.531]
 [50.015]] [[2.669]
 [2.507]
 [2.433]
 [2.408]
 [2.507]]
printing an ep nov before normalisation:  55.61218203020242
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.73270691227806
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.67320794653378
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.096]
 [1.096]
 [1.096]
 [1.096]
 [1.096]] [[34.846]
 [34.846]
 [34.846]
 [34.846]
 [34.846]] [[1.286]
 [1.286]
 [1.286]
 [1.286]
 [1.286]]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.076]
 [1.217]
 [1.131]
 [1.131]
 [1.131]] [[32.634]
 [44.751]
 [34.206]
 [34.206]
 [34.206]] [[0.21 ]
 [1.451]
 [1.278]
 [1.278]
 [1.278]]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.577039674212763
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  27.229231730392556
printing an ep nov before normalisation:  40.40945580512844
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3381 1.0 1.0
printing an ep nov before normalisation:  55.20236510400706
maxi score, test score, baseline:  0.3381 1.0 1.0
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  21.080929481246503
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3401 1.0 1.0
line 256 mcts: sample exp_bonus 51.70027477563261
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9805,     0.0010,     0.0003,     0.0001,     0.0181],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9815,     0.0001,     0.0003,     0.0181],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0018,     0.0002,     0.9617,     0.0011,     0.0352],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0073,     0.0003,     0.0001,     0.9282,     0.0640],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0432, 0.0638, 0.0723, 0.0561, 0.7646], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.6642278458241
actions average: 
K:  2  action  0 :  tensor([    0.9867,     0.0011,     0.0000,     0.0001,     0.0122],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9819,     0.0101,     0.0002,     0.0078],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0011,     0.0001,     0.9617,     0.0172,     0.0199],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0056,     0.0001,     0.9707,     0.0231],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0012, 0.1495, 0.0842, 0.0206, 0.7444], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84376436
maxi score, test score, baseline:  0.3421 1.0 1.0
printing an ep nov before normalisation:  47.858628364228714
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  42.35150457696784
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.574]
 [0.475]
 [0.411]
 [0.519]] [[35.101]
 [37.241]
 [45.578]
 [33.454]
 [37.773]] [[0.446]
 [0.574]
 [0.475]
 [0.411]
 [0.519]]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.635]
 [0.635]
 [0.719]
 [0.627]] [[30.007]
 [30.007]
 [30.007]
 [33.475]
 [32.928]] [[0.635]
 [0.635]
 [0.635]
 [0.719]
 [0.627]]
siam score:  -0.85022825
printing an ep nov before normalisation:  33.90107406017365
printing an ep nov before normalisation:  30.000961972327268
printing an ep nov before normalisation:  47.49277101183473
Printing some Q and Qe and total Qs values:  [[0.894]
 [0.932]
 [0.859]
 [0.859]
 [0.91 ]] [[49.322]
 [51.713]
 [49.539]
 [49.539]
 [50.017]] [[0.894]
 [0.932]
 [0.859]
 [0.859]
 [0.91 ]]
Printing some Q and Qe and total Qs values:  [[0.898]
 [0.939]
 [0.898]
 [0.898]
 [0.913]] [[54.059]
 [60.266]
 [54.059]
 [54.059]
 [52.392]] [[0.898]
 [0.939]
 [0.898]
 [0.898]
 [0.913]]
printing an ep nov before normalisation:  43.21136954861104
printing an ep nov before normalisation:  0.011124622596980771
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  49.70720885318133
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.969989873903835
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  2  action  0 :  tensor([    0.9992,     0.0003,     0.0000,     0.0002,     0.0003],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9753,     0.0003,     0.0002,     0.0241],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0009, 0.0232, 0.8695, 0.0221, 0.0843], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0000,     0.0023,     0.9439,     0.0538],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0014, 0.1236, 0.0548, 0.0250, 0.7952], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.013599098593886083
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.85620284
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.771]
 [0.744]
 [0.668]
 [0.767]] [[57.391]
 [51.714]
 [57.391]
 [49.072]
 [55.265]] [[2.039]
 [1.872]
 [2.039]
 [1.678]
 [1.989]]
printing an ep nov before normalisation:  50.86553182812111
printing an ep nov before normalisation:  42.43810486383879
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3821 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.572]
 [0.568]
 [0.568]
 [0.568]] [[26.624]
 [26.303]
 [26.624]
 [26.624]
 [26.624]] [[0.568]
 [0.572]
 [0.568]
 [0.568]
 [0.568]]
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.566]
 [0.596]
 [0.593]
 [0.553]] [[25.018]
 [37.891]
 [30.138]
 [27.892]
 [27.353]] [[0.549]
 [0.566]
 [0.596]
 [0.593]
 [0.553]]
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.38910661058984
siam score:  -0.8569275
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9904,     0.0010,     0.0016,     0.0000,     0.0070],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9811,     0.0002,     0.0000,     0.0186],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0022,     0.9557,     0.0034,     0.0384],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0013,     0.0051,     0.9484,     0.0452],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0003,     0.0058,     0.0536,     0.0329,     0.9074],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3821 1.0 1.0
printing an ep nov before normalisation:  35.31029631188642
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.9995,     0.0001,     0.0001,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0591,     0.9209,     0.0082,     0.0005,     0.0113],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0005,     0.0029,     0.8853,     0.0529,     0.0584],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0018,     0.0152,     0.9310,     0.0518],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0003,     0.1275,     0.0258,     0.0664,     0.7800],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.35641098022461
printing an ep nov before normalisation:  28.113318491988064
siam score:  -0.8523628
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.145625348800912
printing an ep nov before normalisation:  40.00996708981336
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85170984
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8509614
printing an ep nov before normalisation:  43.27065222546849
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.08141149827637
printing an ep nov before normalisation:  36.51803049157208
printing an ep nov before normalisation:  40.057859579538096
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.56153446011467
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.16876861339875
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 47.05615141353875
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.016]
 [0.597]
 [0.597]
 [0.577]] [[41.45 ]
 [35.734]
 [38.274]
 [44.092]
 [39.328]] [[1.452]
 [0.549]
 [1.204]
 [1.377]
 [1.216]]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.821]
 [0.558]
 [0.659]
 [0.659]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.659]
 [0.821]
 [0.558]
 [0.659]
 [0.659]]
siam score:  -0.85351515
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
maxi score, test score, baseline:  0.3841 1.0 1.0
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8484841
Printing some Q and Qe and total Qs values:  [[1.058]
 [1.04 ]
 [0.995]
 [0.966]
 [0.967]] [[50.636]
 [42.137]
 [45.686]
 [46.202]
 [45.789]] [[2.117]
 [1.789]
 [1.873]
 [1.863]
 [1.849]]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.724]
 [0.666]
 [0.654]
 [0.647]] [[38.643]
 [43.172]
 [45.035]
 [45.743]
 [45.965]] [[1.523]
 [1.946]
 [2.002]
 [2.034]
 [2.04 ]]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
printing an ep nov before normalisation:  58.41560228050902
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.774]
 [0.744]
 [0.697]
 [0.773]] [[28.823]
 [30.946]
 [31.617]
 [35.58 ]
 [30.701]] [[1.521]
 [1.565]
 [1.568]
 [1.715]
 [1.551]]
printing an ep nov before normalisation:  38.058425505703845
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.063768412457165
actions average: 
K:  0  action  0 :  tensor([    0.9882,     0.0005,     0.0000,     0.0001,     0.0113],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9891,     0.0001,     0.0002,     0.0105],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0006,     0.0002,     0.9710,     0.0154,     0.0127],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0004,     0.0007,     0.0141,     0.8988,     0.0860],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0445, 0.1772, 0.0012, 0.0476, 0.7295], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.190454976459876
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.27484825044506
maxi score, test score, baseline:  0.3841 1.0 1.0
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.141]
 [1.29 ]
 [1.141]
 [1.141]
 [1.06 ]] [[33.523]
 [38.651]
 [33.523]
 [33.523]
 [32.965]] [[1.388]
 [1.636]
 [1.388]
 [1.388]
 [1.296]]
line 256 mcts: sample exp_bonus 53.01918495716296
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.28800915820883
printing an ep nov before normalisation:  35.147175788879395
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8475084
printing an ep nov before normalisation:  45.905442237854004
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  3  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0012,     0.9665,     0.0006,     0.0014,     0.0302],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0001,     0.9075,     0.0288,     0.0635],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0001,     0.0326,     0.9450,     0.0222],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0027, 0.0881, 0.1244, 0.0257, 0.7591], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8500319
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  22.349283097812187
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.29344406743986
printing an ep nov before normalisation:  46.70635068822794
printing an ep nov before normalisation:  53.446100996963104
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.641 0.128 0.077 0.128 0.026]
siam score:  -0.8500406
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9988,     0.0008,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0011,     0.9591,     0.0005,     0.0019,     0.0374],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9997,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0001,     0.0009,     0.9571,     0.0419],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0021, 0.0750, 0.0432, 0.0469, 0.8328], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.099750995635986
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.66816603829781
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.556]
 [0.567]
 [0.563]
 [0.554]] [[45.587]
 [41.605]
 [41.91 ]
 [34.919]
 [42.829]] [[0.074]
 [0.556]
 [0.567]
 [0.563]
 [0.554]]
printing an ep nov before normalisation:  32.68605052145004
printing an ep nov before normalisation:  16.66554684910252
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.90983428548516
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.437328259331444
siam score:  -0.85139483
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.13230487582958
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.391]
 [1.23 ]
 [1.258]
 [1.194]
 [1.284]] [[26.708]
 [35.177]
 [38.455]
 [40.112]
 [36.235]] [[2.173]
 [2.261]
 [2.385]
 [2.37 ]
 [2.346]]
printing an ep nov before normalisation:  44.13359318712463
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.83883086693499
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.04387339567936
actions average: 
K:  0  action  0 :  tensor([    0.9810,     0.0002,     0.0028,     0.0002,     0.0158],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.8933,     0.0080,     0.0009,     0.0974],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0140,     0.9278,     0.0007,     0.0573],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0004,     0.0003,     0.0014,     0.9526,     0.0452],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0060, 0.1184, 0.0253, 0.0885, 0.7619], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.2188077679448
printing an ep nov before normalisation:  39.67336722926707
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.878]
 [0.878]
 [0.878]
 [0.931]
 [0.878]] [[26.92 ]
 [26.92 ]
 [26.92 ]
 [35.953]
 [26.92 ]] [[1.301]
 [1.301]
 [1.301]
 [1.593]
 [1.301]]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8485837
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.616]
 [0.578]
 [0.104]
 [0.602]] [[25.302]
 [44.171]
 [26.056]
 [28.813]
 [29.362]] [[0.553]
 [1.812]
 [1.105]
 [0.733]
 [1.251]]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.766199588775635
printing an ep nov before normalisation:  33.195514868761656
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
printing an ep nov before normalisation:  48.94415794449146
printing an ep nov before normalisation:  60.95237126436297
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.285930125032245
printing an ep nov before normalisation:  37.54353084782343
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.945091247558594
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.489]
 [0.39 ]
 [0.417]
 [0.462]] [[29.589]
 [29.883]
 [40.538]
 [35.555]
 [31.853]] [[0.444]
 [0.489]
 [0.39 ]
 [0.417]
 [0.462]]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.18497955325879
siam score:  -0.8463189
printing an ep nov before normalisation:  57.12209452316209
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.553]
 [0.641]
 [0.646]
 [0.642]] [[42.617]
 [35.917]
 [35.776]
 [36.935]
 [32.676]] [[1.832]
 [1.552]
 [1.633]
 [1.693]
 [1.487]]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.638]
 [0.625]
 [0.625]
 [0.625]] [[48.198]
 [41.798]
 [48.198]
 [48.198]
 [48.116]] [[1.816]
 [1.578]
 [1.816]
 [1.816]
 [1.813]]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9973,     0.0002,     0.0000,     0.0006,     0.0018],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9992,     0.0001,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0039, 0.0021, 0.8815, 0.0449, 0.0676], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0000,     0.0005,     0.0001,     0.9974,     0.0020],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0028, 0.0047, 0.0368, 0.0535, 0.9021], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.019]
 [0.786]
 [0.786]
 [0.786]
 [0.786]] [[32.663]
 [32.886]
 [32.886]
 [32.886]
 [32.886]] [[1.195]
 [0.964]
 [0.964]
 [0.964]
 [0.964]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.926]
 [0.881]
 [0.772]
 [0.926]] [[38.839]
 [45.415]
 [39.185]
 [35.464]
 [43.034]] [[0.859]
 [0.926]
 [0.881]
 [0.772]
 [0.926]]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.386]
 [1.386]
 [1.386]
 [1.386]
 [1.386]] [[59.57]
 [59.57]
 [59.57]
 [59.57]
 [59.57]] [[2.25]
 [2.25]
 [2.25]
 [2.25]
 [2.25]]
Printing some Q and Qe and total Qs values:  [[1.35 ]
 [1.363]
 [0.026]
 [1.231]
 [1.358]] [[41.453]
 [41.657]
 [41.548]
 [35.189]
 [40.264]] [[2.137]
 [2.157]
 [0.816]
 [1.811]
 [2.106]]
printing an ep nov before normalisation:  37.63454912124837
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9665,     0.0002,     0.0010,     0.0323],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0007,     0.0008,     0.9927,     0.0002,     0.0056],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0010, 0.0009, 0.0536, 0.7769, 0.1677], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0029, 0.1203, 0.0293, 0.0019, 0.8456], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.31751210377126
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
printing an ep nov before normalisation:  33.76157283782959
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
siam score:  -0.84287935
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9990,     0.0001,     0.0000,     0.0006,     0.0003],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.9657,     0.0001,     0.0001,     0.0336],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0031,     0.9480,     0.0104,     0.0384],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0003,     0.0164,     0.9386,     0.0442],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0022, 0.0468, 0.1241, 0.0878, 0.7392], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
siam score:  -0.84652436
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8482271
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.97575715373117
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8500908
actions average: 
K:  4  action  0 :  tensor([    0.9994,     0.0002,     0.0000,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0011,     0.9326,     0.0002,     0.0004,     0.0656],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0001,     0.9709,     0.0008,     0.0279],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0015,     0.0008,     0.0257,     0.9010,     0.0710],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0119, 0.0494, 0.0280, 0.1899, 0.7208], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.40273936078293
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.903]
 [0.924]
 [0.903]
 [0.903]
 [0.903]] [[35.355]
 [39.436]
 [35.355]
 [35.355]
 [35.355]] [[0.903]
 [0.924]
 [0.903]
 [0.903]
 [0.903]]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.944]
 [0.882]
 [0.864]
 [0.824]
 [0.885]] [[28.768]
 [54.18 ]
 [42.593]
 [46.987]
 [52.48 ]] [[0.944]
 [0.882]
 [0.864]
 [0.824]
 [0.885]]
actor:  0 policy actor:  0  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.30097070277266
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.5195],
        [0.5184],
        [0.4461],
        [0.3225],
        [0.6080],
        [0.6561],
        [0.0000],
        [0.2801],
        [0.5184],
        [0.4930]], dtype=torch.float64)
0.0 0.5195387663796354
0.0 0.518425659980178
0.0 0.44610697189675497
0.0 0.32248645125916003
0.0 0.6079799282877476
0.0 0.6561075076063846
0.0 0.0
0.0 0.2800738268523075
0.0 0.518425659980178
0.0 0.49303286001506247
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.906]
 [0.906]
 [0.967]
 [0.917]] [[38.619]
 [38.619]
 [38.619]
 [35.1  ]
 [38.64 ]] [[1.902]
 [1.902]
 [1.902]
 [1.794]
 [1.914]]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.968444568511515
actions average: 
K:  0  action  0 :  tensor([    0.9984,     0.0003,     0.0000,     0.0003,     0.0010],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9579,     0.0002,     0.0015,     0.0402],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0003,     0.9685,     0.0003,     0.0308],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0000,     0.0010,     0.0006,     0.9637,     0.0346],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0006,     0.0471,     0.0314,     0.1566,     0.7643],
       grad_fn=<DivBackward0>)
siam score:  -0.8516342
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.55413157341508
Printing some Q and Qe and total Qs values:  [[1.171]
 [1.329]
 [1.171]
 [1.171]
 [1.202]] [[44.061]
 [47.456]
 [44.061]
 [44.061]
 [41.087]] [[2.635]
 [2.996]
 [2.635]
 [2.635]
 [2.489]]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.788]
 [0.627]
 [0.698]
 [0.673]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.698]
 [0.788]
 [0.627]
 [0.698]
 [0.673]]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.9325,     0.0020,     0.0001,     0.0252,     0.0402],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9726,     0.0000,     0.0012,     0.0260],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0005,     0.0006,     0.9027,     0.0218,     0.0744],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0003,     0.0109,     0.9347,     0.0538],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0181, 0.0774, 0.1210, 0.0437, 0.7398], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3901 1.0 1.0
maxi score, test score, baseline:  0.3901 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.051 0.154 0.641 0.077 0.077]
maxi score, test score, baseline:  0.3901 1.0 1.0
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.80501170834975
maxi score, test score, baseline:  0.3901 1.0 1.0
printing an ep nov before normalisation:  47.188557763638364
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.425944203981945
printing an ep nov before normalisation:  23.368107147079165
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.849863
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.567]
 [0.567]
 [0.578]
 [0.567]] [[25.612]
 [25.612]
 [25.612]
 [28.392]
 [25.612]] [[0.567]
 [0.567]
 [0.567]
 [0.578]
 [0.567]]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.85829574088865
actions average: 
K:  0  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9447,     0.0081,     0.0002,     0.0466],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0163,     0.9212,     0.0150,     0.0474],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0024,     0.0004,     0.0004,     0.9303,     0.0665],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0018, 0.0564, 0.0161, 0.1153, 0.8103], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.786461702203898
actions average: 
K:  4  action  0 :  tensor([    0.9996,     0.0001,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0057,     0.9282,     0.0002,     0.0002,     0.0658],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0000,     0.9974,     0.0000,     0.0022],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0010,     0.0013,     0.0004,     0.8974,     0.0999],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0006,     0.0747,     0.0531,     0.0806,     0.7910],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.74438004659022
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.477191418917954
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.537]
 [0.591]
 [0.537]
 [0.537]] [[51.312]
 [39.934]
 [39.397]
 [39.934]
 [39.934]] [[1.996]
 [1.51 ]
 [1.539]
 [1.51 ]
 [1.51 ]]
maxi score, test score, baseline:  0.3941 1.0 1.0
printing an ep nov before normalisation:  60.29323555649863
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.18039147834897
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9531,     0.0205,     0.0005,     0.0014,     0.0245],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9453,     0.0094,     0.0006,     0.0443],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0008,     0.0231,     0.9510,     0.0006,     0.0246],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0006,     0.0135,     0.9414,     0.0444],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0011, 0.0419, 0.0315, 0.0523, 0.8733], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.901545571137856
printing an ep nov before normalisation:  33.6782103784176
printing an ep nov before normalisation:  39.77997303009033
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.026 0.231 0.026 0.256 0.462]
actor:  1 policy actor:  1  step number:  112 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.9575399965579
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  24.65203613621013
printing an ep nov before normalisation:  63.45022275045256
printing an ep nov before normalisation:  57.58056726171852
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.026 0.615 0.359]
printing an ep nov before normalisation:  51.535353660583496
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8448581
siam score:  -0.84480315
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9700,     0.0021,     0.0005,     0.0005,     0.0269],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0006,     0.9511,     0.0089,     0.0002,     0.0393],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0001,     0.9479,     0.0196,     0.0324],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0018, 0.0016, 0.0696, 0.7543, 0.1727], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0056, 0.1872, 0.0109, 0.0368, 0.7595], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.488]
 [0.459]
 [0.459]
 [0.459]] [[28.496]
 [28.166]
 [28.496]
 [28.496]
 [28.496]] [[0.459]
 [0.488]
 [0.459]
 [0.459]
 [0.459]]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.724]
 [0.721]
 [0.733]
 [0.727]] [[30.478]
 [30.387]
 [27.966]
 [32.55 ]
 [21.993]] [[1.105]
 [1.104]
 [1.051]
 [1.156]
 [0.936]]
printing an ep nov before normalisation:  0.019509647900406435
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.9991,     0.0001,     0.0000,     0.0001,     0.0008],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0007,     0.8953,     0.0098,     0.0013,     0.0929],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0196,     0.9464,     0.0017,     0.0321],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0004,     0.0004,     0.9527,     0.0464],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0490, 0.0972, 0.0371, 0.0420, 0.7747], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.573]
 [0.582]
 [0.555]
 [0.573]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.573]
 [0.573]
 [0.582]
 [0.555]
 [0.573]]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8252775
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.651]
 [0.618]
 [0.567]
 [0.562]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.541]
 [0.651]
 [0.618]
 [0.567]
 [0.562]]
printing an ep nov before normalisation:  36.45550259867052
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8238052
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.336030541539
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3961 1.0 1.0
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3961 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.026 0.308 0.026 0.538 0.103]
maxi score, test score, baseline:  0.3961 1.0 1.0
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.542]
 [0.026]
 [0.457]
 [0.512]] [[39.594]
 [53.644]
 [37.103]
 [44.713]
 [49.233]] [[0.404]
 [0.542]
 [0.026]
 [0.457]
 [0.512]]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.348226905321475
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.6678127699819
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.05445885248924
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
