append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Starting evaluation
siam score:  -0.0017926217406056821
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.024627274839946513
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
deleting a thread, now have 2 threads
Frames:  1049 train batches done:  32 episodes:  34
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
siam score:  -0.17259432
first move QE:  -0.08555989993773568
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
deleting a thread, now have 1 threads
Frames:  1049 train batches done:  82 episodes:  34
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
siam score:  -0.36239517
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.3477215371922643, 0.23457028360122134, 0.06998664201425017, 0.3477215371922643]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.4302913722497453, 0.2414146965595359, 0.08687923463118283, 0.2414146965595359]
using another actor
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.08472324804404975, 0.4152767519559502, 0.08472324804404975, 0.4152767519559502]
maxi score, test score, baseline:  -0.9564217391304348 -1.0 -0.9564217391304348
probs:  [0.08472324804404975, 0.4152767519559502, 0.08472324804404975, 0.4152767519559502]
maxi score, test score, baseline:  -0.9582333333333334 -1.0 -0.9582333333333334
probs:  [0.12652057207640638, 0.12652057207640638, 0.12652057207640638, 0.6204382837707807]
maxi score, test score, baseline:  -0.9582333333333334 -1.0 -0.9582333333333334
maxi score, test score, baseline:  -0.9582333333333334 -1.0 -0.9582333333333334
probs:  [0.07304101595883936, 0.36097427812750743, 0.36097427812750743, 0.20501042778614567]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.08651965288640377, 0.4276960294713005, 0.24289215882114795, 0.24289215882114795]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.08651965288640377, 0.4276960294713005, 0.24289215882114795, 0.24289215882114795]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.08651965288640377, 0.4276960294713005, 0.24289215882114795, 0.24289215882114795]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.08651965288640377, 0.4276960294713005, 0.24289215882114795, 0.24289215882114795]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.08651965288640377, 0.4276960294713005, 0.24289215882114795, 0.24289215882114795]
from probs:  [0.08651965288640377, 0.4276960294713005, 0.24289215882114795, 0.24289215882114795]
maxi score, test score, baseline:  -0.9614384615384616 -1.0 -0.9614384615384616
probs:  [0.1268133386687802, 0.1268133386687802, 0.1268133386687802, 0.6195599839936595]
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.085116662834563, 0.414883337165437, 0.414883337165437, 0.085116662834563]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.20640082243422184, 0.36028027266637974, 0.36028027266637974, 0.07303863223301883]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.0852474328045407, 0.4147525671954593, 0.4147525671954593, 0.0852474328045407]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.0852474328045407, 0.4147525671954593, 0.4147525671954593, 0.0852474328045407]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.06425812364809987, 0.3119139587839667, 0.3119139587839667, 0.3119139587839667]
siam score:  -0.44150802
siam score:  -0.44846374
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.08537802895655823, 0.08537802895655823, 0.4146219710434418, 0.4146219710434418]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3118770570756641, 0.06436882877300759, 0.3118770570756641, 0.3118770570756641]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.31217301914101603, 0.06348094257695201, 0.31217301914101603, 0.31217301914101603]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.31217301914101603, 0.06348094257695201, 0.31217301914101603, 0.31217301914101603]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.31217301914101603, 0.06348094257695201, 0.31217301914101603, 0.31217301914101603]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.3595156899745257, 0.07309003927191986, 0.3595156899745257, 0.20787858077902868]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
siam score:  -0.440338
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.31215440242782094, 0.06353679271653728, 0.31215440242782094, 0.31215440242782094]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.31215440242782094, 0.06353679271653728, 0.31215440242782094, 0.31215440242782094]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.31215440242782094, 0.06353679271653728, 0.31215440242782094, 0.31215440242782094]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.31215440242782094, 0.06353679271653728, 0.31215440242782094, 0.31215440242782094]
using another actor
from probs:  [0.31215440242782094, 0.06353679271653728, 0.31215440242782094, 0.31215440242782094]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.359309790425068, 0.07311688458489014, 0.20826353456497398, 0.359309790425068]
first move QE:  -0.33923799002697236
siam score:  -0.44345814
using another actor
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.3591218794887577, 0.20860894226288487, 0.07314729875959973, 0.3591218794887577]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.3591218794887577, 0.20860894226288487, 0.07314729875959973, 0.3591218794887577]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.29216912054827954, 0.10240807808102169, 0.10240807808102169, 0.5030147232896771]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.2921691218107617, 0.10240807366233277, 0.10240807366233277, 0.5030147308645727]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.2921691218107617, 0.10240807366233277, 0.10240807366233277, 0.5030147308645727]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.3117298201873251, 0.0648105394380247, 0.3117298201873251, 0.3117298201873251]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.3117298201873251, 0.0648105394380247, 0.3117298201873251, 0.3117298201873251]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.3117298201873251, 0.0648105394380247, 0.3117298201873251, 0.3117298201873251]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.3117298201873251, 0.0648105394380247, 0.3117298201873251, 0.3117298201873251]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.3117298201873251, 0.0648105394380247, 0.3117298201873251, 0.3117298201873251]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.3117298201873251, 0.0648105394380247, 0.3117298201873251, 0.3117298201873251]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.3117298201873251, 0.0648105394380247, 0.3117298201873251, 0.3117298201873251]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.08602828198488588, 0.08602828198488588, 0.4139717180151141, 0.4139717180151141]
siam score:  -0.47292113
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.2460019043591839, 0.086078078726534, 0.4219181125550983, 0.2460019043591839]
using another actor
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.0862871290321095, 0.0862871290321095, 0.4137128709678905, 0.4137128709678905]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.12825827211264762, 0.12825827211264762, 0.6152251836620571, 0.12825827211264762]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.2463575823406473, 0.2463575823406473, 0.4211936299895811, 0.0860912053291242]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.2463575823406473, 0.2463575823406473, 0.4211936299895811, 0.0860912053291242]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.10250515150720346, 0.2933808377919993, 0.5016088591935939, 0.10250515150720346]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.1266690732743337, 0.1266690732743337, 0.6199927801769991, 0.1266690732743337]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.24651287692073462, 0.08610521527452111, 0.4208690308840097, 0.24651287692073462]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.24651287692073462, 0.08610521527452111, 0.4208690308840097, 0.24651287692073462]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.31158316569167016, 0.06525050292498959, 0.31158316569167016, 0.31158316569167016]
Printing some Q and Qe and total Qs values:  [[1.134]
 [0.99 ]
 [1.021]
 [1.064]
 [1.134]
 [1.096]
 [1.134]] [[ 0.   ]
 [-0.033]
 [-0.026]
 [-0.023]
 [ 0.   ]
 [-0.026]
 [ 0.   ]] [[0.761]
 [0.611]
 [0.643]
 [0.686]
 [0.761]
 [0.718]
 [0.761]]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.08654527158008019, 0.08654527158008019, 0.4134547284199198, 0.4134547284199198]
from probs:  [0.08654527158008019, 0.08654527158008019, 0.4134547284199198, 0.4134547284199198]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.07338026549317807, 0.21011812446620157, 0.35825080502031015, 0.35825080502031015]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.07338026122148608, 0.21011812350162595, 0.35825080763844397, 0.35825080763844397]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.0861233839654309, 0.24665557926460066, 0.4205654575053679, 0.24665557926460066]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.0861233839654309, 0.24665557926460066, 0.4205654575053679, 0.24665557926460066]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.0861233839654309, 0.24665557926460066, 0.4205654575053679, 0.24665557926460066]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.0861233839654309, 0.24665557926460066, 0.4205654575053679, 0.24665557926460066]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.0861233839654309, 0.24665557926460066, 0.4205654575053679, 0.24665557926460066]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.24702178680895964, 0.08619827449279543, 0.4197581518892854, 0.24702178680895964]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.3114370848781043, 0.06568874536568706, 0.3114370848781043, 0.3114370848781043]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.4129405695990282, 0.08705943040097182, 0.08705943040097182, 0.4129405695990282]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.4129405695990282, 0.08705943040097182, 0.08705943040097182, 0.4129405695990282]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.4129405695990282, 0.08705943040097182, 0.08705943040097182, 0.4129405695990282]
first move QE:  -0.24424199465855345
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.4129405695990282, 0.08705943040097182, 0.08705943040097182, 0.4129405695990282]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.6126686341757244, 0.12911045527475853, 0.12911045527475853, 0.12911045527475853]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.3793068275481915, 0.07730161958328795, 0.27169577643426035, 0.27169577643426035]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.3793068275481915, 0.07730161958328795, 0.27169577643426035, 0.27169577643426035]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.3793068275481915, 0.07730161958328795, 0.27169577643426035, 0.27169577643426035]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.42166825965303745, 0.08592141986702903, 0.3020343282350347, 0.19037599224489896]
siam score:  -0.4905337
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.42166825965303745, 0.08592141986702903, 0.3020343282350347, 0.19037599224489896]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.42166825965303745, 0.08592141986702903, 0.3020343282350347, 0.19037599224489896]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.42166825965303745, 0.08592141986702903, 0.3020343282350347, 0.19037599224489896]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.4746859565109506, 0.09670958107196757, 0.2143022312085409, 0.2143022312085409]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
from probs:  [0.31136425733939294, 0.06590722798182116, 0.31136425733939294, 0.31136425733939294]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.08731546287921126, 0.08731546287921126, 0.4126845371207888, 0.4126845371207888]
using another actor
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.08731543208244953, 0.08731543208244953, 0.4126845679175504, 0.4126845679175504]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.08629666117751074, 0.2473163387078279, 0.2473163387078279, 0.41907066140683347]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.06601628521070442, 0.31132790492976514, 0.31132790492976514, 0.31132790492976514]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.21119465052860215, 0.3575462542493026, 0.3575462542493026, 0.07371284097279271]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
siam score:  -0.51118654
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.1908347720316813, 0.30229169835627107, 0.4209393941211571, 0.08593413549089048]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.1908347720316813, 0.30229169835627107, 0.4209393941211571, 0.08593413549089048]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.1908347720316813, 0.30229169835627107, 0.4209393941211571, 0.08593413549089048]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.1908347720316813, 0.30229169835627107, 0.4209393941211571, 0.08593413549089048]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.1908347720316813, 0.30229169835627107, 0.4209393941211571, 0.08593413549089048]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.1908347720316813, 0.30229169835627107, 0.4209393941211571, 0.08593413549089048]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.1908347720316813, 0.30229169835627107, 0.4209393941211571, 0.08593413549089048]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.1908347720316813, 0.30229169835627107, 0.4209393941211571, 0.08593413549089048]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.24748264747258725, 0.41866261933665716, 0.24748264747258725, 0.0863720857181684]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.24755839107764466, 0.41847101564249406, 0.08641220220221676, 0.24755839107764466]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.610144581564761, 0.12995180614507965, 0.12995180614507965, 0.12995180614507965]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.3111828189639964, 0.3111828189639964, 0.3111828189639964, 0.06645154310801077]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.3111828189639964, 0.3111828189639964, 0.3111828189639964, 0.06645154310801077]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.3111828189639964, 0.3111828189639964, 0.3111828189639964, 0.06645154310801077]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.3118205674394169, 0.3118205674394169, 0.3118205674394169, 0.06453829768174923]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.3118205674394169, 0.3118205674394169, 0.3118205674394169, 0.06453829768174923]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.3118205674394169, 0.3118205674394169, 0.3118205674394169, 0.06453829768174923]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.3118205674394169, 0.3118205674394169, 0.3118205674394169, 0.06453829768174923]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.3118205674394169, 0.3118205674394169, 0.3118205674394169, 0.06453829768174923]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.3118205674394169, 0.3118205674394169, 0.3118205674394169, 0.06453829768174923]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.3118205674394169, 0.3118205674394169, 0.3118205674394169, 0.06453829768174923]
siam score:  -0.5093643
from probs:  [0.3346795617041755, 0.3346795617041755, 0.2624937058252062, 0.0681471707664428]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.46043336357646175, 0.17837100675439646, 0.26744332996136433, 0.09375229970777742]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.46043336357646175, 0.17837100675439646, 0.26744332996136433, 0.09375229970777742]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.46043336357646175, 0.17837100675439646, 0.26744332996136433, 0.09375229970777742]
from probs:  [0.46043336357646175, 0.17837100675439646, 0.26744332996136433, 0.09375229970777742]
UNIT TEST: sample policy line 217 mcts : [0.  0.2 0.2 0.2 0.2 0.2 0. ]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.38903693121819094, 0.2262110070705325, 0.3054815227739978, 0.07927053893727883]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.3416988401267732, 0.24651776556480606, 0.3416988401267732, 0.0700845541816476]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.37567494620922914, 0.17160869691899577, 0.37567494620922914, 0.07704141066254586]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
first move QE:  -0.19536367555028084
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.37567494620922914, 0.17160869691899577, 0.37567494620922914, 0.07704141066254586]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.37567494620922914, 0.17160869691899577, 0.37567494620922914, 0.07704141066254586]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.37567494620922914, 0.17160869691899577, 0.37567494620922914, 0.07704141066254586]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.3027069893307734, 0.19166237894896856, 0.41959605289056723, 0.08603457882969102]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.24793264300315926, 0.24793264300315926, 0.4174559167440622, 0.08667879724961924]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.35686872170348044, 0.35686872170348044, 0.0741837159071772, 0.212078840685862]
from probs:  [0.35686872170348044, 0.35686872170348044, 0.0741837159071772, 0.212078840685862]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.24798427996232886, 0.4173047631267335, 0.08672667694860892, 0.24798427996232886]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
Printing some Q and Qe and total Qs values:  [[1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.178]] [[0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]] [[0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.0887111079560823, 0.4112888920439177, 0.0887111079560823, 0.4112888920439177]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.0887111079560823, 0.4112888920439177, 0.0887111079560823, 0.4112888920439177]
siam score:  -0.5418545
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.0887111079560823, 0.4112888920439177, 0.0887111079560823, 0.4112888920439177]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.0887111079560823, 0.4112888920439177, 0.0887111079560823, 0.4112888920439177]
from probs:  [0.0887111079560823, 0.4112888920439177, 0.0887111079560823, 0.4112888920439177]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.07423774724513915, 0.35680289999936526, 0.2121564527561303, 0.35680289999936526]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.07423774724513915, 0.35680289999936526, 0.2121564527561303, 0.35680289999936526]
siam score:  -0.5586836
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.07423774724513915, 0.35680289999936526, 0.2121564527561303, 0.35680289999936526]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.07423774724513915, 0.35680289999936526, 0.2121564527561303, 0.35680289999936526]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.07423774724513915, 0.35680289999936526, 0.2121564527561303, 0.35680289999936526]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.07423774724513915, 0.35680289999936526, 0.2121564527561303, 0.35680289999936526]
first move QE:  -0.18314389853609017
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.07423774724513915, 0.35680289999936526, 0.2121564527561303, 0.35680289999936526]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.07423774724513915, 0.35680289999936526, 0.2121564527561303, 0.35680289999936526]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.395]
 [0.427]
 [0.424]
 [0.429]
 [0.425]
 [0.416]] [[-0.056]
 [-0.027]
 [-0.06 ]
 [-0.075]
 [-0.09 ]
 [-0.082]
 [-0.062]] [[0.046]
 [0.048]
 [0.058]
 [0.044]
 [0.04 ]
 [0.041]
 [0.045]]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.07423774724513915, 0.35680289999936526, 0.2121564527561303, 0.35680289999936526]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.07423774724513915, 0.35680289999936526, 0.2121564527561303, 0.35680289999936526]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.07423774724513915, 0.35680289999936526, 0.2121564527561303, 0.35680289999936526]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.08677532358893322, 0.41715780114386364, 0.24803343763360164, 0.24803343763360164]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.08677532358893322, 0.41715780114386364, 0.24803343763360164, 0.24803343763360164]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.08677532358893322, 0.41715780114386364, 0.24803343763360164, 0.24803343763360164]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.408]
 [0.548]
 [0.598]
 [0.598]
 [0.59 ]
 [0.578]] [[ 0.048]
 [ 0.094]
 [ 0.089]
 [-0.001]
 [ 0.024]
 [ 0.034]
 [ 0.003]] [[ 0.082]
 [-0.04 ]
 [ 0.099]
 [ 0.118]
 [ 0.127]
 [ 0.122]
 [ 0.1  ]]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.08677532358893322, 0.41715780114386364, 0.24803343763360164, 0.24803343763360164]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
using explorer policy with actor:  1
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[1.181]
 [1.   ]
 [1.114]
 [1.175]
 [1.165]
 [1.16 ]
 [1.142]] [[-0.012]
 [-0.031]
 [-0.115]
 [-0.094]
 [-0.107]
 [-0.059]
 [-0.061]] [[1.121]
 [0.933]
 [1.02 ]
 [1.088]
 [1.073]
 [1.084]
 [1.066]]
first move QE:  -0.1774655074171457
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.1310570427497779, 0.1310570427497779, 0.1310570427497779, 0.6068288717506664]
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.08682471227734417, 0.24808029073267446, 0.24808029073267446, 0.4170147062573069]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.06731684961007697, 0.31089438346330767, 0.31089438346330767, 0.31089438346330767]
main train batch thing paused
add a thread
from probs:  [0.06731684961007697, 0.31089438346330767, 0.31089438346330767, 0.31089438346330767]
Adding thread: now have 4 threads
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.142]
 [0.153]
 [0.233]
 [0.235]
 [0.228]
 [0.251]] [[ 0.498]
 [ 0.077]
 [-0.001]
 [-0.04 ]
 [-0.014]
 [ 0.056]
 [ 0.058]] [[0.152]
 [0.142]
 [0.153]
 [0.233]
 [0.235]
 [0.228]
 [0.251]]
siam score:  -0.56501764
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.24812499743841193, 0.4168752279813336, 0.08687477714184245, 0.24812499743841193]
from probs:  [0.24812499743841193, 0.4168752279813336, 0.08687477714184245, 0.24812499743841193]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.2726888681416454, 0.37705766159321596, 0.07756460212349334, 0.2726888681416454]
first move QE:  -0.1708470909472163
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.2726888681416454, 0.37705766159321596, 0.07756460212349334, 0.2726888681416454]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.27268886870119685, 0.3770576647267054, 0.07756459787090107, 0.27268886870119685]
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.596]
 [0.744]
 [0.762]
 [0.769]
 [0.772]
 [0.718]] [[ 0.092]
 [ 0.317]
 [ 0.138]
 [ 0.029]
 [ 0.004]
 [-0.   ]
 [ 0.003]] [[0.659]
 [0.596]
 [0.744]
 [0.762]
 [0.769]
 [0.772]
 [0.718]]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.27268886870119685, 0.3770576647267054, 0.07756459787090107, 0.27268886870119685]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.27268886870119685, 0.3770576647267054, 0.07756459787090107, 0.27268886870119685]
siam score:  -0.5630193
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.28334291716506393, 0.3597876540800896, 0.07352651158978242, 0.28334291716506393]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.31192455418391096, 0.31192455418391096, 0.0642263374482672, 0.31192455418391096]
main train batch thing paused
add a thread
Adding thread: now have 5 threads
main train batch thing paused
add a thread
Adding thread: now have 6 threads
Printing some Q and Qe and total Qs values:  [[1.203]
 [1.153]
 [1.056]
 [1.091]
 [1.077]
 [1.061]
 [1.052]] [[ 0.404]
 [ 0.006]
 [-0.266]
 [-0.505]
 [-0.363]
 [-0.119]
 [-0.084]] [[1.075]
 [0.854]
 [0.659]
 [0.571]
 [0.628]
 [0.73 ]
 [0.741]]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.2727292069272953, 0.37695093625245185, 0.07759064989295758, 0.2727292069272953]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.4136413953381826, 0.08635860466181734, 0.08635860466181734, 0.4136413953381826]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
using explorer policy with actor:  1
siam score:  -0.5718215
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.35843499327144995, 0.07331920676749769, 0.20981080668960242, 0.35843499327144995]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.35843499327144995, 0.07331920676749769, 0.20981080668960242, 0.35843499327144995]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.3584349951498002, 0.073319203706969, 0.20981080599343058, 0.3584349951498002]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.3584349951498002, 0.073319203706969, 0.20981080599343058, 0.3584349951498002]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.3584349951498002, 0.073319203706969, 0.20981080599343058, 0.3584349951498002]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.3584349951498002, 0.073319203706969, 0.20981080599343058, 0.3584349951498002]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.3584349951498002, 0.073319203706969, 0.20981080599343058, 0.3584349951498002]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.3584349951498002, 0.073319203706969, 0.20981080599343058, 0.3584349951498002]
siam score:  -0.5636777
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.676]
 [0.676]
 [0.852]
 [0.676]
 [0.676]
 [0.676]] [[0.012]
 [0.012]
 [0.012]
 [0.016]
 [0.012]
 [0.012]
 [0.012]] [[-0.017]
 [-0.017]
 [-0.017]
 [ 0.162]
 [-0.017]
 [-0.017]
 [-0.017]]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.3584349969725046, 0.07331920073710796, 0.20981080531788293, 0.3584349969725046]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.3584349969725046, 0.07331920073710796, 0.20981080531788293, 0.3584349969725046]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.3584349969725046, 0.07331920073710796, 0.20981080531788293, 0.3584349969725046]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.3584349969725046, 0.07331920073710796, 0.20981080531788293, 0.3584349969725046]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.569]
 [-0.569]
 [-0.569]
 [-0.569]
 [-0.569]
 [-0.569]
 [-0.569]]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.3878930586618305, 0.07933515160250985, 0.22704904327984407, 0.30572274645581565]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.3878930586618305, 0.07933515160250985, 0.22704904327984407, 0.30572274645581565]
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.214]
 [0.214]
 [0.407]
 [0.214]
 [0.292]
 [0.183]] [[-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [ 0.236]
 [-0.02 ]
 [ 0.753]
 [ 0.776]] [[-0.254]
 [-0.254]
 [-0.254]
 [ 0.023]
 [-0.254]
 [ 0.081]
 [-0.021]]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.34126903436784267, 0.07034411129698301, 0.24711781996733165, 0.34126903436784267]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.3340960969354923, 0.06838069821494529, 0.26342710791407015, 0.3340960969354923]
using another actor
Printing some Q and Qe and total Qs values:  [[1.253]
 [1.253]
 [1.253]
 [1.229]
 [1.242]
 [1.253]
 [1.253]] [[-0.309]
 [-0.309]
 [-0.309]
 [-0.701]
 [-0.657]
 [-0.309]
 [-0.309]] [[1.157]
 [1.157]
 [1.157]
 [1.067]
 [1.087]
 [1.157]
 [1.157]]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.4208650430444031, 0.0861090403451639, 0.2465129583052165, 0.2465129583052165]
line 256 mcts: sample exp_bonus -0.1627687468644135
using another actor
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.3766502243852617, 0.07767264550857775, 0.27283856505308024, 0.27283856505308024]
Printing some Q and Qe and total Qs values:  [[1.14]
 [1.14]
 [1.14]
 [1.14]
 [1.14]
 [1.14]
 [1.14]] [[-0.197]
 [-0.197]
 [-0.197]
 [-0.197]
 [-0.197]
 [-0.197]
 [-0.197]] [[0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.3766502243852617, 0.07767264550857775, 0.27283856505308024, 0.27283856505308024]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.58371854
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.41831363367971414, 0.0862512824301507, 0.3030142061625045, 0.1924208777276306]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.41831363367971414, 0.0862512824301507, 0.3030142061625045, 0.1924208777276306]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.41831363367971414, 0.0862512824301507, 0.3030142061625045, 0.1924208777276306]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.41831363367971414, 0.0862512824301507, 0.3030142061625045, 0.1924208777276306]
Printing some Q and Qe and total Qs values:  [[0.947]
 [1.073]
 [0.979]
 [1.065]
 [1.104]
 [1.128]
 [1.123]] [[ 0.089]
 [ 0.008]
 [ 0.001]
 [-0.001]
 [ 0.071]
 [ 0.015]
 [ 0.033]] [[0.864]
 [0.95 ]
 [0.852]
 [0.937]
 [1.012]
 [1.008]
 [1.012]]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.4134469605428223, 0.08655303945717768, 0.4134469605428223, 0.08655303945717768]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.37500024121055275, 0.07738061928066539, 0.37500024121055275, 0.1726188982982291]
siam score:  -0.5813292
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.3030381863984872, 0.0862793147827045, 0.4181913369443707, 0.19249116187443752]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.3030381863984872, 0.0862793147827045, 0.4181913369443707, 0.19249116187443752]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.3030381863984872, 0.0862793147827045, 0.4181913369443707, 0.19249116187443752]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.3030381863984872, 0.0862793147827045, 0.4181913369443707, 0.19249116187443752]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.3030381863984872, 0.0862793147827045, 0.4181913369443707, 0.19249116187443752]
Printing some Q and Qe and total Qs values:  [[0.876]
 [0.875]
 [0.899]
 [0.912]
 [0.914]
 [0.905]
 [0.917]] [[0.756]
 [0.475]
 [0.448]
 [0.22 ]
 [0.267]
 [0.539]
 [0.394]] [[0.161]
 [0.113]
 [0.132]
 [0.108]
 [0.117]
 [0.154]
 [0.142]]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.3030381863984872, 0.0862793147827045, 0.4181913369443707, 0.19249116187443752]
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.04 ]
 [-0.033]
 [-0.028]
 [-0.024]
 [-0.035]
 [-0.028]] [[0.976]
 [0.614]
 [0.419]
 [0.121]
 [0.204]
 [0.154]
 [0.234]] [[-0.461]
 [-0.505]
 [-0.531]
 [-0.576]
 [-0.558]
 [-0.578]
 [-0.557]]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.21641271590197814, 0.09699126133123555, 0.470183306864808, 0.21641271590197814]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.21641271590197814, 0.09699126133123555, 0.470183306864808, 0.21641271590197814]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.21641271590197814, 0.09699126133123555, 0.470183306864808, 0.21641271590197814]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.21641271590197814, 0.09699126133123555, 0.470183306864808, 0.21641271590197814]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.24835542359703025, 0.08718693610600403, 0.4161022166999355, 0.24835542359703025]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.1039217159541493, 0.1039217159541493, 0.49607818319133223, 0.29607838490036914]
Printing some Q and Qe and total Qs values:  [[1.187]
 [1.118]
 [1.192]
 [1.206]
 [1.214]
 [1.223]
 [1.223]] [[ 0.1  ]
 [ 0.157]
 [ 0.   ]
 [-0.032]
 [ 0.064]
 [ 0.314]
 [ 0.231]] [[0.378]
 [0.319]
 [0.366]
 [0.375]
 [0.4  ]
 [0.45 ]
 [0.436]]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.08983751776807675, 0.08983751776807675, 0.41016248223192325, 0.41016248223192325]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.132143835964629, 0.132143835964629, 0.132143835964629, 0.6035684921061129]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.132143835964629, 0.132143835964629, 0.132143835964629, 0.6035684921061129]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.08724060435349089, 0.24838852083518315, 0.24838852083518315, 0.41598235397614286]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.07775956659250952, 0.2729328622383913, 0.2729328622383913, 0.37637470893070785]
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.745]
 [0.752]
 [0.74 ]
 [0.745]
 [0.743]
 [0.741]] [[-0.101]
 [-0.328]
 [-0.601]
 [-0.63 ]
 [-0.337]
 [-0.237]
 [-0.125]] [[0.737]
 [0.745]
 [0.752]
 [0.74 ]
 [0.745]
 [0.743]
 [0.741]]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.0965942130698736, 0.3391330066352903, 0.0965942130698736, 0.46767856722496254]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.0965942130698736, 0.3391330066352903, 0.0965942130698736, 0.46767856722496254]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.0965942130698736, 0.3391330066352903, 0.0965942130698736, 0.46767856722496254]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.08674715500462671, 0.41325284499537335, 0.08674715500462671, 0.41325284499537335]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.10404484733650347, 0.2961406611645887, 0.10404484733650347, 0.4957696441624044]
line 256 mcts: sample exp_bonus -0.0398876076149295
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.08681177674548321, 0.41318822325451676, 0.08681177674548321, 0.41318822325451676]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.0868117696236874, 0.4131882303763126, 0.0868117696236874, 0.4131882303763126]
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.0868117696236874, 0.4131882303763126, 0.0868117696236874, 0.4131882303763126]
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.0868117696236874, 0.4131882303763126, 0.0868117696236874, 0.4131882303763126]
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.0868117696236874, 0.4131882303763126, 0.0868117696236874, 0.4131882303763126]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.463]
 [0.644]
 [0.636]
 [0.672]
 [0.69 ]
 [0.659]] [[0.13 ]
 [0.767]
 [0.184]
 [0.034]
 [0.11 ]
 [0.323]
 [0.199]] [[0.592]
 [0.463]
 [0.644]
 [0.636]
 [0.672]
 [0.69 ]
 [0.659]]
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.10410676039362778, 0.49561773250186597, 0.10410676039362778, 0.29616874671087867]
Printing some Q and Qe and total Qs values:  [[1.169]
 [1.173]
 [1.197]
 [1.211]
 [1.221]
 [1.226]
 [1.231]] [[ 0.003]
 [-0.168]
 [-0.227]
 [-0.206]
 [-0.096]
 [-0.01 ]
 [-0.001]] [[1.202]
 [1.177]
 [1.191]
 [1.209]
 [1.237]
 [1.256]
 [1.263]]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[0.114]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[0.682]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]]
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.10416890853627316, 0.4954672409109949, 0.2961949420164589, 0.10416890853627316]
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.10416890853627316, 0.4954672409109949, 0.2961949420164589, 0.10416890853627316]
Printing some Q and Qe and total Qs values:  [[1.251]
 [1.221]
 [1.221]
 [1.221]
 [1.197]
 [1.19 ]
 [1.205]] [[-0.108]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.051]
 [-0.052]
 [-0.046]] [[0.855]
 [0.854]
 [0.854]
 [0.854]
 [0.82 ]
 [0.813]
 [0.83 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.09033377195687892, 0.4096662280431211, 0.4096662280431211, 0.09033377195687892]
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.77 ]
 [0.77 ]
 [0.743]
 [0.692]
 [0.77 ]
 [0.77 ]] [[0.466]
 [0.466]
 [0.466]
 [0.056]
 [0.141]
 [0.466]
 [0.466]] [[0.441]
 [0.441]
 [0.441]
 [0.346]
 [0.309]
 [0.441]
 [0.441]]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.09033377195687892, 0.4096662280431211, 0.4096662280431211, 0.09033377195687892]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.09033377195687892, 0.4096662280431211, 0.4096662280431211, 0.09033377195687892]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.09033377195687892, 0.4096662280431211, 0.4096662280431211, 0.09033377195687892]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.09033377195687892, 0.4096662280431211, 0.4096662280431211, 0.09033377195687892]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.1326804758142523, 0.1326804758142523, 0.6019585725572429, 0.1326804758142523]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.1326804758142523, 0.1326804758142523, 0.6019585725572429, 0.1326804758142523]
siam score:  -0.60867006
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.504]
 [0.919]
 [0.965]
 [0.963]
 [0.94 ]
 [0.92 ]] [[-0.049]
 [-0.003]
 [-0.091]
 [-0.396]
 [-0.317]
 [-0.258]
 [-0.103]] [[0.425]
 [0.063]
 [0.449]
 [0.393]
 [0.418]
 [0.415]
 [0.446]]
siam score:  -0.6225772
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.12897992462862254, 0.12897992462862254, 0.6130602261141324, 0.12897992462862254]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.21677488588999927, 0.09716447509399762, 0.4692857531260038, 0.21677488588999927]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.21677488588999927, 0.09716447509399762, 0.4692857531260038, 0.21677488588999927]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.21677488588999927, 0.09716447509399762, 0.4692857531260038, 0.21677488588999927]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.21677488588999927, 0.09716447509399762, 0.4692857531260038, 0.21677488588999927]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.21677488588999927, 0.09716447509399762, 0.4692857531260038, 0.21677488588999927]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.21677488588999927, 0.09716447509399762, 0.4692857531260038, 0.21677488588999927]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.21677488588999927, 0.09716447509399762, 0.4692857531260038, 0.21677488588999927]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.21677488588999927, 0.09716447509399762, 0.4692857531260038, 0.21677488588999927]
first move QE:  -0.13529416397260066
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.21677488588999927, 0.09716447509399762, 0.4692857531260038, 0.21677488588999927]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.2730631862120639, 0.0779131490330647, 0.3759604785428077, 0.2730631862120639]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.704]
 [0.703]] [[-0.017]
 [-0.017]
 [-0.   ]
 [-0.017]
 [-0.017]
 [ 0.004]
 [-0.017]] [[0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.704]
 [0.703]]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.35594346077810435, 0.07507009964545523, 0.35594346077810435, 0.21304297879833609]
Starting evaluation
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.35594346936864574, 0.07507008546107298, 0.35594346936864574, 0.21304297580163545]
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]]
first move QE:  -0.13353342066778118
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.4129301267762022, 0.08706987322379783, 0.4129301267762022, 0.08706987322379783]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.4129301267762022, 0.08706987322379783, 0.4129301267762022, 0.08706987322379783]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.5  ]
 [0.453]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[-0.004]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[0.531]
 [0.5  ]
 [0.453]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]]
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.4129301339430513, 0.08706986605694877, 0.4129301339430513, 0.08706986605694877]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.4129301339430513, 0.08706986605694877, 0.4129301339430513, 0.08706986605694877]
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.725]
 [0.728]
 [0.732]
 [0.73 ]
 [0.729]
 [0.72 ]] [[-0.005]
 [-0.004]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.028]
 [0.027]
 [0.029]
 [0.034]
 [0.032]
 [0.031]
 [0.022]]
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.705]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
using explorer policy with actor:  0
using another actor
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.631]
 [0.645]
 [0.658]
 [0.668]
 [0.659]
 [0.649]] [[0.292]
 [0.361]
 [0.19 ]
 [0.053]
 [0.006]
 [0.043]
 [0.065]] [[0.216]
 [0.207]
 [0.192]
 [0.182]
 [0.185]
 [0.182]
 [0.175]]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus -0.005190771171310916
Printing some Q and Qe and total Qs values:  [[1.027]
 [0.792]
 [0.87 ]
 [0.91 ]
 [0.91 ]
 [0.666]
 [0.819]] [[-0.001]
 [-0.001]
 [ 0.   ]
 [ 0.   ]
 [-0.   ]
 [ 0.   ]
 [-0.   ]] [[0.647]
 [0.411]
 [0.49 ]
 [0.529]
 [0.529]
 [0.285]
 [0.439]]
maxi score, test score, baseline:  -0.9879952380952381 -1.0 -0.9879952380952381
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
probs:  [0.09082715889215869, 0.09082715889215869, 0.4091728411078413, 0.4091728411078413]
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
probs:  [0.09082714905612181, 0.09082714905612181, 0.4091728509438782, 0.4091728509438782]
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.09082713943046294, 0.09082713943046294, 0.40917286056953706, 0.40917286056953706]
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.13321241374126497, 0.13321241374126497, 0.13321241374126497, 0.600362758776205]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.13321240421461547, 0.13321240421461547, 0.13321240421461547, 0.6003627873561536]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.08768193048460546, 0.24861266607251847, 0.24861266607251847, 0.4150927373703575]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.22794823283518034, 0.3059208530167837, 0.07954872990890449, 0.3865821842391315]
siam score:  -0.71550333
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.1649696162151357, 0.3308780590205464, 0.08602930875127109, 0.41812301601304686]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.09137625216829444, 0.37069852538996995, 0.09137625216829444, 0.44654897027344126]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.003293112156819552
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.1480607935119534, 0.41863038253154916, 0.08570295854259274, 0.34760586541390476]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.1589781822977107, 0.4495186708743272, 0.09201767907106827, 0.2994854677568939]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.1589781822977107, 0.4495186708743272, 0.09201767907106827, 0.2994854677568939]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.1589781822977107, 0.4495186708743272, 0.09201767907106827, 0.2994854677568939]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.51 ]
 [0.501]
 [0.491]
 [0.496]
 [0.499]
 [0.503]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.502]
 [0.51 ]
 [0.501]
 [0.491]
 [0.496]
 [0.499]
 [0.503]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.18027185284800593, 0.4564008826874406, 0.09398153102318133, 0.26934573344137214]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.18027185284800593, 0.4564008826874406, 0.09398153102318133, 0.26934573344137214]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.22868022496059898, 0.4504058853703731, 0.09223366470842881, 0.22868022496059898]
siam score:  -0.78261274
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.24736002172864735, 0.4189586093665849, 0.08632134717612042, 0.24736002172864735]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.24736002172864735, 0.4189586093665849, 0.08632134717612042, 0.24736002172864735]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.24736002172864735, 0.4189586093665849, 0.08632134717612042, 0.24736002172864735]
deleting a thread, now have 5 threads
Frames:  11459 train batches done:  1332 episodes:  308
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.30324358307210625, 0.41674258670006475, 0.086722406920308, 0.1932914233075211]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.30324358307210625, 0.41674258670006475, 0.086722406920308, 0.1932914233075211]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.30324358307210625, 0.41674258670006475, 0.086722406920308, 0.1932914233075211]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.30324358307210625, 0.41674258670006475, 0.086722406920308, 0.1932914233075211]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.3555954939683784, 0.3555954939683784, 0.07546651515071118, 0.213342496912532]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.3555954939683784, 0.3555954939683784, 0.07546651515071118, 0.213342496912532]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.003700485518876812
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.2963729402591602, 0.49402809546213344, 0.10479948213935326, 0.10479948213935326]
STARTED EXPV TRAINING ON FRAME NO.  11715
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.2963729402591602, 0.49402809546213344, 0.10479948213935326, 0.10479948213935326]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.2963729402591602, 0.49402809546213344, 0.10479948213935326, 0.10479948213935326]
deleting a thread, now have 4 threads
Frames:  11715 train batches done:  1365 episodes:  313
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.2963729402591602, 0.49402809546213344, 0.10479948213935326, 0.10479948213935326]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.2963729402591602, 0.49402809546213344, 0.10479948213935326, 0.10479948213935326]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.2963729402591602, 0.49402809546213344, 0.10479948213935326, 0.10479948213935326]
siam score:  -0.8097224
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.2963729402591602, 0.49402809546213344, 0.10479948213935326, 0.10479948213935326]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.2963729402591602, 0.49402809546213344, 0.10479948213935326, 0.10479948213935326]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.2747],
        [-0.4352],
        [-0.5137],
        [-0.5105],
        [-0.4706],
        [-0.5070],
        [-0.3109],
        [-0.0000],
        [-0.4971],
        [-0.4013]], dtype=torch.float64)
-0.032346567066 -0.3070134826970311
-0.08397170119799999 -0.5191443608070451
-0.09703970119800001 -0.6107226144069141
-0.032346567066 -0.5428424788701532
-0.09703970119800001 -0.5676781827244477
-0.032346567066 -0.5393396735484114
-0.09703970119800001 -0.40791582615940314
-0.8136953615999999 -0.8136953615999999
-0.032346567066 -0.5294377849418923
-0.083839701198 -0.4851892740253274
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.2963729402591602, 0.49402809546213344, 0.10479948213935326, 0.10479948213935326]
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.65 ]
 [0.659]
 [0.655]
 [0.654]
 [0.654]
 [0.651]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[1.212]
 [1.21 ]
 [1.22 ]
 [1.216]
 [1.214]
 [1.214]
 [1.211]]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.2963729402591602, 0.49402809546213344, 0.10479948213935326, 0.10479948213935326]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.2963729402591602, 0.49402809546213344, 0.10479948213935326, 0.10479948213935326]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.29637294203353143, 0.4940281047994075, 0.10479947658353056, 0.10479947658353056]
line 256 mcts: sample exp_bonus 0.006625627320006606
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.565]
 [0.586]
 [0.575]
 [0.58 ]
 [0.577]
 [0.575]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.453]
 [0.45 ]
 [0.472]
 [0.46 ]
 [0.466]
 [0.462]
 [0.461]]
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.29637294203353143, 0.4940281047994075, 0.10479947658353056, 0.10479947658353056]
using explorer policy with actor:  1
siam score:  -0.8031284
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.13400246854660341, 0.5979925943601897, 0.13400246854660341, 0.13400246854660341]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.555]
 [0.578]
 [0.6  ]
 [0.665]
 [0.585]
 [0.573]] [[0.002]
 [0.001]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.147]
 [1.143]
 [1.166]
 [1.188]
 [1.253]
 [1.174]
 [1.162]]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8164741
UNIT TEST: sample policy line 217 mcts : [0.082 0.122 0.061 0.082 0.122 0.49  0.041]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.31011277012464583, 0.06966168962606258, 0.31011277012464583, 0.31011277012464583]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.31011277012464583, 0.06966168962606258, 0.31011277012464583, 0.31011277012464583]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.31126950905671963, 0.06619147282984116, 0.31126950905671963, 0.31126950905671963]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.17]
 [-0.17]
 [-0.17]
 [-0.17]
 [-0.17]
 [-0.17]
 [-0.17]]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.31126950905671963, 0.06619147282984116, 0.31126950905671963, 0.31126950905671963]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.31126950905671963, 0.06619147282984116, 0.31126950905671963, 0.31126950905671963]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.21341958058330054, 0.07558031901312807, 0.3555000502017857, 0.3555000502017857]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
Printing some Q and Qe and total Qs values:  [[1.132]
 [1.126]
 [1.126]
 [1.126]
 [1.126]
 [1.126]
 [1.126]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[1.135]
 [1.129]
 [1.129]
 [1.129]
 [1.129]
 [1.129]
 [1.129]]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.24878299899408268, 0.24878299899408268, 0.08813886621301745, 0.4142951357988172]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
line 256 mcts: sample exp_bonus -0.00516481764793265
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.31123300195453596, 0.31123300195453596, 0.06630099413639218, 0.31123300195453596]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.31123300195453596, 0.31123300195453596, 0.06630099413639218, 0.31123300195453596]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
Printing some Q and Qe and total Qs values:  [[1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]] [[-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]] [[0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.257]
 [0.257]
 [0.266]
 [0.257]
 [0.263]
 [0.264]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.135]
 [-0.135]
 [-0.135]
 [-0.126]
 [-0.135]
 [-0.129]
 [-0.129]]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.31123300195453596, 0.31123300195453596, 0.06630099413639218, 0.31123300195453596]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.412223594997884, 0.412223594997884, 0.08777640500211605, 0.08777640500211605]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.412223594997884, 0.412223594997884, 0.08777640500211605, 0.08777640500211605]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.412223594997884, 0.412223594997884, 0.08777640500211605, 0.08777640500211605]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.374023715726213, 0.374023715726213, 0.17387016873290143, 0.07808239981467258]
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.246]
 [0.187]
 [0.182]
 [0.183]
 [0.186]
 [0.192]] [[0.001]
 [0.001]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.192]
 [0.246]
 [0.187]
 [0.182]
 [0.183]
 [0.186]
 [0.192]]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.374023715726213, 0.374023715726213, 0.17387016873290143, 0.07808239981467258]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.3573350261792266, 0.3573350261792266, 0.21148264610793333, 0.07384730153361346]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.3059810580102632, 0.3858969036261398, 0.22838161371658744, 0.07974042464700955]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.351]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.3404156393508432, 0.3404156393508432, 0.2480485833233638, 0.07112013797494982]
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.025]
 [0.062]
 [0.061]
 [0.06 ]
 [0.056]
 [0.058]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.061]
 [0.025]
 [0.062]
 [0.061]
 [0.06 ]
 [0.056]
 [0.058]]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.3404156393508432, 0.3404156393508432, 0.2480485833233638, 0.07112013797494982]
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.53 ]
 [0.54 ]
 [0.535]
 [0.534]
 [0.531]
 [0.531]] [[-0.003]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.528]
 [0.53 ]
 [0.54 ]
 [0.535]
 [0.534]
 [0.531]
 [0.531]]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.3404156393508432, 0.3404156393508432, 0.2480485833233638, 0.07112013797494982]
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
siam score:  -0.84683657
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.385]
 [0.418]
 [0.428]
 [0.425]
 [0.426]
 [0.427]] [[-0.003]
 [-0.001]
 [-0.002]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.203]
 [0.163]
 [0.195]
 [0.205]
 [0.202]
 [0.203]
 [0.204]]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.33334953767890924, 0.33334953767890924, 0.2643706099446398, 0.06893031469754177]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[1.233]
 [1.233]
 [1.233]
 [1.233]
 [1.233]
 [1.233]
 [1.233]] [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]]
deleting a thread, now have 3 threads
Frames:  12821 train batches done:  1500 episodes:  340
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.37393408409258505, 0.17397320891799375, 0.37393408409258505, 0.07815862289683619]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.37393408409258505, 0.17397320891799375, 0.37393408409258505, 0.07815862289683619]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
first move QE:  -0.10812074573965418
using another actor
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.37393408409258505, 0.17397320891799375, 0.37393408409258505, 0.07815862289683619]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.37393408409258505, 0.17397320891799375, 0.37393408409258505, 0.07815862289683619]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.30328426905166245, 0.19360279043195747, 0.4161449209646898, 0.08696801955169027]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.3059889030592485, 0.22846403062177068, 0.38576087324853797, 0.07978619307044288]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.3059889030592485, 0.22846403062177068, 0.38576087324853797, 0.07978619307044288]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.24766404549974336, 0.24766404549974336, 0.41818872401848745, 0.08648318498202577]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.27331854082913676, 0.27331854082913676, 0.3749456142934353, 0.07841730404829116]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.27331854082913676, 0.27331854082913676, 0.3749456142934353, 0.07841730404829116]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.27331854082913676, 0.27331854082913676, 0.3749456142934353, 0.07841730404829116]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.30328777529648415, 0.19364391309997286, 0.41606431927004106, 0.0870039923335019]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.21748799837875296, 0.21748799837875296, 0.46731706346833707, 0.09770693977415695]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.24708374083203008, 0.11099164632679252, 0.5309329665143848, 0.11099164632679252]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.2697356770339469, 0.09419292743696352, 0.4553094408936139, 0.18076195463547565]
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.295]
 [0.356]
 [0.303]
 [0.306]
 [0.314]
 [0.595]] [[-0.007]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.003]] [[0.277]
 [0.295]
 [0.356]
 [0.303]
 [0.306]
 [0.314]
 [0.595]]
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.26973567742098686, 0.09419292438140324, 0.45530944491997444, 0.18076195327763536]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.26973567742098686, 0.09419292438140324, 0.45530944491997444, 0.18076195327763536]
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.30329077884886846, 0.08704015698652041, 0.4159847648898102, 0.1936842992748008]
line 256 mcts: sample exp_bonus 0.018131396833336547
using another actor
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]] [[0.004]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]] [[0.48 ]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11715
siam score:  -0.8667936
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.3551320966903767, 0.07603702386211146, 0.21369878275713508, 0.3551320966903767]
line 256 mcts: sample exp_bonus 0.008097892897669226
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.3551320966903767, 0.07603702386211146, 0.21369878275713508, 0.3551320966903767]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.3551320966903767, 0.07603702386211146, 0.21369878275713508, 0.3551320966903767]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.3551320966903767, 0.07603702386211146, 0.21369878275713508, 0.3551320966903767]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.0027970172507222743
from probs:  [0.10544082732983356, 0.10544082732983356, 0.29644208714742853, 0.4926762581929044]
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.155]
 [0.155]
 [0.15 ]
 [0.151]
 [0.152]
 [0.155]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.115]
 [0.115]
 [0.115]
 [0.109]
 [0.11 ]
 [0.112]
 [0.115]]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.065]
 [0.073]
 [0.07 ]
 [0.068]
 [0.067]
 [0.067]] [[-0.]
 [ 0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[0.068]
 [0.065]
 [0.073]
 [0.07 ]
 [0.068]
 [0.067]
 [0.067]]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.07988049454878089, 0.22861675357849778, 0.3059998072628773, 0.3855029446098441]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.08657212176600534, 0.2477915151589999, 0.2477915151589999, 0.41784484791599485]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.09426297788331277, 0.1808912267049191, 0.26982956216176746, 0.4550162332500006]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.10345639450105255, 0.19854691184703635, 0.19854691184703635, 0.4994497818048747]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.10345639450105255, 0.19854691184703635, 0.19854691184703635, 0.4994497818048747]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.10345639450105255, 0.19854691184703635, 0.19854691184703635, 0.4994497818048747]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.11115826445057866, 0.2472415549228591, 0.11115826445057866, 0.5304419161759836]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.10556983064277492, 0.29644670644527443, 0.10556983064277492, 0.4924136322691758]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.10556983064277492, 0.29644670644527443, 0.10556983064277492, 0.4924136322691758]
siam score:  -0.86650205
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.1355536663929872, 0.1355536663929872, 0.1355536663929872, 0.5933390008210385]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.1355536663929872, 0.1355536663929872, 0.1355536663929872, 0.5933390008210385]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8638799
Printing some Q and Qe and total Qs values:  [[1.134]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]] [[0.379]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]] [[1.033]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.21382402330820643, 0.35495499410582293, 0.35495499410582293, 0.07626598848014766]
line 256 mcts: sample exp_bonus 0.2989308912709356
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.21382402330820643, 0.35495499410582293, 0.35495499410582293, 0.07626598848014766]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.508]
 [0.278]
 [0.389]
 [0.384]
 [0.384]
 [0.384]] [[0.29 ]
 [0.484]
 [0.541]
 [0.242]
 [0.29 ]
 [0.29 ]
 [0.29 ]] [[0.057]
 [0.278]
 [0.077]
 [0.038]
 [0.057]
 [0.057]
 [0.057]]
from probs:  [0.21382402330820643, 0.35495499410582293, 0.35495499410582293, 0.07626598848014766]
Printing some Q and Qe and total Qs values:  [[0.891]
 [0.887]
 [0.888]
 [0.893]
 [0.893]
 [0.884]
 [0.891]] [[0.04 ]
 [0.036]
 [0.038]
 [0.023]
 [0.021]
 [0.031]
 [0.047]] [[0.891]
 [0.887]
 [0.888]
 [0.893]
 [0.893]
 [0.884]
 [0.891]]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.2733923461288892, 0.3745484374970627, 0.2733923461288892, 0.07866687024515878]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.35491133118800444, 0.35491133118800444, 0.21385407917051946, 0.07632325845347174]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.24897351167658863, 0.41321164342234024, 0.24897351167658863, 0.08884133322448258]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.24897351167658863, 0.41321164342234024, 0.24897351167658863, 0.08884133322448258]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.24897351167658863, 0.41321164342234024, 0.24897351167658863, 0.08884133322448258]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.24897351167658863, 0.41321164342234024, 0.24897351167658863, 0.08884133322448258]
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]] [[4.907]
 [4.907]
 [4.907]
 [4.907]
 [4.907]
 [4.907]
 [4.907]] [[0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.09337525597694192, 0.4066247440230581, 0.4066247440230581, 0.09337525597694192]
siam score:  -0.86756074
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8690541
from probs:  [0.3109964230375853, 0.3109964230375853, 0.06701073088724417, 0.3109964230375853]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.3548246594631344, 0.3548246594631344, 0.076437858921696, 0.21391282215203505]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.3548246594631344, 0.3548246594631344, 0.076437858921696, 0.21391282215203505]
from probs:  [0.41304077916515347, 0.24899974982107229, 0.0889597211927021, 0.24899974982107229]
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.424]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[0.858]
 [1.024]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[0.899]
 [1.097]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.581]
 [0.581]
 [0.584]
 [0.581]
 [0.581]
 [0.581]] [[0.307]
 [0.219]
 [0.219]
 [0.131]
 [0.219]
 [0.219]
 [0.219]] [[0.374]
 [0.334]
 [0.334]
 [0.289]
 [0.334]
 [0.334]
 [0.334]]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.38503404566283794, 0.22888193993356412, 0.08007816859155112, 0.3060058458120469]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.38503404566283794, 0.22888193993356412, 0.08007816859155112, 0.3060058458120469]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.41721937113993857, 0.24800929320071496, 0.08676204245863153, 0.24800929320071496]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.49138354853149174, 0.29644196539442136, 0.10608724303704341, 0.10608724303704341]
from probs:  [0.3094165150471299, 0.3094165150471299, 0.3094165150471299, 0.07175045485861022]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.3546115389290303, 0.07672451660017698, 0.3546115389290303, 0.21405240554176233]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.3093820616284926, 0.07185381511452221, 0.3093820616284926, 0.3093820616284926]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.3093820616284926, 0.07185381511452221, 0.3093820616284926, 0.3093820616284926]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.09420888303285442, 0.09420888303285442, 0.4057911169671456, 0.4057911169671456]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.07678187068438111, 0.2140791897732587, 0.35456946977118015, 0.35456946977118015]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.24908214888207392, 0.24908214888207392, 0.4124596478729536, 0.0893760543628986]
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.611]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[1.095]
 [1.575]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]] [[ 0.077]
 [ 0.467]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.5884240679011746, 0.13719197736627514, 0.13719197736627514, 0.13719197736627514]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.49062919802607097, 0.10647615461416224, 0.10647615461416224, 0.2964184927456046]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.681]
 [0.712]
 [0.692]
 [0.701]
 [0.714]
 [0.716]] [[3.246]
 [3.508]
 [3.936]
 [4.281]
 [4.789]
 [4.177]
 [4.749]] [[0.345]
 [0.389]
 [0.492]
 [0.529]
 [0.623]
 [0.534]
 [0.632]]
line 256 mcts: sample exp_bonus 2.647107536574595
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.49062921010122224, 0.10647614741190953, 0.10647614741190953, 0.2964184950749587]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.4906292219808362, 0.10647614032628637, 0.10647614032628637, 0.29641849736659104]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.4906292219808362, 0.10647614032628637, 0.10647614032628637, 0.29641849736659104]
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.283]
 [0.293]
 [0.292]
 [0.267]
 [0.161]
 [0.284]] [[4.369]
 [3.775]
 [4.314]
 [5.855]
 [5.59 ]
 [5.778]
 [5.824]] [[-0.228]
 [-0.311]
 [-0.211]
 [ 0.046]
 [-0.024]
 [-0.099]
 [ 0.032]]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.4053181959634671, 0.09468180403653288, 0.09468180403653288, 0.4053181959634671]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[-0.058]
 [-0.054]
 [-0.058]
 [-0.059]
 [-0.058]
 [-0.059]
 [-0.056]] [[3.605]
 [2.551]
 [3.666]
 [3.516]
 [3.472]
 [2.931]
 [3.421]] [[-0.467]
 [-0.64 ]
 [-0.457]
 [-0.483]
 [-0.49 ]
 [-0.581]
 [-0.496]]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.44 ]
 [0.471]
 [0.476]
 [0.467]
 [0.48 ]
 [0.474]] [[-0.422]
 [ 0.613]
 [-0.599]
 [-0.581]
 [-0.391]
 [-0.483]
 [-0.1  ]] [[0.355]
 [0.496]
 [0.325]
 [0.333]
 [0.355]
 [0.354]
 [0.411]]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.07458675773227778, 0.2841512507069902, 0.2841512507069902, 0.35711074085374184]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.06596426756214083, 0.3113452441459531, 0.3113452441459531, 0.3113452441459531]
siam score:  -0.857866
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.489]
 [0.489]
 [0.611]
 [0.489]
 [0.489]
 [0.489]] [[1.122]
 [1.272]
 [1.272]
 [1.272]
 [1.272]
 [1.272]
 [1.272]] [[0.215]
 [0.315]
 [0.315]
 [0.436]
 [0.315]
 [0.315]
 [0.315]]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.09825585068690908, 0.46403214073418514, 0.33945615789199685, 0.09825585068690908]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.09825585068690908, 0.46403214073418514, 0.33945615789199685, 0.09825585068690908]
siam score:  -0.8605406
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.20117775673730218, 0.39124448572473147, 0.326511614257998, 0.08106614327996842]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.2291804148761506, 0.38446634301794513, 0.3059885083656414, 0.08036473374026282]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.2291804148761506, 0.38446634301794513, 0.3059885083656414, 0.08036473374026282]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.2291804148761506, 0.38446634301794513, 0.3059885083656414, 0.08036473374026282]
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.616]
 [0.606]
 [0.608]
 [0.609]
 [0.61 ]
 [0.61 ]] [[2.509]
 [3.299]
 [2.939]
 [3.052]
 [3.022]
 [3.001]
 [2.92 ]] [[0.345]
 [0.613]
 [0.483]
 [0.523]
 [0.514]
 [0.507]
 [0.48 ]]
Printing some Q and Qe and total Qs values:  [[1.13 ]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[0.256]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]] [[0.933]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.24857635778844833, 0.33968945932781847, 0.33968945932781847, 0.07204472355591462]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.19443330135893863, 0.3032628473647299, 0.41443281371473106, 0.08787103756160046]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
Printing some Q and Qe and total Qs values:  [[1.437]
 [1.41 ]
 [1.436]
 [1.41 ]
 [1.434]
 [1.435]
 [1.436]] [[0.009]
 [0.01 ]
 [0.007]
 [0.01 ]
 [0.006]
 [0.006]
 [0.007]] [[1.44 ]
 [1.414]
 [1.438]
 [1.414]
 [1.436]
 [1.436]
 [1.438]]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.2963885952918295, 0.10680042322957194, 0.4900105582490267, 0.10680042322957194]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.2963885952918295, 0.10680042322957194, 0.4900105582490267, 0.10680042322957194]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.2963885952918295, 0.10680042322957194, 0.4900105582490267, 0.10680042322957194]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.2963885952918295, 0.10680042322957194, 0.4900105582490267, 0.10680042322957194]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.30325841029939876, 0.08790996421817537, 0.41436904471364633, 0.19446258076877965]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.30325841029939876, 0.08790996421817537, 0.41436904471364633, 0.19446258076877965]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.30325841029939876, 0.08790996421817537, 0.41436904471364633, 0.19446258076877965]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.30325841029939876, 0.08790996421817537, 0.41436904471364633, 0.19446258076877965]
using explorer policy with actor:  1
siam score:  -0.8547205
using explorer policy with actor:  1
deleting a thread, now have 2 threads
Frames:  15885 train batches done:  1862 episodes:  468
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.10693012984146365, 0.2963743717065611, 0.10693012984146365, 0.4897653686105116]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.10693012984146365, 0.2963743717065611, 0.10693012984146365, 0.4897653686105116]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.10693012984146365, 0.2963743717065611, 0.10693012984146365, 0.4897653686105116]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.10693012984146365, 0.2963743717065611, 0.10693012984146365, 0.4897653686105116]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.08047213276225602, 0.3059766348321058, 0.22927442324372252, 0.3842768091619157]
siam score:  -0.8551407
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.08047213276225602, 0.3059766348321058, 0.22927442324372252, 0.3842768091619157]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.08047213276225602, 0.3059766348321058, 0.22927442324372252, 0.3842768091619157]
using explorer policy with actor:  1
siam score:  -0.8578888
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.08047213276225602, 0.3059766348321058, 0.22927442324372252, 0.3842768091619157]
siam score:  -0.85976213
using explorer policy with actor:  1
siam score:  -0.8597312
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.07924712925069498, 0.3728135698442899, 0.1751257310607252, 0.3728135698442899]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.366]
 [0.358]
 [0.373]
 [0.37 ]
 [0.373]
 [0.369]] [[4.585]
 [4.976]
 [4.639]
 [4.441]
 [4.433]
 [4.493]
 [4.624]] [[ 0.017]
 [ 0.068]
 [ 0.003]
 [-0.014]
 [-0.018]
 [-0.006]
 [ 0.013]]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.08802715990920715, 0.4141803636646188, 0.19454857662393873, 0.30324389980223526]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.07747007163142469, 0.3540767584085734, 0.21437641155142836, 0.3540767584085734]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.07747007163142469, 0.3540767584085734, 0.21437641155142836, 0.3540767584085734]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.381]
 [0.328]
 [0.348]
 [0.348]
 [0.411]
 [0.368]] [[1.707]
 [2.246]
 [1.69 ]
 [1.878]
 [2.101]
 [1.69 ]
 [1.799]] [[ 0.033]
 [ 0.313]
 [-0.019]
 [ 0.096]
 [ 0.207]
 [ 0.065]
 [ 0.076]]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.07223186679396848, 0.3395591354632919, 0.24864986227944777, 0.3395591354632919]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.07223186679396848, 0.3395591354632919, 0.24864986227944777, 0.3395591354632919]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.07223186679396848, 0.3395591354632919, 0.24864986227944777, 0.3395591354632919]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.07223186679396848, 0.3395591354632919, 0.24864986227944777, 0.3395591354632919]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.07223186679396848, 0.3395591354632919, 0.24864986227944777, 0.3395591354632919]
actor:  1 policy actor:  1  step number:  94 total reward:  0.19333333333333236  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
siam score:  -0.8535142
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.12220157888783888, 0.13221256478319843, 0.6099000923575782, 0.13568576397138435]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.12261868026320112, 0.12924851068104332, 0.6119838626336929, 0.13614894642206268]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.419]] [[1.929]
 [2.307]
 [2.307]
 [2.08 ]
 [2.307]
 [2.307]
 [2.311]] [[0.314]
 [0.499]
 [0.499]
 [0.385]
 [0.499]
 [0.499]
 [0.499]]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[1.621]
 [1.621]
 [1.621]
 [1.621]
 [1.621]
 [1.621]
 [1.621]] [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.12249717385037309, 0.12946004713415651, 0.6113356596879883, 0.13670711932748206]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.12249717385037309, 0.12946004713415651, 0.6113356596879883, 0.13670711932748206]
first move QE:  -0.02234578916865412
from probs:  [0.12249712468131062, 0.12946013237853993, 0.611335398468575, 0.1367073444715745]
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
probs:  [0.12249707626731846, 0.12946021631387578, 0.6113351412605541, 0.1367075661582518]
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
probs:  [0.12249707626731846, 0.12946021631387578, 0.6113351412605541, 0.1367075661582518]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12249702859113612, 0.12946029897008765, 0.6113348879722288, 0.1367077844665474]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12249702859113612, 0.12946029897008765, 0.6113348879722288, 0.1367077844665474]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12249702859113612, 0.12946029897008765, 0.6113348879722288, 0.1367077844665474]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12249702859113612, 0.12946029897008765, 0.6113348879722288, 0.1367077844665474]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12245545524585995, 0.12953267602167995, 0.611113105780845, 0.13689876295161504]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12245545524585995, 0.12953267602167995, 0.611113105780845, 0.13689876295161504]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12241333018087175, 0.12960601358885732, 0.6108883803188163, 0.13709227591145454]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12241333018087175, 0.12960601358885732, 0.6108883803188163, 0.13709227591145454]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12285927173464867, 0.12643299345799217, 0.6131159776009679, 0.13759175720639122]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12202462712938755, 0.1290048486254744, 0.608961142750833, 0.14000938149430509]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12248270565777589, 0.12948916302679425, 0.6112494217329905, 0.13677870958243948]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12248270565777589, 0.12948916302679425, 0.6112494217329905, 0.13677870958243948]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12248270565777589, 0.12948916302679425, 0.6112494217329905, 0.13677870958243948]
siam score:  -0.8626527
line 256 mcts: sample exp_bonus 4.48455612988919
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12248270565777589, 0.12948916302679425, 0.6112494217329905, 0.13677870958243948]
line 256 mcts: sample exp_bonus 4.319667030986398
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]] [[4.281]
 [4.281]
 [4.281]
 [4.281]
 [4.281]
 [4.281]
 [4.281]] [[0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12248270565777589, 0.12948916302679425, 0.6112494217329905, 0.13677870958243948]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12248270565777589, 0.12948916302679425, 0.6112494217329905, 0.13677870958243948]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12248270565777589, 0.12948916302679425, 0.6112494217329905, 0.13677870958243948]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [ 0.004]
 [ 0.003]
 [-0.001]
 [-0.001]] [[4.671]
 [4.671]
 [4.671]
 [4.596]
 [4.723]
 [4.671]
 [4.671]] [[0.476]
 [0.476]
 [0.476]
 [0.447]
 [0.503]
 [0.476]
 [0.476]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.6456],
        [-0.4097],
        [-0.0000],
        [-0.6450],
        [-0.5118],
        [-0.3946],
        [-0.3582],
        [-0.4098],
        [-0.0000],
        [-0.6446]], dtype=torch.float64)
-0.032346567066 -0.677898679726985
-0.09703970119800001 -0.5067524609775945
-0.9446210399999999 -0.9446210399999999
-0.032346567066 -0.6773880963711232
-0.045026434398 -0.5568710136413815
-0.09703970119800001 -0.49167444673996186
-0.045546567066 -0.403770582259701
-0.09703970119800001 -0.506881641694065
-0.858 -0.858
-0.057834381198 -0.7024620234052233
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.12248270565777589, 0.12948916302679425, 0.6112494217329905, 0.13677870958243948]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.1223995900969452, 0.1296339907440702, 0.6108057287517441, 0.1371606904072405]
UNIT TEST: sample policy line 217 mcts : [0.041 0.061 0.286 0.082 0.061 0.367 0.102]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.1223995900969452, 0.1296339907440702, 0.6108057287517441, 0.1371606904072405]
from probs:  [0.1223995900969452, 0.1296339907440702, 0.6108057287517441, 0.1371606904072405]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12235715101518446, 0.1297079398915256, 0.6105791787546619, 0.13735573033862797]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.963770190656185
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12235715101518446, 0.1297079398915256, 0.6105791787546619, 0.13735573033862797]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12235715101518446, 0.1297079398915256, 0.6105791787546619, 0.13735573033862797]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12231419316610395, 0.12978279334460802, 0.610349858342802, 0.13755315514648597]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12227065530450724, 0.12985865746141312, 0.6101174416700271, 0.13775324556405255]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.024]
 [-0.026]
 [-0.027]
 [-0.026]
 [-0.026]
 [-0.027]] [[1.479]
 [1.019]
 [1.462]
 [1.556]
 [1.585]
 [1.608]
 [1.563]] [[ 0.039]
 [-0.187]
 [ 0.032]
 [ 0.078]
 [ 0.093]
 [ 0.104]
 [ 0.082]]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12227065530450724, 0.12985865746141312, 0.6101174416700271, 0.13775324556405255]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12227065530450724, 0.12985865746141312, 0.6101174416700271, 0.13775324556405255]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12227065530450724, 0.12985865746141312, 0.6101174416700271, 0.13775324556405255]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12227065530450724, 0.12985865746141312, 0.6101174416700271, 0.13775324556405255]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12227065530450724, 0.12985865746141312, 0.6101174416700271, 0.13775324556405255]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12275976551930462, 0.13037815865816224, 0.6125604446978591, 0.13430163112467397]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12317508462124771, 0.12708410934330164, 0.6146041057647178, 0.13513670027073285]
start point for exploration sampling:  11715
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12314526394232142, 0.1271168796826943, 0.6144394482671215, 0.13529840810786267]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12311502158054922, 0.12715011341495647, 0.6142724624106586, 0.13546240259383566]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12311502158054922, 0.12715011341495647, 0.6142724624106586, 0.13546240259383566]
first move QE:  0.009432556654883092
UNIT TEST: sample policy line 217 mcts : [0.041 0.735 0.02  0.041 0.082 0.02  0.061]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.12311498378691986, 0.12715015464047882, 0.614272254973791, 0.1354626065988103]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.12311498378691986, 0.12715015464047882, 0.614272254973791, 0.1354626065988103]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.12311498378691986, 0.12715015464047882, 0.614272254973791, 0.1354626065988103]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.12311498378691986, 0.12715015464047882, 0.614272254973791, 0.1354626065988103]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.12311498378691986, 0.12715015464047882, 0.614272254973791, 0.1354626065988103]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.12311498378691986, 0.12715015464047882, 0.614272254973791, 0.1354626065988103]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[-0.24]
 [-0.24]
 [-0.24]
 [-0.24]
 [-0.24]
 [-0.24]
 [-0.24]]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12361107911409697, 0.12772827956971472, 0.6167336325689625, 0.13192700874722588]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12361107911409697, 0.12772827956971472, 0.6167336325689625, 0.13192700874722588]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12361107911409697, 0.12772827956971472, 0.6167336325689625, 0.13192700874722588]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.954696541066095
line 256 mcts: sample exp_bonus 1.350474740757933
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12413195849861566, 0.12826652931331547, 0.6193349828747534, 0.12826652931331547]
siam score:  -0.8732368
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12413195849861566, 0.12826652931331547, 0.6193349828747534, 0.12826652931331547]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12413195849861566, 0.12826652931331547, 0.6193349828747534, 0.12826652931331547]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12413195849861566, 0.12826652931331547, 0.6193349828747534, 0.12826652931331547]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12413195849861566, 0.12826652931331547, 0.6193349828747534, 0.12826652931331547]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12464070872422556, 0.12464070872422556, 0.621859135300942, 0.1288594472506069]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.1276989431235799, 0.12362868856614039, 0.6168233617163692, 0.13184900659391047]
siam score:  -0.87178147
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.1276989431235799, 0.12362868856614039, 0.6168233617163692, 0.13184900659391047]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12774112467922577, 0.1236063604044567, 0.6166955521530135, 0.13195696276330407]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12778391519499677, 0.12358370989903947, 0.6165658974484222, 0.13206647745754152]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12787137662761883, 0.12353741352663761, 0.6163008904484968, 0.13229031939724678]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[1.751]
 [1.751]
 [1.751]
 [1.751]
 [1.751]
 [1.751]
 [1.751]] [[0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12787137662761883, 0.12353741352663761, 0.6163008904484968, 0.13229031939724678]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12787137662761883, 0.12353741352663761, 0.6163008904484968, 0.13229031939724678]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.322]
 [0.322]
 [0.326]
 [0.322]
 [0.322]
 [0.322]] [[3.572]
 [3.584]
 [3.584]
 [3.651]
 [3.584]
 [3.584]
 [3.584]] [[0.483]
 [0.477]
 [0.477]
 [0.515]
 [0.477]
 [0.477]
 [0.477]]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12787137662761883, 0.12353741352663761, 0.6163008904484968, 0.13229031939724678]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12787137662761883, 0.12353741352663761, 0.6163008904484968, 0.13229031939724678]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12787137662761883, 0.12353741352663761, 0.6163008904484968, 0.13229031939724678]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12796143844101476, 0.1234897406824231, 0.6160280043287479, 0.13252081654781406]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12796143844101476, 0.1234897406824231, 0.6160280043287479, 0.13252081654781406]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
first move QE:  0.03068651978026824
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12404409042147226, 0.12404409042147226, 0.6187960627824339, 0.13311575637462159]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12402837625654492, 0.12402837625654492, 0.6186997042762826, 0.1332435432106276]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.1240124226865092, 0.1240124226865092, 0.6186018777494815, 0.13337327687750025]
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[3.529]
 [3.529]
 [3.529]
 [3.529]
 [3.529]
 [3.529]
 [3.529]] [[0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.265]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.1240124226865092, 0.1240124226865092, 0.6186018777494815, 0.13337327687750025]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.12399622419836911, 0.12399622419836911, 0.6185025493966032, 0.13350500220665867]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.12399620255874139, 0.12399620255874139, 0.618502418242418, 0.13350517664009925]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.12397975295835019, 0.12397975295835019, 0.6184015501055634, 0.1336389439777363]
siam score:  -0.8627888
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9599902241880772
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.12745209620281056, 0.1228468992706659, 0.6127676205222385, 0.13693338400428484]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
from probs:  [0.12752750894343615, 0.12277848303327082, 0.6123890927942224, 0.1373049152290707]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.1275660769446795, 0.1227434932224374, 0.6121955040482935, 0.13749492578458963]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.1275660769446795, 0.1227434932224374, 0.6121955040482935, 0.13749492578458963]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.12756612875040385, 0.12274344589238506, 0.6121952436638274, 0.13749518169338382]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.12756612875040385, 0.12274344589238506, 0.6121952436638274, 0.13749518169338382]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.052]
 [-0.05 ]
 [-0.052]
 [-0.052]
 [-0.053]
 [-0.054]] [[3.894]
 [3.894]
 [3.605]
 [4.236]
 [3.894]
 [4.377]
 [4.591]] [[0.369]
 [0.369]
 [0.281]
 [0.473]
 [0.369]
 [0.516]
 [0.581]]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.12760534225659148, 0.12270787012806744, 0.6119984145059061, 0.13768837310943496]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.12764511056876648, 0.12267179135619669, 0.611798800892156, 0.13788429718288078]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.12768549890684328, 0.12263515008187523, 0.6115960751118571, 0.13808327589942448]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.12772652188493813, 0.12259793304682418, 0.6113901638107122, 0.1382853812575254]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.12772657707524793, 0.1225978826353911, 0.6113898864261725, 0.1382856538631884]
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.387]
 [0.405]] [[0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [1.184]
 [0.977]] [[-0.329]
 [-0.329]
 [-0.329]
 [-0.329]
 [-0.329]
 [-0.244]
 [-0.329]]
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.1284632806918625, 0.12322674004824868, 0.6145084775235657, 0.13380150173632305]
using explorer policy with actor:  1
siam score:  -0.8715094
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.12851668992078696, 0.1231985098772829, 0.6143466646057264, 0.13393813559620374]
UNIT TEST: sample policy line 217 mcts : [0.041 0.469 0.143 0.082 0.102 0.082 0.082]
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.12851668992078696, 0.1231985098772829, 0.6143466646057264, 0.13393813559620374]
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.12383828046694974, 0.12383828046694974, 0.6175187202651664, 0.13480471880093414]
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.1225579107629804, 0.12777477856893085, 0.6111549012752601, 0.13851240939282872]
Printing some Q and Qe and total Qs values:  [[0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]] [[3.927]
 [3.927]
 [3.927]
 [3.927]
 [3.927]
 [3.927]
 [3.927]] [[0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.184]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]] [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[-0.327]
 [-0.561]
 [-0.561]
 [-0.561]
 [-0.561]
 [-0.561]
 [-0.561]]
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.12251980818449491, 0.12781684129107096, 0.6109438779848766, 0.13871947253955744]
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.12251980818449491, 0.12781684129107096, 0.6109438779848766, 0.13871947253955744]
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.12251980818449491, 0.12781684129107096, 0.6109438779848766, 0.13871947253955744]
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.1225197557813446, 0.1278168987498795, 0.6109435893467664, 0.1387197561220095]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.12248104088922902, 0.12785963741968878, 0.6107291749077285, 0.13893014678335355]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.12248104088922902, 0.12785963741968878, 0.6107291749077285, 0.13893014678335355]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.12240171799559095, 0.12794720462636988, 0.6102898614339496, 0.13936121594408962]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.12236107888363727, 0.12799206750709147, 0.610064790100507, 0.1395820635087643]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.12100621155359569, 0.13208649945642775, 0.6033101991878884, 0.1435970898020882]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.1217129940244508, 0.13285808080962963, 0.6068375545196834, 0.1385913706462361]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.1217129940244508, 0.13285808080962963, 0.6068375545196834, 0.1385913706462361]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.1217129940244508, 0.13285808080962963, 0.6068375545196834, 0.1385913706462361]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.1217129940244508, 0.13285808080962963, 0.6068375545196834, 0.1385913706462361]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.12166246004103233, 0.13297695772221885, 0.6065631858035351, 0.13879739643321376]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.12161110059898374, 0.13309777645626722, 0.6062843353503601, 0.13900678759438903]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.12161102745820608, 0.13309794784198412, 0.6062839399296323, 0.1390070847701776]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.12161102745820608, 0.13309794784198412, 0.6062839399296323, 0.1390070847701776]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.12161102745820608, 0.13309794784198412, 0.6062839399296323, 0.1390070847701776]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.12161102745820608, 0.13309794784198412, 0.6062839399296323, 0.1390070847701776]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
from probs:  [0.12024886614118588, 0.13719557879298222, 0.5994938065087665, 0.14306174855706544]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.12017745009695396, 0.13737690080692302, 0.5991150922742109, 0.14333055682191223]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.583]
 [0.547]
 [0.547]
 [0.549]
 [0.543]
 [0.545]] [[2.12 ]
 [2.289]
 [2.081]
 [2.051]
 [2.219]
 [1.991]
 [2.321]] [[0.6  ]
 [0.751]
 [0.577]
 [0.557]
 [0.67 ]
 [0.513]
 [0.734]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.12010478376789377, 0.13756139652103302, 0.5987297495447228, 0.1436040701663505]
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.12010478376789377, 0.13756139652103302, 0.5987297495447228, 0.1436040701663505]
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.12003104471324252, 0.1377486165396727, 0.598338716575186, 0.14388162217189865]
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.12003104471324252, 0.1377486165396727, 0.598338716575186, 0.14388162217189865]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.12077125999464655, 0.13859823033029656, 0.6020322793447603, 0.13859823033029656]
using explorer policy with actor:  1
from probs:  [0.12077125999464655, 0.13859823033029656, 0.6020322793447603, 0.13859823033029656]
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
Starting evaluation
siam score:  -0.86353165
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.12225065098978322, 0.1341675657062087, 0.6094142175977996, 0.1341675657062087]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9930034482758621 -1.0 -0.9930034482758621
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.12225036156323461, 0.13416850962792742, 0.6094126191809104, 0.13416850962792742]
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.12225036156323461, 0.13416850962792742, 0.6094126191809104, 0.13416850962792742]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9931432432432432 -1.0 -0.9931432432432432
probs:  [0.12225030603506784, 0.13416869072489776, 0.6094123125151365, 0.13416869072489776]
line 256 mcts: sample exp_bonus -0.36756896002018535
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.12225025125553962, 0.1341688693803246, 0.6094120099838112, 0.1341688693803246]
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.12224998807485798, 0.13416972770609106, 0.60941055651296, 0.13416972770609106]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.426]
 [0.434]
 [0.433]
 [0.433]
 [0.435]
 [0.434]] [[0.651]
 [1.325]
 [0.729]
 [0.433]
 [0.421]
 [0.711]
 [0.718]] [[0.433]
 [0.426]
 [0.434]
 [0.433]
 [0.433]
 [0.435]
 [0.434]]
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.12224993748490405, 0.13416989269799762, 0.6094102771191008, 0.13416989269799762]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.433]
 [0.433]
 [0.432]
 [0.432]
 [0.433]
 [0.435]] [[0.606]
 [1.997]
 [0.808]
 [0.751]
 [0.794]
 [0.704]
 [0.5  ]] [[0.433]
 [0.433]
 [0.433]
 [0.432]
 [0.432]
 [0.433]
 [0.435]]
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.12224993748490405, 0.13416989269799762, 0.6094102771191008, 0.13416989269799762]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.426]
 [0.432]
 [0.433]
 [0.433]
 [0.435]
 [0.433]] [[0.726]
 [1.403]
 [0.7  ]
 [0.6  ]
 [0.645]
 [0.796]
 [0.769]] [[0.433]
 [0.426]
 [0.432]
 [0.433]
 [0.433]
 [0.435]
 [0.433]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.12220736870793311, 0.13430935315641257, 0.6091739249792418, 0.13430935315641257]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.12216412612348213, 0.13445102183937613, 0.6089338301977657, 0.13445102183937613]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.445]
 [0.431]
 [0.431]
 [0.432]
 [0.431]
 [0.431]] [[0.362]
 [2.104]
 [0.372]
 [0.215]
 [0.08 ]
 [0.252]
 [0.698]] [[0.429]
 [0.445]
 [0.431]
 [0.431]
 [0.432]
 [0.431]
 [0.431]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.1221640744448923, 0.13445119039450745, 0.6089335447660928, 0.13445119039450745]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
main train batch thing paused
add a thread
Adding thread: now have 4 threads
main train batch thing paused
add a thread
Adding thread: now have 5 threads
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.12443638773618403, 0.12443638773618403, 0.6201939345428125, 0.13093328998481946]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.1252496396225176, 0.1252496396225176, 0.6242510811324472, 0.1252496396225176]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.1252496396225176, 0.1252496396225176, 0.6242510811324472, 0.1252496396225176]
UNIT TEST: sample policy line 217 mcts : [0.163 0.02  0.143 0.122 0.122 0.163 0.265]
from probs:  [0.12524964311777592, 0.12524964311777592, 0.6242510706466722, 0.12524964311777592]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.12993713776112628, 0.1236781869035441, 0.6164475375742035, 0.12993713776112628]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.1300126757450666, 0.12365723052051747, 0.6163174179893494, 0.1300126757450666]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.1300126757450666, 0.12365723052051747, 0.6163174179893494, 0.1300126757450666]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.587]
 [0.54 ]
 [0.542]
 [0.543]
 [0.539]
 [0.527]] [[2.229]
 [2.713]
 [2.459]
 [2.644]
 [2.753]
 [2.917]
 [2.974]] [[0.239]
 [0.494]
 [0.347]
 [0.432]
 [0.482]
 [0.553]
 [0.571]]
siam score:  -0.86069626
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]] [[3.53]
 [3.53]
 [3.53]
 [3.53]
 [3.53]
 [3.53]
 [3.53]] [[0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.12444765195915389, 0.12444765195915389, 0.620260709361205, 0.13084398672048717]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.12444765195915389, 0.12444765195915389, 0.620260709361205, 0.13084398672048717]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.12525160372495162, 0.12525160372495162, 0.624245188825145, 0.12525160372495162]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.12525160372495162, 0.12525160372495162, 0.624245188825145, 0.12525160372495162]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.12525160372495162, 0.12525160372495162, 0.624245188825145, 0.12525160372495162]
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.096]
 [0.11 ]
 [0.118]
 [0.116]
 [0.11 ]
 [0.116]] [[0.161]
 [1.178]
 [0.149]
 [0.153]
 [0.167]
 [0.25 ]
 [0.898]] [[0.123]
 [0.096]
 [0.11 ]
 [0.118]
 [0.116]
 [0.11 ]
 [0.116]]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.13009037018362854, 0.13009037018362854, 0.6161810581581835, 0.12363820147455938]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.4640778188562487
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.13102598693736875, 0.12443139037255677, 0.6201112323175177, 0.12443139037255677]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.13111891846693827, 0.12442198473396955, 0.6200371120651228, 0.12442198473396955]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.13111901947413562, 0.124421974320757, 0.6200370318843503, 0.124421974320757]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.13576760464854873, 0.12283434674374369, 0.6121558604474924, 0.12924218816021527]
line 256 mcts: sample exp_bonus 0.28062983305928146
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.584]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]] [[2.337]
 [2.689]
 [2.523]
 [2.523]
 [2.523]
 [2.523]
 [2.523]] [[0.574]
 [0.584]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.1300906954363462, 0.12364063611801454, 0.6161779730092931, 0.1300906954363462]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13024638469152167, 0.12359759888244638, 0.6159096317345101, 0.13024638469152167]
using another actor
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13576452195300467, 0.12283728834064528, 0.6121555153415628, 0.12924267436478734]
siam score:  -0.85747427
using another actor
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13699387457718884, 0.1235856382504051, 0.6158348489220009, 0.1235856382504051]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.1371759622067929, 0.12356352755403577, 0.6156969826851356, 0.12356352755403577]
using explorer policy with actor:  1
siam score:  -0.86009127
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13130395011028226, 0.12440791392397213, 0.6198802220417735, 0.12440791392397213]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13130395011028226, 0.12440791392397213, 0.6198802220417735, 0.12440791392397213]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13140064794359194, 0.12439819894793665, 0.6198029541605347, 0.12439819894793665]
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.855]
 [0.742]
 [0.74 ]
 [0.738]
 [0.737]
 [0.742]] [[1.561]
 [1.905]
 [2.462]
 [2.569]
 [2.679]
 [2.651]
 [2.523]] [[0.769]
 [0.855]
 [0.742]
 [0.74 ]
 [0.738]
 [0.737]
 [0.742]]
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.1314992725641341, 0.12438829019070102, 0.619724147054464, 0.12438829019070102]
start point for exploration sampling:  11715
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.3243391421096056
from probs:  [0.1304026374737399, 0.1304026374737399, 0.6156349623215516, 0.12355976273096858]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.13547375742035514, 0.13547375742035514, 0.6071842071536341, 0.12186827800565557]
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.623]
 [0.565]
 [0.571]
 [0.568]
 [0.641]
 [0.558]] [[1.743]
 [1.746]
 [1.648]
 [1.629]
 [1.571]
 [1.338]
 [1.461]] [[0.465]
 [0.497]
 [0.391]
 [0.387]
 [0.355]
 [0.312]
 [0.29 ]]
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.13562938524850504, 0.13562938524850504, 0.606920211660729, 0.12182101784226086]
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.12345563280153744, 0.13810599393874026, 0.6149827404581849, 0.12345563280153744]
from probs:  [0.12345563280153744, 0.13810599393874026, 0.6149827404581849, 0.12345563280153744]
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.1225349156159198, 0.137277299292885, 0.6103463372143233, 0.12984144787687188]
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.1225349156159198, 0.137277299292885, 0.6103463372143233, 0.12984144787687188]
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.12345232416151761, 0.13081362681778164, 0.6149204222029191, 0.13081362681778164]
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.12340344776299754, 0.13099172619458643, 0.6146130998478295, 0.13099172619458643]
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.12335265256238805, 0.1311768174420425, 0.6142937125535269, 0.1311768174420425]
from probs:  [0.12335265256238805, 0.1311768174420425, 0.6142937125535269, 0.1311768174420425]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.513]
 [0.562]
 [0.562]] [[2.121]
 [2.121]
 [2.121]
 [2.121]
 [1.73 ]
 [2.121]
 [2.121]] [[0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.022]
 [0.202]
 [0.202]]
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.387]
 [0.387]
 [0.386]
 [0.383]
 [0.387]
 [0.384]] [[6.585]
 [6.585]
 [6.585]
 [6.254]
 [6.398]
 [6.585]
 [6.345]] [[0.597]
 [0.597]
 [0.597]
 [0.508]
 [0.544]
 [0.597]
 [0.53 ]]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[1.693]
 [1.693]
 [1.693]
 [1.693]
 [1.693]
 [1.693]
 [1.693]] [[1.347]
 [1.347]
 [1.347]
 [1.347]
 [1.347]
 [1.347]
 [1.347]]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.035]
 [0.034]
 [0.024]
 [0.012]
 [0.004]
 [0.026]] [[4.005]
 [4.625]
 [3.889]
 [3.995]
 [4.022]
 [4.274]
 [4.016]] [[-0.13 ]
 [-0.004]
 [-0.192]
 [-0.173]
 [-0.175]
 [-0.117]
 [-0.166]]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.12430223257763784, 0.13243784040916945, 0.6189576944355548, 0.12430223257763784]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.13013397027852636, 0.13801749027776, 0.6094621664058136, 0.12238637303790002]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
from probs:  [0.13013397027852636, 0.13801749027776, 0.6094621664058136, 0.12238637303790002]
siam score:  -0.8525074
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
probs:  [0.13145684563771284, 0.13145684563771284, 0.6138073078661583, 0.12327900085841613]
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.659]
 [0.618]
 [0.567]
 [0.596]
 [0.562]
 [0.552]] [[1.767]
 [1.766]
 [1.608]
 [1.06 ]
 [0.975]
 [1.21 ]
 [1.176]] [[0.6  ]
 [0.602]
 [0.529]
 [0.333]
 [0.325]
 [0.375]
 [0.359]]
using explorer policy with actor:  1
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.1314569540217929, 0.1314569540217929, 0.6138071210484629, 0.12327897090795137]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.13155730489410922, 0.13155730489410922, 0.6136339089454739, 0.1232514812663078]
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.665]
 [0.336]
 [0.39 ]
 [0.401]
 [0.395]
 [0.356]] [[1.719]
 [1.621]
 [2.058]
 [1.629]
 [1.434]
 [1.795]
 [1.016]] [[0.364]
 [0.665]
 [0.336]
 [0.39 ]
 [0.401]
 [0.395]
 [0.356]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.13155730489410922, 0.13155730489410922, 0.6136339089454739, 0.1232514812663078]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.13155730489410922, 0.13155730489410922, 0.6136339089454739, 0.1232514812663078]
Printing some Q and Qe and total Qs values:  [[0.585]
 [1.083]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.698]] [[1.547]
 [1.334]
 [1.559]
 [1.559]
 [1.559]
 [1.559]
 [2.167]] [[0.627]
 [1.017]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [1.049]]
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
actor:  1 policy actor:  1  step number:  67 total reward:  0.19999999999999862  reward:  1.0 rdn_beta:  0.667
from probs:  [0.1302828621153654, 0.13839146212544085, 0.6090128051067232, 0.12231287065247061]
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
probs:  [0.10552665598853214, 0.10813935184083104, 0.2597770109903114, 0.5265569811803253]
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[0.97]
 [0.97]
 [0.97]
 [0.97]
 [0.97]
 [0.97]
 [0.97]] [[0.952]
 [0.952]
 [0.952]
 [0.952]
 [0.952]
 [0.952]
 [0.952]]
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.654]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]] [[1.809]
 [2.196]
 [1.809]
 [1.809]
 [1.809]
 [1.809]
 [1.809]] [[0.571]
 [0.839]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]]
siam score:  -0.8522812
from probs:  [0.10509071926194659, 0.10784723960095217, 0.262725950499268, 0.5243360906378333]
Printing some Q and Qe and total Qs values:  [[1.06]
 [1.06]
 [1.06]
 [1.06]
 [1.06]
 [1.06]
 [1.06]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.1046319174740448, 0.10749691574532934, 0.26586047729549683, 0.522010689485129]
line 256 mcts: sample exp_bonus 1.7202314712203692
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.1043870366678925, 0.10728641687716316, 0.26755044943530704, 0.5207760970196373]
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.179]
 [0.202]
 [0.201]
 [0.2  ]
 [0.198]
 [0.189]] [[ 0.008]
 [ 0.561]
 [ 0.246]
 [ 0.055]
 [-0.116]
 [-0.17 ]
 [ 0.298]] [[0.203]
 [0.179]
 [0.202]
 [0.201]
 [0.2  ]
 [0.198]
 [0.189]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10441614177256346, 0.10735796337002783, 0.26731605368237205, 0.5209098411750368]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.277]
 [0.302]
 [0.298]
 [0.298]
 [0.301]
 [0.3  ]] [[0.524]
 [1.287]
 [0.519]
 [0.654]
 [0.539]
 [0.574]
 [0.722]] [[0.301]
 [0.277]
 [0.302]
 [0.298]
 [0.298]
 [0.301]
 [0.3  ]]
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.10441636775794892, 0.1073582575790123, 0.2673144209500654, 0.5209109537129734]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.10441636775794892, 0.1073582575790123, 0.2673144209500654, 0.5209109537129734]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
probs:  [0.10523568910245563, 0.1082007268321348, 0.2615605204553301, 0.5250030636100794]
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
probs:  [0.1055483870697347, 0.1055483870697347, 0.2623383856168947, 0.5265648402436359]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.534]
 [0.518]
 [0.524]
 [0.524]
 [0.471]
 [0.527]] [[2.244]
 [2.728]
 [2.33 ]
 [2.435]
 [2.383]
 [2.026]
 [2.645]] [[0.543]
 [0.534]
 [0.518]
 [0.524]
 [0.524]
 [0.471]
 [0.527]]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.10535329850629321, 0.10535329850629321, 0.2637268592628917, 0.5255665437245218]
siam score:  -0.85214543
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.10463801058396893, 0.10463801058396893, 0.26876661014293607, 0.5219573686891261]
siam score:  -0.85596883
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.1071927451121014, 0.10411992976226497, 0.26930337696550444, 0.5193839481601292]
from probs:  [0.1071927451121014, 0.10411992976226497, 0.26930337696550444, 0.5193839481601292]
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.10419804657975475, 0.10419804657975475, 0.2718420950579072, 0.5197618117825833]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.10369273228789244, 0.10672141225828653, 0.27233392698793224, 0.5172519284658887]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.10369273228789244, 0.10672141225828653, 0.27233392698793224, 0.5172519284658887]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.10369273228789244, 0.10672141225828653, 0.27233392698793224, 0.5172519284658887]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.586]
 [0.555]
 [0.532]
 [0.455]
 [0.532]
 [0.455]] [[0.875]
 [1.333]
 [0.938]
 [0.807]
 [0.984]
 [0.971]
 [0.984]] [[0.427]
 [0.757]
 [0.462]
 [0.352]
 [0.393]
 [0.462]
 [0.393]]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.598]
 [0.504]
 [0.508]
 [0.502]
 [0.498]
 [0.502]] [[1.192]
 [0.322]
 [1.178]
 [1.186]
 [0.952]
 [1.102]
 [1.027]] [[0.656]
 [0.226]
 [0.65 ]
 [0.658]
 [0.514]
 [0.6  ]
 [0.559]]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.544]
 [0.501]
 [0.504]
 [0.501]
 [0.504]
 [0.498]] [[3.542]
 [5.986]
 [5.13 ]
 [4.938]
 [2.881]
 [4.938]
 [4.298]] [[0.44 ]
 [0.938]
 [0.755]
 [0.718]
 [0.308]
 [0.718]
 [0.589]]
Printing some Q and Qe and total Qs values:  [[-0.047]
 [-0.038]
 [-0.06 ]
 [-0.047]
 [-0.054]
 [-0.057]
 [-0.052]] [[4.794]
 [4.313]
 [3.668]
 [4.793]
 [4.592]
 [4.632]
 [5.033]] [[0.412]
 [0.251]
 [0.016]
 [0.412]
 [0.339]
 [0.351]
 [0.493]]
actor:  1 policy actor:  1  step number:  69 total reward:  0.05333333333333301  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.11254654132845095, 0.11254654132845095, 0.21301681041081538, 0.5618901069322828]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.11242307824148823, 0.11242307824148823, 0.21388641607571782, 0.5612674274413056]
from probs:  [0.11242307824148823, 0.11242307824148823, 0.21388641607571782, 0.5612674274413056]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
using another actor
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.11362750262962974, 0.11181384352094315, 0.2163408885322593, 0.5582177653171679]
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]] [[2.24]
 [2.24]
 [2.24]
 [2.24]
 [2.24]
 [2.24]
 [2.24]] [[0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.11362762049109879, 0.11181394761955668, 0.21634014944208363, 0.5582182824472608]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.11362773713872443, 0.11181405064606065, 0.2163394179637839, 0.558218794251431]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.1134067446581643, 0.11155789689774032, 0.21810802478820568, 0.5569273336558896]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.11329540749630826, 0.11142886134439146, 0.21899895741037634, 0.5562767737489239]
deleting a thread, now have 4 threads
Frames:  24287 train batches done:  2843 episodes:  703
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.11626525514364235, 0.11077705721726072, 0.21993095707291693, 0.55302673056618]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.11626525514364235, 0.11077705721726072, 0.21993095707291693, 0.55302673056618]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.11626539274999302, 0.11077715437774063, 0.2199302398054556, 0.5530272130668107]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.11626539274999302, 0.11077715437774063, 0.2199302398054556, 0.5530272130668107]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.11626539274999302, 0.11077715437774063, 0.2199302398054556, 0.5530272130668107]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.11626539274999302, 0.11077715437774063, 0.2199302398054556, 0.5530272130668107]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.11626539274999302, 0.11077715437774063, 0.2199302398054556, 0.5530272130668107]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.11626539274999302, 0.11077715437774063, 0.2199302398054556, 0.5530272130668107]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.11626539274999302, 0.11077715437774063, 0.2199302398054556, 0.5530272130668107]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.11626539274999302, 0.11077715437774063, 0.2199302398054556, 0.5530272130668107]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.11626539274999302, 0.11077715437774063, 0.2199302398054556, 0.5530272130668107]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.11626539274999302, 0.11077715437774063, 0.2199302398054556, 0.5530272130668107]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.11618329590014764, 0.11064369385365032, 0.22081832811770608, 0.552354682128496]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.191]
 [0.175]
 [0.176]
 [0.177]
 [0.17 ]
 [0.165]] [[-1.119]
 [ 1.17 ]
 [-0.845]
 [-0.93 ]
 [-0.853]
 [-0.776]
 [-0.263]] [[0.178]
 [0.191]
 [0.175]
 [0.176]
 [0.177]
 [0.17 ]
 [0.165]]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.11618329590014764, 0.11064369385365032, 0.22081832811770608, 0.552354682128496]
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]] [[7.026]
 [7.026]
 [7.026]
 [7.026]
 [7.026]
 [7.026]
 [7.026]] [[0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.11618329590014764, 0.11064369385365032, 0.22081832811770608, 0.552354682128496]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.11618343313935729, 0.11064379052838541, 0.22081761414465015, 0.552355162187607]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.11451962035466337, 0.11077560884982118, 0.22170204595489618, 0.5530027248406192]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.11175548976657965, 0.11175548976657965, 0.21858933290524168, 0.557899687561599]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.11175548976657965, 0.11175548976657965, 0.21858933290524168, 0.557899687561599]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.130795147719692
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.11126853062510662, 0.11311416343971341, 0.22015018650481757, 0.5554671194303623]
start point for exploration sampling:  11715
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.11114142534798672, 0.11300409573857975, 0.22102819961900533, 0.5548262792944283]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.11114152640509652, 0.1130042103761993, 0.22102748205182096, 0.5548267811668833]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.11101383084244011, 0.11289363176246507, 0.22190957253943297, 0.5541829648556619]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.058]
 [-0.058]
 [-0.043]
 [-0.046]
 [-0.044]
 [-0.044]] [[5.273]
 [5.72 ]
 [5.72 ]
 [5.636]
 [5.754]
 [5.989]
 [6.007]] [[0.309]
 [0.405]
 [0.405]
 [0.39 ]
 [0.418]
 [0.476]
 [0.481]]
siam score:  -0.85648376
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.11094764572087211, 0.11286522590685352, 0.22234635429074287, 0.5538407740815315]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
from probs:  [0.11094764572087211, 0.11286522590685352, 0.22234635429074287, 0.5538407740815315]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.517]
 [0.454]
 [0.457]
 [0.459]
 [0.458]
 [0.456]] [[1.461]
 [2.108]
 [1.239]
 [1.284]
 [1.28 ]
 [1.349]
 [1.378]] [[0.452]
 [0.517]
 [0.454]
 [0.457]
 [0.459]
 [0.458]
 [0.456]]
deleting a thread, now have 3 threads
Frames:  25247 train batches done:  2954 episodes:  730
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.11056187667377894, 0.11253195872790497, 0.2250105960601845, 0.5518955685381315]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.11056187667377894, 0.11253195872790497, 0.2250105960601845, 0.5518955685381315]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.11056187667377894, 0.11253195872790497, 0.2250105960601845, 0.5518955685381315]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.11075625217706045, 0.11272981520661979, 0.22364704304204988, 0.5528668895742699]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
from probs:  [0.11075625217706045, 0.11272981520661979, 0.22364704304204988, 0.5528668895742699]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.11062795723575909, 0.11261924969313643, 0.22453289819273067, 0.5522198948783739]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
deleting a thread, now have 2 threads
Frames:  25483 train batches done:  2980 episodes:  735
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.11049915241659442, 0.1125082740348207, 0.22542225294255225, 0.5515703206060327]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.11069520155364052, 0.11270789262600202, 0.22404691835550677, 0.5525499874648506]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.11069520155364052, 0.11270789262600202, 0.22404691835550677, 0.5525499874648506]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.363]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[6.786]
 [6.606]
 [6.786]
 [6.786]
 [6.786]
 [6.786]
 [6.786]] [[0.711]
 [0.668]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.11069520155364052, 0.11270789262600202, 0.22404691835550677, 0.5525499874648506]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.11069520155364052, 0.11270789262600202, 0.22404691835550677, 0.5525499874648506]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.11056719151186832, 0.11259784657174865, 0.2249306129184944, 0.5519043489978888]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.11056719151186832, 0.11259784657174865, 0.2249306129184944, 0.5519043489978888]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.11056719151186832, 0.11259784657174865, 0.2249306129184944, 0.5519043489978888]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.1104385613859459, 0.1124872674513929, 0.2258185881221608, 0.5512555830405003]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.1104385613859459, 0.1124872674513929, 0.2258185881221608, 0.5512555830405003]
siam score:  -0.84485215
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.537]
 [0.602]
 [0.537]
 [0.537]
 [0.537]
 [0.537]] [[1.151]
 [1.151]
 [2.004]
 [1.151]
 [1.151]
 [1.151]
 [1.151]] [[0.418]
 [0.418]
 [1.052]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
from probs:  [0.11004890511793666, 0.1121522926774153, 0.2285085108868652, 0.5492902913177828]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.109917749048074, 0.112039542085509, 0.22941392349540657, 0.5486287853710105]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.10978605757312934, 0.11192636242018358, 0.23032300977661305, 0.5479645702300739]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.10978605757312934, 0.11192636242018358, 0.23032300977661305, 0.5479645702300739]
line 256 mcts: sample exp_bonus 4.810666637458282
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.10999153258625957, 0.11213584869398376, 0.22888135950945435, 0.5489912592103022]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.11019575344381376, 0.11234405632872034, 0.22744850862716331, 0.5500116816003027]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.11019575344381376, 0.11234405632872034, 0.22744850862716331, 0.5500116816003027]
siam score:  -0.8431053
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.1094087114801251, 0.1156450976058552, 0.22885129626040232, 0.5460948946536174]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.10914014313163581, 0.11548369668915189, 0.23063525557756975, 0.5447409046016426]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.10914014313163581, 0.11548369668915189, 0.23063525557756975, 0.5447409046016426]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.10920758781823992, 0.11561705714481119, 0.2301035253256661, 0.5450718297112828]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.10920758781823992, 0.11561705714481119, 0.2301035253256661, 0.5450718297112828]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.10940894205581628, 0.11583024524714587, 0.22868288841340953, 0.5460779242836283]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.10902319364276218, 0.11744663631425652, 0.2293718094269434, 0.544158360616038]
siam score:  -0.8461126
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
line 256 mcts: sample exp_bonus 1.0474921625217353
using another actor
from probs:  [0.10932544194921727, 0.1156695127038633, 0.2293424536249809, 0.5456625917219385]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.1094298299572381, 0.1136694184681135, 0.2307226343437045, 0.546178117230944]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.3803806533921508
first move QE:  0.14324780084011737
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.35 ]
 [0.538]
 [0.507]
 [0.428]
 [0.539]
 [0.491]] [[1.187]
 [1.716]
 [2.138]
 [0.481]
 [0.456]
 [1.687]
 [1.415]] [[0.401]
 [0.625]
 [0.936]
 [0.184]
 [0.121]
 [0.737]
 [0.585]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.10916628528859933, 0.11347752239516017, 0.2325071394063458, 0.5448490529098947]
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.10916628528859933, 0.11347752239516017, 0.2325071394063458, 0.5448490529098947]
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.10916628528859933, 0.11347752239516017, 0.2325071394063458, 0.5448490529098947]
UNIT TEST: sample policy line 217 mcts : [0.02  0.163 0.02  0.082 0.02  0.082 0.612]
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.10903351211333862, 0.11338081558633535, 0.23340619452797068, 0.5441794777723553]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.848]
 [0.598]
 [0.595]
 [0.595]
 [0.583]
 [0.595]] [[1.495]
 [2.55 ]
 [1.955]
 [2.249]
 [2.249]
 [1.777]
 [2.249]] [[0.18 ]
 [0.701]
 [0.363]
 [0.464]
 [0.464]
 [0.293]
 [0.464]]
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.10923908588473175, 0.11359459712780062, 0.23195970374319558, 0.5452066132442721]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.10923919112102158, 0.11359473459019234, 0.23195893904081555, 0.5452071352479706]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.10923919112102158, 0.11359473459019234, 0.23195893904081555, 0.5452071352479706]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.10884142037996312, 0.11330662805433885, 0.23465101545978276, 0.5432009361059152]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.841]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]] [[3.619]
 [3.825]
 [3.619]
 [3.619]
 [3.619]
 [3.619]
 [3.619]] [[0.708]
 [0.856]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]]
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.751]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.66 ]] [[3.407]
 [4.502]
 [3.767]
 [3.767]
 [3.767]
 [3.767]
 [3.49 ]] [[0.305]
 [0.578]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.317]]
first move QE:  0.14652444597758524
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.10830473334689633, 0.11494594855480844, 0.2362275318238523, 0.540521786274443]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.10830473334689633, 0.11494594855480844, 0.2362275318238523, 0.540521786274443]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.10816829134761319, 0.11486383976694282, 0.23713403370665628, 0.5398338351787877]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.10816829134761319, 0.11486383976694282, 0.23713403370665628, 0.5398338351787877]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.10837735431073993, 0.11508586155407284, 0.23565842376367563, 0.5408783603715115]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.10862398092205147, 0.11307036112958514, 0.23619509601968344, 0.5421105619286799]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.10862398092205147, 0.11307036112958514, 0.23619509601968344, 0.5421105619286799]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.1084898266801068, 0.11297254895249766, 0.2371036321286274, 0.541433992238768]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.1084898266801068, 0.11297254895249766, 0.2371036321286274, 0.541433992238768]
using another actor
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.10873529998129541, 0.11096376800930155, 0.2376405146941486, 0.5426604173152545]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.10873551018595899, 0.11096401071975047, 0.23763901934422962, 0.5426614597500609]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.1083332004717955, 0.1127166540375399, 0.2382900937589577, 0.5406600517317068]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.1083332004717955, 0.1127166540375399, 0.2382900937589577, 0.5406600517317068]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.10857286365485307, 0.11075201267394003, 0.23881765614146822, 0.5418574675297388]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.10902024625180237, 0.10902024625180237, 0.23786680724330997, 0.5440927002530852]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.10902024625180237, 0.10902024625180237, 0.23786680724330997, 0.5440927002530852]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.493]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.416]] [[2.112]
 [2.779]
 [2.379]
 [2.379]
 [2.379]
 [2.379]
 [2.544]] [[0.471]
 [0.493]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.416]]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.10888978169194607, 0.10888978169194607, 0.23878583106243678, 0.543434605553671]
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]] [[0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.10888978169194607, 0.10888978169194607, 0.23878583106243678, 0.543434605553671]
maxi score, test score, baseline:  -0.995160663507109 -1.0 -0.995160663507109
probs:  [0.10875879019075434, 0.10875879019075434, 0.23970857571444112, 0.5427738439040501]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.1087588944878754, 0.1087588944878754, 0.23970784986686508, 0.5427743611573841]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.563]
 [0.572]
 [0.56 ]
 [0.557]
 [0.558]
 [0.571]] [[2.201]
 [2.629]
 [2.328]
 [2.249]
 [2.234]
 [2.175]
 [2.221]] [[0.561]
 [0.563]
 [0.572]
 [0.56 ]
 [0.557]
 [0.558]
 [0.571]]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]] [[2.433]
 [2.433]
 [2.433]
 [2.433]
 [2.433]
 [2.433]
 [2.433]] [[0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]]
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.1087588944878754, 0.1087588944878754, 0.23970784986686508, 0.5427743611573841]
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.1087588944878754, 0.1087588944878754, 0.23970784986686508, 0.5427743611573841]
from probs:  [0.1087588944878754, 0.1087588944878754, 0.23970784986686508, 0.5427743611573841]
first move QE:  0.16129962315939664
line 256 mcts: sample exp_bonus 0.2521232166581359
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.10862726534869849, 0.10862726534869849, 0.24063508624146182, 0.5421103830611413]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.10862726534869849, 0.10862726534869849, 0.24063508624146182, 0.5421103830611413]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.10862726534869849, 0.10862726534869849, 0.24063508624146182, 0.5421103830611413]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.10862736808123663, 0.10862736808123663, 0.24063437131989374, 0.5421108925176331]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.10862736808123663, 0.10862736808123663, 0.24063437131989374, 0.5421108925176331]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.10862736808123663, 0.10862736808123663, 0.24063437131989374, 0.5421108925176331]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.10862736808123663, 0.10862736808123663, 0.24063437131989374, 0.5421108925176331]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.10862736808123663, 0.10862736808123663, 0.24063437131989374, 0.5421108925176331]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.11041114547239606, 0.10822355053382836, 0.2412632105692029, 0.5401020934245727]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.11051094719263442, 0.10830152221934254, 0.24070213138584837, 0.5404853992021746]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.11039494658475028, 0.10816787841408042, 0.241625762712683, 0.5398114122884864]
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]] [[5.515]
 [5.515]
 [5.515]
 [5.515]
 [5.515]
 [5.515]
 [5.515]] [[0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.11039494658475028, 0.10816787841408042, 0.241625762712683, 0.5398114122884864]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.11039494658475028, 0.10816787841408042, 0.241625762712683, 0.5398114122884864]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.11061198649466258, 0.10838053388465337, 0.24013363325340453, 0.5408738463672795]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[2.094]
 [2.011]
 [2.011]
 [2.011]
 [2.011]
 [2.011]
 [2.011]] [[0.259]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.11407214531359829, 0.1075937418138179, 0.2413740235628458, 0.536960089309738]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
first move QE:  0.16849524641787583
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.11420667424077915, 0.10766480978605601, 0.24081944811623116, 0.5373090678569337]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.11420667424077915, 0.10766480978605601, 0.24081944811623116, 0.5373090678569337]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.514]
 [0.539]
 [0.545]
 [0.539]
 [0.538]
 [0.538]] [[1.153]
 [1.498]
 [0.97 ]
 [1.004]
 [1.012]
 [1.084]
 [1.218]] [[0.478]
 [0.679]
 [0.352]
 [0.381]
 [0.379]
 [0.427]
 [0.517]]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.11420681229413834, 0.10766490318515728, 0.24081875366306366, 0.5373095308576408]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.11420681229413834, 0.10766490318515728, 0.24081875366306366, 0.5373095308576408]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.11420681229413834, 0.10766490318515728, 0.24081875366306366, 0.5373095308576408]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.11412203189003146, 0.10752891400383524, 0.24172506662181537, 0.536623987484318]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.542]
 [0.204]
 [0.528]
 [0.491]
 [0.128]
 [0.534]] [[2.2  ]
 [1.803]
 [0.901]
 [1.06 ]
 [1.016]
 [0.611]
 [1.671]] [[0.866]
 [0.796]
 [0.351]
 [0.579]
 [0.546]
 [0.226]
 [0.755]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.368]
 [0.555]
 [0.478]
 [0.317]
 [0.492]
 [0.439]] [[2.14 ]
 [1.792]
 [2.067]
 [0.569]
 [0.462]
 [1.544]
 [1.297]] [[0.868]
 [0.799]
 [1.006]
 [0.479]
 [0.342]
 [0.799]
 [0.686]]
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.1140370006271019, 0.10739239359369486, 0.2426348475668543, 0.5359357582123488]
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.1142592369324342, 0.10760166378574725, 0.24115782250590984, 0.5369812767759089]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.11409085276196096, 0.10732927110347697, 0.24297192245447546, 0.5356079536800866]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.11185623813927924, 0.10730367396678082, 0.24537263948020685, 0.5354674484137331]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.11175453255877088, 0.1071668411289643, 0.24630113683744131, 0.5347774894748236]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.2818714866049653
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.11175453255877088, 0.1071668411289643, 0.24630113683744131, 0.5347774894748236]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.11197975588923005, 0.10738280628427126, 0.24478104497572148, 0.5358563928507772]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.11197975588923005, 0.10738280628427126, 0.24478104497572148, 0.5358563928507772]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.11197975588923005, 0.10738280628427126, 0.24478104497572148, 0.5358563928507772]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.11197975588923005, 0.10738280628427126, 0.24478104497572148, 0.5358563928507772]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.11187922669150178, 0.10724680899707754, 0.24570340328694737, 0.5351705610244734]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
using another actor
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.11232917647345633, 0.1076781033769026, 0.24266755519049635, 0.5373251649591447]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.11232917647345633, 0.1076781033769026, 0.24266755519049635, 0.5373251649591447]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.11232917647345633, 0.1076781033769026, 0.24266755519049635, 0.5373251649591447]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.11232917647345633, 0.1076781033769026, 0.24266755519049635, 0.5373251649591447]
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.767]
 [0.703]
 [0.714]
 [0.734]
 [0.74 ]
 [0.713]] [[3.7  ]
 [4.609]
 [3.237]
 [3.178]
 [2.9  ]
 [3.051]
 [3.772]] [[0.716]
 [0.767]
 [0.703]
 [0.714]
 [0.734]
 [0.74 ]
 [0.713]]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.11223110193674876, 0.10754413486552006, 0.2435753479811198, 0.5366494152166115]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.11223110193674876, 0.10754413486552006, 0.2435753479811198, 0.5366494152166115]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.11223110193674876, 0.10754413486552006, 0.2435753479811198, 0.5366494152166115]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.11213255283358228, 0.10740951810267292, 0.24448753343088317, 0.5359703956328615]
using another actor
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.683]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[1.616]
 [2.51 ]
 [1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]] [[0.295]
 [0.688]
 [0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]]
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.11235896302227366, 0.10762635212266292, 0.24296108852370596, 0.5370535963313574]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.11235896302227366, 0.10762635212266292, 0.24296108852370596, 0.5370535963313574]
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.11235896302227366, 0.10762635212266292, 0.24296108852370596, 0.5370535963313574]
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.11226160552035544, 0.10749257186798214, 0.24386711617333132, 0.5363787064383312]
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.11226160552035544, 0.10749257186798214, 0.24386711617333132, 0.5363787064383312]
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.11226160552035544, 0.10749257186798214, 0.24386711617333132, 0.5363787064383312]
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]] [[0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]] [[0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.11261901235403046, 0.10779384659131469, 0.2417100725628138, 0.5378770684918412]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3892966535656712
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.11261901235403046, 0.10779384659131469, 0.2417100725628138, 0.5378770684918412]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.11045607392103805, 0.10805635533193082, 0.24229915965943086, 0.5391884110876003]
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.1104561904250899, 0.10805645560328918, 0.24229844598253766, 0.5391889079890834]
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.1104561904250899, 0.10805645560328918, 0.24229844598253766, 0.5391889079890834]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.5  ]
 [0.507]
 [0.521]
 [0.502]
 [0.537]
 [0.52 ]] [[0.426]
 [0.958]
 [0.341]
 [0.238]
 [0.1  ]
 [0.181]
 [0.47 ]] [[0.592]
 [0.759]
 [0.57 ]
 [0.545]
 [0.493]
 [0.534]
 [0.617]]
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.11023115836287241, 0.10779438861705005, 0.24410812536970752, 0.5378663276503699]
siam score:  -0.84748244
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.11023115836287241, 0.10779438861705005, 0.24410812536970752, 0.5378663276503699]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.11205636559582037, 0.10722964230577925, 0.24566548417497047, 0.5350485079234301]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.11205636559582037, 0.10722964230577925, 0.24566548417497047, 0.5350485079234301]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.513]
 [0.505]
 [0.504]
 [0.499]
 [0.351]
 [0.555]] [[0.264]
 [1.382]
 [0.264]
 [0.736]
 [0.84 ]
 [0.739]
 [1.485]] [[0.35 ]
 [0.545]
 [0.35 ]
 [0.428]
 [0.44 ]
 [0.275]
 [0.604]]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10761791644278716, 0.10761791644278716, 0.24778281220839685, 0.5369813549060289]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.1095562100847816, 0.10719083034908815, 0.2483951319599155, 0.5348578276062148]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10943997663639314, 0.1070568376283996, 0.24932130121000884, 0.5341818845251984]
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[3.072]
 [3.072]
 [3.072]
 [3.072]
 [3.072]
 [3.072]
 [3.072]] [[0.55]
 [0.55]
 [0.55]
 [0.55]
 [0.55]
 [0.55]
 [0.55]]
siam score:  -0.8505556
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10943997663639314, 0.1070568376283996, 0.24932130121000884, 0.5341818845251984]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10920581412008577, 0.10678689749431343, 0.2511871507149072, 0.5328201376706936]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10920581412008577, 0.10678689749431343, 0.2511871507149072, 0.5328201376706936]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10920581412008577, 0.10678689749431343, 0.2511871507149072, 0.5328201376706936]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.576]
 [0.601]
 [0.533]
 [0.538]
 [0.486]
 [0.534]] [[2.634]
 [2.217]
 [1.573]
 [2.401]
 [2.46 ]
 [1.464]
 [2.567]] [[0.53 ]
 [0.576]
 [0.601]
 [0.533]
 [0.538]
 [0.486]
 [0.534]]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10920581412008577, 0.10678689749431343, 0.2511871507149072, 0.5328201376706936]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.10920592727114728, 0.10678699437731506, 0.2511864608122725, 0.532820617539265]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.10920592727114728, 0.10678699437731506, 0.2511864608122725, 0.532820617539265]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.1090879903787001, 0.10665103769390427, 0.25212620499143884, 0.5321347669359568]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.1090879903787001, 0.10665103769390427, 0.25212620499143884, 0.5321347669359568]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.1090879903787001, 0.10665103769390427, 0.25212620499143884, 0.5321347669359568]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.1090879903787001, 0.10665103769390427, 0.25212620499143884, 0.5321347669359568]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.1090879903787001, 0.10665103769390427, 0.25212620499143884, 0.5321347669359568]
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.431]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.403]
 [0.48 ]] [[3.972]
 [5.774]
 [3.972]
 [3.972]
 [3.972]
 [3.392]
 [3.972]] [[0.424]
 [0.799]
 [0.424]
 [0.424]
 [0.424]
 [0.274]
 [0.424]]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.1090879903787001, 0.10665103769390427, 0.25212620499143884, 0.5321347669359568]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.1090879903787001, 0.10665103769390427, 0.25212620499143884, 0.5321347669359568]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.1090879903787001, 0.10665103769390427, 0.25212620499143884, 0.5321347669359568]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.222]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]] [[6.065]
 [6.463]
 [6.065]
 [6.065]
 [6.065]
 [6.065]
 [6.065]] [[0.506]
 [0.602]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.1090879903787001, 0.10665103769390427, 0.25212620499143884, 0.5321347669359568]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.11102222411087304, 0.10623095976320275, 0.2527003588170872, 0.530046457308837]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.11102222411087304, 0.10623095976320275, 0.2527003588170872, 0.530046457308837]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]] [[0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.11091908080883713, 0.1060925558479461, 0.2536398751445792, 0.5293484881986376]
line 256 mcts: sample exp_bonus 3.6749746653692057
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.11091908080883713, 0.1060925558479461, 0.2536398751445792, 0.5293484881986376]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.11115211658857294, 0.10631543760197229, 0.25207063156122594, 0.5304618142482289]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.11115211658857294, 0.10631543760197229, 0.25207063156122594, 0.5304618142482289]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.11115211658857294, 0.10631543760197229, 0.25207063156122594, 0.5304618142482289]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.11115211658857294, 0.10631543760197229, 0.25207063156122594, 0.5304618142482289]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.11138402227695253, 0.10653723850590777, 0.2505089979191928, 0.531569741297947]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.11138402227695253, 0.10653723850590777, 0.2505089979191928, 0.531569741297947]
from probs:  [0.11138402227695253, 0.10653723850590777, 0.2505089979191928, 0.531569741297947]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
from probs:  [0.11161493377269018, 0.10675806170887013, 0.24895422398996106, 0.5326727805284787]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]] [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]]
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.26 ]
 [0.209]
 [0.208]
 [0.213]
 [0.211]
 [0.212]] [[0.483]
 [1.575]
 [0.494]
 [0.322]
 [0.453]
 [0.377]
 [0.661]] [[0.211]
 [0.26 ]
 [0.209]
 [0.208]
 [0.213]
 [0.211]
 [0.212]]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.491]
 [0.491]
 [0.471]
 [0.471]
 [0.474]
 [0.488]] [[1.61 ]
 [1.41 ]
 [1.572]
 [1.463]
 [1.379]
 [1.572]
 [1.572]] [[0.745]
 [0.629]
 [0.737]
 [0.644]
 [0.588]
 [0.72 ]
 [0.734]]
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]] [[8.792]
 [3.089]
 [3.089]
 [3.089]
 [3.089]
 [3.089]
 [3.089]] [[0.8  ]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.11161518579742774, 0.10675824980425579, 0.24895285232701728, 0.5326737120712992]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.11151603719166955, 0.10662322371655132, 0.24986819715052974, 0.5319925419412492]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
siam score:  -0.84705573
siam score:  -0.8457388
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.066]
 [-0.061]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.051]] [[0.147]
 [0.147]
 [0.092]
 [1.178]
 [1.178]
 [1.178]
 [0.067]] [[-0.319]
 [-0.312]
 [-0.332]
 [ 0.091]
 [ 0.091]
 [ 0.091]
 [-0.335]]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.10920425967639466, 0.10675240509162762, 0.25141227559038465, 0.5326310596415931]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.249780562988622
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.10943286872617825, 0.10697587435424297, 0.24984396477505144, 0.5337472921445273]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.419214477112866
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.10966039338254986, 0.10719828360372059, 0.2482830931486646, 0.5348582298650649]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.10966039338254986, 0.10719828360372059, 0.2482830931486646, 0.5348582298650649]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.10966039338254986, 0.10719828360372059, 0.2482830931486646, 0.5348582298650649]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.10966039338254986, 0.10719828360372059, 0.2482830931486646, 0.5348582298650649]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.10966039338254986, 0.10719828360372059, 0.2482830931486646, 0.5348582298650649]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.10988684134288669, 0.10741964036437872, 0.24672960790543996, 0.5359639103872947]
actor:  1 policy actor:  1  step number:  54 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.1153795694324874, 0.1138068295123208, 0.20261124761102048, 0.5682023534441714]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.1153795694324874, 0.1138068295123208, 0.20261124761102048, 0.5682023534441714]
Starting evaluation
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.1153795694324874, 0.1138068295123208, 0.20261124761102048, 0.5682023534441714]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.1153795694324874, 0.1138068295123208, 0.20261124761102048, 0.5682023534441714]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.11537964571903345, 0.1138068960413562, 0.20261077486597853, 0.5682026833736318]
line 256 mcts: sample exp_bonus 3.8338155888726453
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.11537964571903345, 0.1138068960413562, 0.20261077486597853, 0.5682026833736318]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.514]
 [0.522]
 [0.509]
 [0.509]
 [0.491]
 [0.511]] [[4.868]
 [4.998]
 [4.757]
 [4.248]
 [4.32 ]
 [3.764]
 [4.369]] [[0.519]
 [0.514]
 [0.522]
 [0.509]
 [0.509]
 [0.491]
 [0.511]]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.11531752600927683, 0.11373455158128397, 0.20311041637127472, 0.5678375060381645]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.11648613224556965, 0.11334305806484639, 0.20428808626189038, 0.5658827234276935]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.402335758429766
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11637684822855415, 0.11319351910212411, 0.20530142504333998, 0.5651282076259818]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.672]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[3.076]
 [3.053]
 [3.076]
 [3.076]
 [3.076]
 [3.076]
 [3.076]] [[1.025]
 [1.037]
 [1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.025]]
siam score:  -0.83651507
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11653188180816135, 0.11334430648100285, 0.2042419869867128, 0.565881824724123]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11511082244186613, 0.11352647251450261, 0.20457043649744042, 0.5667922685461908]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.533]
 [0.505]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[2.936]
 [1.057]
 [1.021]
 [2.936]
 [2.936]
 [2.936]
 [2.936]] [[0.554]
 [0.027]
 [0.005]
 [0.554]
 [0.554]
 [0.554]
 [0.554]]
siam score:  -0.8346151
siam score:  -0.83651304
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11370649382777546, 0.11370649382777546, 0.20489501902871168, 0.5676919933157374]
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.034]
 [-0.021]
 [-0.023]
 [-0.023]
 [-0.017]
 [-0.003]] [[4.114]
 [4.299]
 [4.173]
 [4.117]
 [4.187]
 [3.954]
 [3.627]] [[0.26 ]
 [0.338]
 [0.29 ]
 [0.263]
 [0.295]
 [0.194]
 [0.056]]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11339145534556914, 0.11496691937635248, 0.20552577520624848, 0.56611585007183]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.7  ]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[2.155]
 [2.41 ]
 [2.155]
 [2.155]
 [2.155]
 [2.155]
 [2.155]] [[0.661]
 [0.87 ]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]]
siam score:  -0.834817
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11339145534556914, 0.11496691937635248, 0.20552577520624848, 0.56611585007183]
using explorer policy with actor:  1
siam score:  -0.83650744
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
siam score:  -0.8361244
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11331803899342194, 0.11490343471665618, 0.20603317168838373, 0.5657453546015381]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.429992702742922
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11292719884950653, 0.11607466746898913, 0.2072041674872631, 0.5637939661942413]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11307777284791647, 0.11622944379341672, 0.20614627079521025, 0.5645465125634567]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11276772967351081, 0.1174319765197091, 0.20679791721501917, 0.5630023765917609]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
first move QE:  0.2238043348407046
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]] [[3.409]
 [3.409]
 [3.409]
 [3.409]
 [3.409]
 [3.409]
 [3.409]] [[0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.1126917202872611, 0.11738486146934564, 0.20730441126648594, 0.5626190069769073]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.1126917202872611, 0.11738486146934564, 0.20730441126648594, 0.5626190069769073]
first move QE:  0.22416070522694073
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.1126917202872611, 0.11738486146934564, 0.20730441126648594, 0.5626190069769073]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.1123884653587338, 0.11856313005064341, 0.20793969640767343, 0.5611087081829493]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.1123884653587338, 0.11856313005064341, 0.20793969640767343, 0.5611087081829493]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.1126656888144276, 0.1157359726189942, 0.2091075394319778, 0.5624907991346004]
line 256 mcts: sample exp_bonus 5.716045180429308
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.1126656888144276, 0.1157359726189942, 0.2091075394319778, 0.5624907991346004]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11258975742391619, 0.11567888496340384, 0.20962351575554372, 0.5621078418571364]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11258975742391619, 0.11567888496340384, 0.20962351575554372, 0.5621078418571364]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11266484000480151, 0.11577705045095345, 0.20907849413357504, 0.56247961541067]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11266484000480151, 0.11577705045095345, 0.20907849413357504, 0.56247961541067]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.546]
 [0.574]
 [0.587]
 [0.591]
 [0.567]
 [0.578]] [[2.894]
 [3.176]
 [2.711]
 [2.51 ]
 [2.713]
 [2.577]
 [3.166]] [[0.629]
 [0.749]
 [0.55 ]
 [0.465]
 [0.563]
 [0.482]
 [0.767]]
siam score:  -0.840561
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.656]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.653]] [[2.548]
 [2.463]
 [2.548]
 [2.548]
 [2.548]
 [2.548]
 [2.653]] [[1.035]
 [0.984]
 [1.035]
 [1.035]
 [1.035]
 [1.035]
 [1.075]]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11306933841210048, 0.11463015485849937, 0.20780278571028363, 0.5644977210191164]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.11306933841210048, 0.11463015485849937, 0.20780278571028363, 0.5644977210191164]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.11314686586506836, 0.11471933057173277, 0.20725214222017516, 0.5648816613430238]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
siam score:  -0.83623606
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.11300148342308633, 0.11459317409975525, 0.2082573517201306, 0.5641479907570278]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5400],
        [-0.3367],
        [-0.6138],
        [-0.5740],
        [-0.4164],
        [-0.0000],
        [-0.6206],
        [-0.6985],
        [-0.4761],
        [-0.6103]], dtype=torch.float64)
-0.09703970119800001 -0.6369962467919594
-0.084359833866 -0.42107111035419215
-0.032346567066 -0.64612908212485
-0.032346567066 -0.6063416210506255
-0.09703970119800001 -0.5134217198759851
-0.932617147638 -0.932617147638
-0.032346567066 -0.6529129587140999
-0.032346567066 -0.7308714472652046
-0.084359833866 -0.560410215983371
-0.032346567066 -0.6426442697370034
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.11292845735735864, 0.11452980530027018, 0.20876227166791128, 0.5637794656744598]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.11292845735735864, 0.11452980530027018, 0.20876227166791128, 0.5637794656744598]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.11289196342973803, 0.11289196342973803, 0.21062977102619418, 0.5635863021143298]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.11289196342973803, 0.11289196342973803, 0.21062977102619418, 0.5635863021143298]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.468]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.416]] [[0.684]
 [0.355]
 [1.593]
 [1.593]
 [1.593]
 [1.593]
 [0.643]] [[ 0.297]
 [-0.003]
 [ 0.879]
 [ 0.879]
 [ 0.879]
 [ 0.879]
 [ 0.138]]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.11281907484163348, 0.11281907484163348, 0.2111434388431842, 0.5632184114735488]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.448]
 [0.46 ]
 [0.46 ]
 [0.45 ]
 [0.452]
 [0.457]] [[2.274]
 [2.16 ]
 [2.058]
 [2.13 ]
 [1.999]
 [2.255]
 [2.364]] [[0.26 ]
 [0.215]
 [0.194]
 [0.218]
 [0.164]
 [0.251]
 [0.292]]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.11274595898622135, 0.11274595898622135, 0.21165870828567188, 0.5628493737418854]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.11274595898622135, 0.11274595898622135, 0.21165870828567188, 0.5628493737418854]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[4.668]
 [4.668]
 [4.668]
 [4.668]
 [4.668]
 [4.668]
 [4.668]] [[0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.11403903543801581, 0.11242053283746185, 0.2123115186239151, 0.5612289131006072]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.11403903543801581, 0.11242053283746185, 0.2123115186239151, 0.5612289131006072]
actor:  1 policy actor:  1  step number:  60 total reward:  0.03333333333333266  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.10001686858017796, 0.09859763797726454, 0.3092373057810031, 0.4921481876615544]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.10119914596947267, 0.09839663946600202, 0.30925551110577537, 0.49114870345874995]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.36 ]
 [0.354]
 [0.374]
 [0.351]
 [0.337]
 [0.351]] [[2.926]
 [2.545]
 [2.799]
 [3.06 ]
 [2.73 ]
 [2.836]
 [2.843]] [[0.509]
 [0.343]
 [0.455]
 [0.59 ]
 [0.422]
 [0.461]
 [0.474]]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.10119914596947267, 0.09839663946600202, 0.30925551110577537, 0.49114870345874995]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.10119914596947267, 0.09839663946600202, 0.30925551110577537, 0.49114870345874995]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.1010759118209774, 0.0982589175443533, 0.31020784035650717, 0.49045733027816213]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[-0.036]
 [-0.036]
 [-0.036]
 [-0.128]
 [-0.036]
 [-0.036]
 [-0.036]] [[0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08631567735584919, 0.08396918362925192, 0.4190533800055627, 0.4106617590093362]
line 256 mcts: sample exp_bonus 2.4255498176452948
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08631567735584919, 0.08396918362925192, 0.4190533800055627, 0.4106617590093362]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08650002686234427, 0.08414851287104054, 0.4199496135142148, 0.4094018467524005]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08650002686234427, 0.08414851287104054, 0.4199496135142148, 0.4094018467524005]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.08650001022071631, 0.08414848337827237, 0.41994946399591154, 0.40940204240509975]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.08650001022071631, 0.08414848337827237, 0.41994946399591154, 0.40940204240509975]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.08633659509092323, 0.08397508499325254, 0.4190803795268229, 0.41060794038900134]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.08633659509092323, 0.08397508499325254, 0.4190803795268229, 0.41060794038900134]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.0861732014587094, 0.08380172115942697, 0.4182114699156968, 0.4118136074661669]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.0861732014587094, 0.08380172115942697, 0.4182114699156968, 0.4118136074661669]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.0861732014587094, 0.08380172115942697, 0.4182114699156968, 0.4118136074661669]
siam score:  -0.8408176
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.219]
 [0.175]
 [0.178]
 [0.183]
 [0.178]
 [0.181]] [[1.897]
 [2.084]
 [1.92 ]
 [1.74 ]
 [1.661]
 [1.705]
 [1.892]] [[-0.134]
 [ 0.041]
 [-0.085]
 [-0.172]
 [-0.207]
 [-0.19 ]
 [-0.093]]
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.08654112774445519, 0.08415947797570242, 0.4199994051266553, 0.40929998915318705]
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.08654112774445519, 0.08415947797570242, 0.4199994051266553, 0.40929998915318705]
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.08654112774445519, 0.08415947797570242, 0.4199994051266553, 0.40929998915318705]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7520399568198481
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.0862160517714091, 0.0838142905662571, 0.41826926217212745, 0.4117003954902063]
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.504]
 [0.479]
 [0.472]
 [0.476]
 [0.477]
 [0.483]] [[-0.037]
 [ 0.502]
 [ 0.026]
 [-0.046]
 [ 0.001]
 [ 0.005]
 [-0.032]] [[0.474]
 [0.504]
 [0.479]
 [0.472]
 [0.476]
 [0.477]
 [0.483]]
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.0862160517714091, 0.0838142905662571, 0.41826926217212745, 0.4117003954902063]
from probs:  [0.0862160517714091, 0.0838142905662571, 0.41826926217212745, 0.4117003954902063]
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.0863996522524956, 0.08399275420060823, 0.41916114616218775, 0.41044644738470837]
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.0863996522524956, 0.08399275420060823, 0.41916114616218775, 0.41044644738470837]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.3499936019539833
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.08639963612426883, 0.08399272533333067, 0.4191609997888417, 0.4104466387535589]
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.08607539683348757, 0.08364828784325197, 0.4174345936631315, 0.4128417216601289]
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.08607539683348757, 0.08364828784325197, 0.4174345936631315, 0.4128417216601289]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.354]
 [0.295]
 [0.278]
 [0.287]
 [0.286]
 [0.296]] [[1.412]
 [2.077]
 [1.803]
 [1.383]
 [1.584]
 [1.675]
 [1.806]] [[0.279]
 [0.354]
 [0.295]
 [0.278]
 [0.287]
 [0.286]
 [0.296]]
siam score:  -0.8338484
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.08591327150831403, 0.08347606306461267, 0.4165713603583771, 0.4140393050686963]
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.08609735021771156, 0.08365491065693124, 0.4174651490559933, 0.41278259006936385]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.08628063736343228, 0.0838329891800934, 0.418355094339724, 0.4115312791167502]
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
siam score:  -0.83974355
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.08646312222102526, 0.08401027480201602, 0.41924107496429497, 0.4102855280126637]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using another actor
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.08664482653261556, 0.08418680210697124, 0.4201232659165275, 0.4090451054438858]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.42 ]
 [0.41 ]
 [0.41 ]
 [0.406]
 [0.405]
 [0.412]] [[3.634]
 [3.997]
 [3.657]
 [3.657]
 [3.083]
 [3.175]
 [3.454]] [[0.333]
 [0.418]
 [0.337]
 [0.337]
 [0.205]
 [0.226]
 [0.292]]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.08664482653261556, 0.08418680210697124, 0.4201232659165275, 0.4090451054438858]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.419]
 [0.433]
 [0.432]
 [0.429]
 [0.43 ]
 [0.448]] [[ 0.01 ]
 [ 0.05 ]
 [ 0.052]
 [ 0.005]
 [ 0.003]
 [-0.019]
 [ 0.007]] [[0.439]
 [0.419]
 [0.433]
 [0.432]
 [0.429]
 [0.43 ]
 [0.448]]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08682575529642139, 0.08436257595094653, 0.4210016914641304, 0.40780997728850166]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08682575529642139, 0.08436257595094653, 0.4210016914641304, 0.40780997728850166]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08682575529642139, 0.08436257595094653, 0.4210016914641304, 0.40780997728850166]
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.337]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]] [[3.592]
 [5.593]
 [3.592]
 [3.592]
 [3.592]
 [3.592]
 [3.592]] [[0.159]
 [0.695]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08700592782132324, 0.08453762828070385, 0.4218765133692878, 0.40657993052868513]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08700592782132324, 0.08453762828070385, 0.4218765133692878, 0.40657993052868513]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08700592782132324, 0.08453762828070385, 0.4218765133692878, 0.40657993052868513]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.0871853338250965, 0.08471193587255263, 0.4227476134557357, 0.40535511684661507]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.56 ]
 [0.549]
 [0.522]
 [0.531]
 [0.539]
 [0.537]] [[0.799]
 [2.453]
 [3.187]
 [2.444]
 [0.366]
 [2.033]
 [1.923]] [[0.29 ]
 [0.839]
 [1.059]
 [0.813]
 [0.18 ]
 [0.697]
 [0.663]]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.08754185250262758, 0.08505830963058816, 0.4244786100092702, 0.40292122785751416]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.08754185250262758, 0.08505830963058816, 0.4244786100092702, 0.40292122785751416]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.08722513499150238, 0.08472061417120397, 0.42278579927506166, 0.40526845156223207]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.08708713212385388, 0.08455642011181466, 0.4219600364490866, 0.4063964113152448]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.08708713212385388, 0.08455642011181466, 0.4219600364490866, 0.4063964113152448]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.166]
 [0.183]
 [0.212]
 [0.213]
 [0.212]
 [0.211]] [[4.518]
 [5.702]
 [5.465]
 [5.623]
 [5.683]
 [5.651]
 [5.736]] [[0.381]
 [0.724]
 [0.649]
 [0.721]
 [0.743]
 [0.731]
 [0.761]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]] [[4.102]
 [4.102]
 [4.102]
 [4.102]
 [4.102]
 [4.102]
 [4.102]] [[2.699]
 [2.699]
 [2.699]
 [2.699]
 [2.699]
 [2.699]
 [2.699]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.08708713212385388, 0.08455642011181466, 0.4219600364490866, 0.4063964113152448]
actor:  1 policy actor:  1  step number:  55 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8375284
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.09395773972249209, 0.09198622830426459, 0.3548352126990448, 0.4592208192741985]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.09395773972249209, 0.09198622830426459, 0.3548352126990448, 0.4592208192741985]
siam score:  -0.8396381
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.09398120954360971, 0.09199831178458033, 0.3547411251806037, 0.45927935349120635]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.09413274569357376, 0.09214663617034229, 0.3536998259744723, 0.4600207921616117]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.09400479798863773, 0.092010465839036, 0.3546465033565214, 0.4593382328158048]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.09400479798863773, 0.092010465839036, 0.3546465033565214, 0.4593382328158048]
siam score:  -0.8458058
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.09400479798863773, 0.092010465839036, 0.3546465033565214, 0.4593382328158048]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.09387683965437525, 0.09187428419530404, 0.35559325938431807, 0.4586556167660025]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.09387683965437525, 0.09187428419530404, 0.35559325938431807, 0.4586556167660025]
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 3 threads
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.09387683965437525, 0.09187428419530404, 0.35559325938431807, 0.4586556167660025]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.10137961423924113, 0.09984932337981992, 0.30014179905472693, 0.498629263326212]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.10140782430699737, 0.09986941581304186, 0.29999434941430014, 0.4987284104656605]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.10140782430699737, 0.09986941581304186, 0.29999434941430014, 0.4987284104656605]
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.1013119404717547, 0.09976728045219065, 0.30070434029472964, 0.4982164387813251]
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.10061224930140006, 0.09984487833707577, 0.3009384599867703, 0.49860441237475384]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.10061224930140006, 0.09984487833707577, 0.3009384599867703, 0.49860441237475384]
maxi score, test score, baseline:  -0.9961121212121212 -1.0 -0.9961121212121212
probs:  [0.10061224306896242, 0.09984486758978221, 0.3009385321517183, 0.49860435718953694]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9961121212121212 -1.0 -0.9961121212121212
maxi score, test score, baseline:  -0.9961121212121212 -1.0 -0.9961121212121212
probs:  [0.10051344805727046, 0.09974296870472835, 0.30165001509292205, 0.49809356814507916]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9961121212121212 -1.0 -0.9961121212121212
probs:  [0.10051344805727046, 0.09974296870472835, 0.30165001509292205, 0.49809356814507916]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.502]
 [0.662]
 [0.519]
 [0.528]
 [0.519]
 [0.52 ]] [[4.625]
 [4.032]
 [2.221]
 [3.42 ]
 [3.216]
 [0.892]
 [4.056]] [[0.517]
 [0.502]
 [0.662]
 [0.519]
 [0.528]
 [0.519]
 [0.52 ]]
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.009]
 [-0.009]
 [-0.013]
 [-0.009]
 [-0.012]
 [-0.009]] [[4.42 ]
 [3.713]
 [3.713]
 [4.236]
 [3.713]
 [4.306]
 [3.713]] [[ 0.291]
 [-0.013]
 [-0.013]
 [ 0.212]
 [-0.013]
 [ 0.244]
 [-0.013]]
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.014]
 [-0.018]
 [-0.017]
 [-0.017]
 [-0.016]
 [-0.012]] [[4.229]
 [3.874]
 [4.13 ]
 [4.198]
 [4.291]
 [4.125]
 [4.067]] [[0.204]
 [0.05 ]
 [0.161]
 [0.192]
 [0.234]
 [0.16 ]
 [0.137]]
maxi score, test score, baseline:  -0.9961121212121212 -1.0 -0.9961121212121212
probs:  [0.1003158315865963, 0.09953914365642771, 0.303073171438345, 0.4970718533186311]
maxi score, test score, baseline:  -0.9961121212121212 -1.0 -0.9961121212121212
probs:  [0.1003158315865963, 0.09953914365642771, 0.303073171438345, 0.4970718533186311]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  0.667
siam score:  -0.84333396
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.10507871462092276, 0.1044477017808434, 0.2687978761404575, 0.5216757074577762]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.10507871462092276, 0.1044477017808434, 0.2687978761404575, 0.5216757074577762]
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.263]
 [0.276]
 [0.287]
 [0.285]
 [0.287]
 [0.284]] [[5.495]
 [5.189]
 [5.77 ]
 [5.509]
 [5.522]
 [5.586]
 [5.565]] [[ 0.021]
 [-0.056]
 [ 0.054]
 [ 0.022]
 [ 0.022]
 [ 0.035]
 [ 0.028]]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.10507871462092276, 0.1044477017808434, 0.2687978761404575, 0.5216757074577762]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.10507871462092276, 0.1044477017808434, 0.2687978761404575, 0.5216757074577762]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.10507871462092276, 0.1044477017808434, 0.2687978761404575, 0.5216757074577762]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.10507871462092276, 0.1044477017808434, 0.2687978761404575, 0.5216757074577762]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.08923219893596174, 0.23589820444400622, 0.22927834844064626, 0.4455912481793858]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.08917455420631996, 0.23606323344774935, 0.2294576373520918, 0.4453045749938389]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.08917455420631996, 0.23606323344774935, 0.2294576373520918, 0.4453045749938389]
Printing some Q and Qe and total Qs values:  [[0.938]
 [0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]] [[0.   ]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]] [[0.938]
 [0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.607]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[1.582]
 [1.927]
 [1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.619]] [[0.308]
 [0.563]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.08911798002795857, 0.2362249890261148, 0.2296338059871411, 0.44502322495878555]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.08911798002795857, 0.2362249890261148, 0.2296338059871411, 0.44502322495878555]
UNIT TEST: sample policy line 217 mcts : [0.551 0.082 0.02  0.02  0.224 0.02  0.082]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
actor:  1 policy actor:  1  step number:  39 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
using explorer policy with actor:  1
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.0946843114513873, 0.21899461760339178, 0.2134248491262896, 0.47289622181893126]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.0946843114513873, 0.21899461760339178, 0.2134248491262896, 0.47289622181893126]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.0946843114513873, 0.21899461760339178, 0.2134248491262896, 0.47289622181893126]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.09475246011461307, 0.21915238000578785, 0.21285814117915733, 0.4732370187004417]
from probs:  [0.09475246011461307, 0.21915238000578785, 0.21285814117915733, 0.4732370187004417]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.09482016721574014, 0.21930912020328355, 0.21229510515658886, 0.4735756074243874]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.09488743703249275, 0.2194648480987006, 0.2117357054860548, 0.47391200938275185]
line 256 mcts: sample exp_bonus 1.772448871244004
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.09501270128778623, 0.2184334002391673, 0.21201546935514423, 0.4745384291179022]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.09501270128778623, 0.2184334002391673, 0.21201546935514423, 0.4745384291179022]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.09501270128778623, 0.2184334002391673, 0.21201546935514423, 0.4745384291179022]
siam score:  -0.8340027
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[3.274]
 [3.274]
 [3.274]
 [3.274]
 [3.274]
 [3.274]
 [3.274]] [[0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]]
from probs:  [0.09501269630076466, 0.21843324501341177, 0.2120156555272151, 0.47453840315860846]
UNIT TEST: sample policy line 217 mcts : [0.224 0.02  0.061 0.122 0.204 0.143 0.224]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.54 ]
 [0.113]
 [0.513]
 [0.494]
 [0.098]
 [0.482]] [[1.182]
 [1.086]
 [0.754]
 [0.584]
 [0.98 ]
 [0.502]
 [1.142]] [[0.909]
 [0.93 ]
 [0.439]
 [0.7  ]
 [0.848]
 [0.322]
 [0.905]]
using another actor
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.09504243111032126, 0.21906591603128123, 0.21120525464029075, 0.47468639821810676]
maxi score, test score, baseline:  -0.9961962962962964 -1.0 -0.9961962962962964
probs:  [0.09528998368267795, 0.21702962446725624, 0.21175603903268503, 0.47592435281738094]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.589]
 [0.551]
 [0.572]
 [0.595]
 [0.545]
 [0.557]] [[2.239]
 [3.07 ]
 [2.236]
 [1.73 ]
 [1.793]
 [2.344]
 [2.888]] [[0.587]
 [1.034]
 [0.578]
 [0.347]
 [0.401]
 [0.626]
 [0.911]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.09518680461521829, 0.2173485318099826, 0.212056989753465, 0.47540767382133414]
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.09518680461521829, 0.2173485318099826, 0.212056989753465, 0.47540767382133414]
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.09518680461521829, 0.2173485318099826, 0.212056989753465, 0.47540767382133414]
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.09518680461521829, 0.2173485318099826, 0.212056989753465, 0.47540767382133414]
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.09530878427653224, 0.21634457746652136, 0.21232897206024282, 0.47601766619670366]
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.549]
 [0.488]
 [0.467]
 [0.488]
 [0.464]
 [0.488]] [[4.497]
 [4.59 ]
 [3.441]
 [3.487]
 [3.441]
 [3.545]
 [3.441]] [[0.27 ]
 [0.292]
 [0.04 ]
 [0.026]
 [0.04 ]
 [0.033]
 [0.04 ]]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.019]
 [0.021]
 [0.018]
 [0.019]
 [0.011]
 [0.005]] [[0.903]
 [1.2  ]
 [1.102]
 [1.002]
 [0.986]
 [0.906]
 [1.237]] [[0.016]
 [0.019]
 [0.021]
 [0.018]
 [0.019]
 [0.011]
 [0.005]]
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.0955633004807582, 0.21565428088835348, 0.21149197513415374, 0.4772904434967346]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.09556329707359437, 0.2156541256377919, 0.21149215182940012, 0.47729042545921346]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.09556329707359437, 0.2156541256377919, 0.21149215182940012, 0.47729042545921346]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.09556329707359437, 0.2156541256377919, 0.21149215182940012, 0.47729042545921346]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.09546110755064321, 0.21596830120519214, 0.21179189748053207, 0.4767786937636325]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.09529956238624189, 0.21645613751001788, 0.21227289076118644, 0.47597140934255383]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.09529956238624189, 0.21645613751001788, 0.21227289076118644, 0.47597140934255383]
siam score:  -0.82661986
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.09529956238624189, 0.21645613751001788, 0.21227289076118644, 0.47597140934255383]
siam score:  -0.82989335
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.09529956238624189, 0.21645613751001788, 0.21227289076118644, 0.47597140934255383]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.09541945278533279, 0.21546942378328013, 0.21254016981255422, 0.47657095361883284]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.09553801944411554, 0.21449360461309258, 0.21280449776811694, 0.4771638781746749]
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.068]
 [-0.073]
 [-0.063]
 [-0.068]
 [-0.063]
 [-0.07 ]] [[4.234]
 [3.991]
 [3.808]
 [4.234]
 [3.56 ]
 [4.234]
 [3.829]] [[-0.379]
 [-0.425]
 [-0.46 ]
 [-0.379]
 [-0.497]
 [-0.379]
 [-0.454]]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.09553801944411554, 0.21449360461309258, 0.21280449776811694, 0.4771638781746749]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.468]
 [0.436]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[1.989]
 [2.371]
 [2.029]
 [1.996]
 [1.912]
 [1.912]
 [1.912]] [[0.436]
 [0.468]
 [0.436]
 [0.429]
 [0.429]
 [0.429]
 [0.429]]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.09547904752132454, 0.21466841113759352, 0.21298231673532664, 0.4768702246057553]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.579]
 [0.58 ]
 [0.581]
 [0.58 ]
 [0.58 ]
 [0.585]] [[3.859]
 [4.233]
 [3.841]
 [3.701]
 [3.841]
 [3.841]
 [3.799]] [[0.795]
 [0.952]
 [0.784]
 [0.725]
 [0.784]
 [0.784]
 [0.769]]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.09554568970361643, 0.21481837558114147, 0.21243244702803404, 0.47720348768720805]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.09554568970361643, 0.21481837558114147, 0.21243244702803404, 0.47720348768720805]
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[3.871]
 [3.871]
 [3.871]
 [3.871]
 [3.871]
 [3.871]
 [3.871]] [[0.885]
 [0.885]
 [0.885]
 [0.885]
 [0.885]
 [0.885]
 [0.885]]
actor:  1 policy actor:  1  step number:  69 total reward:  0.10666666666666635  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
siam score:  -0.82770574
first move QE:  0.29671565602025085
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.09437147653573665, 0.20075157122442105, 0.23353067496298519, 0.4713462772768572]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.09444193242112756, 0.20090140281600188, 0.2329580426635757, 0.47169862209929486]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.562]
 [0.488]
 [0.547]
 [0.548]
 [0.547]
 [0.54 ]] [[2.185]
 [1.97 ]
 [1.565]
 [2.026]
 [1.621]
 [2.026]
 [2.084]] [[0.462]
 [0.381]
 [0.173]
 [0.386]
 [0.251]
 [0.386]
 [0.398]]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.416]
 [0.336]
 [0.185]
 [0.034]
 [0.315]
 [0.444]] [[1.703]
 [1.749]
 [0.75 ]
 [2.021]
 [1.977]
 [1.082]
 [1.612]] [[ 0.424]
 [ 0.355]
 [-0.058]
 [ 0.214]
 [ 0.048]
 [ 0.032]
 [ 0.336]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.09444193242112756, 0.20090140281600188, 0.2329580426635757, 0.47169862209929486]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.09444193242112756, 0.20090140281600188, 0.2329580426635757, 0.47169862209929486]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.528]
 [0.525]
 [0.525]
 [0.522]
 [0.524]
 [0.519]] [[1.969]
 [1.933]
 [2.02 ]
 [1.982]
 [1.883]
 [1.992]
 [1.957]] [[0.503]
 [0.496]
 [0.551]
 [0.525]
 [0.457]
 [0.532]
 [0.502]]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.507]
 [0.483]
 [0.509]
 [0.496]
 [0.496]
 [0.513]] [[0.159]
 [1.037]
 [0.692]
 [0.339]
 [0.597]
 [0.597]
 [0.626]] [[0.496]
 [0.817]
 [0.679]
 [0.564]
 [0.652]
 [0.651]
 [0.671]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.446]
 [0.481]
 [0.481]] [[1.508]
 [1.508]
 [1.508]
 [1.508]
 [2.111]
 [1.508]
 [1.508]] [[0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.462]
 [0.095]
 [0.095]]
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.332]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]] [[4.919]
 [5.163]
 [4.919]
 [4.919]
 [4.919]
 [4.919]
 [4.919]] [[0.751]
 [0.825]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.09461634041575366, 0.1995572908674864, 0.2332561530400801, 0.47257021567667984]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.09451498707035938, 0.19980556654592446, 0.233616702337491, 0.4720627440462253]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.09451498707035938, 0.19980556654592446, 0.233616702337491, 0.4720627440462253]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.09465501491209552, 0.20010183452506386, 0.2324801375429562, 0.4727630130198844]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.09465501491209552, 0.20010183452506386, 0.2324801375429562, 0.4727630130198844]
siam score:  -0.8301346
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.09472439281805006, 0.2002486228475805, 0.2319170174916236, 0.4731099668427459]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.09472439281805006, 0.2002486228475805, 0.2319170174916236, 0.4731099668427459]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.192]
 [0.428]
 [0.462]
 [0.422]
 [0.752]
 [0.359]] [[0.013]
 [0.246]
 [0.203]
 [0.212]
 [0.202]
 [0.176]
 [0.475]] [[0.368]
 [0.268]
 [0.473]
 [0.513]
 [0.467]
 [0.779]
 [0.586]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.09027942588556978, 0.19128603718523443, 0.26755410753735154, 0.4508804293918442]
siam score:  -0.83460325
maxi score, test score, baseline:  -0.9962768115942029 -1.0 -0.9962768115942029
probs:  [0.09024796671204365, 0.19166149281715142, 0.2673680207474004, 0.45072251972340455]
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.09024795632225417, 0.1916613050674003, 0.2673682716217058, 0.45072246698863966]
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.09013849134905492, 0.19187093634892702, 0.26781611424111895, 0.45017445806089923]
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.09002916617549822, 0.19208029990665687, 0.26826338491284607, 0.44962714900499884]
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.09002916617549822, 0.19208029990665687, 0.26826338491284607, 0.44962714900499884]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.369]
 [0.384]
 [0.371]
 [0.392]
 [0.405]
 [0.39 ]] [[5.083]
 [4.476]
 [4.637]
 [4.695]
 [4.624]
 [4.406]
 [4.681]] [[0.835]
 [0.61 ]
 [0.681]
 [0.696]
 [0.681]
 [0.604]
 [0.701]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.09280893266483124, 0.18675689832464698, 0.25689081641393724, 0.4635433525965845]
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.0927071474654677, 0.18695144065232935, 0.25730762267763624, 0.4630337892045667]
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.0927071474654677, 0.18695144065232935, 0.25730762267763624, 0.4630337892045667]
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.0927071474654677, 0.18695144065232935, 0.25730762267763624, 0.4630337892045667]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.557]
 [0.519]
 [0.521]
 [0.521]
 [0.521]
 [0.497]] [[5.218]
 [5.238]
 [4.652]
 [5.22 ]
 [5.22 ]
 [5.22 ]
 [5.243]] [[0.791]
 [0.821]
 [0.615]
 [0.798]
 [0.798]
 [0.798]
 [0.794]]
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.092754020815225, 0.1874465123648012, 0.25653179019601025, 0.4632676766239636]
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.09271088719805838, 0.18762465076293902, 0.256611524157131, 0.4630529378818716]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.3154028167117384
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.0928574190146665, 0.18792143905506625, 0.2554353895981395, 0.4637857523321279]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.0928574190146665, 0.18792143905506625, 0.2554353895981395, 0.4637857523321279]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.0928574190146665, 0.18792143905506625, 0.2554353895981395, 0.4637857523321279]
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
probs:  [0.09281472110031162, 0.1880963108412543, 0.25551579082395254, 0.46357317723448155]
siam score:  -0.83162856
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.691]
 [0.573]
 [0.574]
 [0.575]
 [0.574]
 [0.578]] [[1.615]
 [2.336]
 [1.646]
 [1.493]
 [1.485]
 [1.543]
 [1.591]] [[0.316]
 [0.781]
 [0.327]
 [0.249]
 [0.245]
 [0.274]
 [0.303]]
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.607]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[2.42 ]
 [2.705]
 [2.545]
 [2.545]
 [2.545]
 [2.545]
 [2.545]] [[0.626]
 [0.804]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]]
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.188]
 [0.215]
 [0.209]
 [0.216]
 [0.213]
 [0.215]] [[5.024]
 [5.176]
 [4.949]
 [4.758]
 [4.753]
 [4.78 ]
 [4.925]] [[0.511]
 [0.546]
 [0.483]
 [0.416]
 [0.417]
 [0.425]
 [0.475]]
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
probs:  [0.09238455095375418, 0.18923157118849587, 0.2569646076114144, 0.46141927024633567]
line 256 mcts: sample exp_bonus 2.7179091889023117
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
probs:  [0.09228436700363998, 0.18942852794600948, 0.2573693791506704, 0.4609177258996802]
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
probs:  [0.09228436700363998, 0.18942852794600948, 0.2573693791506704, 0.4609177258996802]
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
probs:  [0.09228436700363998, 0.18942852794600948, 0.2573693791506704, 0.4609177258996802]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
probs:  [0.09445042474663981, 0.18517016411465512, 0.24861789225329717, 0.47176151888540796]
start point for exploration sampling:  11715
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.027]
 [0.027]
 [0.015]
 [0.027]
 [0.027]
 [0.014]] [[2.557]
 [2.615]
 [2.615]
 [2.816]
 [2.615]
 [2.615]
 [2.921]] [[-0.179]
 [-0.148]
 [-0.148]
 [-0.06 ]
 [-0.148]
 [-0.148]
 [-0.009]]
from probs:  [0.094450422887083, 0.18516999267856846, 0.24861807547155898, 0.4717615089627897]
using explorer policy with actor:  0
from probs:  [0.0945201517772149, 0.1853068025921726, 0.24806281159330035, 0.47211023403731217]
maxi score, test score, baseline:  -0.9963539007092199 -1.0 -0.9963539007092199
maxi score, test score, baseline:  -0.9963539007092199 -1.0 -0.9963539007092199
probs:  [0.0945201500862017, 0.18530663255681384, 0.24806299239540833, 0.4721102249615761]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.288]
 [0.295]
 [0.295]] [[3.044]
 [3.044]
 [3.044]
 [3.044]
 [3.41 ]
 [3.044]
 [3.044]] [[0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.24 ]
 [0.138]
 [0.138]]
maxi score, test score, baseline:  -0.9963539007092199 -1.0 -0.9963539007092199
maxi score, test score, baseline:  -0.9963539007092199 -1.0 -0.9963539007092199
probs:  [0.0945201500862017, 0.18530663255681384, 0.24806299239540833, 0.4721102249615761]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.575]
 [0.555]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[1.98 ]
 [1.745]
 [1.854]
 [1.98 ]
 [1.98 ]
 [1.98 ]
 [1.98 ]] [[1.037]
 [0.94 ]
 [0.974]
 [1.037]
 [1.037]
 [1.037]
 [1.037]]
siam score:  -0.83989924
maxi score, test score, baseline:  -0.9963788732394366 -1.0 -0.9963788732394366
probs:  [0.09442600924704779, 0.1854922085580628, 0.24844285065657187, 0.4716389315383176]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.544]
 [0.526]
 [0.523]
 [0.523]
 [0.523]
 [0.546]] [[0.573]
 [1.645]
 [0.516]
 [0.368]
 [0.381]
 [0.418]
 [0.851]] [[0.579]
 [0.918]
 [0.566]
 [0.519]
 [0.523]
 [0.535]
 [0.677]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9963788732394366 -1.0 -0.9963788732394366
probs:  [0.09451318640918081, 0.18473944860934483, 0.248672447278883, 0.4720749177025914]
maxi score, test score, baseline:  -0.9963788732394366 -1.0 -0.9963788732394366
probs:  [0.09451318640918081, 0.18473944860934483, 0.248672447278883, 0.4720749177025914]
maxi score, test score, baseline:  -0.9963788732394366 -1.0 -0.9963788732394366
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[1.837]
 [1.837]
 [1.837]
 [1.837]
 [1.837]
 [1.837]
 [1.837]] [[0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]]
siam score:  -0.8397487
maxi score, test score, baseline:  -0.9963788732394366 -1.0 -0.9963788732394366
probs:  [0.09451318640918081, 0.18473944860934483, 0.248672447278883, 0.4720749177025914]
line 256 mcts: sample exp_bonus 5.428061819925633
maxi score, test score, baseline:  -0.9963788732394366 -1.0 -0.9963788732394366
probs:  [0.09451318640918081, 0.18473944860934483, 0.248672447278883, 0.4720749177025914]
maxi score, test score, baseline:  -0.9963788732394366 -1.0 -0.9963788732394366
probs:  [0.09451318640918081, 0.18473944860934483, 0.248672447278883, 0.4720749177025914]
maxi score, test score, baseline:  -0.9963912280701754 -1.0 -0.9963912280701754
probs:  [0.09441932898556159, 0.18492285141274079, 0.24905277723470373, 0.4716050423669939]
siam score:  -0.84209406
maxi score, test score, baseline:  -0.9963912280701754 -1.0 -0.9963912280701754
probs:  [0.09441932898556159, 0.18492285141274079, 0.24905277723470373, 0.4716050423669939]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9963912280701754 -1.0 -0.9963912280701754
probs:  [0.09457556690230326, 0.1843127785970842, 0.24872524349313524, 0.47238641100747736]
first move QE:  0.31840486746855046
maxi score, test score, baseline:  -0.9963912280701754 -1.0 -0.9963912280701754
probs:  [0.09457556690230326, 0.1843127785970842, 0.24872524349313524, 0.47238641100747736]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9963912280701754 -1.0 -0.9963912280701754
probs:  [0.0946610874153022, 0.18357442494624432, 0.24895037605502734, 0.4728141115834262]
maxi score, test score, baseline:  -0.9963912280701754 -1.0 -0.9963912280701754
probs:  [0.0946610874153022, 0.18357442494624432, 0.24895037605502734, 0.4728141115834262]
maxi score, test score, baseline:  -0.9963912280701754 -1.0 -0.9963912280701754
probs:  [0.09470264070090906, 0.1830171415609682, 0.24925738827169588, 0.4730228294664269]
maxi score, test score, baseline:  -0.9963912280701754 -1.0 -0.9963912280701754
probs:  [0.09460955459997536, 0.1831950590439901, 0.2496385718558211, 0.47255681450021336]
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.512]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]] [[4.362]
 [5.045]
 [4.362]
 [4.362]
 [4.362]
 [4.362]
 [4.362]] [[0.737]
 [0.919]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]]
maxi score, test score, baseline:  -0.9964034965034965 -1.0 -0.9964034965034965
probs:  [0.09467929042816842, 0.1833300292600137, 0.24908510704047324, 0.47290557327134464]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.0946792889226454, 0.1833298626995301, 0.249085283236856, 0.4729055651409686]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.0946792889226454, 0.1833298626995301, 0.249085283236856, 0.4729055651409686]
actor:  1 policy actor:  1  step number:  66 total reward:  0.09999999999999909  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.09198383225433608, 0.17724697383347765, 0.27134401336948594, 0.45942518054270026]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.09198383225433608, 0.17724697383347765, 0.27134401336948594, 0.45942518054270026]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.09198383225433608, 0.17724697383347765, 0.27134401336948594, 0.45942518054270026]
UNIT TEST: sample policy line 217 mcts : [0.163 0.041 0.102 0.224 0.163 0.163 0.143]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.09198383225433608, 0.17724697383347765, 0.27134401336948594, 0.45942518054270026]
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.431]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[2.631]
 [2.759]
 [1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.417]] [[0.393]
 [0.431]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.0918478675221067, 0.17757556176442618, 0.27183096271316987, 0.4587456080002973]
actor:  1 policy actor:  1  step number:  87 total reward:  0.03999999999999915  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.08956342340172689, 0.17256379285153942, 0.2905511991190939, 0.44732158462763977]
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.08956342340172689, 0.17256379285153942, 0.2905511991190939, 0.44732158462763977]
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.08956342340172689, 0.17256379285153942, 0.2905511991190939, 0.44732158462763977]
actor:  1 policy actor:  1  step number:  62 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  60 total reward:  0.11333333333333262  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.08456586769059711, 0.16292712885756538, 0.33017901623971774, 0.4223279872121198]
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
probs:  [0.08456583736459283, 0.16292692741491663, 0.33017940019594527, 0.4223278350245453]
actor:  1 policy actor:  1  step number:  81 total reward:  0.05333333333333201  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
siam score:  -0.84534574
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
probs:  [0.08096850117087542, 0.198619810181648, 0.3160751250330735, 0.4043365636144031]
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
probs:  [0.08104780306983973, 0.1988145554716842, 0.31540447640967034, 0.4047331650488058]
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
probs:  [0.08104780306983973, 0.1988145554716842, 0.31540447640967034, 0.4047331650488058]
actor:  1 policy actor:  1  step number:  58 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07939710428735187, 0.2339183131375154, 0.290198785777243, 0.3964857967978898]
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.07946657642304923, 0.23412333560260756, 0.2895768433439509, 0.39683324463039227]
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.07946657642304923, 0.23412333560260756, 0.2895768433439509, 0.39683324463039227]
UNIT TEST: sample policy line 217 mcts : [0.041 0.469 0.082 0.143 0.061 0.061 0.143]
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.07953571881499319, 0.23432729629629678, 0.2889579411236278, 0.39717904376508223]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.4403966361522675
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.07960449538099418, 0.23453017785299055, 0.2883423134571927, 0.39752301330882267]
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.07960449538099418, 0.23453017785299055, 0.2883423134571927, 0.39752301330882267]
actor:  1 policy actor:  1  step number:  82 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.536]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[1.643]
 [2.697]
 [1.643]
 [1.643]
 [1.643]
 [1.643]
 [1.643]] [[-0.029]
 [ 0.536]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]]
siam score:  -0.8350618
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.07763913329650615, 0.22755441988172984, 0.3071127218707874, 0.3876937249509765]
actor:  1 policy actor:  1  step number:  70 total reward:  0.2733333333333332  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[2.095]
 [2.095]
 [2.095]
 [2.095]
 [2.095]
 [2.095]
 [2.095]] [[0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]]
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.07542311119123818, 0.22105114885217447, 0.3269149195003897, 0.3766108204561976]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.394]
 [0.313]
 [0.333]
 [0.306]
 [0.281]
 [0.295]] [[3.639]
 [3.739]
 [3.533]
 [3.671]
 [3.149]
 [3.734]
 [3.481]] [[0.287]
 [0.413]
 [0.23 ]
 [0.319]
 [0.031]
 [0.298]
 [0.185]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.07300014697733172, 0.21449687888687846, 0.34801035720350937, 0.3644926169322804]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.07300014697733172, 0.21449687888687846, 0.34801035720350937, 0.3644926169322804]
UNIT TEST: sample policy line 217 mcts : [0.082 0.367 0.02  0.122 0.    0.02  0.388]
actor:  1 policy actor:  1  step number:  60 total reward:  0.13999999999999968  reward:  1.0 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  -0.9964753424657534 -1.0 -0.9964753424657534
probs:  [0.07224340438197999, 0.2103322521012949, 0.36070909411204255, 0.35671524940468263]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.501]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.476]] [[1.93 ]
 [1.631]
 [1.93 ]
 [1.93 ]
 [1.93 ]
 [1.93 ]
 [1.759]] [[0.644]
 [0.531]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.549]]
siam score:  -0.8366176
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.07228460379357272, 0.20990734106775838, 0.3609148299815979, 0.35689322515707106]
siam score:  -0.83715177
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.07228460379357272, 0.20990734106775838, 0.3609148299815979, 0.35689322515707106]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.07446824641809939, 0.2051493229780921, 0.34854002723770905, 0.37184240336609936]
line 256 mcts: sample exp_bonus 4.82121212488012
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.07446824641809939, 0.2051493229780921, 0.34854002723770905, 0.37184240336609936]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.594]
 [0.362]
 [0.559]
 [0.484]
 [0.276]
 [0.529]] [[2.141]
 [2.002]
 [1.967]
 [2.318]
 [2.566]
 [2.157]
 [2.841]] [[0.578]
 [0.575]
 [0.354]
 [0.683]
 [0.726]
 [0.362]
 [0.888]]
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
probs:  [0.0744682119443817, 0.2051493050763628, 0.3485402523968707, 0.37184223058238475]
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
probs:  [0.0744682119443817, 0.2051493050763628, 0.3485402523968707, 0.37184223058238475]
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
actor:  1 policy actor:  1  step number:  75 total reward:  0.19999999999999818  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  45 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.498]
 [0.576]
 [0.518]
 [0.541]
 [0.471]
 [0.51 ]] [[2.725]
 [3.261]
 [2.979]
 [2.553]
 [2.098]
 [2.502]
 [2.543]] [[0.86 ]
 [1.051]
 [0.995]
 [0.82 ]
 [0.677]
 [0.779]
 [0.813]]
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
probs:  [0.07073476077327995, 0.22918283325221184, 0.35317154269693474, 0.3469108632775734]
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
probs:  [0.07073476077327995, 0.22918283325221184, 0.35317154269693474, 0.3469108632775734]
actor:  1 policy actor:  1  step number:  62 total reward:  0.29999999999999927  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.325]
 [0.286]
 [0.286]
 [0.286]
 [0.298]
 [0.294]] [[1.634]
 [1.334]
 [1.634]
 [1.634]
 [1.634]
 [1.092]
 [0.823]] [[0.539]
 [0.399]
 [0.539]
 [0.539]
 [0.539]
 [0.236]
 [0.076]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
probs:  [0.07149032793434242, 0.21791630256482075, 0.3569561685611911, 0.3536372009396457]
maxi score, test score, baseline:  -0.9965216216216216 -1.0 -0.9965216216216216
probs:  [0.07149035289070622, 0.21791633813623024, 0.35695629313037275, 0.3536370158426908]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.635]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.569]] [[2.318]
 [2.531]
 [2.318]
 [2.318]
 [2.318]
 [2.318]
 [2.563]] [[0.704]
 [0.789]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.754]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9965216216216216 -1.0 -0.9965216216216216
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  56 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9965329966329967 -1.0 -0.9965329966329967
Starting evaluation
maxi score, test score, baseline:  -0.9965329966329967 -1.0 -0.9965329966329967
maxi score, test score, baseline:  -0.9965555183946488 -1.0 -0.9965555183946488
probs:  [0.07126025410975159, 0.23907587573328024, 0.3558098925802377, 0.3338539775767305]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9965777408637874 -1.0 -0.9965777408637874
probs:  [0.07126029689127364, 0.23907596107163676, 0.35581010609957703, 0.33385363593751255]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.07114702331434435, 0.23931651188575437, 0.3552433338535578, 0.33429313094634344]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.07114702331434435, 0.23931651188575437, 0.3552433338535578, 0.33429313094634344]
maxi score, test score, baseline:  -0.9966320261437909 -1.0 -0.9966320261437909
probs:  [0.07114704391438101, 0.2393165532605142, 0.3552434366659101, 0.33429296615919474]
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
actor:  1 policy actor:  1  step number:  46 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[-0.777]
 [-0.701]
 [-0.701]
 [-0.701]
 [-0.701]
 [-0.701]
 [-0.701]] [[0.599]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]] [[1.003]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]] [[0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.06979859765268054, 0.23477398756644857, 0.3484991561904923, 0.34692825859037857]
using another actor
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
probs:  [0.06986633986563717, 0.23507142727755295, 0.3488384129257234, 0.34622381993108653]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.223]
 [0.202]
 [0.206]
 [0.223]
 [0.197]
 [0.223]] [[-0.117]
 [ 0.321]
 [ 0.477]
 [ 0.224]
 [ 0.321]
 [ 0.25 ]
 [ 0.321]] [[0.206]
 [0.223]
 [0.202]
 [0.206]
 [0.223]
 [0.197]
 [0.223]]
line 256 mcts: sample exp_bonus 1.824205777173202
siam score:  -0.8363027
siam score:  -0.83457667
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
probs:  [0.06993866241911662, 0.235315130577317, 0.34920013602262795, 0.3455460709809386]
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
probs:  [0.06993866241911662, 0.235315130577317, 0.34920013602262795, 0.3455460709809386]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
probs:  [0.06982510185146461, 0.23554241657300912, 0.3486319348564297, 0.3460005467190965]
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.06989217713512207, 0.2358371448851247, 0.3489678514188968, 0.3453028265608564]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.06966614811483561, 0.23629128934434732, 0.34783691474191336, 0.3462056477989037]
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.06966614811483561, 0.23629128934434732, 0.34783691474191336, 0.3462056477989037]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.06966614811483561, 0.23629128934434732, 0.34783691474191336, 0.3462056477989037]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.06966616797291929, 0.23629133137990518, 0.34783701385853705, 0.34620548678863844]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.385744592728
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.252057946448215
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.06966616797291929, 0.23629133137990518, 0.34783701385853705, 0.34620548678863844]
siam score:  -0.8292023
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.06973785829708284, 0.23653483265379216, 0.3481955743998924, 0.34553173464923265]
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
probs:  [0.06969677613204447, 0.23700496949313718, 0.34798987670384307, 0.34530837767097533]
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
probs:  [0.06969677613204447, 0.23700496949313718, 0.34798987670384307, 0.34530837767097533]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[3.233]
 [3.233]
 [3.233]
 [3.233]
 [3.233]
 [3.233]
 [3.233]] [[0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
probs:  [0.06969677613204447, 0.23700496949313718, 0.34798987670384307, 0.34530837767097533]
siam score:  -0.8398163
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.06983850026342682, 0.23748761364705565, 0.34869871243222034, 0.3439751736572973]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.06999011935244476, 0.23683969165149008, 0.34945703817444085, 0.3437131508216244]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.438]
 [0.404]
 [0.334]
 [0.331]
 [0.35 ]
 [0.397]] [[5.345]
 [5.504]
 [5.036]
 [4.925]
 [5.29 ]
 [5.159]
 [5.549]] [[0.746]
 [0.805]
 [0.668]
 [0.604]
 [0.696]
 [0.672]
 [0.795]]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.06999011935244476, 0.23683969165149008, 0.34945703817444085, 0.3437131508216244]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.06999011935244476, 0.23683969165149008, 0.34945703817444085, 0.3437131508216244]
line 256 mcts: sample exp_bonus 0.5972443240161607
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.584]
 [0.691]
 [0.696]
 [0.705]
 [0.707]
 [0.712]] [[-0.183]
 [ 1.094]
 [-0.174]
 [-0.205]
 [-0.21 ]
 [-0.143]
 [-0.018]] [[0.382]
 [0.925]
 [0.397]
 [0.387]
 [0.394]
 [0.429]
 [0.497]]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.0698364735721419, 0.237533690374449, 0.3486881279079408, 0.3439417081454684]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.369]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.37 ]] [[3.611]
 [3.051]
 [2.628]
 [2.628]
 [2.628]
 [2.628]
 [3.344]] [[ 0.133]
 [ 0.025]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [ 0.075]]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.06972499529032607, 0.23776087611055097, 0.34813034405016907, 0.34438378454895385]
siam score:  -0.83699197
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.06972499529032607, 0.23776087611055097, 0.34813034405016907, 0.34438378454895385]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.028]] [[3.534]
 [3.534]
 [3.534]
 [3.534]
 [3.534]
 [3.534]
 [3.814]] [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.488]]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.553]
 [0.554]
 [0.562]
 [0.549]
 [0.554]
 [0.552]] [[2.743]
 [2.593]
 [3.151]
 [1.533]
 [0.785]
 [3.151]
 [2.63 ]] [[ 0.571]
 [ 0.494]
 [ 0.665]
 [ 0.177]
 [-0.064]
 [ 0.665]
 [ 0.504]]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.06980574172950711, 0.23687676967014315, 0.3485341981145143, 0.3447832904858354]
actor:  1 policy actor:  1  step number:  57 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8336531
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.07075468719077628, 0.23394103249609494, 0.34202163478843933, 0.3532826455246895]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.07075468719077628, 0.23394103249609494, 0.34202163478843933, 0.3532826455246895]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
actor:  1 policy actor:  1  step number:  90 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.07233001291393652, 0.21899795914289452, 0.3476346313195055, 0.3610373966236634]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.629]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[0.797]
 [1.582]
 [1.199]
 [1.199]
 [1.199]
 [1.199]
 [1.199]] [[0.355]
 [0.817]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.0722010996990499, 0.21922776992622764, 0.34817906477634936, 0.36039206559837317]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.07207241720284688, 0.21945716941238338, 0.3487225238492203, 0.35974788953554937]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.07207241720284688, 0.21945716941238338, 0.3487225238492203, 0.35974788953554937]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.07207241720284688, 0.21945716941238338, 0.3487225238492203, 0.35974788953554937]
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.07225304013207548, 0.21866585452582654, 0.3484301380039876, 0.36065096733811036]
actor:  1 policy actor:  1  step number:  64 total reward:  0.11333333333333251  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.0741000760633015, 0.21455071156728242, 0.3414526369100209, 0.36989657545939514]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.07427748221090878, 0.21379328201345665, 0.34114563982245716, 0.37078359595317745]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.0741293272705968, 0.21420736265201387, 0.34161973864404493, 0.3700435714333443]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.574]
 [0.531]
 [0.54 ]
 [0.531]
 [0.531]
 [0.538]] [[1.483]
 [2.17 ]
 [1.483]
 [1.739]
 [1.483]
 [1.483]
 [2.142]] [[0.266]
 [0.675]
 [0.266]
 [0.412]
 [0.266]
 [0.266]
 [0.63 ]]
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.07431489176859944, 0.21223746659423362, 0.342476256304083, 0.37097138533308394]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
actor:  1 policy actor:  1  step number:  54 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.041 0.571 0.286 0.    0.    0.    0.102]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.512]
 [0.473]
 [0.505]
 [0.528]
 [0.452]
 [0.441]] [[1.096]
 [1.448]
 [1.347]
 [1.007]
 [0.912]
 [1.22 ]
 [1.192]] [[0.481]
 [0.639]
 [0.566]
 [0.484]
 [0.476]
 [0.503]
 [0.482]]
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.07611307532212018, 0.20914611135333633, 0.33476776267500347, 0.37997305064954007]
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.07599239691003877, 0.2093535761813404, 0.33528508960132064, 0.3793689373073002]
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.07599239691003877, 0.2093535761813404, 0.33528508960132064, 0.3793689373073002]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.587]
 [0.555]
 [0.587]
 [0.587]
 [0.587]
 [0.587]] [[3.847]
 [3.847]
 [3.482]
 [3.847]
 [3.847]
 [3.847]
 [3.847]] [[1.03 ]
 [1.03 ]
 [0.826]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]]
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.0759624781253787, 0.2086158683840535, 0.3362030231977181, 0.3792186302928497]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.07396298136458669, 0.20311774295419363, 0.35369828426958116, 0.36922099141163844]
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.07396298136458669, 0.20311774295419363, 0.35369828426958116, 0.36922099141163844]
siam score:  -0.8233435
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
probs:  [0.07401101171549958, 0.20261650392074096, 0.3539120312158256, 0.36946045314793385]
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
probs:  [0.07401101171549958, 0.20261650392074096, 0.3539120312158256, 0.36946045314793385]
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
probs:  [0.07401101171549958, 0.20261650392074096, 0.3539120312158256, 0.36946045314793385]
actor:  1 policy actor:  1  step number:  62 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07401101171549958, 0.20261650392074096, 0.3539120312158256, 0.36946045314793385]
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
probs:  [0.07304285127926181, 0.1992815858175965, 0.36462049700581506, 0.3630550658973266]
actor:  1 policy actor:  1  step number:  58 total reward:  0.15333333333333254  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
probs:  [0.07093602059695306, 0.22241195792059182, 0.3540861734850169, 0.3525658479974383]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
probs:  [0.07093602059695306, 0.22241195792059182, 0.3540861734850169, 0.3525658479974383]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.07093602770669034, 0.22241201943513925, 0.35408620889983433, 0.35256574395833606]
maxi score, test score, baseline:  -0.9968418960244648 -1.0 -0.9968418960244648
using another actor
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.476]
 [0.499]
 [0.501]
 [0.497]
 [0.497]
 [0.498]] [[0.295]
 [1.545]
 [0.347]
 [0.354]
 [0.304]
 [0.492]
 [0.852]] [[0.499]
 [0.476]
 [0.499]
 [0.501]
 [0.497]
 [0.497]
 [0.498]]
maxi score, test score, baseline:  -0.9968418960244648 -1.0 -0.9968418960244648
probs:  [0.07098171720827952, 0.22191305497101485, 0.3543139983309525, 0.35279122948975317]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9968418960244648 -1.0 -0.9968418960244648
probs:  [0.0735618862772473, 0.2167848496252374, 0.3424238039946032, 0.36722946010291213]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
probs:  [0.07356186214557471, 0.21678491651101786, 0.3424238821431099, 0.3672293392002975]
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
probs:  [0.0735293828702603, 0.21607334143062135, 0.34333095779444484, 0.36706631790467353]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
probs:  [0.07617574933799337, 0.21139613143122324, 0.3321155479731956, 0.3803125712575878]
start point for exploration sampling:  11715
from probs:  [0.07617574933799337, 0.21139613143122324, 0.3321155479731956, 0.3803125712575878]
using explorer policy with actor:  1
siam score:  -0.8223379
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.07633141046798521, 0.21182896797034165, 0.3307486766473868, 0.38109094491428636]
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.555]
 [0.554]
 [0.553]
 [0.552]
 [0.549]
 [0.563]] [[2.51 ]
 [3.533]
 [2.75 ]
 [2.693]
 [2.555]
 [2.677]
 [2.784]] [[0.254]
 [0.594]
 [0.332]
 [0.312]
 [0.266]
 [0.303]
 [0.352]]
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.07621701194439796, 0.21203281967493176, 0.33123184131612826, 0.38051832706454203]
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.139]
 [0.157]
 [0.156]
 [0.156]
 [0.156]
 [0.156]] [[-0.147]
 [ 0.979]
 [-0.094]
 [-0.142]
 [-0.226]
 [-0.18 ]
 [-0.167]] [[0.158]
 [0.139]
 [0.157]
 [0.156]
 [0.156]
 [0.156]
 [0.156]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.07344789361059392, 0.24070131045795923, 0.31917925849037676, 0.36667153744107006]
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.07333161569561064, 0.2409620054292919, 0.31961683510119737, 0.3660895437739001]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.539]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[4.241]
 [4.091]
 [4.241]
 [4.241]
 [4.241]
 [4.241]
 [4.241]] [[0.56 ]
 [0.498]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]]
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.645]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.552]] [[2.617]
 [2.364]
 [2.617]
 [2.617]
 [2.617]
 [2.617]
 [3.243]] [[0.516]
 [0.461]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.828]]
start point for exploration sampling:  11715
actor:  1 policy actor:  1  step number:  52 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
probs:  [0.0749515942971826, 0.23659536594457864, 0.314255618279031, 0.3741974214792077]
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
probs:  [0.0749515942971826, 0.23659536594457864, 0.314255618279031, 0.3741974214792077]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.18486972507834434
actor:  1 policy actor:  1  step number:  58 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
probs:  [0.07631893062385332, 0.23354985503445158, 0.3090899958023991, 0.381041218539296]
actor:  1 policy actor:  1  step number:  70 total reward:  0.006666666666665488  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07631893062385332, 0.23354985503445158, 0.3090899958023991, 0.381041218539296]
actor:  1 policy actor:  1  step number:  49 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8263144
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
probs:  [0.07507495081423098, 0.25201830263004166, 0.2980826830748863, 0.37482406348084113]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9968879518072289 -1.0 -0.9968879518072289
probs:  [0.07517080080215405, 0.25106184820798244, 0.2984639751428325, 0.375303375847031]
maxi score, test score, baseline:  -0.9968879518072289 -1.0 -0.9968879518072289
probs:  [0.07533286895803866, 0.2503364717691222, 0.29821683676530547, 0.37611382250753367]
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.547]
 [0.519]
 [0.514]
 [0.525]
 [0.514]
 [0.524]] [[2.26 ]
 [2.546]
 [2.549]
 [2.071]
 [2.332]
 [2.305]
 [3.083]] [[0.534]
 [0.547]
 [0.519]
 [0.514]
 [0.525]
 [0.514]
 [0.524]]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 1.6517664450734746
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[1.649]
 [3.673]
 [3.673]
 [3.673]
 [3.673]
 [3.673]
 [3.673]] [[0.64 ]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]]
Printing some Q and Qe and total Qs values:  [[0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.572]
 [0.557]
 [0.541]
 [0.586]
 [0.53 ]
 [0.603]] [[2.161]
 [2.459]
 [2.066]
 [1.832]
 [2.1  ]
 [1.497]
 [2.034]] [[0.576]
 [0.711]
 [0.499]
 [0.366]
 [0.546]
 [0.188]
 [0.529]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8131061789287255
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
probs:  [0.07506036544831582, 0.25040227130836923, 0.2997856748349619, 0.374751688408353]
siam score:  -0.82004386
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.662]
 [0.653]
 [0.653]
 [0.653]
 [0.667]
 [0.674]] [[3.236]
 [3.895]
 [3.236]
 [3.236]
 [3.236]
 [3.706]
 [3.941]] [[0.575]
 [0.867]
 [0.575]
 [0.575]
 [0.575]
 [0.789]
 [0.895]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
probs:  [0.07677151855314177, 0.24648778356973283, 0.29342495717607653, 0.38331574070104885]
actor:  1 policy actor:  1  step number:  61 total reward:  0.18666666666666643  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8161136
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
probs:  [0.0757945384370774, 0.24334680011907764, 0.3024285581632311, 0.3784301032806138]
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
probs:  [0.0757945384370774, 0.24334680011907764, 0.3024285581632311, 0.3784301032806138]
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
probs:  [0.07586144570744666, 0.243561906471298, 0.3018119577272149, 0.3787646900940405]
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
actor:  1 policy actor:  1  step number:  51 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  61 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
probs:  [0.07299560346078922, 0.26313176391889714, 0.29943931518279765, 0.36443331743751595]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07299558523851109, 0.26313175322517035, 0.29943943540660967, 0.3644332261297088]
maxi score, test score, baseline:  -0.9969149253731343 -1.0 -0.9969149253731343
probs:  [0.07113466275919036, 0.28194437001101635, 0.29179375700301197, 0.3551272102267814]
maxi score, test score, baseline:  -0.9969149253731343 -1.0 -0.9969149253731343
probs:  [0.071194807967218, 0.2821831337704047, 0.2911940766339873, 0.3554279816283899]
using another actor
siam score:  -0.8257797
maxi score, test score, baseline:  -0.9969414201183432 -1.0 -0.9969414201183432
probs:  [0.07120379781137529, 0.28298129636199576, 0.29034239120482114, 0.35547251462180773]
maxi score, test score, baseline:  -0.9969414201183432 -1.0 -0.9969414201183432
maxi score, test score, baseline:  -0.9969414201183432 -1.0 -0.9969414201183432
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9969414201183432 -1.0 -0.9969414201183432
maxi score, test score, baseline:  -0.9969414201183432 -1.0 -0.9969414201183432
probs:  [0.07109359906869875, 0.2833046107903297, 0.29068077395528213, 0.3549210161856895]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.683]
 [0.638]
 [0.625]
 [0.625]
 [0.625]
 [0.599]] [[1.461]
 [1.448]
 [2.192]
 [1.981]
 [1.981]
 [1.819]
 [1.687]] [[1.052]
 [1.102]
 [1.219]
 [1.171]
 [1.171]
 [1.14 ]
 [1.1  ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9969414201183432 -1.0 -0.9969414201183432
probs:  [0.07098362638527285, 0.2836272619786336, 0.2910184625548921, 0.3543706490812014]
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.453]
 [0.517]
 [0.472]
 [0.385]
 [0.43 ]
 [0.434]] [[1.34 ]
 [1.539]
 [1.718]
 [1.697]
 [1.2  ]
 [1.684]
 [1.179]] [[0.482]
 [0.713]
 [0.837]
 [0.785]
 [0.533]
 [0.739]
 [0.575]]
from probs:  [0.07098362638527285, 0.2836272619786336, 0.2910184625548921, 0.3543706490812014]
maxi score, test score, baseline:  -0.9969501474926253 -1.0 -0.9969501474926253
probs:  [0.07098361019078657, 0.2836272405186073, 0.2910185813693817, 0.3543705679212245]
maxi score, test score, baseline:  -0.9969501474926253 -1.0 -0.9969501474926253
probs:  [0.07098361019078657, 0.2836272405186073, 0.2910185813693817, 0.3543705679212245]
maxi score, test score, baseline:  -0.9969501474926253 -1.0 -0.9969501474926253
probs:  [0.07104302116449673, 0.28386500395882197, 0.29042430834065197, 0.3546676665360294]
first move QE:  0.37051153941578124
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.07114292641192074, 0.2828560670601101, 0.2908337405402286, 0.3551672659877406]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4905944721287057
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.07113245050947094, 0.28217471612050765, 0.2915783779970269, 0.3551144553729945]
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
siam score:  -0.82505304
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
probs:  [0.07113243459749843, 0.28217469463176037, 0.2915784951428526, 0.3551143756278887]
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
probs:  [0.0713284880858343, 0.28019330576357654, 0.29238341989123084, 0.3560947862593584]
actor:  1 policy actor:  1  step number:  71 total reward:  0.02666666666666584  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.9969845481049563 -1.0 -0.9969845481049563
probs:  [0.07061011688568768, 0.2781064074282427, 0.298781490985612, 0.3525019847004575]
maxi score, test score, baseline:  -0.9969845481049563 -1.0 -0.9969845481049563
probs:  [0.07056130537012535, 0.2786535226817845, 0.2985277010514476, 0.35225747089664255]
maxi score, test score, baseline:  -0.9969845481049563 -1.0 -0.9969845481049563
probs:  [0.07045218741789307, 0.2789612216799758, 0.2988752088158065, 0.3517113820863247]
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.59 ]
 [0.563]
 [0.615]
 [0.544]
 [0.55 ]
 [0.563]] [[2.147]
 [3.568]
 [3.415]
 [3.139]
 [3.013]
 [3.313]
 [3.721]] [[0.269]
 [0.757]
 [0.692]
 [0.625]
 [0.547]
 [0.651]
 [0.794]]
maxi score, test score, baseline:  -0.9969845481049563 -1.0 -0.9969845481049563
probs:  [0.07045218741789307, 0.2789612216799758, 0.2988752088158065, 0.3517113820863247]
maxi score, test score, baseline:  -0.9969845481049563 -1.0 -0.9969845481049563
probs:  [0.07045218741789307, 0.2789612216799758, 0.2988752088158065, 0.3517113820863247]
from probs:  [0.07034327502186569, 0.27926826139071986, 0.2992221416942466, 0.3511663218931678]
maxi score, test score, baseline:  -0.996993023255814 -1.0 -0.996993023255814
probs:  [0.07043883881543207, 0.2782876433337806, 0.2996293094802472, 0.35164420837054006]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.612]
 [0.239]
 [0.524]
 [0.532]
 [0.524]
 [0.582]] [[1.672]
 [1.809]
 [1.616]
 [1.425]
 [1.279]
 [1.425]
 [1.839]] [[1.008]
 [1.081]
 [0.863]
 [0.932]
 [0.893]
 [0.932]
 [1.077]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9970014492753624 -1.0 -0.9970014492753624
probs:  [0.07049932736528358, 0.2785270404003654, 0.2990269385552568, 0.3519466936790943]
maxi score, test score, baseline:  -0.9970014492753624 -1.0 -0.9970014492753624
probs:  [0.07049932736528358, 0.2785270404003654, 0.2990269385552568, 0.3519466936790943]
maxi score, test score, baseline:  -0.9970014492753624 -1.0 -0.9970014492753624
probs:  [0.07049932736528358, 0.2785270404003654, 0.2990269385552568, 0.3519466936790943]
maxi score, test score, baseline:  -0.9970014492753624 -1.0 -0.9970014492753624
probs:  [0.07049932736528358, 0.2785270404003654, 0.2990269385552568, 0.3519466936790943]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9970098265895954 -1.0 -0.9970098265895954
probs:  [0.07192610280168739, 0.2745115163648821, 0.2944752689443146, 0.3590871118891159]
first move QE:  0.37450070833721355
maxi score, test score, baseline:  -0.9970098265895954 -1.0 -0.9970098265895954
probs:  [0.07208048083134722, 0.2737865188601275, 0.29427387731280624, 0.359859122995719]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[3.429]
 [3.429]
 [3.429]
 [3.429]
 [3.429]
 [3.429]
 [3.429]] [[0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9970098265895954 -1.0 -0.9970098265895954
probs:  [0.07208048083134722, 0.2737865188601275, 0.29427387731280624, 0.359859122995719]
siam score:  -0.821279
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
probs:  [0.07208046356434587, 0.2737864913344594, 0.29427400861989056, 0.3598590364813042]
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
probs:  [0.07208046356434587, 0.2737864913344594, 0.29427400861989056, 0.3598590364813042]
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
probs:  [0.07208046356434587, 0.2737864913344594, 0.29427400861989056, 0.3598590364813042]
actor:  1 policy actor:  1  step number:  67 total reward:  0.19999999999999896  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  63 total reward:  0.1999999999999993  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
probs:  [0.07059493712576993, 0.26686075959572775, 0.3101140629266171, 0.35243024035188525]
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.517]
 [0.474]
 [0.474]
 [0.475]
 [0.475]
 [0.477]] [[3.461]
 [3.61 ]
 [3.489]
 [3.468]
 [3.445]
 [3.466]
 [3.366]] [[0.112]
 [0.18 ]
 [0.117]
 [0.113]
 [0.11 ]
 [0.114]
 [0.099]]
Printing some Q and Qe and total Qs values:  [[0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.503]
 [0.506]
 [0.5  ]
 [0.506]
 [0.506]
 [0.506]] [[3.612]
 [3.513]
 [3.612]
 [3.381]
 [3.612]
 [3.612]
 [3.612]] [[0.602]
 [0.567]
 [0.602]
 [0.519]
 [0.602]
 [0.602]
 [0.602]]
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
probs:  [0.07048743674391285, 0.26714079892738984, 0.3104795087369552, 0.35189225559174214]
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
probs:  [0.07057649588395615, 0.2662134453409266, 0.3108724382084523, 0.35233762056666496]
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
probs:  [0.07057649588395615, 0.2662134453409266, 0.3108724382084523, 0.35233762056666496]
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.376]
 [0.398]
 [0.313]
 [0.37 ]
 [0.242]
 [0.37 ]] [[1.42 ]
 [1.155]
 [1.052]
 [1.123]
 [1.117]
 [1.389]
 [1.232]] [[-0.037]
 [ 0.06 ]
 [ 0.064]
 [-0.009]
 [ 0.048]
 [-0.036]
 [ 0.067]]
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
probs:  [0.07056661205384807, 0.266269904598636, 0.3108744345030174, 0.3522890488444986]
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
probs:  [0.07056661205384807, 0.266269904598636, 0.3108744345030174, 0.3522890488444986]
maxi score, test score, baseline:  -0.9970181556195965 -1.0 -0.9970181556195965
probs:  [0.07045948276789371, 0.266547670053864, 0.3112399248440655, 0.3517529223341768]
from probs:  [0.07045948276789371, 0.266547670053864, 0.3112399248440655, 0.3517529223341768]
line 256 mcts: sample exp_bonus 3.472676475817314
maxi score, test score, baseline:  -0.997034670487106 -1.0 -0.997034670487106
probs:  [0.07052104954767927, 0.26678101510729385, 0.31063713114224456, 0.35206080420278235]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.997034670487106 -1.0 -0.997034670487106
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.997034670487106 -1.0 -0.997034670487106
probs:  [0.07029801558315109, 0.26739092957232835, 0.31136553646607235, 0.35094551837844823]
maxi score, test score, baseline:  -0.997034670487106 -1.0 -0.997034670487106
probs:  [0.07029801558315109, 0.26739092957232835, 0.31136553646607235, 0.35094551837844823]
maxi score, test score, baseline:  -0.997034670487106 -1.0 -0.997034670487106
probs:  [0.07035932275839352, 0.2676244895792877, 0.3107640852689269, 0.3512521023933919]
maxi score, test score, baseline:  -0.997034670487106 -1.0 -0.997034670487106
probs:  [0.07035932275839352, 0.2676244895792877, 0.3107640852689269, 0.3512521023933919]
maxi score, test score, baseline:  -0.997034670487106 -1.0 -0.997034670487106
probs:  [0.07035932275839352, 0.2676244895792877, 0.3107640852689269, 0.3512521023933919]
actor:  1 policy actor:  1  step number:  58 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.997034670487106 -1.0 -0.997034670487106
probs:  [0.0712854734828017, 0.26559485349105383, 0.3072328954761532, 0.3558867775499913]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]] [[4.336]
 [4.336]
 [4.336]
 [4.336]
 [4.336]
 [4.336]
 [4.336]] [[3.499]
 [3.499]
 [3.499]
 [3.499]
 [3.499]
 [3.499]
 [3.499]]
maxi score, test score, baseline:  -0.997034670487106 -1.0 -0.997034670487106
probs:  [0.0712854734828017, 0.26559485349105383, 0.3072328954761532, 0.3558867775499913]
first move QE:  0.3804781655451359
maxi score, test score, baseline:  -0.997034670487106 -1.0 -0.997034670487106
probs:  [0.0712854734828017, 0.26559485349105383, 0.3072328954761532, 0.3558867775499913]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.997034670487106 -1.0 -0.997034670487106
maxi score, test score, baseline:  -0.997034670487106 -1.0 -0.997034670487106
probs:  [0.06890315138030477, 0.28939321084031905, 0.2977308185021785, 0.3439728192771978]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.595]
 [0.635]
 [0.616]
 [0.62 ]
 [0.632]
 [0.619]] [[3.612]
 [5.369]
 [3.648]
 [3.276]
 [2.687]
 [3.35 ]
 [4.051]] [[0.403]
 [0.904]
 [0.423]
 [0.294]
 [0.12 ]
 [0.33 ]
 [0.529]]
maxi score, test score, baseline:  -0.997034670487106 -1.0 -0.997034670487106
probs:  [0.06890315138030477, 0.28939321084031905, 0.2977308185021785, 0.3439728192771978]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.997034670487106 -1.0 -0.997034670487106
actor:  1 policy actor:  1  step number:  40 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9970428571428571 -1.0 -0.9970428571428571
probs:  [0.06710804242497682, 0.3079348340782048, 0.2899613296095061, 0.3349957938873122]
maxi score, test score, baseline:  -0.9970428571428571 -1.0 -0.9970428571428571
probs:  [0.06710804242497682, 0.3079348340782048, 0.2899613296095061, 0.3349957938873122]
maxi score, test score, baseline:  -0.9970428571428571 -1.0 -0.9970428571428571
probs:  [0.0672594274763346, 0.307175816097429, 0.2898119124312513, 0.33575284399498506]
maxi score, test score, baseline:  -0.9970509971509972 -1.0 -0.9970509971509972
probs:  [0.06725942131587931, 0.30717569777260395, 0.28981206786346914, 0.3357528130480476]
maxi score, test score, baseline:  -0.9970509971509972 -1.0 -0.9970509971509972
probs:  [0.06715337165379487, 0.30751000621135627, 0.29011450595476784, 0.335222116180081]
maxi score, test score, baseline:  -0.9970509971509972 -1.0 -0.9970509971509972
probs:  [0.06726066369651659, 0.3080021791794459, 0.28897849282246907, 0.33575866430156837]
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]] [[5.09]
 [5.09]
 [5.09]
 [5.09]
 [5.09]
 [5.09]
 [5.09]] [[2.458]
 [2.458]
 [2.458]
 [2.458]
 [2.458]
 [2.458]
 [2.458]]
maxi score, test score, baseline:  -0.9970509971509972 -1.0 -0.9970509971509972
probs:  [0.06726066369651659, 0.3080021791794459, 0.28897849282246907, 0.33575866430156837]
maxi score, test score, baseline:  -0.9970509971509972 -1.0 -0.9970509971509972
probs:  [0.06726066369651659, 0.3080021791794459, 0.28897849282246907, 0.33575866430156837]
maxi score, test score, baseline:  -0.9970509971509972 -1.0 -0.9970509971509972
probs:  [0.06726066369651659, 0.3080021791794459, 0.28897849282246907, 0.33575866430156837]
actor:  1 policy actor:  1  step number:  62 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
probs:  [0.0683375174324562, 0.30459178610835214, 0.28592320212922284, 0.3411474943299689]
actor:  1 policy actor:  1  step number:  52 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.484]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[1.625]
 [2.272]
 [1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]] [[0.195]
 [0.378]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]]
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
probs:  [0.06681549657033034, 0.32014739692706734, 0.2795013903087607, 0.33353571619384165]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
probs:  [0.06681549657033034, 0.32014739692706734, 0.2795013903087607, 0.33353571619384165]
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
probs:  [0.06681549657033034, 0.32014739692706734, 0.2795013903087607, 0.33353571619384165]
maxi score, test score, baseline:  -0.9970751412429378 -1.0 -0.9970751412429378
probs:  [0.06681549311034123, 0.32014727082953937, 0.27950153729731503, 0.3335356987628044]
maxi score, test score, baseline:  -0.9970751412429378 -1.0 -0.9970751412429378
maxi score, test score, baseline:  -0.9970751412429378 -1.0 -0.9970751412429378
probs:  [0.06681549311034123, 0.32014727082953937, 0.27950153729731503, 0.3335356987628044]
maxi score, test score, baseline:  -0.9970751412429378 -1.0 -0.9970751412429378
probs:  [0.06691523909808954, 0.31913073845843604, 0.27991950715088887, 0.3340345152925855]
maxi score, test score, baseline:  -0.9970751412429378 -1.0 -0.9970751412429378
maxi score, test score, baseline:  -0.9970751412429378 -1.0 -0.9970751412429378
probs:  [0.06691523909808954, 0.31913073845843604, 0.27991950715088887, 0.3340345152925855]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]] [[0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]]
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.042]
 [-0.052]
 [-0.045]
 [-0.048]
 [-0.057]
 [-0.049]] [[2.422]
 [2.659]
 [2.089]
 [2.299]
 [1.99 ]
 [1.968]
 [2.467]] [[-0.511]
 [-0.469]
 [-0.574]
 [-0.533]
 [-0.587]
 [-0.599]
 [-0.508]]
from probs:  [0.0669152321805404, 0.31913048848543346, 0.2799197988895848, 0.33403448044444145]
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
probs:  [0.06690949322917754, 0.31846983755254127, 0.28061523398231136, 0.3340054352359699]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[2.221]
 [2.221]
 [2.221]
 [2.221]
 [2.221]
 [2.221]
 [2.221]] [[1.11]
 [1.11]
 [1.11]
 [1.11]
 [1.11]
 [1.11]
 [1.11]]
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
probs:  [0.06695561013175105, 0.3187061801681089, 0.2801013969789636, 0.33423681272117645]
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
probs:  [0.06695561013175105, 0.3187061801681089, 0.2801013969789636, 0.33423681272117645]
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
probs:  [0.06695561013175105, 0.3187061801681089, 0.2801013969789636, 0.33423681272117645]
line 256 mcts: sample exp_bonus 1.481608914364589
siam score:  -0.8089081
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
probs:  [0.06695561013175105, 0.3187061801681089, 0.2801013969789636, 0.33423681272117645]
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.06695560669000405, 0.31870605632414384, 0.2801015416019923, 0.33423679538385975]
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.06695560669000405, 0.31870605632414384, 0.2801015416019923, 0.33423679538385975]
actor:  1 policy actor:  1  step number:  54 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  0.667
first move QE:  0.3908513614246367
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.683]
 [0.707]
 [0.652]
 [0.695]
 [0.695]
 [0.658]] [[2.601]
 [2.64 ]
 [2.265]
 [2.17 ]
 [2.489]
 [2.333]
 [2.505]] [[0.503]
 [0.53 ]
 [0.429]
 [0.342]
 [0.492]
 [0.44 ]
 [0.46 ]]
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.06801682867008339, 0.31557109933964506, 0.27686492077765057, 0.33954715121262097]
actor:  1 policy actor:  1  step number:  64 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.0687659324097917, 0.3130679813465911, 0.2748703016877705, 0.34329578455584675]
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.0687659324097917, 0.3130679813465911, 0.2748703016877705, 0.34329578455584675]
siam score:  -0.81577873
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.06886400167388271, 0.3120868283952475, 0.2752629450633746, 0.34378622486749516]
siam score:  -0.8163024
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.0689613716760292, 0.31111267134549436, 0.27565278877923954, 0.3442731681992368]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.0705375991802662, 0.30588233736475384, 0.2714191864752101, 0.35216087697977]
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.07063370838521009, 0.30493518489657906, 0.27178958469123576, 0.352641522026975]
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.07053310514406846, 0.3052678509573462, 0.27206095804793573, 0.3521380858506496]
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.07043270721201482, 0.3055998381198516, 0.27233177759130184, 0.35163567707683174]
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.07043270721201482, 0.3055998381198516, 0.27233177759130184, 0.35163567707683174]
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.07043270721201482, 0.3055998381198516, 0.27233177759130184, 0.35163567707683174]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.07043270721201482, 0.3055998381198516, 0.27233177759130184, 0.35163567707683174]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.485]
 [0.53 ]
 [0.536]
 [0.532]
 [0.535]
 [0.525]] [[-0.54 ]
 [ 0.585]
 [-0.805]
 [-1.013]
 [-0.862]
 [-0.779]
 [-0.474]] [[0.327]
 [0.466]
 [0.279]
 [0.251]
 [0.271]
 [0.289]
 [0.329]]
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.07052820837359121, 0.3046568894185287, 0.2727016217859116, 0.3521132804219685]
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.07052820837359121, 0.3046568894185287, 0.2727016217859116, 0.3521132804219685]
UNIT TEST: sample policy line 217 mcts : [0.204 0.082 0.327 0.122 0.041 0.163 0.061]
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.07052820837359121, 0.3046568894185287, 0.2727016217859116, 0.3521132804219685]
actor:  1 policy actor:  1  step number:  76 total reward:  0.24666666666666504  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.243]
 [0.207]
 [0.178]
 [0.243]
 [0.163]
 [0.169]] [[4.214]
 [4.214]
 [3.839]
 [3.969]
 [4.214]
 [4.116]
 [4.198]] [[-0.099]
 [-0.099]
 [-0.198]
 [-0.204]
 [-0.099]
 [-0.195]
 [-0.176]]
from probs:  [0.07165575916942545, 0.3003114926412258, 0.2702765839440165, 0.35775616424533235]
using explorer policy with actor:  1
siam score:  -0.8192956
maxi score, test score, baseline:  -0.9971067039106145 -1.0 -0.9971067039106145
probs:  [0.07155688423158636, 0.3006360146491604, 0.2705457241986088, 0.35726137692064447]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.489]
 [0.427]
 [0.395]
 [0.434]
 [0.397]
 [0.401]] [[1.213]
 [1.74 ]
 [1.299]
 [1.499]
 [1.54 ]
 [1.516]
 [1.544]] [[0.037]
 [0.487]
 [0.133]
 [0.234]
 [0.3  ]
 [0.247]
 [0.269]]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.805]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [1.185]] [[1.985]
 [0.695]
 [1.985]
 [1.985]
 [1.985]
 [1.985]
 [0.973]] [[0.98 ]
 [0.78 ]
 [0.98 ]
 [0.98 ]
 [0.98 ]
 [0.98 ]
 [1.209]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  51 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.479]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]] [[1.989]
 [1.861]
 [1.989]
 [1.989]
 [1.989]
 [1.989]
 [1.989]] [[0.618]
 [0.586]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.07191610931011043, 0.3061479509941629, 0.26287306900306584, 0.35906287069266074]
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.07191610931011043, 0.3061479509941629, 0.26287306900306584, 0.35906287069266074]
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.07181850190051593, 0.306480730813796, 0.26312633377290295, 0.35857443351278523]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.484]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.407]] [[1.116]
 [1.509]
 [1.353]
 [1.353]
 [1.353]
 [1.353]
 [1.228]] [[0.048]
 [0.283]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.112]]
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]] [[1.777]
 [1.777]
 [1.777]
 [1.777]
 [1.777]
 [1.777]
 [1.777]] [[-0.102]
 [-0.102]
 [-0.102]
 [-0.102]
 [-0.102]
 [-0.102]
 [-0.102]]
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.522]
 [0.501]
 [0.495]
 [0.495]
 [0.509]
 [0.475]] [[1.698]
 [3.134]
 [2.22 ]
 [1.357]
 [1.357]
 [2.806]
 [3.767]] [[0.371]
 [0.796]
 [0.519]
 [0.265]
 [0.265]
 [0.695]
 [0.961]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.07076775614390268, 0.31604015019604687, 0.25987286938932813, 0.35331922427072227]
siam score:  -0.82644445
actor:  1 policy actor:  1  step number:  64 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.0712038292462477, 0.3170553172935063, 0.2562942831807973, 0.3554465702794487]
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.07120434691500419, 0.3157963745227857, 0.25755109245859265, 0.35544818610361745]
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.07120434691500419, 0.3157963745227857, 0.25755109245859265, 0.35544818610361745]
first move QE:  0.39600017787945396
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.513]
 [0.533]
 [0.526]
 [0.531]
 [0.533]
 [0.544]] [[2.628]
 [3.152]
 [2.6  ]
 [2.215]
 [2.407]
 [2.547]
 [2.5  ]] [[0.416]
 [0.544]
 [0.417]
 [0.311]
 [0.365]
 [0.403]
 [0.399]]
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.066]
 [-0.027]
 [-0.026]
 [-0.03 ]
 [-0.025]
 [-0.022]] [[0.467]
 [0.613]
 [0.358]
 [0.5  ]
 [0.296]
 [0.38 ]
 [0.42 ]] [[-0.299]
 [-0.291]
 [-0.337]
 [-0.288]
 [-0.36 ]
 [-0.327]
 [-0.311]]
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.07130710137322555, 0.3148074191081011, 0.2579234836018105, 0.35596199591686284]
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.674]
 [0.063]
 [0.041]
 [0.048]
 [0.058]
 [0.046]] [[3.894]
 [5.566]
 [4.169]
 [4.296]
 [4.189]
 [3.933]
 [3.774]] [[ 0.031]
 [ 0.724]
 [ 0.082]
 [ 0.094]
 [ 0.077]
 [ 0.032]
 [-0.007]]
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.07130710137322555, 0.3148074191081011, 0.2579234836018105, 0.35596199591686284]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6200000000000003  reward:  1.0 rdn_beta:  0.5
UNIT TEST: sample policy line 217 mcts : [0.041 0.429 0.102 0.041 0.041 0.122 0.224]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]] [[1.462]
 [1.462]
 [1.462]
 [1.462]
 [1.462]
 [1.462]
 [1.462]] [[0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.769]]
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.07002647447941683, 0.30914447962211666, 0.2712706547030689, 0.34955839119539767]
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.06992280466736867, 0.3094914344527451, 0.27154623523960864, 0.34903952564027774]
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.06992280466736867, 0.3094914344527451, 0.27154623523960864, 0.34903952564027774]
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.002]
 [0.002]] [[-0.421]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.313]
 [-0.174]] [[0.04 ]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.07002128456551858, 0.30851746133320157, 0.27192929348322353, 0.3495319606180563]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.07002128456551858, 0.30851746133320157, 0.27192929348322353, 0.3495319606180563]
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[1.97]
 [1.97]
 [1.97]
 [1.97]
 [1.97]
 [1.97]
 [1.97]] [[1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.102]]
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.0698961179130285, 0.30887300967685405, 0.2723231467225185, 0.34890772568759887]
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.06979311923056972, 0.3092163964326724, 0.27259826191252395, 0.348392222424234]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.0687444933513242, 0.3045626794739012, 0.28354412984335636, 0.3431486973314181]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[3.718]
 [3.718]
 [3.718]
 [3.718]
 [3.718]
 [3.718]
 [3.718]] [[0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]]
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.0687444933513242, 0.3045626794739012, 0.28354412984335636, 0.3431486973314181]
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.0687444933513242, 0.3045626794739012, 0.28354412984335636, 0.3431486973314181]
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.0687444933513242, 0.3045626794739012, 0.28354412984335636, 0.3431486973314181]
actor:  1 policy actor:  1  step number:  61 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.06961084562095027, 0.3017830319088156, 0.2811204015165725, 0.34748572095366165]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
probs:  [0.06976001696403454, 0.30107363565214157, 0.2809347032294742, 0.34823164415434976]
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
probs:  [0.06965809854405261, 0.30139820414941304, 0.2812221403582839, 0.34772155694825047]
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
probs:  [0.06965809854405261, 0.30139820414941304, 0.2812221403582839, 0.34772155694825047]
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
probs:  [0.06965809854405261, 0.30139820414941304, 0.2812221403582839, 0.34772155694825047]
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
probs:  [0.0697518959201686, 0.30045608735261414, 0.2816014314644732, 0.34819058526274393]
actor:  1 policy actor:  1  step number:  55 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 3.6583480827299377
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
probs:  [0.07075413179181847, 0.29664636053630083, 0.27939327219389026, 0.35320623547799035]
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.328]
 [0.311]
 [0.34 ]
 [0.274]
 [0.309]
 [0.291]] [[1.915]
 [1.385]
 [2.219]
 [2.068]
 [1.313]
 [2.161]
 [1.805]] [[0.215]
 [0.328]
 [0.311]
 [0.34 ]
 [0.274]
 [0.309]
 [0.291]]
line 256 mcts: sample exp_bonus 1.501620738458541
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.074]
 [0.1  ]
 [0.112]
 [0.12 ]
 [0.122]
 [0.129]] [[0.021]
 [0.002]
 [0.055]
 [0.1  ]
 [0.129]
 [0.194]
 [0.398]] [[-0.078]
 [-0.112]
 [-0.078]
 [-0.058]
 [-0.045]
 [-0.032]
 [ 0.009]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.07088984450477576, 0.29598043353529596, 0.27924387866890193, 0.35388584329102646]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.07088984450477576, 0.29598043353529596, 0.27924387866890193, 0.35388584329102646]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.07088984450477576, 0.29598043353529596, 0.27924387866890193, 0.35388584329102646]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.07088984450477576, 0.29598043353529596, 0.27924387866890193, 0.35388584329102646]
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07078950321314377, 0.29629714035923166, 0.2795297080155087, 0.35338364841211584]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07078950321314377, 0.29629714035923166, 0.2795297080155087, 0.35338364841211584]
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07068933775136788, 0.2966133604347948, 0.2798149680916031, 0.35288233372223415]
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.489]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]] [[3.546]
 [3.371]
 [3.546]
 [3.546]
 [3.546]
 [3.546]
 [3.546]] [[0.786]
 [0.666]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07074401474660581, 0.2968431571767563, 0.27925708202772725, 0.3531557460489106]
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07074401474660581, 0.2968431571767563, 0.27925708202772725, 0.3531557460489106]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2666666666666665  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.44 ]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[3.857]
 [4.106]
 [3.857]
 [3.857]
 [3.857]
 [3.857]
 [3.857]] [[0.615]
 [0.708]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]]
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.071728781253108, 0.2937269729394117, 0.2764598712601119, 0.35808437454736836]
UNIT TEST: sample policy line 217 mcts : [0.041 0.102 0.143 0.061 0.041 0.571 0.041]
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.071728781253108, 0.2937269729394117, 0.2764598712601119, 0.35808437454736836]
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.071728781253108, 0.2937269729394117, 0.2764598712601119, 0.35808437454736836]
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07182020266006676, 0.29282548681947523, 0.2768127777112336, 0.3585415328092244]
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07182020266006676, 0.29282548681947523, 0.2768127777112336, 0.3585415328092244]
siam score:  -0.81565547
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07182020266006676, 0.29282548681947523, 0.2768127777112336, 0.3585415328092244]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
actor:  1 policy actor:  1  step number:  64 total reward:  0.01999999999999935  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  51 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.608]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[1.606]
 [1.648]
 [1.606]
 [1.606]
 [1.606]
 [1.606]
 [1.606]] [[0.706]
 [0.806]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]]
siam score:  -0.81407094
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07066216158949877, 0.28880009639850424, 0.2877874832295631, 0.3527502587824339]
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07066216158949877, 0.28880009639850424, 0.2877874832295631, 0.3527502587824339]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07056316960005958, 0.28909823136471213, 0.28808377470172036, 0.35225482433350797]
actor:  1 policy actor:  1  step number:  61 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07006990883642056, 0.2870740832065281, 0.29306775795937395, 0.34978824999767727]
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07006990883642056, 0.2870740832065281, 0.29306775795937395, 0.34978824999767727]
line 256 mcts: sample exp_bonus 2.9036500039740987
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07006002884507803, 0.28710715759688593, 0.2930930385674114, 0.34973977499062475]
siam score:  -0.8194319
start point for exploration sampling:  11715
line 256 mcts: sample exp_bonus 2.480026093596127
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]] [[2.092]
 [2.092]
 [2.092]
 [2.092]
 [2.092]
 [2.092]
 [2.092]] [[0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]]
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.06991836515967731, 0.28792054563047503, 0.29313053812387013, 0.3490305510859776]
first move QE:  0.40328499436056636
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.06991836515967731, 0.28792054563047503, 0.29313053812387013, 0.3490305510859776]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.251]
 [0.241]
 [0.231]
 [0.237]
 [0.233]
 [0.241]] [[1.796]
 [1.961]
 [1.953]
 [1.504]
 [1.752]
 [2.117]
 [1.733]] [[0.242]
 [0.251]
 [0.241]
 [0.231]
 [0.237]
 [0.233]
 [0.241]]
maxi score, test score, baseline:  -0.997175204359673 -1.0 -0.997175204359673
maxi score, test score, baseline:  -0.997175204359673 -1.0 -0.997175204359673
probs:  [0.06902380437035471, 0.28300100834163733, 0.3034179322339225, 0.34455725505408563]
maxi score, test score, baseline:  -0.9971826086956522 -1.0 -0.9971826086956522
probs:  [0.06902380473624117, 0.2830009347487281, 0.30341800379887485, 0.34455725671615595]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.495583775607031
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.997189972899729 -1.0 -0.997189972899729
probs:  [0.06916495308795632, 0.2823595511563406, 0.30321241983152014, 0.3452630759241829]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06924860452064478, 0.2814901931115649, 0.3035798234473483, 0.345681378920442]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.64 ]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[1.646]
 [1.923]
 [1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]] [[0.604]
 [0.888]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06924860452064478, 0.2814901931115649, 0.3035798234473483, 0.345681378920442]
actor:  1 policy actor:  1  step number:  53 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06856048392447107, 0.2793617426130682, 0.3098377814525883, 0.3422399920098725]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06861802378680554, 0.27959658657795017, 0.30925766701432633, 0.3425277226209179]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.547]
 [0.483]
 [0.457]
 [0.457]
 [0.484]
 [0.489]] [[1.14 ]
 [1.42 ]
 [1.312]
 [1.284]
 [1.284]
 [1.087]
 [1.245]] [[0.793]
 [0.987]
 [0.886]
 [0.851]
 [0.851]
 [0.812]
 [0.87 ]]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06722216917935074, 0.29276687278545965, 0.3044640641831884, 0.3355468938520012]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.628]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]] [[1.441]
 [1.734]
 [1.441]
 [1.441]
 [1.441]
 [1.441]
 [1.441]] [[0.449]
 [0.673]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06722216917935074, 0.29276687278545965, 0.3044640641831884, 0.3355468938520012]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06722216917935074, 0.29276687278545965, 0.3044640641831884, 0.3355468938520012]
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.591]
 [0.626]
 [0.627]
 [0.621]
 [0.615]
 [0.606]] [[3.729]
 [3.964]
 [3.713]
 [3.641]
 [3.555]
 [3.433]
 [3.872]] [[0.75 ]
 [0.761]
 [0.712]
 [0.69 ]
 [0.655]
 [0.609]
 [0.746]]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06722216917935074, 0.29276687278545965, 0.3044640641831884, 0.3355468938520012]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06722216917935074, 0.29276687278545965, 0.3044640641831884, 0.3355468938520012]
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.901]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[3.456]
 [2.836]
 [2.725]
 [3.456]
 [3.456]
 [3.456]
 [3.456]] [[0.705]
 [0.756]
 [0.461]
 [0.705]
 [0.705]
 [0.705]
 [0.705]]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.0673057959007297, 0.2918856444059279, 0.3048434877378873, 0.33596507195545505]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06853826072861756, 0.2883237449832902, 0.3010049623345653, 0.34213303195352684]
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999963  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.494]
 [0.478]
 [0.478]
 [0.478]
 [0.489]
 [0.478]] [[-0.434]
 [ 0.426]
 [-0.181]
 [-0.181]
 [-0.181]
 [-0.099]
 [-0.181]] [[0.216]
 [0.537]
 [0.297]
 [0.297]
 [0.297]
 [0.335]
 [0.297]]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06978417320278528, 0.2847229810328898, 0.2971245540874558, 0.3483682916768691]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06978417320278528, 0.2847229810328898, 0.2971245540874558, 0.3483682916768691]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06968797179692875, 0.2850010090335464, 0.29742417443512004, 0.34788684473440484]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.504]
 [0.441]
 [0.449]
 [0.442]
 [0.442]
 [0.44 ]] [[0.452]
 [1.386]
 [0.521]
 [0.331]
 [0.101]
 [0.233]
 [0.554]] [[0.196]
 [0.416]
 [0.209]
 [0.185]
 [0.138]
 [0.161]
 [0.213]]
line 256 mcts: sample exp_bonus 2.156970661943343
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06954409595799702, 0.28596845260591786, 0.2973207218011427, 0.34716672963494244]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.646]
 [0.574]
 [0.635]
 [0.574]
 [0.608]
 [0.574]] [[1.638]
 [1.718]
 [1.638]
 [2.109]
 [1.638]
 [2.103]
 [1.638]] [[0.367]
 [0.48 ]
 [0.367]
 [0.664]
 [0.367]
 [0.634]
 [0.367]]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.06968110078900022, 0.28534602890564553, 0.2971210259147215, 0.3478518443906327]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.0697182281550009, 0.2856335864007889, 0.29660899048288447, 0.3480391949613257]
maxi score, test score, baseline:  -0.9971972972972973 -1.0 -0.9971972972972973
probs:  [0.0694340858947, 0.28645847338748126, 0.29749025161648107, 0.3466171891013377]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[1.957]
 [1.957]
 [1.957]
 [1.957]
 [1.957]
 [1.957]
 [1.957]] [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.331]
 [0.287]
 [0.277]
 [0.226]
 [0.249]
 [0.282]] [[1.519]
 [1.486]
 [2.515]
 [2.007]
 [0.922]
 [1.973]
 [1.742]] [[0.204]
 [0.331]
 [0.287]
 [0.277]
 [0.226]
 [0.249]
 [0.282]]
maxi score, test score, baseline:  -0.9972045822102426 -1.0 -0.9972045822102426
probs:  [0.06943408403780021, 0.2864583984741191, 0.2974903378401681, 0.34661717964791267]
line 256 mcts: sample exp_bonus 4.112461031300662
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
probs:  [0.06943408219090193, 0.28645832396429205, 0.29749042359940103, 0.34661717024540506]
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.06943408035392452, 0.28645824985474844, 0.2974905088979226, 0.34661716089340444]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.725]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[3.767]
 [3.622]
 [3.767]
 [3.767]
 [3.767]
 [3.767]
 [3.767]] [[0.739]
 [0.725]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]]
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.871]
 [0.868]
 [0.867]
 [0.867]
 [0.867]
 [0.868]] [[0.074]
 [0.07 ]
 [0.078]
 [0.085]
 [0.085]
 [0.085]
 [0.077]] [[0.867]
 [0.871]
 [0.868]
 [0.867]
 [0.867]
 [0.867]
 [0.868]]
actor:  0 policy actor:  1  step number:  30 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  32 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  34 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  34 total reward:  0.5133333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  34 total reward:  0.54  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9770948051948052 -1.0 -0.9770948051948052
probs:  [0.06934496675877255, 0.28694239516930925, 0.2975410006798896, 0.34617163739202866]
actor:  0 policy actor:  1  step number:  42 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9733024179620035 -1.0 -0.9733024179620035
probs:  [0.06942793731650783, 0.28613865267992167, 0.2978467737485864, 0.346586636254984]
maxi score, test score, baseline:  -0.9733024179620035 -1.0 -0.9733024179620035
probs:  [0.06942793731650783, 0.28613865267992167, 0.2978467737485864, 0.346586636254984]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9733711455641688 -1.0 -0.9733711455641688
probs:  [0.06942791898158782, 0.28613791963262336, 0.29784761846010954, 0.3465865429256793]
actor:  1 policy actor:  1  step number:  52 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.9733711455641688 -1.0 -0.9733711455641688
probs:  [0.06852203427412369, 0.28305166663642856, 0.3063701497070454, 0.3420561493824023]
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.269]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]] [[1.932]
 [2.539]
 [1.932]
 [1.932]
 [1.932]
 [1.932]
 [1.932]] [[0.233]
 [0.269]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]]
maxi score, test score, baseline:  -0.9733711455641688 -1.0 -0.9733711455641688
probs:  [0.06852203427412369, 0.28305166663642856, 0.3063701497070454, 0.3420561493824023]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9733711455641688 -1.0 -0.9733711455641688
probs:  [0.06852203427412369, 0.28305166663642856, 0.3063701497070454, 0.3420561493824023]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9733711455641688 -1.0 -0.9733711455641688
maxi score, test score, baseline:  -0.9733711455641688 -1.0 -0.9733711455641688
probs:  [0.06860159252568021, 0.2822179441689231, 0.30672646958354327, 0.3424539937218534]
maxi score, test score, baseline:  -0.9735075407026564 -1.0 -0.9735075407026564
probs:  [0.06860157653584133, 0.2822165878765392, 0.30672792505708435, 0.3424539105305352]
maxi score, test score, baseline:  -0.9735075407026564 -1.0 -0.9735075407026564
probs:  [0.06860157653584133, 0.2822165878765392, 0.30672792505708435, 0.3424539105305352]
maxi score, test score, baseline:  -0.9735075407026564 -1.0 -0.9735075407026564
probs:  [0.06860157653584133, 0.2822165878765392, 0.30672792505708435, 0.3424539105305352]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9735752136752137 -1.0 -0.9735752136752137
probs:  [0.06860156860333441, 0.2822159150459856, 0.3067286470910354, 0.3424538692596446]
actor:  1 policy actor:  1  step number:  72 total reward:  0.006666666666665377  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8057614
maxi score, test score, baseline:  -0.9737761662425785 -0.5366666666666667 -0.5366666666666667
probs:  [0.06897292189860253, 0.2868116210914006, 0.29991451375839073, 0.34430094325160615]
maxi score, test score, baseline:  -0.9737761662425785 -0.5366666666666667 -0.5366666666666667
probs:  [0.06897292189860253, 0.2868116210914006, 0.29991451375839073, 0.34430094325160615]
maxi score, test score, baseline:  -0.9738424703891709 -0.5366666666666667 -0.5366666666666667
probs:  [0.06895127100292524, 0.28627268685249835, 0.30058286191618905, 0.34419318022838735]
maxi score, test score, baseline:  -0.9738424703891709 -0.5366666666666667 -0.5366666666666667
probs:  [0.06895127100292524, 0.28627268685249835, 0.30058286191618905, 0.34419318022838735]
maxi score, test score, baseline:  -0.9738424703891709 -0.5366666666666667 -0.5366666666666667
probs:  [0.06895127100292524, 0.28627268685249835, 0.30058286191618905, 0.34419318022838735]
actor:  1 policy actor:  1  step number:  59 total reward:  0.1466666666666664  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7766581124208347
maxi score, test score, baseline:  -0.9739084388185655 -0.5366666666666667 -0.5366666666666667
actor:  1 policy actor:  1  step number:  52 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9739084388185655 -0.5366666666666667 -0.5366666666666667
probs:  [0.06912152266552074, 0.2900261077176659, 0.2958047739574574, 0.3450475956593559]
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]] [[3.063]
 [1.479]
 [1.479]
 [1.479]
 [1.479]
 [1.479]
 [1.479]] [[ 0.319]
 [-0.468]
 [-0.468]
 [-0.468]
 [-0.468]
 [-0.468]
 [-0.468]]
maxi score, test score, baseline:  -0.9739084388185655 -0.5366666666666667 -0.5366666666666667
probs:  [0.07022310072351566, 0.2867757330002839, 0.2924405560503819, 0.3505606102258185]
actor:  1 policy actor:  1  step number:  65 total reward:  0.02666666666666606  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9739084388185655 -0.5366666666666667 -0.5366666666666667
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9739084388185655 -0.5366666666666667 -0.5366666666666667
probs:  [0.070635382945632, 0.2859347561709202, 0.2908061357477449, 0.35262372513570284]
line 256 mcts: sample exp_bonus 1.9701637735990325
maxi score, test score, baseline:  -0.9739084388185655 -0.5366666666666667 -0.5366666666666667
probs:  [0.070635382945632, 0.2859347561709202, 0.2908061357477449, 0.35262372513570284]
line 256 mcts: sample exp_bonus 4.49129600453682
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.546]
 [0.513]
 [0.513]
 [0.513]
 [0.51 ]
 [0.508]] [[0.931]
 [1.223]
 [0.902]
 [0.902]
 [0.902]
 [0.799]
 [0.916]] [[0.509]
 [0.546]
 [0.513]
 [0.513]
 [0.513]
 [0.51 ]
 [0.508]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9739740740740741 -0.5366666666666667 -0.5366666666666667
probs:  [0.07178492320800284, 0.2829017390398741, 0.28693676846933136, 0.3583765692827917]
siam score:  -0.7986029
maxi score, test score, baseline:  -0.9739740740740741 -0.5366666666666667 -0.5366666666666667
probs:  [0.07169385098800883, 0.2831717175392016, 0.28721364765192403, 0.3579207838208655]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9739740740740741 -0.5366666666666667 -0.5366666666666667
probs:  [0.07169385098800883, 0.2831717175392016, 0.28721364765192403, 0.3579207838208655]
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.498]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]] [[2.636]
 [2.427]
 [2.636]
 [2.636]
 [2.636]
 [2.636]
 [2.636]] [[0.185]
 [0.355]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]]
siam score:  -0.7967636
actor:  1 policy actor:  1  step number:  55 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9741043551088777 -0.5366666666666667 -0.5366666666666667
probs:  [0.07248855669070758, 0.2808158569591502, 0.2847975709890359, 0.3618980153611064]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9741043551088777 -0.5366666666666667 -0.5366666666666667
probs:  [0.07254127744788377, 0.28102040766506425, 0.2842766557489945, 0.3621616591380574]
maxi score, test score, baseline:  -0.9741043551088777 -0.5366666666666667 -0.5366666666666667
probs:  [0.07254127744788377, 0.28102040766506425, 0.2842766557489945, 0.3621616591380574]
siam score:  -0.7905368
maxi score, test score, baseline:  -0.9741043551088777 -0.5366666666666667 -0.5366666666666667
probs:  [0.07254127744788377, 0.28102040766506425, 0.2842766557489945, 0.3621616591380574]
maxi score, test score, baseline:  -0.9741043551088777 -0.5366666666666667 -0.5366666666666667
probs:  [0.07254127744788377, 0.28102040766506425, 0.2842766557489945, 0.3621616591380574]
Printing some Q and Qe and total Qs values:  [[-0.06 ]
 [-0.036]
 [-0.041]
 [-0.033]
 [-0.038]
 [-0.048]
 [-0.033]] [[1.318]
 [1.318]
 [1.752]
 [0.886]
 [0.483]
 [1.736]
 [1.866]] [[-0.041]
 [-0.017]
 [ 0.122]
 [-0.159]
 [-0.297]
 [ 0.11 ]
 [ 0.168]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[1.084]
 [1.084]
 [1.084]
 [1.084]
 [1.084]
 [1.084]
 [1.084]] [[-0.374]
 [-0.374]
 [-0.374]
 [-0.374]
 [-0.374]
 [-0.374]
 [-0.374]]
maxi score, test score, baseline:  -0.9741690058479533 -0.5366666666666667 -0.5366666666666667
probs:  [0.07169482057185667, 0.2901378766732561, 0.2802385700726517, 0.3579287326822356]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.79042405
maxi score, test score, baseline:  -0.9741690058479533 -0.5366666666666667 -0.5366666666666667
probs:  [0.07169482057185667, 0.2901378766732561, 0.2802385700726517, 0.3579287326822356]
maxi score, test score, baseline:  -0.9742333333333334 -0.5366666666666667 -0.5366666666666667
probs:  [0.07169482057185667, 0.2901378766732561, 0.2802385700726517, 0.3579287326822356]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.466]
 [0.05 ]
 [0.041]
 [0.044]
 [0.048]
 [0.059]] [[2.022]
 [2.194]
 [2.527]
 [2.057]
 [1.946]
 [2.361]
 [2.367]] [[-0.24 ]
 [ 0.257]
 [-0.049]
 [-0.215]
 [-0.248]
 [-0.106]
 [-0.093]]
maxi score, test score, baseline:  -0.9742333333333334 -0.5366666666666667 -0.5366666666666667
probs:  [0.07169482057185667, 0.2901378766732561, 0.2802385700726517, 0.3579287326822356]
actor:  1 policy actor:  1  step number:  53 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  64 total reward:  0.046666666666665746  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9742333333333334 -0.5366666666666667 -0.5366666666666667
probs:  [0.07218815630410796, 0.2914457273352842, 0.27596683764699304, 0.36039927871361477]
maxi score, test score, baseline:  -0.9742333333333334 -0.5366666666666667 -0.5366666666666667
probs:  [0.07218815630410796, 0.2914457273352842, 0.27596683764699304, 0.36039927871361477]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.9742333333333334 -0.5366666666666667 -0.5366666666666667
probs:  [0.07219882962932679, 0.29213361747648814, 0.2752152363782391, 0.36045231651594595]
maxi score, test score, baseline:  -0.9742333333333334 -0.5366666666666667 -0.5366666666666667
probs:  [0.07219202641859601, 0.2916051808698278, 0.27578483539284737, 0.36041795731872883]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9742333333333334 -0.5366666666666667 -0.5366666666666667
probs:  [0.07219202641859601, 0.2916051808698278, 0.27578483539284737, 0.36041795731872883]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9742333333333334 -0.5366666666666667 -0.5366666666666667
probs:  [0.07219202641859601, 0.2916051808698278, 0.27578483539284737, 0.36041795731872883]
line 256 mcts: sample exp_bonus 0.9192900436945057
maxi score, test score, baseline:  -0.9742333333333334 -0.5366666666666667 -0.5366666666666667
actor:  1 policy actor:  1  step number:  58 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  80 total reward:  0.11333333333333162  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9742333333333334 -0.5366666666666667 -0.5366666666666667
line 256 mcts: sample exp_bonus 0.9482121530888602
start point for exploration sampling:  11715
start point for exploration sampling:  11715
actor:  1 policy actor:  1  step number:  56 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07214385377065076, 0.2888853491412442, 0.2787924130346708, 0.36017838405343433]
maxi score, test score, baseline:  -0.9742333333333334 -0.5366666666666667 -0.5366666666666667
probs:  [0.07107713796142827, 0.3000998316542311, 0.2739790959409572, 0.35484393444338347]
line 256 mcts: sample exp_bonus 1.118225840105305
maxi score, test score, baseline:  -0.9742333333333334 -0.5366666666666667 -0.5366666666666667
probs:  [0.07107713796142827, 0.3000998316542311, 0.2739790959409572, 0.35484393444338347]
using another actor
maxi score, test score, baseline:  -0.9743610281923715 -0.5366666666666667 -0.5366666666666667
probs:  [0.0709881222505087, 0.300388623307359, 0.2742247975135882, 0.3543984569285441]
maxi score, test score, baseline:  -0.9743610281923715 -0.5366666666666667 -0.5366666666666667
probs:  [0.07107105996073111, 0.2995700415809074, 0.27454568567395427, 0.3548132127844073]
maxi score, test score, baseline:  -0.9743610281923715 -0.5366666666666667 -0.5366666666666667
probs:  [0.07107105996073111, 0.2995700415809074, 0.27454568567395427, 0.3548132127844073]
maxi score, test score, baseline:  -0.9743610281923715 -0.5366666666666667 -0.5366666666666667
probs:  [0.07107105996073111, 0.2995700415809074, 0.27454568567395427, 0.3548132127844073]
maxi score, test score, baseline:  -0.9743610281923715 -0.5366666666666667 -0.5366666666666667
probs:  [0.07107105996073111, 0.2995700415809074, 0.27454568567395427, 0.3548132127844073]
maxi score, test score, baseline:  -0.974424400330852 -0.5366666666666667 -0.5366666666666667
probs:  [0.07107105996073111, 0.2995700415809074, 0.27454568567395427, 0.3548132127844073]
maxi score, test score, baseline:  -0.974424400330852 -0.5366666666666667 -0.5366666666666667
probs:  [0.07107105996073111, 0.2995700415809074, 0.27454568567395427, 0.3548132127844073]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.974424400330852 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.974424400330852 -0.5366666666666667 -0.5366666666666667
probs:  [0.07107105996073111, 0.2995700415809074, 0.27454568567395427, 0.3548132127844073]
maxi score, test score, baseline:  -0.974424400330852 -0.5366666666666667 -0.5366666666666667
probs:  [0.07107105996073111, 0.2995700415809074, 0.27454568567395427, 0.3548132127844073]
actor:  1 policy actor:  1  step number:  39 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  79 total reward:  0.2933333333333322  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9744874587458746 -0.5366666666666667 -0.5366666666666667
probs:  [0.07079835592736353, 0.2939955060010884, 0.28181223868896976, 0.35339389938257826]
maxi score, test score, baseline:  -0.9744874587458746 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9744874587458746 -0.5366666666666667 -0.5366666666666667
probs:  [0.07079835592736353, 0.2939955060010884, 0.28181223868896976, 0.35339389938257826]
actor:  1 policy actor:  1  step number:  55 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  60 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.06969269311668275, 0.3000793551677773, 0.2823623939301295, 0.3478655577854103]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.06969269311668275, 0.3000793551677773, 0.2823623939301295, 0.3478655577854103]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.06969269311668275, 0.3000793551677773, 0.2823623939301295, 0.3478655577854103]
actor:  1 policy actor:  1  step number:  84 total reward:  0.05999999999999828  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.06932255526594744, 0.2991930874074535, 0.2854700070985911, 0.346014350228008]
Printing some Q and Qe and total Qs values:  [[ 0.061]
 [ 0.152]
 [ 0.061]
 [ 0.061]
 [ 0.061]
 [-0.002]
 [ 0.061]] [[1.467]
 [1.489]
 [1.467]
 [1.467]
 [1.467]
 [1.823]
 [1.467]] [[-0.236]
 [-0.13 ]
 [-0.236]
 [-0.236]
 [-0.236]
 [-0.062]
 [-0.236]]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.06932255526594744, 0.2991930874074535, 0.2854700070985911, 0.346014350228008]
actor:  1 policy actor:  1  step number:  58 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.06997934128912205, 0.29712742014729165, 0.28359021136718465, 0.34930302719640166]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.06997934128912205, 0.29712742014729165, 0.28359021136718465, 0.34930302719640166]
actor:  1 policy actor:  1  step number:  48 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7985339
line 256 mcts: sample exp_bonus 3.6758192145249913
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
actor:  1 policy actor:  1  step number:  59 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [-0.014]
 [ 0.001]
 [-0.01 ]
 [ 0.001]
 [-0.008]] [[1.75 ]
 [1.899]
 [1.725]
 [1.512]
 [1.554]
 [1.512]
 [1.96 ]] [[-0.049]
 [ 0.05 ]
 [-0.077]
 [-0.203]
 [-0.186]
 [-0.203]
 [ 0.087]]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.07050525477470589, 0.2996325675760348, 0.2779224518426781, 0.3519397258065811]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.07059075499051964, 0.2987819838025801, 0.278260018410852, 0.3523672427960482]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.07040476651940282, 0.2993754899831329, 0.2787834221499224, 0.3514363213475419]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.07062658631524042, 0.2979052538268508, 0.27892270123567886, 0.3525454586222298]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.07062658631524042, 0.2979052538268508, 0.27892270123567886, 0.3525454586222298]
siam score:  -0.7870852
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.07067856008075428, 0.2981248366340817, 0.2783912668624897, 0.35280533642267436]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.07067856008075428, 0.2981248366340817, 0.2783912668624897, 0.35280533642267436]
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.005]
 [-0.015]
 [-0.025]
 [-0.019]
 [-0.01 ]
 [-0.011]] [[3.047]
 [3.111]
 [3.029]
 [2.912]
 [2.987]
 [3.158]
 [3.011]] [[0.26 ]
 [0.301]
 [0.251]
 [0.182]
 [0.225]
 [0.322]
 [0.244]]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.07067856008075428, 0.2981248366340817, 0.2783912668624897, 0.35280533642267436]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.07067856008075428, 0.2981248366340817, 0.2783912668624897, 0.35280533642267436]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.07067856008075428, 0.2981248366340817, 0.2783912668624897, 0.35280533642267436]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.0719238818555673, 0.2941595369395523, 0.2748780482654257, 0.3590385329394547]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]] [[2.125]
 [1.533]
 [1.533]
 [1.533]
 [1.533]
 [1.533]
 [1.533]] [[ 0.175]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
actor:  1 policy actor:  1  step number:  86 total reward:  0.033333333333331217  reward:  1.0 rdn_beta:  0.667
from probs:  [0.0719238818555673, 0.2941595369395523, 0.2748780482654257, 0.3590385329394547]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.07221972560844439, 0.29321752406654367, 0.27404343363372613, 0.3605193166912858]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.07221972560844439, 0.29321752406654367, 0.27404343363372613, 0.3605193166912858]
siam score:  -0.7780672
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.07221972560844439, 0.29321752406654367, 0.27404343363372613, 0.3605193166912858]
maxi score, test score, baseline:  -0.9746126436781609 -0.5366666666666667 -0.5366666666666667
probs:  [0.07221972560844439, 0.29321752406654367, 0.27404343363372613, 0.3605193166912858]
maxi score, test score, baseline:  -0.9746747747747748 -0.5366666666666667 -0.5366666666666667
probs:  [0.07227112638864049, 0.29342654227887793, 0.27352599624150314, 0.3607763350909785]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.021]
 [0.03 ]
 [0.032]
 [0.032]
 [0.028]
 [0.027]] [[-2.998]
 [ 0.719]
 [-0.469]
 [-0.77 ]
 [-0.858]
 [-0.699]
 [-0.32 ]] [[0.398]
 [0.021]
 [0.03 ]
 [0.032]
 [0.032]
 [0.028]
 [0.027]]
siam score:  -0.77571046
maxi score, test score, baseline:  -0.9746747747747748 -0.5366666666666667 -0.5366666666666667
probs:  [0.07235497642639277, 0.29260558411673493, 0.27384383052539857, 0.3611956089314738]
rdn beta is 0 so we're just using the maxi policy
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.9746747747747748 -0.5366666666666667 -0.5366666666666667
probs:  [0.07235497642639277, 0.29260558411673493, 0.27384383052539857, 0.3611956089314738]
maxi score, test score, baseline:  -0.9746747747747748 -0.5366666666666667 -0.5366666666666667
probs:  [0.07235497642639277, 0.29260558411673493, 0.27384383052539857, 0.3611956089314738]
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.032]
 [-0.018]
 [-0.026]
 [-0.026]
 [-0.022]
 [-0.026]] [[1.781]
 [2.663]
 [1.737]
 [2.153]
 [2.153]
 [1.719]
 [2.153]] [[-0.8  ]
 [-0.369]
 [-0.817]
 [-0.618]
 [-0.618]
 [-0.831]
 [-0.618]]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]] [[-1.598]
 [-2.215]
 [-2.215]
 [-2.215]
 [-2.215]
 [-2.215]
 [-2.215]] [[0.423]
 [0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]]
maxi score, test score, baseline:  -0.9746747747747748 -0.5366666666666667 -0.5366666666666667
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.248]
 [-0.018]
 [-0.019]
 [-0.011]
 [-0.017]
 [ 0.202]] [[3.018]
 [2.984]
 [3.229]
 [3.091]
 [3.151]
 [3.182]
 [2.915]] [[0.091]
 [0.302]
 [0.172]
 [0.109]
 [0.143]
 [0.152]
 [0.229]]
maxi score, test score, baseline:  -0.9746747747747748 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9746747747747748 -0.5366666666666667 -0.5366666666666667
probs:  [0.07240629354084394, 0.2928134363084407, 0.27332806117187314, 0.36145220897884217]
maxi score, test score, baseline:  -0.9746747747747748 -0.5366666666666667 -0.5366666666666667
probs:  [0.07240629354084394, 0.2928134363084407, 0.27332806117187314, 0.36145220897884217]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.55 ]
 [0.623]
 [0.637]
 [0.86 ]
 [0.625]
 [0.624]] [[-0.538]
 [ 0.437]
 [-0.611]
 [-0.879]
 [-0.689]
 [-0.788]
 [-0.515]] [[0.618]
 [0.55 ]
 [0.623]
 [0.637]
 [0.86 ]
 [0.625]
 [0.624]]
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.009]
 [-0.01 ]
 [-0.021]
 [-0.019]
 [-0.012]
 [-0.017]] [[3.357]
 [3.882]
 [3.992]
 [3.135]
 [3.062]
 [3.523]
 [3.815]] [[-0.239]
 [-0.062]
 [-0.027]
 [-0.322]
 [-0.345]
 [-0.184]
 [-0.092]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9747981255093725 -0.5366666666666667 -0.5366666666666667
probs:  [0.07239916761301483, 0.2922848048238478, 0.27389990630406297, 0.3614161212590744]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  0.333
using another actor
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.02 ]
 [-0.025]
 [-0.027]
 [-0.02 ]
 [-0.024]
 [-0.026]] [[2.883]
 [3.607]
 [2.871]
 [2.612]
 [2.998]
 [2.583]
 [3.213]] [[-0.402]
 [-0.155]
 [-0.405]
 [-0.493]
 [-0.359]
 [-0.501]
 [-0.293]]
maxi score, test score, baseline:  -0.974859349593496 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.974859349593496 -0.5366666666666667 -0.5366666666666667
probs:  [0.07200081867401907, 0.2968889572888481, 0.2716859573214905, 0.3594242667156424]
maxi score, test score, baseline:  -0.974859349593496 -0.5366666666666667 -0.5366666666666667
probs:  [0.07200081867401907, 0.2968889572888481, 0.2716859573214905, 0.3594242667156424]
maxi score, test score, baseline:  -0.974859349593496 -0.5366666666666667 -0.5366666666666667
probs:  [0.07200081867401907, 0.2968889572888481, 0.2716859573214905, 0.3594242667156424]
maxi score, test score, baseline:  -0.974859349593496 -0.5366666666666667 -0.5366666666666667
probs:  [0.07200081867401907, 0.2968889572888481, 0.2716859573214905, 0.3594242667156424]
UNIT TEST: sample policy line 217 mcts : [0.061 0.469 0.041 0.041 0.041 0.143 0.204]
maxi score, test score, baseline:  -0.974859349593496 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.974859349593496 -0.5366666666666667 -0.5366666666666667
probs:  [0.07208441307680927, 0.2960714512347919, 0.2720018739068655, 0.3598422617815334]
siam score:  -0.7642134
actor:  0 policy actor:  1  step number:  65 total reward:  0.13333333333333275  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.019]
 [-0.033]
 [-0.026]
 [-0.025]
 [-0.037]
 [-0.024]] [[1.654]
 [1.503]
 [0.97 ]
 [1.249]
 [1.318]
 [0.887]
 [1.526]] [[ 0.247]
 [ 0.151]
 [-0.218]
 [-0.025]
 [ 0.022]
 [-0.278]
 [ 0.162]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9721627737226278 -0.5366666666666667 -0.5366666666666667
probs:  [0.07165526695263286, 0.30026825799312795, 0.2703800622346534, 0.35769641281958575]
siam score:  -0.7585027
maxi score, test score, baseline:  -0.9721627737226278 -0.5366666666666667 -0.5366666666666667
probs:  [0.0717387744189359, 0.29945160212882355, 0.27069565027317305, 0.3581139731790675]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.9721627737226278 -0.5366666666666667 -0.5366666666666667
probs:  [0.07151697853011642, 0.3005523864269514, 0.2709270568682052, 0.357003578174727]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [ 0.38 ]
 [-0.067]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[2.216]
 [1.923]
 [1.391]
 [2.216]
 [2.216]
 [2.216]
 [2.216]] [[ 0.203]
 [ 0.413]
 [-0.025]
 [ 0.203]
 [ 0.203]
 [ 0.203]
 [ 0.203]]
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
probs:  [0.07160023762962012, 0.2997369131901984, 0.27124295412854277, 0.3574198950516386]
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
probs:  [0.07168302812392442, 0.2989260296619699, 0.2715570734318815, 0.3578338687822241]
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
probs:  [0.07168302812392442, 0.2989260296619699, 0.2715570734318815, 0.3578338687822241]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
probs:  [0.07168302812392442, 0.2989260296619699, 0.2715570734318815, 0.3578338687822241]
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
probs:  [0.07168302812392442, 0.2989260296619699, 0.2715570734318815, 0.3578338687822241]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
probs:  [0.07158499211823288, 0.298705696350026, 0.2723665541647434, 0.3573427573669977]
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
probs:  [0.0714950071894574, 0.2989980591445615, 0.2726145761710511, 0.35689235749493]
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
probs:  [0.0714950071894574, 0.2989980591445615, 0.2726145761710511, 0.35689235749493]
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
probs:  [0.07140515254310163, 0.2992899986489658, 0.2728622390849244, 0.3564426097230081]
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
probs:  [0.07140515254310163, 0.2992899986489658, 0.2728622390849244, 0.3564426097230081]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
probs:  [0.07140515254310163, 0.2992899986489658, 0.2728622390849244, 0.3564426097230081]
using another actor
Printing some Q and Qe and total Qs values:  [[-0.015]
 [ 0.008]
 [-0.013]
 [-0.014]
 [-0.004]
 [-0.014]
 [-0.019]] [[3.343]
 [2.927]
 [2.758]
 [3.098]
 [3.356]
 [3.236]
 [3.173]] [[ 0.095]
 [-0.02 ]
 [-0.098]
 [ 0.015]
 [ 0.11 ]
 [ 0.06 ]
 [ 0.034]]
maxi score, test score, baseline:  -0.9722300970873787 -0.5366666666666667 -0.5366666666666667
probs:  [0.0713920465447758, 0.299309392769443, 0.2729204053142202, 0.35637815537156103]
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  68 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9722970944309928 -0.5366666666666667 -0.5366666666666667
probs:  [0.07159101119254391, 0.3029101812768575, 0.26812154669301813, 0.3573772608375805]
maxi score, test score, baseline:  -0.972363768115942 -0.5366666666666667 -0.5366666666666667
probs:  [0.07164017467999326, 0.3031185312400747, 0.26761820077201015, 0.3576230933079219]
maxi score, test score, baseline:  -0.9724301204819278 -0.5366666666666667 -0.5366666666666667
probs:  [0.07164017467999326, 0.3031185312400747, 0.26761820077201015, 0.3576230933079219]
maxi score, test score, baseline:  -0.9724301204819278 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9724301204819278 -0.5366666666666667 -0.5366666666666667
probs:  [0.07168914452715566, 0.3033260605750422, 0.2671168373801843, 0.3578679575176178]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.413]
 [0.388]
 [0.391]
 [0.388]
 [0.392]
 [0.388]] [[ 0.241]
 [ 0.664]
 [ 0.228]
 [-0.083]
 [ 0.228]
 [ 0.115]
 [ 0.228]] [[0.39 ]
 [0.413]
 [0.388]
 [0.391]
 [0.388]
 [0.392]
 [0.388]]
maxi score, test score, baseline:  -0.9724301204819278 -0.5366666666666667 -0.5366666666666667
probs:  [0.07168914452715566, 0.3033260605750422, 0.2671168373801843, 0.3578679575176178]
from probs:  [0.07168914452715566, 0.3033260605750422, 0.2671168373801843, 0.3578679575176178]
maxi score, test score, baseline:  -0.9724301204819278 -0.5366666666666667 -0.5366666666666667
probs:  [0.07173792187581941, 0.3035327741205448, 0.2666174448276771, 0.3581118591759587]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.972496153846154 -0.5366666666666667 -0.5366666666666667
probs:  [0.07173792187581941, 0.3035327741205448, 0.2666174448276771, 0.3581118591759587]
maxi score, test score, baseline:  -0.9725618705035972 -0.5366666666666667 -0.5366666666666667
probs:  [0.07173792187581941, 0.3035327741205448, 0.2666174448276771, 0.3581118591759587]
maxi score, test score, baseline:  -0.9725618705035972 -0.5366666666666667 -0.5366666666666667
probs:  [0.07173792187581941, 0.3035327741205448, 0.2666174448276771, 0.3581118591759587]
maxi score, test score, baseline:  -0.9725618705035972 -0.5366666666666667 -0.5366666666666667
probs:  [0.07173792187581941, 0.3035327741205448, 0.2666174448276771, 0.3581118591759587]
maxi score, test score, baseline:  -0.9725618705035972 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9725618705035972 -0.5366666666666667 -0.5366666666666667
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.248]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]] [[3.463]
 [2.7  ]
 [3.463]
 [3.463]
 [3.463]
 [3.463]
 [3.463]] [[0.508]
 [0.308]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]]
maxi score, test score, baseline:  -0.9725618705035972 -0.5366666666666667 -0.5366666666666667
probs:  [0.07173177610357286, 0.3030258356780231, 0.2671616960750846, 0.3580806921433195]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9725618705035972 -0.5366666666666667 -0.5366666666666667
probs:  [0.07181398270150902, 0.3022259277226753, 0.26746833980692647, 0.35849174976888915]
maxi score, test score, baseline:  -0.9725618705035972 -0.5366666666666667 -0.5366666666666667
probs:  [0.07181398270150902, 0.3022259277226753, 0.26746833980692647, 0.35849174976888915]
maxi score, test score, baseline:  -0.9725618705035972 -0.5366666666666667 -0.5366666666666667
probs:  [0.07181398270150902, 0.3022259277226753, 0.26746833980692647, 0.35849174976888915]
siam score:  -0.7450436
maxi score, test score, baseline:  -0.9725618705035972 -0.5366666666666667 -0.5366666666666667
probs:  [0.07181398270150902, 0.3022259277226753, 0.26746833980692647, 0.35849174976888915]
maxi score, test score, baseline:  -0.9725618705035972 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9725618705035972 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9725618705035972 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9725618705035972 -0.5366666666666667 -0.5366666666666667
probs:  [0.07181398270150902, 0.3022259277226753, 0.26746833980692647, 0.35849174976888915]
maxi score, test score, baseline:  -0.9726272727272728 -0.5366666666666667 -0.5366666666666667
probs:  [0.07181398270150902, 0.3022259277226753, 0.26746833980692647, 0.35849174976888915]
actor:  1 policy actor:  1  step number:  61 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9726272727272728 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9726272727272728 -0.5366666666666667 -0.5366666666666667
probs:  [0.07142642151822935, 0.3073471868648429, 0.2646725639097886, 0.3565538277071391]
maxi score, test score, baseline:  -0.9726272727272728 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9726923627684965 -0.5366666666666667 -0.5366666666666667
probs:  [0.07176843699418278, 0.3053680817250433, 0.26459947369777304, 0.35826400758300103]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9726923627684965 -0.5366666666666667 -0.5366666666666667
probs:  [0.07181619069073769, 0.3055715946507586, 0.2641094242817751, 0.35850279037672855]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.598085482086707
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.184 0.163 0.082 0.102 0.102 0.02  0.347]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9726923627684965 -0.5366666666666667 -0.5366666666666667
probs:  [0.07083228453761078, 0.31509922581270516, 0.26048552489816623, 0.3535829647515179]
actor:  1 policy actor:  1  step number:  64 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  0.333
siam score:  -0.74323714
maxi score, test score, baseline:  -0.9726923627684965 -0.5366666666666667 -0.5366666666666667
probs:  [0.07034269661831011, 0.31984013845118914, 0.25868228649647773, 0.3511348784340231]
actor:  1 policy actor:  1  step number:  63 total reward:  0.013333333333332864  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9726923627684965 -0.5366666666666667 -0.5366666666666667
probs:  [0.07060288262352125, 0.3189134519925541, 0.25804653115881854, 0.35243713422510614]
maxi score, test score, baseline:  -0.9726923627684965 -0.5366666666666667 -0.5366666666666667
probs:  [0.07051413513553312, 0.31922953776853474, 0.2582633825192386, 0.3519929445766936]
maxi score, test score, baseline:  -0.9726923627684965 -0.5366666666666667 -0.5366666666666667
actor:  1 policy actor:  1  step number:  50 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9726923627684965 -0.5366666666666667 -0.5366666666666667
siam score:  -0.74061257
maxi score, test score, baseline:  -0.9726923627684965 -0.5366666666666667 -0.5366666666666667
probs:  [0.06975111042779794, 0.32660688793691683, 0.25546441817816856, 0.3481775834571167]
Printing some Q and Qe and total Qs values:  [[-0.034]
 [-0.028]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]] [[1.498]
 [1.465]
 [1.498]
 [1.498]
 [1.498]
 [1.498]
 [1.498]] [[0.285]
 [0.269]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]]
siam score:  -0.7416241
maxi score, test score, baseline:  -0.9726923627684965 -0.5366666666666667 -0.5366666666666667
probs:  [0.06983451602791291, 0.3258004758878515, 0.25577037065677216, 0.34859463742746344]
maxi score, test score, baseline:  -0.9726923627684965 -0.5366666666666667 -0.5366666666666667
probs:  [0.06974566474964393, 0.32612361152992486, 0.25598079018846426, 0.3481499335319669]
maxi score, test score, baseline:  -0.9726923627684965 -0.5366666666666667 -0.5366666666666667
probs:  [0.06974566474964393, 0.32612361152992486, 0.25598079018846426, 0.3481499335319669]
maxi score, test score, baseline:  -0.9727571428571429 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9727571428571429 -0.5366666666666667 -0.5366666666666667
probs:  [0.06974566474964393, 0.32612361152992486, 0.25598079018846426, 0.3481499335319669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.667
first move QE:  0.4023892086804642
maxi score, test score, baseline:  -0.9727571428571429 -0.5366666666666667 -0.5366666666666667
probs:  [0.07033903343176376, 0.3234689748110094, 0.25507261536223746, 0.3511193763949893]
maxi score, test score, baseline:  -0.9727571428571429 -0.5366666666666667 -0.5366666666666667
probs:  [0.07033903343176376, 0.3234689748110094, 0.25507261536223746, 0.3511193763949893]
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.05 ]
 [0.026]
 [0.029]
 [0.013]
 [0.029]
 [0.007]] [[-1.49 ]
 [ 0.159]
 [-0.46 ]
 [-1.001]
 [-0.642]
 [-0.807]
 [-0.478]] [[0.086]
 [0.05 ]
 [0.026]
 [0.029]
 [0.013]
 [0.029]
 [0.007]]
maxi score, test score, baseline:  -0.9727571428571429 -0.5366666666666667 -0.5366666666666667
probs:  [0.07038378850272627, 0.3236751344977296, 0.25459791004248716, 0.35134316695705714]
maxi score, test score, baseline:  -0.9727571428571429 -0.5366666666666667 -0.5366666666666667
probs:  [0.07038378850272627, 0.3236751344977296, 0.25459791004248716, 0.35134316695705714]
maxi score, test score, baseline:  -0.9727571428571429 -0.5366666666666667 -0.5366666666666667
probs:  [0.07038378850272627, 0.3236751344977296, 0.25459791004248716, 0.35134316695705714]
maxi score, test score, baseline:  -0.9727571428571429 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9728216152019002 -0.5366666666666667 -0.5366666666666667
probs:  [0.07028526127876451, 0.3239786087515046, 0.2548850487697213, 0.35085108120000963]
maxi score, test score, baseline:  -0.9728216152019002 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9728216152019002 -0.5366666666666667 -0.5366666666666667
probs:  [0.07032978851899621, 0.3241841997695721, 0.25441227880975587, 0.3510737329016758]
maxi score, test score, baseline:  -0.972949645390071 -0.5366666666666667 -0.5366666666666667
probs:  [0.07031916707662124, 0.32416841906688243, 0.2544908078800289, 0.3510216059764674]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.972949645390071 -0.5366666666666667 -0.5366666666666667
probs:  [0.07103017674127093, 0.32109333714737287, 0.2532966461663639, 0.3545798399449923]
siam score:  -0.74740034
maxi score, test score, baseline:  -0.972949645390071 -0.5366666666666667 -0.5366666666666667
probs:  [0.07111959300846891, 0.3222072712674379, 0.25164657964102294, 0.3550265560830702]
start point for exploration sampling:  11715
UNIT TEST: sample policy line 217 mcts : [0.041 0.714 0.02  0.061 0.122 0.02  0.02 ]
maxi score, test score, baseline:  -0.972949645390071 -0.5366666666666667 -0.5366666666666667
probs:  [0.07111959300846891, 0.3222072712674379, 0.25164657964102294, 0.3550265560830702]
maxi score, test score, baseline:  -0.972949645390071 -0.5366666666666667 -0.5366666666666667
probs:  [0.07111959300846891, 0.3222072712674379, 0.25164657964102294, 0.3550265560830702]
siam score:  -0.75251013
maxi score, test score, baseline:  -0.972949645390071 -0.5366666666666667 -0.5366666666666667
probs:  [0.07112015144377708, 0.322919557173894, 0.25093134456454125, 0.3550289468177878]
maxi score, test score, baseline:  -0.972949645390071 -0.5366666666666667 -0.5366666666666667
probs:  [0.07112015144377708, 0.322919557173894, 0.25093134456454125, 0.3550289468177878]
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]] [[3.546]
 [3.546]
 [3.546]
 [3.546]
 [3.546]
 [3.546]
 [3.546]] [[0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.4  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.972949645390071 -0.5366666666666667 -0.5366666666666667
probs:  [0.07192201195572853, 0.32388477941919414, 0.2452218869207738, 0.3589713217043035]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.972949645390071 -0.5366666666666667 -0.5366666666666667
probs:  [0.07192201195572853, 0.32388477941919414, 0.2452218869207738, 0.3589713217043035]
maxi score, test score, baseline:  -0.972949645390071 -0.5366666666666667 -0.5366666666666667
probs:  [0.07192201195572853, 0.32388477941919414, 0.2452218869207738, 0.3589713217043035]
actor:  1 policy actor:  1  step number:  40 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.689]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[1.13 ]
 [1.071]
 [1.13 ]
 [1.13 ]
 [1.13 ]
 [1.13 ]
 [1.13 ]] [[0.484]
 [0.689]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]]
maxi score, test score, baseline:  -0.972949645390071 -0.5366666666666667 -0.5366666666666667
probs:  [0.07303753485156554, 0.31970861987236476, 0.24269779307918243, 0.3645560521968873]
maxi score, test score, baseline:  -0.972949645390071 -0.5366666666666667 -0.5366666666666667
probs:  [0.07303753485156554, 0.31970861987236476, 0.24269779307918243, 0.3645560521968873]
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.07303753485156554, 0.31970861987236476, 0.24269779307918243, 0.3645560521968873]
actor:  1 policy actor:  1  step number:  69 total reward:  0.22666666666666657  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.07239624253365481, 0.3256901184975353, 0.2405637669733584, 0.3613498719954515]
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.07239624253365481, 0.3256901184975353, 0.2405637669733584, 0.3613498719954515]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.001]] [[2.039]
 [1.871]
 [1.81 ]
 [1.81 ]
 [1.81 ]
 [1.81 ]
 [1.908]] [[0.185]
 [0.125]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.143]]
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.227]
 [0.078]
 [0.056]
 [0.102]
 [0.093]
 [0.06 ]] [[2.883]
 [2.684]
 [2.964]
 [3.021]
 [3.346]
 [3.664]
 [3.181]] [[0.183]
 [0.208]
 [0.23 ]
 [0.238]
 [0.388]
 [0.504]
 [0.302]]
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.07239624253365481, 0.3256901184975353, 0.2405637669733584, 0.3613498719954515]
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.07230449701554473, 0.32603982487415084, 0.2407651113375549, 0.3608905667727495]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.003]
 [-0.005]
 [-0.012]
 [-0.014]
 [-0.01 ]
 [-0.004]] [[1.123]
 [1.123]
 [1.926]
 [0.578]
 [0.313]
 [1.325]
 [0.9  ]] [[ 0.015]
 [ 0.028]
 [ 0.427]
 [-0.253]
 [-0.388]
 [ 0.122]
 [-0.085]]
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.07230449701554473, 0.32603982487415084, 0.2407651113375549, 0.3608905667727495]
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.07230449701554473, 0.32603982487415084, 0.2407651113375549, 0.3608905667727495]
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.07230449701554473, 0.32603982487415084, 0.2407651113375549, 0.3608905667727495]
actor:  1 policy actor:  1  step number:  41 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.07113735761791119, 0.33643569600196643, 0.23737216320600207, 0.35505478317412037]
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.07113735761791119, 0.33643569600196643, 0.23737216320600207, 0.35505478317412037]
first move QE:  0.39674515875254646
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.07104535981445156, 0.3367965146180342, 0.23756389823677634, 0.35459422733073787]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.5933333333333336  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]]
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.07246414290326202, 0.3319982981806231, 0.23384126018123735, 0.36169629873487763]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[1.399]
 [1.399]
 [1.399]
 [1.399]
 [1.399]
 [1.399]
 [1.399]] [[0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.5  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.07138386952000679, 0.3419705899205355, 0.23035023862859794, 0.35629530193085984]
from probs:  [0.07142730062522819, 0.3421789968380426, 0.22988125995968509, 0.3565124425770442]
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.07133667540273018, 0.342546063214245, 0.23005849287799515, 0.3560587685050297]
actor:  1 policy actor:  1  step number:  61 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.0718527558041673, 0.34045574268798345, 0.2290492105489908, 0.35864229095885836]
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
probs:  [0.0718527558041673, 0.34045574268798345, 0.2290492105489908, 0.35864229095885836]
maxi score, test score, baseline:  -0.9730132075471699 -0.5366666666666667 -0.5366666666666667
actor:  1 policy actor:  1  step number:  58 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9730764705882353 -0.5366666666666667 -0.5366666666666667
probs:  [0.07129093358554735, 0.3456200279496005, 0.22725568741194974, 0.3558333510529024]
maxi score, test score, baseline:  -0.9730764705882353 -0.5366666666666667 -0.5366666666666667
probs:  [0.07129093358554735, 0.3456200279496005, 0.22725568741194974, 0.3558333510529024]
maxi score, test score, baseline:  -0.9730764705882353 -0.5366666666666667 -0.5366666666666667
probs:  [0.07133343585449228, 0.3458264245283161, 0.22679429016999936, 0.35604584944719225]
actor:  0 policy actor:  1  step number:  56 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9704007824726135 -0.5366666666666667 -0.5366666666666667
probs:  [0.07133343585449228, 0.3458264245283161, 0.22679429016999936, 0.35604584944719225]
Printing some Q and Qe and total Qs values:  [[-0.013]
 [ 0.001]
 [ 0.002]
 [ 0.002]
 [-0.013]
 [-0.009]
 [ 0.01 ]] [[0.362]
 [0.555]
 [0.249]
 [0.249]
 [0.361]
 [0.207]
 [0.507]] [[-0.36 ]
 [-0.23 ]
 [-0.415]
 [-0.415]
 [-0.36 ]
 [-0.45 ]
 [-0.252]]
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.02 ]
 [-0.021]
 [-0.027]] [[ 0.488]
 [ 0.488]
 [ 0.488]
 [ 0.488]
 [-0.444]
 [ 1.322]
 [ 0.488]] [[ 0.109]
 [ 0.109]
 [ 0.109]
 [ 0.109]
 [-0.333]
 [ 0.517]
 [ 0.109]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.207]
 [0.171]
 [0.193]
 [0.22 ]
 [0.193]
 [0.169]] [[2.88 ]
 [2.751]
 [2.865]
 [2.569]
 [2.716]
 [2.79 ]
 [2.699]] [[0.416]
 [0.401]
 [0.43 ]
 [0.304]
 [0.394]
 [0.409]
 [0.349]]
maxi score, test score, baseline:  -0.9704007824726135 -0.5366666666666667 -0.5366666666666667
probs:  [0.07133343585449228, 0.3458264245283161, 0.22679429016999936, 0.35604584944719225]
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.01 ]
 [-0.024]
 [-0.022]
 [-0.022]
 [-0.021]
 [-0.01 ]] [[0.288]
 [0.588]
 [0.459]
 [0.358]
 [0.42 ]
 [0.522]
 [0.631]] [[-0.435]
 [-0.263]
 [-0.346]
 [-0.399]
 [-0.365]
 [-0.308]
 [-0.239]]
line 256 mcts: sample exp_bonus 3.0130043712856978
actor:  1 policy actor:  1  step number:  61 total reward:  0.27999999999999914  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[ 0.049]
 [ 0.129]
 [ 0.018]
 [ 0.018]
 [-0.008]
 [ 0.034]
 [ 0.042]] [[2.701]
 [3.073]
 [3.13 ]
 [2.73 ]
 [2.441]
 [3.006]
 [3.472]] [[0.284]
 [0.444]
 [0.408]
 [0.278]
 [0.171]
 [0.376]
 [0.531]]
maxi score, test score, baseline:  -0.9704698672911788 -0.5366666666666667 -0.5366666666666667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9704698672911788 -0.5366666666666667 -0.5366666666666667
probs:  [0.07198994429727067, 0.3435026914207812, 0.22517530665169638, 0.35933205763025156]
maxi score, test score, baseline:  -0.9704698672911788 -0.5366666666666667 -0.5366666666666667
probs:  [0.07198994429727067, 0.3435026914207812, 0.22517530665169638, 0.35933205763025156]
maxi score, test score, baseline:  -0.9704698672911788 -0.5366666666666667 -0.5366666666666667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07251367401811115, 0.34088636629547475, 0.22464668822361009, 0.36195327146280404]
actor:  1 policy actor:  1  step number:  62 total reward:  0.2866666666666666  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.248]
 [0.221]
 [0.257]
 [0.241]
 [0.232]
 [0.249]] [[-0.795]
 [-0.218]
 [-1.822]
 [-1.051]
 [-1.075]
 [-0.904]
 [-0.64 ]] [[0.306]
 [0.248]
 [0.221]
 [0.257]
 [0.241]
 [0.232]
 [0.249]]
maxi score, test score, baseline:  -0.9704698672911788 -0.5366666666666667 -0.5366666666666667
probs:  [0.07207594674490857, 0.33882525681607834, 0.22933406783453095, 0.35976472860448216]
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.   ]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.01 ]] [[1.174]
 [0.824]
 [1.174]
 [1.174]
 [1.174]
 [1.174]
 [0.542]] [[-0.038]
 [-0.205]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.355]]
maxi score, test score, baseline:  -0.9704698672911788 -0.5366666666666667 -0.5366666666666667
probs:  [0.07197070453064462, 0.3391361680266269, 0.22965379702392139, 0.35923933041880723]
maxi score, test score, baseline:  -0.9704698672911788 -0.5366666666666667 -0.5366666666666667
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9704698672911788 -0.5366666666666667 -0.5366666666666667
probs:  [0.07135081938561179, 0.3362103782575406, 0.2362987720647591, 0.35614003029208846]
actor:  0 policy actor:  1  step number:  58 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9676258566978193 -0.5366666666666667 -0.5366666666666667
probs:  [0.07135081938561179, 0.3362103782575406, 0.2362987720647591, 0.35614003029208846]
maxi score, test score, baseline:  -0.9676258566978193 -0.5366666666666667 -0.5366666666666667
probs:  [0.07135119010353165, 0.3357293351323336, 0.23677812810246576, 0.3561413466616689]
maxi score, test score, baseline:  -0.9677010878010878 -0.5366666666666667 -0.5366666666666667
probs:  [0.07135119010353165, 0.3357293351323336, 0.23677812810246576, 0.3561413466616689]
maxi score, test score, baseline:  -0.9677010878010878 -0.5366666666666667 -0.5366666666666667
probs:  [0.07135119010353165, 0.3357293351323336, 0.23677812810246576, 0.3561413466616689]
actor:  1 policy actor:  1  step number:  93 total reward:  0.09333333333333094  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.967775968992248 -0.5366666666666667 -0.5366666666666667
probs:  [0.07236855738532265, 0.33210311176754076, 0.23429444184619108, 0.36123388900094555]
maxi score, test score, baseline:  -0.967775968992248 -0.5366666666666667 -0.5366666666666667
probs:  [0.07241139741337159, 0.33230002606873377, 0.23384049286836836, 0.3614480836495263]
maxi score, test score, baseline:  -0.967775968992248 -0.5366666666666667 -0.5366666666666667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.967775968992248 -0.5366666666666667 -0.5366666666666667
probs:  [0.07249974266309203, 0.3314842611078128, 0.23412619765594586, 0.36188979857314924]
using explorer policy with actor:  1
from probs:  [0.07249974266309203, 0.3314842611078128, 0.23412619765594586, 0.36188979857314924]
maxi score, test score, baseline:  -0.967775968992248 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.967775968992248 -0.5366666666666667 -0.5366666666666667
probs:  [0.07258766166493553, 0.3306724320470538, 0.23441052397598244, 0.36232938231202816]
maxi score, test score, baseline:  -0.967775968992248 -0.5366666666666667 -0.5366666666666667
probs:  [0.07258766166493553, 0.3306724320470538, 0.23441052397598244, 0.36232938231202816]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  93 total reward:  0.09333333333333138  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.452]
 [0.288]
 [0.276]
 [0.276]
 [0.261]
 [0.277]] [[1.055]
 [0.802]
 [0.855]
 [1.035]
 [1.035]
 [0.942]
 [0.929]] [[0.163]
 [0.159]
 [0.031]
 [0.139]
 [0.139]
 [0.062]
 [0.07 ]]
line 256 mcts: sample exp_bonus 1.4721712706173384
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.011]
 [-0.013]
 [-0.009]
 [-0.004]
 [-0.004]
 [-0.032]] [[2.903]
 [3.352]
 [2.613]
 [1.276]
 [2.903]
 [2.903]
 [3.828]] [[ 0.365]
 [ 0.496]
 [ 0.273]
 [-0.126]
 [ 0.365]
 [ 0.365]
 [ 0.626]]
line 256 mcts: sample exp_bonus 1.1988018010333197
Printing some Q and Qe and total Qs values:  [[-0.11 ]
 [-0.115]
 [-0.099]
 [-0.097]
 [-0.11 ]
 [-0.117]
 [-0.125]] [[1.23 ]
 [1.006]
 [0.876]
 [0.397]
 [0.662]
 [1.225]
 [1.399]] [[-0.063]
 [-0.18 ]
 [-0.229]
 [-0.467]
 [-0.347]
 [-0.073]
 [ 0.006]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9678505027068832 -0.5366666666666667 -0.5366666666666667
probs:  [0.07280194497007053, 0.3305567906778232, 0.23323971712062816, 0.3634015472314782]
Printing some Q and Qe and total Qs values:  [[-0.021]
 [-0.021]
 [-0.019]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]] [[0.462]
 [0.462]
 [1.179]
 [0.462]
 [0.462]
 [0.462]
 [0.137]] [[-0.214]
 [-0.214]
 [ 0.027]
 [-0.214]
 [-0.214]
 [-0.214]
 [-0.323]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9678505027068832 -0.5366666666666667 -0.5366666666666667
probs:  [0.07280344280232705, 0.3300879333263723, 0.23370010702993915, 0.36340851684136166]
maxi score, test score, baseline:  -0.9678505027068832 -0.5366666666666667 -0.5366666666666667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.5070616252295674
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9678505027068832 -0.5366666666666667 -0.5366666666666667
probs:  [0.07280344280232705, 0.3300879333263723, 0.23370010702993915, 0.36340851684136166]
maxi score, test score, baseline:  -0.9678505027068832 -0.5366666666666667 -0.5366666666666667
line 256 mcts: sample exp_bonus 4.97253874772015
maxi score, test score, baseline:  -0.9678505027068832 -0.5366666666666667 -0.5366666666666667
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.948]
 [0.685]
 [0.809]
 [0.809]
 [0.734]
 [0.757]] [[2.036]
 [0.709]
 [2.342]
 [1.833]
 [1.833]
 [1.7  ]
 [2.079]] [[0.683]
 [0.948]
 [0.685]
 [0.809]
 [0.809]
 [0.734]
 [0.757]]
actor:  0 policy actor:  1  step number:  69 total reward:  0.026666666666665506  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9655481481481483 -0.5366666666666667 -0.5366666666666667
probs:  [0.0728730188104797, 0.32925538687093664, 0.2341139203651215, 0.3637576739534622]
siam score:  -0.7580346
maxi score, test score, baseline:  -0.9655481481481483 -0.5366666666666667 -0.5366666666666667
probs:  [0.07283092627528263, 0.3293206268246877, 0.2343022632901023, 0.36354618360992746]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9655481481481483 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9655481481481483 -0.5366666666666667 -0.5366666666666667
probs:  [0.07283092627528263, 0.3293206268246877, 0.2343022632901023, 0.36354618360992746]
siam score:  -0.756727
maxi score, test score, baseline:  -0.9655481481481483 -0.5366666666666667 -0.5366666666666667
probs:  [0.07283092627528263, 0.3293206268246877, 0.2343022632901023, 0.36354618360992746]
maxi score, test score, baseline:  -0.9655481481481483 -0.5366666666666667 -0.5366666666666667
probs:  [0.07283092627528263, 0.3293206268246877, 0.2343022632901023, 0.36354618360992746]
maxi score, test score, baseline:  -0.9656274826789839 -0.5366666666666667 -0.5366666666666667
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]] [[2.355]
 [2.355]
 [2.355]
 [2.355]
 [2.355]
 [2.355]
 [2.355]] [[0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.419]
 [0.494]
 [0.494]] [[2.467]
 [2.467]
 [2.467]
 [2.467]
 [1.013]
 [2.467]
 [2.467]] [[ 0.544]
 [ 0.544]
 [ 0.544]
 [ 0.544]
 [-0.015]
 [ 0.544]
 [ 0.544]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9656274826789839 -0.5366666666666667 -0.5366666666666667
probs:  [0.07250733775147644, 0.3323046057516396, 0.2332597762932404, 0.36192828020364354]
actor:  0 policy actor:  0  step number:  66 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7540126
actor:  0 policy actor:  1  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.017]
 [-0.024]
 [-0.019]
 [-0.026]
 [-0.025]
 [-0.031]] [[ 0.367]
 [ 0.259]
 [-0.052]
 [ 0.   ]
 [ 0.075]
 [ 0.11 ]
 [ 0.403]] [[-0.32 ]
 [-0.383]
 [-0.597]
 [-0.557]
 [-0.514]
 [-0.49 ]
 [-0.301]]
maxi score, test score, baseline:  -0.9603137931034483 -0.5366666666666667 -0.5366666666666667
probs:  [0.07246386725943531, 0.33283759151450365, 0.2329881225916708, 0.3617104186343903]
maxi score, test score, baseline:  -0.9603137931034483 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9603137931034483 -0.5366666666666667 -0.5366666666666667
probs:  [0.07246386725943531, 0.33283759151450365, 0.2329881225916708, 0.3617104186343903]
maxi score, test score, baseline:  -0.9604045871559633 -0.5366666666666667 -0.5366666666666667
probs:  [0.07237822381448168, 0.33317610189587876, 0.2331639762594252, 0.3612816980302144]
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6731441651880741
maxi score, test score, baseline:  -0.9604045871559633 -0.5366666666666667 -0.5366666666666667
probs:  [0.07142568533659747, 0.3408863796153258, 0.23116854922924826, 0.35651938581882847]
using another actor
maxi score, test score, baseline:  -0.9604045871559633 -0.5366666666666667 -0.5366666666666667
probs:  [0.07151285124420409, 0.34008087402003334, 0.23145107087531813, 0.3569552038604445]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9604045871559633 -0.5366666666666667 -0.5366666666666667
probs:  [0.07151285124420409, 0.34008087402003334, 0.23145107087531813, 0.3569552038604445]
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.011]
 [-0.025]
 [-0.022]
 [-0.023]
 [-0.025]
 [-0.025]] [[2.452]
 [2.178]
 [2.445]
 [2.412]
 [2.525]
 [2.695]
 [2.719]] [[-0.296]
 [-0.375]
 [-0.3  ]
 [-0.308]
 [-0.271]
 [-0.217]
 [-0.209]]
maxi score, test score, baseline:  -0.9604949656750573 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9604949656750573 -0.5366666666666667 -0.5366666666666667
probs:  [0.07151285124420409, 0.34008087402003334, 0.23145107087531813, 0.3569552038604445]
maxi score, test score, baseline:  -0.9604949656750573 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9604949656750573 -0.5366666666666667 -0.5366666666666667
probs:  [0.07151285124420409, 0.34008087402003334, 0.23145107087531813, 0.3569552038604445]
actor:  1 policy actor:  1  step number:  46 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9604949656750573 -0.5366666666666667 -0.5366666666666667
probs:  [0.07221645426948221, 0.33725404167738093, 0.23005222126519506, 0.36047728278794183]
maxi score, test score, baseline:  -0.9604949656750573 -0.5366666666666667 -0.5366666666666667
probs:  [0.07221645426948221, 0.33725404167738093, 0.23005222126519506, 0.36047728278794183]
maxi score, test score, baseline:  -0.9604949656750573 -0.5366666666666667 -0.5366666666666667
probs:  [0.07221645426948221, 0.33725404167738093, 0.23005222126519506, 0.36047728278794183]
maxi score, test score, baseline:  -0.9604949656750573 -0.5366666666666667 -0.5366666666666667
probs:  [0.07230291764722267, 0.3364594376128334, 0.23032805148008945, 0.3609095932598545]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.172]
 [0.176]
 [0.121]
 [0.041]
 [0.041]
 [0.041]
 [0.086]] [[4.167]
 [2.439]
 [2.594]
 [2.565]
 [2.565]
 [2.565]
 [2.944]] [[0.758]
 [0.245]
 [0.267]
 [0.223]
 [0.223]
 [0.223]
 [0.355]]
maxi score, test score, baseline:  -0.9604949656750573 -0.5366666666666667 -0.5366666666666667
probs:  [0.07221833901151485, 0.33679852788356346, 0.23049692233580202, 0.3604862107691196]
first move QE:  0.36604982675108416
siam score:  -0.74170613
maxi score, test score, baseline:  -0.9605849315068494 -0.5366666666666667 -0.5366666666666667
probs:  [0.07221833901151485, 0.33679852788356346, 0.23049692233580202, 0.3604862107691196]
maxi score, test score, baseline:  -0.9605849315068494 -0.5366666666666667 -0.5366666666666667
probs:  [0.07221833901151485, 0.33679852788356346, 0.23049692233580202, 0.3604862107691196]
maxi score, test score, baseline:  -0.9605849315068494 -0.5366666666666667 -0.5366666666666667
probs:  [0.07221833901151485, 0.33679852788356346, 0.23049692233580202, 0.3604862107691196]
maxi score, test score, baseline:  -0.9605849315068494 -0.5366666666666667 -0.5366666666666667
probs:  [0.07221833901151485, 0.33679852788356346, 0.23049692233580202, 0.3604862107691196]
actor:  1 policy actor:  1  step number:  66 total reward:  0.24666666666666592  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  44 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9605849315068494 -0.5366666666666667 -0.5366666666666667
siam score:  -0.7382617
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.03 ]
 [-0.031]
 [-0.03 ]
 [-0.027]
 [-0.03 ]
 [-0.03 ]] [[-1.332]
 [-0.281]
 [-1.562]
 [-1.526]
 [-1.25 ]
 [-0.728]
 [-0.717]] [[-0.274]
 [-0.098]
 [-0.312]
 [-0.306]
 [-0.257]
 [-0.173]
 [-0.171]]
maxi score, test score, baseline:  -0.9605849315068494 -0.5366666666666667 -0.5366666666666667
probs:  [0.07335415374073703, 0.3322939960691283, 0.22817638822251804, 0.36617546196761663]
maxi score, test score, baseline:  -0.9605849315068494 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9606744874715263 -0.5366666666666667 -0.5366666666666667
probs:  [0.07327170660798134, 0.33262461917556996, 0.22834091914016746, 0.36576275507628125]
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.026]
 [-0.039]
 [-0.024]
 [-0.024]
 [-0.019]
 [ 0.539]] [[1.076]
 [1.16 ]
 [0.971]
 [0.668]
 [0.615]
 [1.054]
 [0.372]] [[-0.179]
 [-0.14 ]
 [-0.248]
 [-0.384]
 [-0.41 ]
 [-0.186]
 [ 0.031]]
from probs:  [0.07327170660798134, 0.33262461917556996, 0.22834091914016746, 0.36576275507628125]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.9609407239819004 -0.5366666666666667 -0.5366666666666667
probs:  [0.07335694252042949, 0.331847203698267, 0.22860691464066854, 0.36618893914063505]
maxi score, test score, baseline:  -0.9609407239819004 -0.5366666666666667 -0.5366666666666667
probs:  [0.07335694252042949, 0.331847203698267, 0.22860691464066854, 0.36618893914063505]
actor:  1 policy actor:  1  step number:  64 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
probs:  [0.07287647527149288, 0.33036872254361693, 0.23296868898552442, 0.3637861131993658]
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
probs:  [0.07287647527149288, 0.33036872254361693, 0.23296868898552442, 0.3637861131993658]
actor:  1 policy actor:  1  step number:  60 total reward:  0.36666666666666603  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
probs:  [0.07234280318161451, 0.3279455562184729, 0.23859391263100002, 0.36111772796891256]
first move QE:  0.3573212806373437
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
probs:  [0.07234280318161451, 0.3279455562184729, 0.23859391263100002, 0.36111772796891256]
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
probs:  [0.07234280318161451, 0.3279455562184729, 0.23859391263100002, 0.36111772796891256]
actor:  1 policy actor:  1  step number:  47 total reward:  0.48  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.006]
 [-0.027]
 [-0.012]
 [-0.01 ]
 [-0.031]
 [-0.014]] [[1.124]
 [0.912]
 [1.2  ]
 [0.971]
 [0.86 ]
 [0.338]
 [1.05 ]] [[0.447]
 [0.363]
 [0.478]
 [0.385]
 [0.336]
 [0.086]
 [0.42 ]]
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
actor:  1 policy actor:  1  step number:  44 total reward:  0.5  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
probs:  [0.07212674935704, 0.3340406860701898, 0.23379093636508466, 0.3600416282076855]
siam score:  -0.75018674
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
probs:  [0.07212674935704, 0.3340406860701898, 0.23379093636508466, 0.3600416282076855]
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
probs:  [0.07212674935704, 0.3340406860701898, 0.23379093636508466, 0.3600416282076855]
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.685]
 [0.215]
 [0.204]
 [0.259]
 [0.178]
 [0.592]] [[2.214]
 [2.251]
 [1.993]
 [2.29 ]
 [1.409]
 [1.495]
 [2.113]] [[0.492]
 [0.779]
 [0.342]
 [0.469]
 [0.107]
 [0.091]
 [0.653]]
actor:  1 policy actor:  1  step number:  86 total reward:  0.16666666666666485  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus -0.5619240502912022
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[1.193]
 [1.193]
 [1.193]
 [1.193]
 [1.193]
 [1.193]
 [1.193]] [[0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.4893858585077582
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
probs:  [0.07333537209751444, 0.32797334834483244, 0.23260118578615824, 0.3660900937714949]
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
probs:  [0.07333537209751444, 0.32797334834483244, 0.23260118578615824, 0.3660900937714949]
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
probs:  [0.07345777940926417, 0.32740532163015534, 0.23243474709859469, 0.36670215186198574]
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[1.056]
 [1.056]
 [1.056]
 [1.056]
 [1.056]
 [1.056]
 [1.056]] [[0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
probs:  [0.07337724103255215, 0.32772130589927745, 0.23260244096427238, 0.36629901210389804]
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.149]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.133]] [[2.308]
 [3.117]
 [2.308]
 [2.308]
 [2.308]
 [2.308]
 [2.784]] [[0.152]
 [0.469]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.343]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.239]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[2.392]
 [2.392]
 [4.387]
 [2.392]
 [2.392]
 [2.392]
 [2.392]] [[-0.165]
 [-0.165]
 [ 0.618]
 [-0.165]
 [-0.165]
 [-0.165]
 [-0.165]]
siam score:  -0.7551904
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[0.511]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[0.775]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]]
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
probs:  [0.07301665949016789, 0.329479310704204, 0.23301015102131664, 0.3644938787843115]
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
probs:  [0.0722480813710744, 0.3359631741229012, 0.23113720453633702, 0.3606515399696873]
maxi score, test score, baseline:  -0.9610286681715575 -0.5366666666666667 -0.5366666666666667
first move QE:  0.3441505622128676
maxi score, test score, baseline:  -0.9611162162162161 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.9611162162162161 -0.5366666666666667 -0.5366666666666667
from probs:  [0.07207647465938195, 0.33612848362611947, 0.23200175676585635, 0.35979328494864227]
maxi score, test score, baseline:  -0.9611162162162161 -0.5366666666666667 -0.5366666666666667
probs:  [0.07199686380692595, 0.33644579503593836, 0.23216254516284007, 0.35939479599429563]
actor:  1 policy actor:  1  step number:  60 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.07243746751532203, 0.3346896452200589, 0.23127266693904833, 0.3616002203255707]
maxi score, test score, baseline:  -0.961376510067114 -0.5366666666666667 -0.5366666666666667
probs:  [0.07255884312581039, 0.33412294840119494, 0.23111108906809247, 0.3622071194049022]
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.039]
 [-0.035]
 [-0.035]
 [-0.031]
 [-0.035]
 [-0.035]] [[1.102]
 [1.492]
 [1.102]
 [1.102]
 [1.112]
 [1.102]
 [1.774]] [[-0.652]
 [-0.526]
 [-0.652]
 [-0.652]
 [-0.644]
 [-0.652]
 [-0.428]]
maxi score, test score, baseline:  -0.961376510067114 -0.5366666666666667 -0.5366666666666667
probs:  [0.07251960655952808, 0.3346206542579944, 0.23084922776721484, 0.3620105114152627]
maxi score, test score, baseline:  -0.961376510067114 -0.5366666666666667 -0.5366666666666667
probs:  [0.07268205842966882, 0.3331280535593765, 0.23136709004123768, 0.362822797969717]
Starting evaluation
maxi score, test score, baseline:  -0.961376510067114 -0.5366666666666667 -0.5366666666666667
maxi score, test score, baseline:  -0.961376510067114 -0.5366666666666667 -0.5366666666666667
probs:  [0.0727627284371992, 0.3323868610480622, 0.23162424899858872, 0.36322616151615]
using explorer policy with actor:  0
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.318]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]] [[0.831]
 [0.896]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[0.345]
 [0.318]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]]
maxi score, test score, baseline:  -0.961376510067114 -0.5366666666666667 -0.5366666666666667
probs:  [0.0727627284371992, 0.3323868610480622, 0.23162424899858872, 0.36322616151615]
actor:  1 policy actor:  1  step number:  63 total reward:  0.11999999999999955  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]] [[0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]]
line 256 mcts: sample exp_bonus 0.9261279859833277
maxi score, test score, baseline:  -0.9615481069042315 -0.5366666666666667 -0.5366666666666667
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.669]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.641]] [[2.73 ]
 [2.487]
 [2.73 ]
 [2.73 ]
 [2.73 ]
 [2.73 ]
 [2.548]] [[0.642]
 [0.669]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.641]]
line 256 mcts: sample exp_bonus 1.495065619114474
first move QE:  0.3346510196063145
maxi score, test score, baseline:  -0.9616333333333332 -0.5366666666666667 -0.5366666666666667
probs:  [0.07236253748181506, 0.3356547339931118, 0.23075800665912846, 0.36122472186594456]
actor:  0 policy actor:  1  step number:  63 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  27 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  28 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  29 total reward:  0.6133333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  29 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  29 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  30 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  30 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  30 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  30 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  32 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9046822983583155 -0.5366666666666667 -0.5366666666666667
probs:  [0.07308356280116464, 0.3327717628114848, 0.2293108829628611, 0.3648337914244894]
actor:  0 policy actor:  1  step number:  34 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9015096866096866 0.3886666666666667 0.3886666666666667
probs:  [0.07586073749055894, 0.2552326891679154, 0.29024335894237746, 0.37866321439914813]
maxi score, test score, baseline:  -0.9015096866096866 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.9015096866096866 0.3886666666666667 0.3886666666666667
first move QE:  0.33203037591476675
maxi score, test score, baseline:  -0.9015096866096866 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.9015096866096866 0.3886666666666667 0.3886666666666667
probs:  [0.07590668111551926, 0.2547810146989457, 0.290419396440189, 0.378892907745346]
actor:  1 policy actor:  1  step number:  52 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[-0.348]
 [-0.348]
 [-0.348]
 [-0.348]
 [-0.348]
 [-0.348]
 [-0.348]] [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]]
maxi score, test score, baseline:  -0.9015096866096866 0.3886666666666667 0.3886666666666667
probs:  [0.07598536050018549, 0.25504546919389864, 0.2896829080710973, 0.37928626223481854]
maxi score, test score, baseline:  -0.9015096866096866 0.3886666666666667 0.3886666666666667
probs:  [0.07598536050018549, 0.25504546919389864, 0.2896829080710973, 0.37928626223481854]
maxi score, test score, baseline:  -0.9015096866096866 0.3886666666666667 0.3886666666666667
probs:  [0.07602447722195836, 0.25517694700024585, 0.2893167510307884, 0.3794818247470074]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9015096866096866 0.3886666666666667 0.3886666666666667
probs:  [0.07596214655776629, 0.2553424475197339, 0.2895256677084378, 0.37916973821406197]
siam score:  -0.7566714
maxi score, test score, baseline:  -0.9015096866096866 0.3886666666666667 0.3886666666666667
actor:  0 policy actor:  1  step number:  59 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8989618336886993 0.3886666666666667 0.3886666666666667
probs:  [0.07596214655776629, 0.2553424475197339, 0.2895256677084378, 0.37916973821406197]
maxi score, test score, baseline:  -0.8989618336886993 0.3886666666666667 0.3886666666666667
probs:  [0.07583779088007772, 0.255672636994297, 0.2899424765996358, 0.3785470955259895]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.459]
 [0.439]
 [0.439]
 [0.469]
 [0.425]
 [0.446]] [[1.156]
 [1.403]
 [1.309]
 [0.969]
 [1.304]
 [1.102]
 [1.189]] [[0.457]
 [0.459]
 [0.439]
 [0.439]
 [0.469]
 [0.425]
 [0.446]]
Printing some Q and Qe and total Qs values:  [[-0.161]
 [-0.16 ]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]] [[-0.7  ]
 [ 0.809]
 [-0.7  ]
 [-0.7  ]
 [-0.7  ]
 [-0.7  ]
 [-0.7  ]] [[-0.379]
 [ 0.224]
 [-0.379]
 [-0.379]
 [-0.379]
 [-0.379]
 [-0.379]]
maxi score, test score, baseline:  -0.8989618336886993 0.3886666666666667 0.3886666666666667
probs:  [0.0758605803245636, 0.25551842415971604, 0.2899604208112052, 0.37866057470451514]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  0 policy actor:  1  step number:  51 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8961695035460993 0.3886666666666667 0.3886666666666667
probs:  [0.07590628935438941, 0.2550692275840694, 0.29013538897926777, 0.37888909408227345]
maxi score, test score, baseline:  -0.8961695035460993 0.3886666666666667 0.3886666666666667
probs:  [0.07590628935438941, 0.2550692275840694, 0.29013538897926777, 0.37888909408227345]
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.701]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.555]] [[2.986]
 [2.712]
 [2.986]
 [2.986]
 [2.986]
 [2.986]
 [3.093]] [[0.56 ]
 [0.701]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.555]]
actor:  1 policy actor:  1  step number:  68 total reward:  0.2999999999999994  reward:  1.0 rdn_beta:  0.5
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.76764905
maxi score, test score, baseline:  -0.8961695035460993 0.3886666666666667 0.3886666666666667
probs:  [0.0759907807334687, 0.25475352311583943, 0.289944192810407, 0.3793115033402848]
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.847]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]] [[2.656]
 [2.446]
 [2.656]
 [2.656]
 [2.656]
 [2.656]
 [2.656]] [[0.684]
 [0.847]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]]
maxi score, test score, baseline:  -0.8961695035460993 0.3886666666666667 0.3886666666666667
probs:  [0.0759907807334687, 0.25475352311583943, 0.289944192810407, 0.3793115033402848]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.04 ]
 [-0.036]
 [-0.069]
 [-0.07 ]
 [-0.069]
 [-0.062]] [[2.364]
 [2.672]
 [2.948]
 [2.657]
 [2.784]
 [2.657]
 [2.854]] [[0.161]
 [0.287]
 [0.385]
 [0.262]
 [0.305]
 [0.262]
 [0.335]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[-0.178]
 [-0.178]
 [-0.178]
 [-0.178]
 [-0.178]
 [-0.178]
 [-0.178]] [[1.306]
 [1.306]
 [1.306]
 [1.306]
 [1.306]
 [1.306]
 [1.306]] [[0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]]
line 256 mcts: sample exp_bonus 2.774884479962648
maxi score, test score, baseline:  -0.8968274136715997 0.3886666666666667 0.3886666666666667
probs:  [0.07592895191855993, 0.25491704456311054, 0.2901520759995952, 0.37900192751873435]
maxi score, test score, baseline:  -0.8968274136715997 0.3886666666666667 0.3886666666666667
probs:  [0.07592895191855993, 0.25491704456311054, 0.2901520759995952, 0.37900192751873435]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.504]
 [0.294]
 [0.352]
 [0.315]
 [0.299]
 [0.308]] [[2.012]
 [1.004]
 [1.99 ]
 [1.671]
 [1.736]
 [1.887]
 [1.782]] [[0.47 ]
 [0.169]
 [0.452]
 [0.351]
 [0.346]
 [0.406]
 [0.361]]
actor:  1 policy actor:  1  step number:  73 total reward:  0.15999999999999892  reward:  1.0 rdn_beta:  0.5
siam score:  -0.76018953
maxi score, test score, baseline:  -0.8968274136715997 0.3886666666666667 0.3886666666666667
probs:  [0.07603545370453278, 0.2544529573862774, 0.28997768042890254, 0.3795339084802874]
maxi score, test score, baseline:  -0.8968274136715997 0.3886666666666667 0.3886666666666667
probs:  [0.07603545370453278, 0.2544529573862774, 0.28997768042890254, 0.3795339084802874]
line 256 mcts: sample exp_bonus -0.630524667170602
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8968274136715997 0.3886666666666667 0.3886666666666667
probs:  [0.07603545370453278, 0.2544529573862774, 0.28997768042890254, 0.3795339084802874]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.589]
 [0.62 ]
 [0.582]
 [0.553]
 [0.576]
 [0.575]] [[0.823]
 [1.201]
 [0.412]
 [0.754]
 [0.743]
 [0.757]
 [1.035]] [[0.578]
 [0.589]
 [0.62 ]
 [0.582]
 [0.553]
 [0.576]
 [0.575]]
start point for exploration sampling:  11715
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.393]
 [0.358]
 [0.364]
 [0.368]
 [0.365]
 [0.366]] [[-0.939]
 [-0.281]
 [-0.938]
 [-0.875]
 [-0.819]
 [-0.9  ]
 [-0.722]] [[0.369]
 [0.393]
 [0.358]
 [0.364]
 [0.368]
 [0.365]
 [0.366]]
maxi score, test score, baseline:  -0.8968274136715997 0.3886666666666667 0.3886666666666667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8970448663853727 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.897477030812325 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.897477030812325 0.3886666666666667 0.3886666666666667
probs:  [0.0759738996233464, 0.25461552308900914, 0.2901848706392513, 0.37922570664839317]
maxi score, test score, baseline:  -0.897477030812325 0.3886666666666667 0.3886666666666667
probs:  [0.0759738996233464, 0.25461552308900914, 0.2901848706392513, 0.37922570664839317]
maxi score, test score, baseline:  -0.8979055788005579 0.3886666666666667 0.3886666666666667
probs:  [0.0759738996233464, 0.25461552308900914, 0.2901848706392513, 0.37922570664839317]
from probs:  [0.0759738996233464, 0.25461552308900914, 0.2901848706392513, 0.37922570664839317]
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  -0.8979055788005579 0.3886666666666667 0.3886666666666667
probs:  [0.07587542377482306, 0.25487560000363046, 0.2905163390224452, 0.3787326371991013]
maxi score, test score, baseline:  -0.8979055788005579 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8979055788005579 0.3886666666666667 0.3886666666666667
actor:  1 policy actor:  1  step number:  63 total reward:  0.05333333333333268  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7733354
maxi score, test score, baseline:  -0.8981185107863605 0.3886666666666667 0.3886666666666667
probs:  [0.0758752381670048, 0.2548763980964571, 0.29051664357450596, 0.37873172016203205]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8981185107863605 0.3886666666666667 0.3886666666666667
probs:  [0.0758752381670048, 0.2548763980964571, 0.29051664357450596, 0.37873172016203205]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.591]
 [0.524]
 [0.518]
 [0.529]
 [0.52 ]
 [0.526]] [[2.018]
 [1.652]
 [2.144]
 [1.296]
 [1.366]
 [1.297]
 [1.645]] [[0.54 ]
 [0.591]
 [0.524]
 [0.518]
 [0.529]
 [0.52 ]
 [0.526]]
first move QE:  0.3056379650206791
maxi score, test score, baseline:  -0.8985417186417186 0.3886666666666667 0.3886666666666667
probs:  [0.07587505453879648, 0.2548771876771829, 0.2905169448783507, 0.37873081290566996]
maxi score, test score, baseline:  -0.8985417186417186 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8985417186417186 0.3886666666666667 0.3886666666666667
probs:  [0.07592000408356997, 0.2544351585958791, 0.2906893032953783, 0.3789555340251725]
maxi score, test score, baseline:  -0.8985417186417186 0.3886666666666667 0.3886666666666667
probs:  [0.07592000408356997, 0.2544351585958791, 0.2906893032953783, 0.3789555340251725]
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]] [[ 0.773]
 [-3.315]
 [-3.315]
 [-3.315]
 [-3.315]
 [-3.315]
 [-3.315]] [[0.192]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]]
siam score:  -0.78058386
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  52 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  59 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07579765261996455, 0.25475748768904816, 0.2911019413222309, 0.3783429183687564]
maxi score, test score, baseline:  -0.8985417186417186 0.3886666666666667 0.3886666666666667
probs:  [0.0758423983717988, 0.25431694165465846, 0.2912740398799035, 0.3785666200936392]
maxi score, test score, baseline:  -0.8987520055325035 0.3886666666666667 0.3886666666666667
probs:  [0.07588690315451449, 0.2538787680910675, 0.2914452116359012, 0.3787891171185168]
maxi score, test score, baseline:  -0.8987520055325035 0.3886666666666667 0.3886666666666667
probs:  [0.07592584070516842, 0.254009213991515, 0.2910811639566457, 0.378983781346671]
maxi score, test score, baseline:  -0.8987520055325035 0.3886666666666667 0.3886666666666667
probs:  [0.07592584070516842, 0.254009213991515, 0.2910811639566457, 0.378983781346671]
maxi score, test score, baseline:  -0.8989614216701174 0.3886666666666667 0.3886666666666667
probs:  [0.07592584070516842, 0.254009213991515, 0.2910811639566457, 0.378983781346671]
maxi score, test score, baseline:  -0.8989614216701174 0.3886666666666667 0.3886666666666667
probs:  [0.07586486812170361, 0.2541693336289982, 0.2912873087766117, 0.3786784894726865]
maxi score, test score, baseline:  -0.8989614216701174 0.3886666666666667 0.3886666666666667
probs:  [0.07586486812170361, 0.2541693336289982, 0.2912873087766117, 0.3786784894726865]
Printing some Q and Qe and total Qs values:  [[-0.137]
 [-0.148]
 [-0.141]
 [-0.142]
 [-0.138]
 [-0.139]
 [-0.138]] [[0.623]
 [0.646]
 [0.616]
 [0.603]
 [0.593]
 [0.626]
 [0.61 ]] [[-0.469]
 [-0.473]
 [-0.476]
 [-0.482]
 [-0.48 ]
 [-0.471]
 [-0.475]]
maxi score, test score, baseline:  -0.8989614216701174 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8989614216701174 0.3886666666666667 0.3886666666666667
probs:  [0.07586486812170361, 0.2541693336289982, 0.2912873087766117, 0.3786784894726865]
maxi score, test score, baseline:  -0.8989614216701174 0.3886666666666667 0.3886666666666667
probs:  [0.07586486812170361, 0.2541693336289982, 0.2912873087766117, 0.3786784894726865]
maxi score, test score, baseline:  -0.8989614216701174 0.3886666666666667 0.3886666666666667
probs:  [0.07586486812170361, 0.2541693336289982, 0.2912873087766117, 0.3786784894726865]
maxi score, test score, baseline:  -0.8989614216701174 0.3886666666666667 0.3886666666666667
probs:  [0.07586486812170361, 0.2541693336289982, 0.2912873087766117, 0.3786784894726865]
maxi score, test score, baseline:  -0.8989614216701174 0.3886666666666667 0.3886666666666667
probs:  [0.0759091987630687, 0.2537329176841607, 0.2914577679294676, 0.37890011562330306]
maxi score, test score, baseline:  -0.8989614216701174 0.3886666666666667 0.3886666666666667
probs:  [0.07584834310857007, 0.2538923338690128, 0.2916639142254836, 0.37859540879693343]
actor:  1 policy actor:  1  step number:  94 total reward:  0.019999999999997797  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8991699724517906 0.3886666666666667 0.3886666666666667
probs:  [0.07578758480285015, 0.2540514950410642, 0.2918697307560849, 0.3782911894000008]
maxi score, test score, baseline:  -0.8991699724517906 0.3886666666666667 0.3886666666666667
probs:  [0.07578758480285015, 0.2540514950410642, 0.2918697307560849, 0.3782911894000008]
maxi score, test score, baseline:  -0.8991699724517906 0.3886666666666667 0.3886666666666667
probs:  [0.07578758480285015, 0.2540514950410642, 0.2918697307560849, 0.3782911894000008]
maxi score, test score, baseline:  -0.8991699724517906 0.3886666666666667 0.3886666666666667
probs:  [0.07578758480285015, 0.2540514950410642, 0.2918697307560849, 0.3782911894000008]
maxi score, test score, baseline:  -0.8991699724517906 0.3886666666666667 0.3886666666666667
probs:  [0.07578758480285015, 0.2540514950410642, 0.2918697307560849, 0.3782911894000008]
siam score:  -0.79138356
maxi score, test score, baseline:  -0.8991699724517906 0.3886666666666667 0.3886666666666667
probs:  [0.07578758480285015, 0.2540514950410642, 0.2918697307560849, 0.3782911894000008]
actor:  1 policy actor:  1  step number:  43 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]] [[1.39]
 [1.39]
 [1.39]
 [1.39]
 [1.39]
 [1.39]
 [1.39]] [[-0.202]
 [-0.202]
 [-0.202]
 [-0.202]
 [-0.202]
 [-0.202]
 [-0.202]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7908652
maxi score, test score, baseline:  -0.8993776632302406 0.3886666666666667 0.3886666666666667
probs:  [0.07491351029097153, 0.26266653351032165, 0.28849859513178044, 0.37392136106692636]
maxi score, test score, baseline:  -0.899584499314129 0.3886666666666667 0.3886666666666667
probs:  [0.07491351029097153, 0.26266653351032165, 0.28849859513178044, 0.37392136106692636]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.8997904859685147 0.3886666666666667 0.3886666666666667
probs:  [0.07491351029097153, 0.26266653351032165, 0.28849859513178044, 0.37392136106692636]
maxi score, test score, baseline:  -0.8997904859685147 0.3886666666666667 0.3886666666666667
probs:  [0.07491351029097153, 0.26266653351032165, 0.28849859513178044, 0.37392136106692636]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8997904859685147 0.3886666666666667 0.3886666666666667
probs:  [0.07485248681657185, 0.2628340900919621, 0.28869760097581787, 0.37361582211564814]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.375]] [[1.425]
 [1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.27 ]
 [1.403]] [[0.135]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.08 ]
 [0.131]]
actor:  1 policy actor:  1  step number:  73 total reward:  0.039999999999998814  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.07489049710374662, 0.26296774948568347, 0.2883359037592911, 0.3738058496512787]
maxi score, test score, baseline:  -0.8999956284153006 0.3886666666666667 0.3886666666666667
probs:  [0.07489032890815221, 0.26296842046838087, 0.2883362311603655, 0.37380501946310135]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.35 ]
 [0.059]
 [0.368]
 [0.368]
 [0.368]
 [0.36 ]] [[0.417]
 [0.699]
 [0.045]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.328]] [[0.356]
 [0.35 ]
 [0.059]
 [0.368]
 [0.368]
 [0.368]
 [0.36 ]]
siam score:  -0.7896199
actor:  1 policy actor:  1  step number:  44 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.551]
 [0.553]
 [0.562]
 [0.556]
 [0.678]
 [0.63 ]] [[-0.828]
 [-0.234]
 [-0.422]
 [-0.581]
 [-0.609]
 [-0.375]
 [-0.274]] [[0.578]
 [0.551]
 [0.553]
 [0.562]
 [0.556]
 [0.678]
 [0.63 ]]
start point for exploration sampling:  11715
using another actor
maxi score, test score, baseline:  -0.9004034013605442 0.3886666666666667 0.3886666666666667
probs:  [0.07832675550053125, 0.2541786565233633, 0.2764844732644668, 0.39101011471163866]
actor:  1 policy actor:  1  step number:  48 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9004034013605442 0.3886666666666667 0.3886666666666667
probs:  [0.07831225379692291, 0.2539158035949315, 0.27683476084039443, 0.39093718176775116]
UNIT TEST: sample policy line 217 mcts : [0.02  0.694 0.102 0.02  0.02  0.082 0.061]
maxi score, test score, baseline:  -0.9004034013605442 0.3886666666666667 0.3886666666666667
probs:  [0.07831225379692291, 0.2539158035949315, 0.27683476084039443, 0.39093718176775116]
actor:  0 policy actor:  1  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8978090291921249 0.3886666666666667 0.3886666666666667
probs:  [0.0783567321948875, 0.25349149919286906, 0.27699220876627184, 0.3911595598459717]
maxi score, test score, baseline:  -0.8978090291921249 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8978090291921249 0.3886666666666667 0.3886666666666667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  70 total reward:  0.07333333333333292  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07833428258215135, 0.2537729178373149, 0.2768459097816353, 0.3910468897988984]
maxi score, test score, baseline:  -0.8978090291921249 0.3886666666666667 0.3886666666666667
probs:  [0.07827530684481576, 0.2539354819838448, 0.27703760997307536, 0.39075160119826396]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [ 0.084]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.003]] [[1.854]
 [2.467]
 [1.854]
 [1.854]
 [1.854]
 [1.854]
 [2.514]] [[-0.658]
 [-0.341]
 [-0.658]
 [-0.658]
 [-0.658]
 [-0.658]
 [-0.413]]
maxi score, test score, baseline:  -0.8984290148448043 0.3886666666666667 0.3886666666666667
probs:  [0.07829734591187966, 0.2537930021347359, 0.27704829136145526, 0.3908613605919292]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8984290148448043 0.3886666666666667 0.3886666666666667
actor:  1 policy actor:  1  step number:  44 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8984290148448043 0.3886666666666667 0.3886666666666667
probs:  [0.07832671674016684, 0.2531134039504921, 0.27755210321571433, 0.39100777609362675]
from probs:  [0.07836325527085372, 0.2532316348526764, 0.2772146533596094, 0.3911904565168605]
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.334]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]] [[1.917]
 [2.126]
 [1.917]
 [1.917]
 [1.917]
 [1.917]
 [1.917]] [[-0.115]
 [ 0.056]
 [-0.115]
 [-0.115]
 [-0.115]
 [-0.115]
 [-0.115]]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.661]
 [0.631]
 [0.454]
 [0.454]
 [0.285]
 [0.308]] [[1.395]
 [1.252]
 [1.569]
 [1.496]
 [1.496]
 [1.425]
 [1.453]] [[0.022]
 [0.372]
 [0.448]
 [0.246]
 [0.246]
 [0.054]
 [0.086]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.333
from probs:  [0.0783996573910458, 0.25334942435955654, 0.27687846331611804, 0.39137245493327966]
maxi score, test score, baseline:  -0.8986340067340067 0.3886666666666667 0.3886666666666667
probs:  [0.078443456137113, 0.25293175321736794, 0.27703335664224804, 0.39159143400327107]
maxi score, test score, baseline:  -0.8986340067340067 0.3886666666666667 0.3886666666666667
probs:  [0.0784282783109676, 0.2526773547731241, 0.27737923670326176, 0.3915151302126465]
Printing some Q and Qe and total Qs values:  [[-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]] [[1.755]
 [1.755]
 [1.755]
 [1.755]
 [1.755]
 [1.755]
 [1.755]] [[-0.431]
 [-0.431]
 [-0.431]
 [-0.431]
 [-0.431]
 [-0.431]
 [-0.431]]
maxi score, test score, baseline:  -0.8986340067340067 0.3886666666666667 0.3886666666666667
probs:  [0.0784282783109676, 0.2526773547731241, 0.27737923670326176, 0.3915151302126465]
maxi score, test score, baseline:  -0.8986340067340067 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8986340067340067 0.3886666666666667 0.3886666666666667
probs:  [0.0784282783109676, 0.2526773547731241, 0.27737923670326176, 0.3915151302126465]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.599]
 [0.596]
 [0.599]
 [0.6  ]
 [0.599]
 [0.603]] [[3.708]
 [3.565]
 [2.969]
 [3.047]
 [3.203]
 [3.02 ]
 [3.131]] [[0.592]
 [0.599]
 [0.596]
 [0.599]
 [0.6  ]
 [0.599]
 [0.603]]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.3319717212985265
actor:  0 policy actor:  1  step number:  55 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.667
from probs:  [0.0784282783109676, 0.2526773547731241, 0.27737923670326176, 0.3915151302126465]
maxi score, test score, baseline:  -0.8967540829986613 0.3886666666666667 0.3886666666666667
probs:  [0.0784282783109676, 0.2526773547731241, 0.27737923670326176, 0.3915151302126465]
from probs:  [0.07846464381180218, 0.25279467054255284, 0.2770437402961068, 0.3916969453495382]
maxi score, test score, baseline:  -0.8967540829986613 0.3886666666666667 0.3886666666666667
probs:  [0.07850804712098822, 0.2523808063485834, 0.27719719939567883, 0.3919139471347496]
maxi score, test score, baseline:  -0.8967540829986613 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8967540829986613 0.3886666666666667 0.3886666666666667
probs:  [0.07850804712098822, 0.2523808063485834, 0.27719719939567883, 0.3919139471347496]
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.095]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]] [[-0.78 ]
 [ 0.256]
 [-0.78 ]
 [-0.78 ]
 [-0.78 ]
 [-0.78 ]
 [-0.78 ]] [[0.125]
 [0.095]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]]
from probs:  [0.07850804712098822, 0.2523808063485834, 0.27719719939567883, 0.3919139471347496]
using explorer policy with actor:  0
siam score:  -0.79512674
line 256 mcts: sample exp_bonus 0.46857343207475444
maxi score, test score, baseline:  -0.896960788243153 0.3886666666666667 0.3886666666666667
probs:  [0.07850804712098822, 0.2523808063485834, 0.27719719939567883, 0.3919139471347496]
actor:  0 policy actor:  1  step number:  48 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.8965133333333334 0.3886666666666667 0.3886666666666667
probs:  [0.07850804712098822, 0.2523808063485834, 0.27719719939567883, 0.3919139471347496]
maxi score, test score, baseline:  -0.8965133333333334 0.3886666666666667 0.3886666666666667
probs:  [0.07850804712098822, 0.2523808063485834, 0.27719719939567883, 0.3919139471347496]
maxi score, test score, baseline:  -0.8965133333333334 0.3886666666666667 0.3886666666666667
probs:  [0.07850804712098822, 0.2523808063485834, 0.27719719939567883, 0.3919139471347496]
actor:  0 policy actor:  1  step number:  43 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07850804712098822, 0.2523808063485834, 0.27719719939567883, 0.3919139471347496]
siam score:  -0.7964195
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07850804712098822, 0.2523808063485834, 0.27719719939567883, 0.3919139471347496]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07850804712098822, 0.2523808063485834, 0.27719719939567883, 0.3919139471347496]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07850804712098822, 0.2523808063485834, 0.27719719939567883, 0.3919139471347496]
actor:  1 policy actor:  1  step number:  67 total reward:  0.14666666666666583  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
siam score:  -0.79619795
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07854411536022318, 0.2524983144674275, 0.2768632844069092, 0.39209428576544014]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07854411536022318, 0.2524983144674275, 0.2768632844069092, 0.39209428576544014]
actor:  1 policy actor:  1  step number:  45 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.52 ]
 [0.507]
 [0.496]
 [0.503]
 [0.507]
 [0.5  ]] [[-0.732]
 [-0.435]
 [-0.919]
 [-0.992]
 [-1.039]
 [-0.93 ]
 [-0.908]] [[0.496]
 [0.52 ]
 [0.507]
 [0.496]
 [0.503]
 [0.507]
 [0.5  ]]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07815678847366486, 0.2512515217813151, 0.2804339067370899, 0.3901577830079301]
actor:  1 policy actor:  1  step number:  76 total reward:  0.05999999999999883  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07819957885678556, 0.250841045285309, 0.28058765548134995, 0.39037172037655543]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.0782362063942419, 0.2509586908703128, 0.280250257123471, 0.39055484561197423]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.0782362063942419, 0.2509586908703128, 0.280250257123471, 0.39055484561197423]
siam score:  -0.8026655
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07827881415700952, 0.25055022211329186, 0.2804030937887811, 0.3907678699409176]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  66 total reward:  0.23333333333333217  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07832120087291226, 0.25014387247101094, 0.2805551375453932, 0.39097978911068354]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.0783633682576692, 0.2497396254953501, 0.2807063945476968, 0.391190611699284]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07839998014128972, 0.24985645896396902, 0.28036990222389585, 0.39137365867084534]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07839978215223461, 0.24985723775017918, 0.28037030172291866, 0.39137267837466755]
using explorer policy with actor:  1
first move QE:  0.2766180874646281
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07839978215223461, 0.24985723775017918, 0.28037030172291866, 0.39137267837466755]
actor:  1 policy actor:  1  step number:  64 total reward:  0.2733333333333322  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07850881346721338, 0.2502051733770177, 0.27936821525903927, 0.3919177978967296]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07855091900776279, 0.24980251868214337, 0.27951825101962097, 0.3921283112904729]
Printing some Q and Qe and total Qs values:  [[-0.094]
 [-0.08 ]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]] [[1.414]
 [2.251]
 [1.414]
 [1.414]
 [1.414]
 [1.414]
 [1.414]] [[-0.353]
 [ 0.039]
 [-0.353]
 [-0.353]
 [-0.353]
 [-0.353]
 [-0.353]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07863448174194702, 0.24900340942486324, 0.27981601226947117, 0.39254609656371847]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07867067623093382, 0.24911817242921672, 0.2794840946314091, 0.3927270567084403]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4744819290311075
siam score:  -0.798441
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07867067623093382, 0.24911817242921672, 0.2794840946314091, 0.3927270567084403]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07867067623093382, 0.24911817242921672, 0.2794840946314091, 0.3927270567084403]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07867067623093382, 0.24911817242921672, 0.2794840946314091, 0.3927270567084403]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07867067623093382, 0.24911817242921672, 0.2794840946314091, 0.3927270567084403]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07867067623093382, 0.24911817242921672, 0.2794840946314091, 0.3927270567084403]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07871217500319097, 0.2487215638348146, 0.2796317246950701, 0.39293453646692433]
actor:  1 policy actor:  1  step number:  67 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7993906
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07875346188193372, 0.24832698033138106, 0.27977860095683965, 0.39314095682984557]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07875346188193372, 0.24832698033138106, 0.27977860095683965, 0.39314095682984557]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]] [[1.427]
 [1.427]
 [1.427]
 [1.427]
 [1.427]
 [1.427]
 [1.427]] [[0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]] [[1.974]
 [1.974]
 [1.974]
 [1.974]
 [1.974]
 [1.974]
 [1.974]] [[-0.377]
 [-0.377]
 [-0.377]
 [-0.377]
 [-0.377]
 [-0.377]
 [-0.377]]
using explorer policy with actor:  1
siam score:  -0.8005169
actor:  1 policy actor:  1  step number:  62 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  58 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.921]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.865]] [[2.927]
 [2.816]
 [2.927]
 [2.927]
 [2.927]
 [2.927]
 [2.574]] [[0.833]
 [0.921]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.865]]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07891231898033346, 0.24726896712766394, 0.27988352555561485, 0.39393518833638774]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07895260741678967, 0.24688302418741878, 0.2800277420193074, 0.39413662637648417]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07895260741678967, 0.24688302418741878, 0.2800277420193074, 0.39413662637648417]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]] [[-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]] [[-0.451]
 [-0.451]
 [-0.451]
 [-0.451]
 [-0.451]
 [-0.451]
 [-0.451]]
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.535]
 [0.155]
 [0.154]
 [0.151]
 [0.142]
 [0.162]] [[3.05 ]
 [1.74 ]
 [2.701]
 [2.623]
 [3.123]
 [2.429]
 [2.643]] [[0.608]
 [0.221]
 [0.431]
 [0.388]
 [0.654]
 [0.275]
 [0.406]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07942289636268782, 0.245363926096032, 0.2787220708705771, 0.39649110667070303]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
Printing some Q and Qe and total Qs values:  [[0.993]
 [0.993]
 [0.993]
 [0.993]
 [0.993]
 [0.993]
 [0.993]] [[1.69]
 [1.69]
 [1.69]
 [1.69]
 [1.69]
 [1.69]
 [1.69]] [[0.993]
 [0.993]
 [0.993]
 [0.993]
 [0.993]
 [0.993]
 [0.993]]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
probs:  [0.07942289636268782, 0.245363926096032, 0.2787220708705771, 0.39649110667070303]
maxi score, test score, baseline:  -0.89366 0.3886666666666667 0.3886666666666667
actor:  0 policy actor:  1  step number:  44 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  74 total reward:  0.1133333333333324  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  0.667
first move QE:  0.26176433647528097
maxi score, test score, baseline:  -0.8909266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.07976686787022336, 0.2445937657501202, 0.2774267863013609, 0.39821258007829546]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8909266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.07980657946390339, 0.24421720619850248, 0.27756508732351026, 0.3984111270140837]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.8909266666666666 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8909266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.07980657946390339, 0.24421720619850248, 0.27756508732351026, 0.3984111270140837]
maxi score, test score, baseline:  -0.8909266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.07980657946390339, 0.24421720619850248, 0.27756508732351026, 0.3984111270140837]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[-0.128]
 [-0.135]
 [-0.241]
 [-0.144]
 [-0.131]
 [-0.146]
 [-0.153]] [[-0.272]
 [ 0.562]
 [ 0.46 ]
 [-0.548]
 [-0.905]
 [ 0.022]
 [ 0.743]] [[-0.352]
 [-0.221]
 [-0.344]
 [-0.415]
 [-0.461]
 [-0.321]
 [-0.209]]
actor:  0 policy actor:  0  step number:  53 total reward:  0.29333333333333267  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.88834 0.3886666666666667 0.3886666666666667
probs:  [0.07984588126677912, 0.24384336799993608, 0.2777031163284706, 0.39860763440481417]
maxi score, test score, baseline:  -0.88834 0.3886666666666667 0.3886666666666667
probs:  [0.07988169944505688, 0.24395289242869952, 0.2773786927768125, 0.39878671534943105]
line 256 mcts: sample exp_bonus 0.37134462181125466
maxi score, test score, baseline:  -0.88834 0.3886666666666667 0.3886666666666667
probs:  [0.07995656253678851, 0.24368982969082933, 0.27719258854152234, 0.39916101923085984]
Printing some Q and Qe and total Qs values:  [[-0.138]
 [-0.191]
 [-0.17 ]
 [-0.161]
 [-0.138]
 [-0.158]
 [-0.143]] [[ 1.535]
 [ 1.243]
 [ 1.06 ]
 [-0.563]
 [ 1.535]
 [ 0.079]
 [ 1.292]] [[ 0.633]
 [ 0.446]
 [ 0.368]
 [-0.448]
 [ 0.633]
 [-0.121]
 [ 0.507]]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.88834 0.3886666666666667 0.3886666666666667
probs:  [0.07737368194301884, 0.2353332277714365, 0.30104575705114167, 0.38624733323440297]
maxi score, test score, baseline:  -0.88834 0.3886666666666667 0.3886666666666667
probs:  [0.07737368194301884, 0.2353332277714365, 0.30104575705114167, 0.38624733323440297]
siam score:  -0.80500907
maxi score, test score, baseline:  -0.88834 0.3886666666666667 0.3886666666666667
probs:  [0.07737368194301884, 0.2353332277714365, 0.30104575705114167, 0.38624733323440297]
maxi score, test score, baseline:  -0.88834 0.3886666666666667 0.3886666666666667
probs:  [0.07737368194301884, 0.2353332277714365, 0.30104575705114167, 0.38624733323440297]
maxi score, test score, baseline:  -0.88834 0.3886666666666667 0.3886666666666667
probs:  [0.07737368194301884, 0.2353332277714365, 0.30104575705114167, 0.38624733323440297]
actor:  0 policy actor:  1  step number:  56 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8858466666666667 0.3886666666666667 0.3886666666666667
probs:  [0.07737368194301884, 0.2353332277714365, 0.30104575705114167, 0.38624733323440297]
maxi score, test score, baseline:  -0.8858466666666667 0.3886666666666667 0.3886666666666667
probs:  [0.07737368194301884, 0.2353332277714365, 0.30104575705114167, 0.38624733323440297]
maxi score, test score, baseline:  -0.8858466666666667 0.3886666666666667 0.3886666666666667
probs:  [0.07737368194301884, 0.2353332277714365, 0.30104575705114167, 0.38624733323440297]
UNIT TEST: sample policy line 217 mcts : [0.02  0.673 0.02  0.102 0.143 0.02  0.02 ]
from probs:  [0.07741263805364672, 0.23545186775908178, 0.3006933912134137, 0.3864421029738579]
Printing some Q and Qe and total Qs values:  [[-0.086]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]] [[1.287]
 [1.287]
 [1.287]
 [1.287]
 [1.287]
 [1.287]
 [1.287]] [[0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]]
maxi score, test score, baseline:  -0.8858466666666667 0.3886666666666667 0.3886666666666667
probs:  [0.07741246608930709, 0.2354527546709348, 0.3006935272123255, 0.3864412520274326]
siam score:  -0.81346834
maxi score, test score, baseline:  -0.8858466666666667 0.3886666666666667 0.3886666666666667
probs:  [0.07741246608930709, 0.2354527546709348, 0.3006935272123255, 0.3864412520274326]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07741246608930709, 0.2354527546709348, 0.3006935272123255, 0.3864412520274326]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.4343036168515682
actor:  0 policy actor:  0  step number:  59 total reward:  0.13333333333333275  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.643]
 [0.606]
 [0.61 ]
 [0.63 ]
 [0.58 ]
 [0.627]] [[1.568]
 [1.658]
 [1.294]
 [1.258]
 [1.52 ]
 [1.141]
 [1.856]] [[0.613]
 [0.643]
 [0.606]
 [0.61 ]
 [0.63 ]
 [0.58 ]
 [0.627]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.88358 0.3886666666666667 0.3886666666666667
probs:  [0.07808521629372145, 0.23389303063173664, 0.2982122132061689, 0.38980953986837313]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.411]
 [0.381]
 [0.371]
 [0.393]
 [0.371]
 [0.394]] [[ 0.057]
 [ 0.143]
 [-0.224]
 [-0.181]
 [-0.58 ]
 [-0.107]
 [-0.254]] [[0.373]
 [0.411]
 [0.381]
 [0.371]
 [0.393]
 [0.371]
 [0.394]]
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.192]
 [0.122]
 [0.114]
 [0.114]
 [0.114]
 [0.099]] [[1.91 ]
 [1.839]
 [1.739]
 [1.865]
 [1.865]
 [1.865]
 [1.7  ]] [[0.253]
 [0.275]
 [0.152]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.108]]
line 256 mcts: sample exp_bonus 1.6847546085604528
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.784]
 [0.773]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.775]] [[1.942]
 [1.931]
 [1.53 ]
 [1.942]
 [1.942]
 [1.942]
 [1.619]] [[0.78 ]
 [0.784]
 [0.773]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.775]]
maxi score, test score, baseline:  -0.88358 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.88358 0.3886666666666667 0.3886666666666667
probs:  [0.07808521629372145, 0.23389303063173664, 0.2982122132061689, 0.38980953986837313]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  56 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8810333333333333 0.3886666666666667 0.3886666666666667
probs:  [0.07812152296020923, 0.23353634425467518, 0.29835106745792284, 0.3899910653271928]
siam score:  -0.8053426
maxi score, test score, baseline:  -0.8810333333333333 0.3886666666666667 0.3886666666666667
probs:  [0.07812152296020923, 0.23353634425467518, 0.29835106745792284, 0.3899910653271928]
maxi score, test score, baseline:  -0.8810333333333333 0.3886666666666667 0.3886666666666667
actor:  1 policy actor:  1  step number:  33 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8810333333333333 0.3886666666666667 0.3886666666666667
probs:  [0.08938727907259171, 0.20707092303368813, 0.25714630061829863, 0.44639549727542155]
maxi score, test score, baseline:  -0.8810333333333333 0.3886666666666667 0.3886666666666667
probs:  [0.08941839826367232, 0.20679456665415943, 0.2572359213665676, 0.44655111371560063]
Printing some Q and Qe and total Qs values:  [[-0.17 ]
 [-0.173]
 [-0.162]
 [-0.157]
 [-0.162]
 [-0.162]
 [-0.163]] [[ 0.348]
 [ 0.471]
 [ 0.   ]
 [-0.471]
 [ 0.   ]
 [ 0.   ]
 [ 0.669]] [[-0.049]
 [ 0.009]
 [-0.215]
 [-0.445]
 [-0.215]
 [-0.215]
 [ 0.119]]
maxi score, test score, baseline:  -0.8810333333333333 0.3886666666666667 0.3886666666666667
probs:  [0.08945210488680157, 0.20687259299583263, 0.2569556330981186, 0.4467196690192473]
maxi score, test score, baseline:  -0.8810333333333333 0.3886666666666667 0.3886666666666667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8810333333333334 0.3886666666666667 0.3886666666666667
probs:  [0.08948568599059444, 0.20695032877696548, 0.25667638858885344, 0.4468875966435866]
maxi score, test score, baseline:  -0.8810333333333334 0.3886666666666667 0.3886666666666667
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]] [[0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]] [[0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5066666666666669  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8810333333333334 0.3886666666666667 0.3886666666666667
using explorer policy with actor:  1
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.8810333333333334 0.3886666666666667 0.3886666666666667
probs:  [0.08786496899390371, 0.20285814939189936, 0.27049393682063405, 0.4387829447935629]
using explorer policy with actor:  1
siam score:  -0.8063621
line 256 mcts: sample exp_bonus 1.4887008666992188
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8810333333333334 0.3886666666666667 0.3886666666666667
probs:  [0.0879301288101649, 0.2026714620042113, 0.27028961645134, 0.4391087927342839]
maxi score, test score, baseline:  -0.8810333333333334 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8810333333333334 0.3886666666666667 0.3886666666666667
from probs:  [0.0879301288101649, 0.2026714620042113, 0.27028961645134, 0.4391087927342839]
maxi score, test score, baseline:  -0.8810333333333333 0.3886666666666667 0.3886666666666667
probs:  [0.0879301288101649, 0.2026714620042113, 0.27028961645134, 0.4391087927342839]
maxi score, test score, baseline:  -0.8810333333333333 0.3886666666666667 0.3886666666666667
probs:  [0.08796565956672767, 0.20275343655454872, 0.26999443397044115, 0.4392864699082825]
maxi score, test score, baseline:  -0.8810333333333333 0.3886666666666667 0.3886666666666667
probs:  [0.08796565956672767, 0.20275343655454872, 0.26999443397044115, 0.4392864699082825]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.716]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]] [[1.729]
 [1.819]
 [1.729]
 [1.729]
 [1.729]
 [1.729]
 [1.729]] [[0.603]
 [0.716]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]]
maxi score, test score, baseline:  -0.8810333333333333 0.3886666666666667 0.3886666666666667
actor:  0 policy actor:  0  step number:  42 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.667
siam score:  -0.81039727
using explorer policy with actor:  1
from probs:  [0.08806599252865265, 0.2026476904005811, 0.2694981163000338, 0.43978820077073244]
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08809549506186382, 0.20238026896693503, 0.269588503111826, 0.4399357328593752]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.107]
 [0.054]
 [0.046]
 [0.059]
 [0.059]
 [0.054]] [[-2.377]
 [-1.244]
 [-2.329]
 [-1.788]
 [-2.209]
 [-2.022]
 [-1.454]] [[0.089]
 [0.107]
 [0.054]
 [0.046]
 [0.059]
 [0.059]
 [0.054]]
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08809549506186382, 0.20238026896693503, 0.269588503111826, 0.4399357328593752]
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08809549506186382, 0.20238026896693503, 0.269588503111826, 0.4399357328593752]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08809549506186382, 0.20238026896693503, 0.269588503111826, 0.4399357328593752]
Printing some Q and Qe and total Qs values:  [[-0.122]
 [-0.093]
 [-0.083]
 [-0.122]
 [-0.122]
 [-0.09 ]
 [-0.068]] [[0.54 ]
 [1.28 ]
 [1.256]
 [0.54 ]
 [0.54 ]
 [0.734]
 [2.351]] [[-0.589]
 [-0.33 ]
 [-0.328]
 [-0.589]
 [-0.589]
 [-0.498]
 [ 0.03 ]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.0881248457036122, 0.2021142243314253, 0.2696784245742543, 0.4400825053907082]
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08815404562388776, 0.20184954588884726, 0.2697678842718132, 0.44022852421545183]
siam score:  -0.81372434
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08815404562388776, 0.20184954588884726, 0.2697678842718132, 0.44022852421545183]
line 256 mcts: sample exp_bonus 0.9499373540282249
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08822440352840195, 0.20201080153469522, 0.2691844348797216, 0.44058036005718126]
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08828851334309139, 0.20182743768682418, 0.2689830976336416, 0.4409009513364429]
Printing some Q and Qe and total Qs values:  [[-0.129]
 [-0.138]
 [-0.205]
 [-0.146]
 [-0.137]
 [-0.175]
 [-0.145]] [[0.88 ]
 [0.866]
 [1.185]
 [0.363]
 [0.097]
 [0.693]
 [1.242]] [[-0.104]
 [-0.123]
 [ 0.023]
 [-0.466]
 [-0.635]
 [-0.275]
 [ 0.122]]
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08828851334309139, 0.20182743768682418, 0.2689830976336416, 0.4409009513364429]
siam score:  -0.8173381
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08828851334309139, 0.20182743768682418, 0.2689830976336416, 0.4409009513364429]
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08828851334309139, 0.20182743768682418, 0.2689830976336416, 0.4409009513364429]
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08828851334309139, 0.20182743768682418, 0.2689830976336416, 0.4409009513364429]
actor:  1 policy actor:  1  step number:  59 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.8781666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08828851334309139, 0.20182743768682418, 0.2689830976336416, 0.4409009513364429]
actor:  0 policy actor:  1  step number:  67 total reward:  0.09333333333333238  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
probs:  [0.08828851334309139, 0.20182743768682418, 0.2689830976336416, 0.4409009513364429]
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
probs:  [0.08828851334309139, 0.20182743768682418, 0.2689830976336416, 0.4409009513364429]
actor:  1 policy actor:  1  step number:  89 total reward:  0.09333333333333105  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  47 total reward:  0.48  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.114]
 [0.145]
 [0.036]
 [0.038]
 [0.014]
 [0.007]] [[1.582]
 [1.583]
 [1.614]
 [1.515]
 [1.595]
 [1.427]
 [1.707]] [[-0.11 ]
 [-0.05 ]
 [-0.012]
 [-0.141]
 [-0.101]
 [-0.201]
 [-0.068]]
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
probs:  [0.0873135491998729, 0.19959653827842325, 0.2770644237555833, 0.43602548876612046]
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
probs:  [0.0873133436003512, 0.19959747278957507, 0.2770647171119818, 0.43602446649809184]
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
probs:  [0.0873133436003512, 0.19959747278957507, 0.2770647171119818, 0.43602446649809184]
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
probs:  [0.0873133436003512, 0.19959747278957507, 0.2770647171119818, 0.43602446649809184]
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
probs:  [0.0873133436003512, 0.19959747278957507, 0.2770647171119818, 0.43602446649809184]
actor:  1 policy actor:  1  step number:  52 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
probs:  [0.08753769511724563, 0.19930743977838025, 0.2760074071514264, 0.43714745795294774]
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
Printing some Q and Qe and total Qs values:  [[-0.1  ]
 [-0.089]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]] [[0.17]
 [0.8 ]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]] [[-0.229]
 [ 0.017]
 [-0.229]
 [-0.229]
 [-0.229]
 [-0.229]
 [-0.229]]
siam score:  -0.82060605
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  46 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.54 ]
 [0.206]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.394]] [[1.703]
 [1.77 ]
 [1.408]
 [1.869]
 [1.869]
 [1.869]
 [1.761]] [[ 0.018]
 [ 0.289]
 [-0.166]
 [ 0.133]
 [ 0.133]
 [ 0.133]
 [ 0.14 ]]
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
probs:  [0.08772402871422891, 0.1989383980909367, 0.27525724921177624, 0.4380803239830581]
line 256 mcts: sample exp_bonus 0.47452089604539927
actor:  1 policy actor:  1  step number:  71 total reward:  0.1466666666666654  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.098]
 [0.03 ]
 [0.031]
 [0.027]
 [0.035]
 [0.033]] [[-3.266]
 [-0.959]
 [-2.766]
 [-3.09 ]
 [-2.844]
 [-2.665]
 [-2.318]] [[0.05 ]
 [0.098]
 [0.03 ]
 [0.031]
 [0.027]
 [0.035]
 [0.033]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
probs:  [0.08775222432184229, 0.1986806308586568, 0.27534582353835957, 0.4382213212811413]
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
probs:  [0.08775222432184229, 0.1986806308586568, 0.27534582353835957, 0.4382213212811413]
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
probs:  [0.08775222432184229, 0.1986806308586568, 0.27534582353835957, 0.4382213212811413]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.795]
 [0.724]
 [0.689]
 [0.69 ]
 [0.693]
 [0.698]] [[1.26 ]
 [1.52 ]
 [1.595]
 [1.613]
 [1.802]
 [1.893]
 [1.826]] [[0.769]
 [0.795]
 [0.724]
 [0.689]
 [0.69 ]
 [0.693]
 [0.698]]
using another actor
from probs:  [0.08775222432184229, 0.1986806308586568, 0.27534582353835957, 0.4382213212811413]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.8759800000000001 0.3886666666666667 0.3886666666666667
probs:  [0.08775222432184229, 0.1986806308586568, 0.27534582353835957, 0.4382213212811413]
actor:  0 policy actor:  1  step number:  41 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
probs:  [0.08775222432184229, 0.1986806308586568, 0.27534582353835957, 0.4382213212811413]
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
probs:  [0.08775222432184229, 0.1986806308586568, 0.27534582353835957, 0.4382213212811413]
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
probs:  [0.08775222432184229, 0.1986806308586568, 0.27534582353835957, 0.4382213212811413]
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
probs:  [0.08775222432184229, 0.1986806308586568, 0.27534582353835957, 0.4382213212811413]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
probs:  [0.08666936160799289, 0.20857933619035293, 0.2719450285087712, 0.432806273692883]
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
probs:  [0.08666936160799289, 0.20857933619035293, 0.2719450285087712, 0.432806273692883]
Printing some Q and Qe and total Qs values:  [[-0.094]
 [-0.038]
 [-0.088]
 [-0.047]
 [-0.044]
 [-0.099]
 [-0.089]] [[0.345]
 [1.261]
 [0.181]
 [0.745]
 [0.811]
 [0.263]
 [0.599]] [[-0.437]
 [-0.045]
 [-0.495]
 [-0.248]
 [-0.222]
 [-0.472]
 [-0.336]]
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
probs:  [0.08666936160799289, 0.20857933619035293, 0.2719450285087712, 0.432806273692883]
Printing some Q and Qe and total Qs values:  [[-0.138]
 [-0.132]
 [-0.138]
 [-0.138]
 [-0.138]
 [-0.138]
 [-0.138]] [[-2.897]
 [ 0.097]
 [-2.897]
 [-2.897]
 [-2.897]
 [-2.897]
 [-2.897]] [[0.088]
 [0.729]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]]
Printing some Q and Qe and total Qs values:  [[-0.128]
 [-0.128]
 [-0.13 ]
 [-0.128]
 [-0.128]
 [-0.118]
 [-0.129]] [[0.764]
 [0.764]
 [0.484]
 [0.764]
 [0.764]
 [0.584]
 [0.212]] [[0.873]
 [0.873]
 [0.813]
 [0.873]
 [0.873]
 [0.838]
 [0.755]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
probs:  [0.08635178568060534, 0.2078142808517995, 0.2746157563640319, 0.4312181771035632]
siam score:  -0.8341494
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
probs:  [0.08638702205801145, 0.2078991669414588, 0.27431942793816166, 0.4313943830623681]
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
probs:  [0.08638702205801145, 0.2078991669414588, 0.27431942793816166, 0.4313943830623681]
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
probs:  [0.08638702205801145, 0.2078991669414588, 0.27431942793816166, 0.4313943830623681]
actor:  1 policy actor:  1  step number:  57 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
probs:  [0.08638702205801145, 0.2078991669414588, 0.27431942793816166, 0.4313943830623681]
maxi score, test score, baseline:  -0.8729666666666668 0.3886666666666667 0.3886666666666667
probs:  [0.08638702205801145, 0.2078991669414588, 0.27431942793816166, 0.4313943830623681]
actor:  0 policy actor:  1  step number:  58 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.8706333333333334 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8706333333333334 0.3886666666666667 0.3886666666666667
probs:  [0.08704720269403518, 0.20510700739027476, 0.2731675936447632, 0.4346781962709269]
maxi score, test score, baseline:  -0.8706333333333334 0.3886666666666667 0.3886666666666667
probs:  [0.08704720269403518, 0.20510700739027476, 0.2731675936447632, 0.4346781962709269]
maxi score, test score, baseline:  -0.8706333333333334 0.3886666666666667 0.3886666666666667
probs:  [0.08704720269403518, 0.20510700739027476, 0.2731675936447632, 0.4346781962709269]
start point for exploration sampling:  11715
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.399]
 [0.5  ]
 [0.5  ]
 [0.239]
 [0.5  ]
 [0.165]] [[-0.233]
 [ 0.336]
 [ 0.291]
 [ 0.291]
 [-0.074]
 [ 0.291]
 [-0.04 ]] [[-0.048]
 [ 0.034]
 [ 0.119]
 [ 0.119]
 [-0.264]
 [ 0.119]
 [-0.326]]
maxi score, test score, baseline:  -0.8706333333333334 0.3886666666666667 0.3886666666666667
probs:  [0.08708333337619162, 0.2051922256924173, 0.272865573860255, 0.4348588670711361]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8706333333333334 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8706333333333334 0.3886666666666667 0.3886666666666667
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.27 ]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.159]] [[1.316]
 [1.474]
 [1.316]
 [1.316]
 [1.316]
 [1.316]
 [1.377]] [[-0.317]
 [-0.123]
 [-0.317]
 [-0.317]
 [-0.317]
 [-0.317]
 [-0.283]]
maxi score, test score, baseline:  -0.8706333333333334 0.3886666666666667 0.3886666666666667
probs:  [0.08610771917296794, 0.21490818379512974, 0.269003677855405, 0.42998041917649726]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.25999999999999934  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  11715
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.602]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[-0.395]
 [-0.029]
 [-0.395]
 [-0.395]
 [-0.395]
 [-0.395]
 [-0.395]] [[0.584]
 [0.602]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]]
maxi score, test score, baseline:  -0.8706333333333334 0.3886666666666667 0.3886666666666667
probs:  [0.08617739182275505, 0.21508225498364197, 0.26841153716012617, 0.4303288160334767]
maxi score, test score, baseline:  -0.8706333333333334 0.3886666666666667 0.3886666666666667
probs:  [0.08617739182275505, 0.21508225498364197, 0.26841153716012617, 0.4303288160334767]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.501]
 [0.394]
 [0.228]
 [0.125]
 [0.362]
 [0.496]] [[1.332]
 [1.26 ]
 [1.779]
 [1.129]
 [1.424]
 [2.576]
 [1.884]] [[0.393]
 [0.397]
 [0.512]
 [0.223]
 [0.268]
 [0.753]
 [0.595]]
siam score:  -0.8310497
actor:  0 policy actor:  1  step number:  54 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  70 total reward:  0.17999999999999972  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.8044680413789829
maxi score, test score, baseline:  -0.8681666666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08617739182275505, 0.21508225498364197, 0.26841153716012617, 0.4303288160334767]
maxi score, test score, baseline:  -0.8681666666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08617739182275505, 0.21508225498364197, 0.26841153716012617, 0.4303288160334767]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.8681666666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08617739182275505, 0.21508225498364197, 0.26841153716012617, 0.4303288160334767]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8681666666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08617739182275505, 0.21508225498364197, 0.26841153716012617, 0.4303288160334767]
maxi score, test score, baseline:  -0.8681666666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08617739182275505, 0.21508225498364197, 0.26841153716012617, 0.4303288160334767]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.397]
 [0.388]] [[0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.441]
 [0.568]] [[0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.173]
 [0.228]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8681666666666666 0.3886666666666667 0.3886666666666667
probs:  [0.0860999735084373, 0.21488883191121566, 0.2690695074627846, 0.4299416871175624]
from probs:  [0.08613304127917684, 0.21458694726454206, 0.2691729695340976, 0.43010704192218346]
maxi score, test score, baseline:  -0.8681666666666666 0.3886666666666667 0.3886666666666667
Printing some Q and Qe and total Qs values:  [[-0.103]
 [-0.098]
 [-0.11 ]
 [-0.111]
 [-0.103]
 [-0.101]
 [-0.11 ]] [[-0.43 ]
 [-0.191]
 [-0.61 ]
 [-0.729]
 [-0.617]
 [-0.614]
 [-0.548]] [[-0.569]
 [-0.444]
 [-0.666]
 [-0.726]
 [-0.662]
 [-0.659]
 [-0.634]]
maxi score, test score, baseline:  -0.8681666666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08620234357259432, 0.21475978324132697, 0.268584286367413, 0.43045358681866575]
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.583]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.544]] [[0.22 ]
 [0.468]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.271]] [[0.66 ]
 [0.583]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.544]]
first move QE:  0.20417347169849362
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.232]
 [0.188]
 [0.113]
 [0.113]
 [0.149]
 [0.147]] [[2.402]
 [2.66 ]
 [3.022]
 [2.402]
 [2.402]
 [3.758]
 [3.453]] [[0.265]
 [0.396]
 [0.484]
 [0.265]
 [0.265]
 [0.686]
 [0.594]]
line 256 mcts: sample exp_bonus 0.858171019785909
actor:  0 policy actor:  0  step number:  65 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8659266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08620234357259432, 0.21475978324132697, 0.268584286367413, 0.43045358681866575]
maxi score, test score, baseline:  -0.8659266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08620234357259432, 0.21475978324132697, 0.268584286367413, 0.43045358681866575]
maxi score, test score, baseline:  -0.8659266666666666 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8659266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.0862680998711635, 0.21416009168769126, 0.2687894084107226, 0.4307824000304227]
maxi score, test score, baseline:  -0.8659266666666666 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8659266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08630073745270135, 0.21386243986717768, 0.2688912189985963, 0.43094560368152457]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.8659266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08630073745270135, 0.21386243986717768, 0.2688912189985963, 0.43094560368152457]
maxi score, test score, baseline:  -0.8659266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08629769247172957, 0.21387504550423217, 0.2688967927617617, 0.4309304692622765]
maxi score, test score, baseline:  -0.8659266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08636473770759759, 0.21366450312809, 0.26870503131047496, 0.4312657278538374]
line 256 mcts: sample exp_bonus 1.3666078633091228
maxi score, test score, baseline:  -0.8659266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08636473770759759, 0.21366450312809, 0.26870503131047496, 0.4312657278538374]
maxi score, test score, baseline:  -0.8659266666666666 0.3886666666666667 0.3886666666666667
actor:  1 policy actor:  1  step number:  65 total reward:  0.15999999999999892  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8659266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.0863647377075976, 0.21366450312809, 0.2687050313104749, 0.4312657278538375]
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]] [[0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]] [[0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]]
maxi score, test score, baseline:  -0.8659266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.0863647377075976, 0.21366450312809, 0.2687050313104749, 0.4312657278538375]
maxi score, test score, baseline:  -0.8659266666666666 0.3886666666666667 0.3886666666666667
probs:  [0.0863647377075976, 0.21366450312809, 0.2687050313104749, 0.4312657278538375]
actor:  0 policy actor:  0  step number:  55 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8634466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.0863647377075976, 0.21366450312809, 0.2687050313104749, 0.4312657278538375]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8309187
maxi score, test score, baseline:  -0.8634466666666666 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8634466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08642852233885971, 0.2134674848071415, 0.2685192198291176, 0.4315847730248812]
maxi score, test score, baseline:  -0.8634466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08642852233885971, 0.2134674848071415, 0.2685192198291176, 0.4315847730248812]
maxi score, test score, baseline:  -0.8634466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08661944523799661, 0.21172798992764397, 0.26911308481262536, 0.432539480021734]
maxi score, test score, baseline:  -0.8634466666666666 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.8634466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08661944523799661, 0.21172798992764397, 0.26911308481262536, 0.432539480021734]
using another actor
maxi score, test score, baseline:  -0.8634466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08661944523799661, 0.21172798992764397, 0.26911308481262536, 0.432539480021734]
from probs:  [0.08661944523799661, 0.21172798992764397, 0.26911308481262536, 0.432539480021734]
siam score:  -0.83011276
from probs:  [0.08661944523799661, 0.21172798992764397, 0.26911308481262536, 0.432539480021734]
maxi score, test score, baseline:  -0.8634466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08665073416689623, 0.21144291709535468, 0.26921040891219095, 0.43269593982555815]
actor:  0 policy actor:  1  step number:  44 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8604466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08668526677618155, 0.21152726887964926, 0.26891884474131184, 0.4328686196028574]
maxi score, test score, baseline:  -0.8604466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08668526677618155, 0.21152726887964926, 0.26891884474131184, 0.4328686196028574]
maxi score, test score, baseline:  -0.8604466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08668526677618155, 0.21152726887964926, 0.26891884474131184, 0.4328686196028574]
maxi score, test score, baseline:  -0.8604466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08668526677618155, 0.21152726887964926, 0.26891884474131184, 0.4328686196028574]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8604466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08671968139402533, 0.21161133244973615, 0.2686282767904559, 0.4330407093657826]
Printing some Q and Qe and total Qs values:  [[-0.141]
 [-0.136]
 [-0.14 ]
 [-0.141]
 [-0.145]
 [-0.138]
 [-0.139]] [[-2.707]
 [ 0.042]
 [-2.21 ]
 [-2.223]
 [-1.31 ]
 [ 0.   ]
 [-2.079]] [[-0.107]
 [ 0.61 ]
 [ 0.022]
 [ 0.019]
 [ 0.255]
 [ 0.598]
 [ 0.057]]
maxi score, test score, baseline:  -0.8604466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08671968139402533, 0.21161133244973615, 0.2686282767904559, 0.4330407093657826]
Starting evaluation
maxi score, test score, baseline:  -0.8604466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08681939805613588, 0.21149455549076074, 0.26814670546455516, 0.43353934098854824]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.061]
 [0.07 ]
 [0.071]
 [0.075]
 [0.066]
 [0.095]] [[-3.866]
 [-0.077]
 [-2.226]
 [-2.067]
 [-1.851]
 [-1.455]
 [ 0.   ]] [[0.084]
 [0.061]
 [0.07 ]
 [0.071]
 [0.075]
 [0.066]
 [0.095]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.087]
 [0.071]
 [0.064]
 [0.067]
 [0.072]
 [0.071]] [[-3.089]
 [-0.176]
 [-2.343]
 [-1.682]
 [-1.495]
 [-1.936]
 [-1.242]] [[0.098]
 [0.087]
 [0.071]
 [0.064]
 [0.067]
 [0.072]
 [0.071]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8604466666666666 0.3886666666666667 0.3886666666666667
probs:  [0.08681939805613588, 0.21149455549076074, 0.26814670546455516, 0.43353934098854824]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.996]
 [0.983]
 [0.771]
 [0.893]
 [0.905]
 [0.982]] [[1.881]
 [1.043]
 [1.09 ]
 [1.644]
 [0.955]
 [1.053]
 [1.222]] [[0.812]
 [0.996]
 [0.983]
 [0.771]
 [0.893]
 [0.905]
 [0.982]]
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8303133333333335 0.3886666666666667 0.3886666666666667
probs:  [0.08681939805613588, 0.21149455549076074, 0.26814670546455516, 0.43353934098854824]
actor:  0 policy actor:  0  step number:  29 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  30 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  30 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8203666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08681939805613588, 0.21149455549076074, 0.26814670546455516, 0.43353934098854824]
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.055]
 [0.066]
 [0.067]
 [0.067]
 [0.068]
 [0.073]] [[-3.623]
 [ 0.029]
 [-2.723]
 [-2.69 ]
 [-2.568]
 [-2.362]
 [-2.34 ]] [[0.091]
 [0.055]
 [0.066]
 [0.067]
 [0.067]
 [0.068]
 [0.073]]
actor:  0 policy actor:  0  step number:  35 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.81714 0.3886666666666667 0.3886666666666667
maxi score, test score, baseline:  -0.81714 0.3886666666666667 0.3886666666666667
probs:  [0.08562438489910257, 0.20858050075492116, 0.27823141812035834, 0.42756369622561796]
actor:  0 policy actor:  1  step number:  38 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8138733333333333 0.3886666666666667 0.3886666666666667
probs:  [0.08431228811575196, 0.20538093611172106, 0.2893041991678439, 0.421002576604683]
maxi score, test score, baseline:  -0.8138733333333333 0.3886666666666667 0.3886666666666667
probs:  [0.08431228811575196, 0.20538093611172106, 0.2893041991678439, 0.421002576604683]
maxi score, test score, baseline:  -0.8138733333333333 0.3886666666666667 0.3886666666666667
probs:  [0.0843416057309642, 0.2051042905192871, 0.28940492486476976, 0.4211491788849789]
maxi score, test score, baseline:  -0.8138733333333333 0.3886666666666667 0.3886666666666667
probs:  [0.0843416057309642, 0.2051042905192871, 0.28940492486476976, 0.4211491788849789]
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]] [[-2.793]
 [-2.793]
 [-2.793]
 [-2.793]
 [-2.793]
 [-2.793]
 [-2.793]] [[0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]]
using another actor
maxi score, test score, baseline:  -0.8138733333333333 0.3886666666666667 0.3886666666666667
probs:  [0.0843416057309642, 0.2051042905192871, 0.28940492486476976, 0.4211491788849789]
maxi score, test score, baseline:  -0.8138733333333333 0.3886666666666667 0.3886666666666667
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.085]
 [-0.081]
 [-0.081]
 [-0.084]
 [-0.08 ]
 [-0.076]] [[0.703]
 [1.997]
 [0.394]
 [0.44 ]
 [0.474]
 [0.557]
 [0.554]] [[-0.353]
 [ 0.197]
 [-0.49 ]
 [-0.47 ]
 [-0.457]
 [-0.419]
 [-0.418]]
maxi score, test score, baseline:  -0.8138733333333333 0.3886666666666667 0.3886666666666667
probs:  [0.08447402996713994, 0.2047349770917746, 0.2889796287163627, 0.42181136422472276]
line 256 mcts: sample exp_bonus 3.6341891990002364
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  97 total reward:  0.11999999999999977  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  105 total reward:  0.013333333333332087  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.713]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.638]] [[1.971]
 [1.452]
 [1.971]
 [1.971]
 [1.971]
 [1.971]
 [1.961]] [[0.228]
 [0.115]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.258]]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.52 ]
 [0.363]
 [0.349]
 [0.434]
 [0.344]
 [0.366]] [[2.171]
 [1.905]
 [1.921]
 [1.856]
 [2.512]
 [2.324]
 [2.533]] [[0.154]
 [0.153]
 [0.083]
 [0.056]
 [0.303]
 [0.201]
 [0.278]]
actor:  0 policy actor:  0  step number:  109 total reward:  0.026666666666666394  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  115 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.17566019790665013
maxi score, test score, baseline:  -0.8053666666666667 0.3886666666666667 0.3886666666666667
probs:  [0.08451095093057495, 0.20482455380822576, 0.28866850833128344, 0.42199598692991586]
actor:  0 policy actor:  0  step number:  118 total reward:  0.059999999999999165  reward:  1.0 rdn_beta:  0.667
first move QE:  0.178484890012825
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.3199999999999993  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08265654670362482, 0.22029915126192387, 0.2842870746709844, 0.41275722736346687]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08265654670362482, 0.22029915126192387, 0.2842870746709844, 0.41275722736346687]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.0720416809084604, 0.3213367188979158, 0.24694823950067354, 0.3596733606929502]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07206969000539627, 0.32146185356015194, 0.2466550250900212, 0.3598134313444306]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07206969000539627, 0.32146185356015194, 0.2466550250900212, 0.3598134313444306]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.39999999999999936  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07224082280777569, 0.32082722926388996, 0.24626209296326249, 0.3606698549650719]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07234303591046569, 0.31986491119640753, 0.2466110397148576, 0.3611810131782692]
Printing some Q and Qe and total Qs values:  [[-0.143]
 [-0.106]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]] [[0.642]
 [0.68 ]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[-0.598]
 [-0.548]
 [-0.598]
 [-0.598]
 [-0.598]
 [-0.598]
 [-0.598]]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07239383902066063, 0.31938660900678356, 0.246784477168637, 0.36143507480391873]
start point for exploration sampling:  11715
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07247249123617859, 0.31903387321970983, 0.2466652283308432, 0.3618284072132684]
actor:  1 policy actor:  1  step number:  61 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07252293327493387, 0.3185592409003057, 0.24683716267050618, 0.36208066315425436]
siam score:  -0.84398794
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07255092901269895, 0.3186824097791449, 0.2465459939750066, 0.36222066723314966]
using explorer policy with actor:  0
siam score:  -0.84227616
line 256 mcts: sample exp_bonus 1.6137228159196608
Printing some Q and Qe and total Qs values:  [[-0.064]
 [-0.048]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]] [[0.938]
 [1.129]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]] [[-0.521]
 [-0.42 ]
 [-0.521]
 [-0.521]
 [-0.521]
 [-0.521]
 [-0.521]]
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.322]
 [0.265]
 [0.325]
 [0.319]
 [0.365]
 [0.336]] [[1.679]
 [1.638]
 [1.593]
 [1.161]
 [1.281]
 [1.928]
 [1.615]] [[ 0.268]
 [ 0.21 ]
 [ 0.13 ]
 [-0.026]
 [ 0.028]
 [ 0.398]
 [ 0.212]]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07260121148532865, 0.31820954737982743, 0.24671711593605086, 0.3624721251987931]
line 256 mcts: sample exp_bonus 1.24034791410405
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07260121148532865, 0.31820954737982743, 0.24671711593605086, 0.3624721251987931]
actor:  1 policy actor:  1  step number:  59 total reward:  0.15999999999999914  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07268184829727967, 0.3185745943418766, 0.2458679773467409, 0.3628755800141028]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07273203263709727, 0.31810343204380054, 0.2460379879556809, 0.3631265473634212]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07273203263709727, 0.31810343204380054, 0.2460379879556809, 0.3631265473634212]
actor:  1 policy actor:  1  step number:  60 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.25 ]
 [0.204]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]] [[1.135]
 [1.135]
 [2.154]
 [1.135]
 [1.135]
 [1.135]
 [1.135]] [[-0.364]
 [-0.364]
 [ 0.27 ]
 [-0.364]
 [-0.364]
 [-0.364]
 [-0.364]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
actor:  1 policy actor:  1  step number:  60 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07280828825425091, 0.31776909401895387, 0.2459147229116142, 0.363507894815181]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07285812048551506, 0.31730149925470924, 0.24608327895833948, 0.3637571013014362]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07285812048551506, 0.31730149925470924, 0.24608327895833948, 0.3637571013014362]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07285812048551506, 0.31730149925470924, 0.24608327895833948, 0.3637571013014362]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07293547516410435, 0.3169563691528102, 0.2459642106229491, 0.3641439450601363]
rdn beta is 0 so we're just using the maxi policy
using another actor
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07293547516410435, 0.3169563691528102, 0.2459642106229491, 0.3641439450601363]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5466666666666671  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07075634324140041, 0.33739259379399467, 0.23860475976732323, 0.3532463031972817]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07075634324140041, 0.33739259379399467, 0.23860475976732323, 0.3532463031972817]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07075634324140041, 0.33739259379399467, 0.23860475976732323, 0.3532463031972817]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07075634324140041, 0.33739259379399467, 0.23860475976732323, 0.3532463031972817]
maxi score, test score, baseline:  -0.8032466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07075634324140041, 0.33739259379399467, 0.23860475976732323, 0.3532463031972817]
actor:  0 policy actor:  0  step number:  65 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07083435991614039, 0.3370291758921472, 0.23850000665279172, 0.3536364575389207]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  44 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.138]
 [-0.123]
 [-0.161]
 [-0.131]
 [-0.038]
 [-0.128]
 [-0.147]] [[ 0.367]
 [ 0.221]
 [ 0.572]
 [-0.426]
 [ 0.267]
 [ 0.266]
 [ 0.445]] [[-0.197]
 [-0.254]
 [-0.117]
 [-0.585]
 [-0.146]
 [-0.237]
 [-0.166]]
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07119053606697832, 0.3346512242384337, 0.23873982755558765, 0.35541841213900033]
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07119053606697832, 0.3346512242384337, 0.23873982755558765, 0.35541841213900033]
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07119053606697832, 0.3346512242384337, 0.23873982755558765, 0.35541841213900033]
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
probs:  [0.0713195811472927, 0.33453817780446243, 0.23807848475959784, 0.356063756288647]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
siam score:  -0.853548
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07751946705507727, 0.3103585247315278, 0.22503178749654995, 0.38709022071684496]
first move QE:  0.16108142126791405
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07751946705507727, 0.3103585247315278, 0.22503178749654995, 0.38709022071684496]
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
actor:  1 policy actor:  1  step number:  61 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  52 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07769410826200186, 0.309627686291854, 0.22471425071609016, 0.387963954730054]
from probs:  [0.07769410826200186, 0.309627686291854, 0.22471425071609016, 0.387963954730054]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8698146123476325
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07769410826200186, 0.309627686291854, 0.22471425071609016, 0.387963954730054]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07769410826200186, 0.309627686291854, 0.22471425071609016, 0.387963954730054]
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07769410826200188, 0.309627686291854, 0.22471425071609014, 0.38796395473005413]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.575]
 [0.477]
 [0.499]
 [0.499]
 [0.499]
 [0.428]] [[1.652]
 [1.382]
 [1.89 ]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [2.311]] [[0.398]
 [0.316]
 [0.506]
 [0.455]
 [0.455]
 [0.455]
 [0.689]]
Printing some Q and Qe and total Qs values:  [[-0.284]
 [-0.   ]
 [-0.   ]
 [-0.284]
 [-0.284]
 [-0.284]
 [-0.   ]] [[0.591]
 [0.   ]
 [0.   ]
 [0.591]
 [0.591]
 [0.591]
 [0.   ]] [[-0.284]
 [-0.295]
 [-0.295]
 [-0.284]
 [-0.284]
 [-0.284]
 [-0.295]]
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
Printing some Q and Qe and total Qs values:  [[-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.121]] [[0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.214]] [[-0.508]
 [-0.508]
 [-0.508]
 [-0.508]
 [-0.508]
 [-0.508]
 [-0.683]]
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]] [[1.986]
 [1.986]
 [1.986]
 [1.986]
 [1.986]
 [1.986]
 [1.986]] [[0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]]
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.8011400000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07779177353359165, 0.30875874344306475, 0.22499708473626154, 0.388452398287082]
actor:  0 policy actor:  0  step number:  67 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  0.667
first move QE:  0.15593654620011785
maxi score, test score, baseline:  -0.7987666666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07784032190193378, 0.3083268011896456, 0.22513767852121144, 0.3886951983872092]
Printing some Q and Qe and total Qs values:  [[-0.085]
 [-0.071]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]] [[0.299]
 [0.679]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]] [[-0.594]
 [-0.391]
 [-0.594]
 [-0.594]
 [-0.594]
 [-0.594]
 [-0.594]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  0 policy actor:  0  step number:  49 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07784032190193381, 0.30832680118964556, 0.22513767852121142, 0.3886951983872093]
maxi score, test score, baseline:  -0.7962600000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07783658032932865, 0.3083278194711059, 0.22515894986135432, 0.38867665033821114]
maxi score, test score, baseline:  -0.7962600000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07783287652858982, 0.3083288274728268, 0.22518000646339098, 0.38865828953519244]
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7962600000000001 0.3483333333333333 0.3483333333333333
probs:  [0.0778812087779297, 0.30789876810952493, 0.22532001412816252, 0.3889000089843828]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.282]
 [0.237]
 [0.491]
 [0.32 ]
 [0.388]
 [0.421]] [[2.344]
 [2.18 ]
 [2.283]
 [2.085]
 [1.625]
 [2.742]
 [2.223]] [[ 0.094]
 [-0.052]
 [-0.062]
 [ 0.126]
 [-0.198]
 [ 0.242]
 [ 0.102]]
actor:  0 policy actor:  0  step number:  54 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7937933333333334 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7937933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07792935415368642, 0.3074703715435331, 0.22545948046215925, 0.3891407938406211]
maxi score, test score, baseline:  -0.7937933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07792935415368642, 0.3074703715435331, 0.22545948046215925, 0.3891407938406211]
actor:  1 policy actor:  1  step number:  61 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  -0.7937933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07802508860296246, 0.3066185283774481, 0.22573680164758703, 0.3896195813720025]
from probs:  [0.07802508860296246, 0.3066185283774481, 0.22573680164758703, 0.3896195813720025]
maxi score, test score, baseline:  -0.7937933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07802508860296246, 0.3066185283774481, 0.22573680164758703, 0.3896195813720025]
UNIT TEST: sample policy line 217 mcts : [0.061 0.306 0.061 0.041 0.041 0.143 0.347]
maxi score, test score, baseline:  -0.7937933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07802508860296246, 0.3066185283774481, 0.22573680164758703, 0.3896195813720025]
line 256 mcts: sample exp_bonus 1.7787992631520606
maxi score, test score, baseline:  -0.7937933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07802508860296246, 0.3066185283774481, 0.22573680164758703, 0.3896195813720025]
siam score:  -0.8518881
maxi score, test score, baseline:  -0.7937933333333334 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7937933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07802508860296246, 0.3066185283774481, 0.22573680164758703, 0.3896195813720025]
actor:  0 policy actor:  0  step number:  47 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07802508860296246, 0.3066185283774481, 0.22573680164758703, 0.3896195813720025]
maxi score, test score, baseline:  -0.7909933333333333 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7909933333333333 0.3483333333333333 0.3483333333333333
probs:  [0.07812008843059634, 0.30577322185966727, 0.2260119947992069, 0.3900946949105295]
maxi score, test score, baseline:  -0.7909933333333333 0.3483333333333333 0.3483333333333333
actor:  1 policy actor:  1  step number:  105 total reward:  0.11999999999999766  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7909933333333333 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7909933333333333 0.3483333333333333 0.3483333333333333
probs:  [0.07816731549864966, 0.30535299637678515, 0.22614880100215554, 0.3903308871224096]
actor:  1 policy actor:  1  step number:  33 total reward:  0.6400000000000002  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07816731549864966, 0.30535299637678515, 0.22614880100215554, 0.3903308871224096]
maxi score, test score, baseline:  -0.7909933333333333 0.3483333333333333 0.3483333333333333
probs:  [0.07481888960590703, 0.2922539973654312, 0.2593423875995933, 0.37358472542906845]
maxi score, test score, baseline:  -0.7909933333333333 0.3483333333333333 0.3483333333333333
probs:  [0.07481888960590703, 0.2922539973654312, 0.2593423875995933, 0.37358472542906845]
maxi score, test score, baseline:  -0.7909933333333333 0.3483333333333333 0.3483333333333333
start point for exploration sampling:  11715
Printing some Q and Qe and total Qs values:  [[0.887]
 [0.761]
 [0.583]
 [0.788]
 [0.835]
 [0.824]
 [0.87 ]] [[1.901]
 [2.154]
 [2.483]
 [2.01 ]
 [1.787]
 [1.035]
 [2.042]] [[0.887]
 [0.761]
 [0.583]
 [0.788]
 [0.835]
 [0.824]
 [0.87 ]]
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]] [[1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]] [[0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]]
actor:  0 policy actor:  1  step number:  46 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  65 total reward:  0.10666666666666569  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7861 0.3483333333333333 0.3483333333333333
probs:  [0.07481605675192889, 0.2922586446841003, 0.25935458603533545, 0.3735707125286353]
Printing some Q and Qe and total Qs values:  [[-0.116]
 [-0.116]
 [-0.266]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.117]] [[0.   ]
 [0.   ]
 [0.549]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.853]] [[-0.282]
 [-0.282]
 [-0.096]
 [-0.282]
 [-0.282]
 [-0.282]
 [ 0.208]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7861 0.3483333333333333 0.3483333333333333
probs:  [0.07490205457498243, 0.2914440112428797, 0.2596531287999683, 0.37400080538216945]
maxi score, test score, baseline:  -0.7861 0.3483333333333333 0.3483333333333333
probs:  [0.07490205457498243, 0.2914440112428797, 0.2596531287999684, 0.37400080538216945]
using another actor
maxi score, test score, baseline:  -0.7861 0.3483333333333333 0.3483333333333333
probs:  [0.07494480458219108, 0.29103905232359517, 0.2598015361071185, 0.3742146069870953]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7861 0.3483333333333333 0.3483333333333333
probs:  [0.07494480458219105, 0.29103905232359517, 0.2598015361071185, 0.37421460698709524]
maxi score, test score, baseline:  -0.7861 0.3483333333333333 0.3483333333333333
probs:  [0.07494480458219105, 0.29103905232359517, 0.2598015361071185, 0.37421460698709524]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5157668305020778
maxi score, test score, baseline:  -0.7861 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7861 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7861 0.3483333333333333 0.3483333333333333
probs:  [0.07494480458219105, 0.29103905232359517, 0.2598015361071185, 0.37421460698709524]
actor:  1 policy actor:  1  step number:  57 total reward:  0.4800000000000002  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  66 total reward:  0.059999999999999054  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.78398 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.78398 0.3483333333333333 0.3483333333333333
probs:  [0.07429533295885449, 0.2885131547388403, 0.2662250468826587, 0.37096646541964656]
from probs:  [0.07429533295885449, 0.2885131547388403, 0.2662250468826587, 0.37096646541964656]
maxi score, test score, baseline:  -0.78398 0.3483333333333333 0.3483333333333333
line 256 mcts: sample exp_bonus 2.0795604906458456
actor:  0 policy actor:  1  step number:  49 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  58 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.7810466666666668 0.3483333333333333 0.3483333333333333
probs:  [0.07428285249713487, 0.2884646162546611, 0.26634848319220483, 0.37090404805599925]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7810466666666668 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7810466666666668 0.3483333333333333 0.3483333333333333
probs:  [0.07432468719080199, 0.28806333647932314, 0.26649870434079065, 0.3711132719890842]
maxi score, test score, baseline:  -0.7810466666666668 0.3483333333333333 0.3483333333333333
probs:  [0.07440787378269235, 0.28726540791862826, 0.2667974130030352, 0.37152930529564415]
maxi score, test score, baseline:  -0.7810466666666668 0.3483333333333333 0.3483333333333333
probs:  [0.07448770411102344, 0.28647948438177295, 0.267104105697051, 0.3719287058101525]
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.845]
 [0.726]
 [0.754]
 [0.762]
 [0.762]
 [0.805]] [[2.178]
 [0.918]
 [2.649]
 [2.151]
 [2.178]
 [2.178]
 [1.613]] [[0.762]
 [0.845]
 [0.726]
 [0.754]
 [0.762]
 [0.762]
 [0.805]]
line 256 mcts: sample exp_bonus 1.4347162156733546
actor:  0 policy actor:  0  step number:  75 total reward:  0.06666666666666632  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.96 ]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.846]] [[0.276]
 [0.814]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.977]] [[0.923]
 [0.96 ]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.846]]
maxi score, test score, baseline:  -0.7789133333333333 0.3483333333333333 0.3483333333333333
probs:  [0.07448770411102346, 0.28647948438177295, 0.26710410569705095, 0.3719287058101526]
maxi score, test score, baseline:  -0.7789133333333333 0.3483333333333333 0.3483333333333333
probs:  [0.07448770411102346, 0.28647948438177295, 0.26710410569705095, 0.3719287058101526]
actor:  0 policy actor:  0  step number:  60 total reward:  0.04666666666666586  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7768200000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07402483754068594, 0.29091944164790773, 0.2654419071540099, 0.3696138136573965]
maxi score, test score, baseline:  -0.7768200000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07410777888782456, 0.2901238429581543, 0.26573975753942697, 0.3700286206145942]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.406]
 [0.009]
 [0.462]
 [0.484]
 [0.046]
 [0.434]] [[ 0.444]
 [ 0.375]
 [ 0.461]
 [-0.129]
 [ 0.086]
 [-0.024]
 [ 0.458]] [[0.509]
 [0.406]
 [0.009]
 [0.462]
 [0.484]
 [0.046]
 [0.434]]
Printing some Q and Qe and total Qs values:  [[-0.102]
 [-0.07 ]
 [-0.102]
 [-0.102]
 [-0.102]
 [-0.102]
 [-0.126]] [[0.604]
 [0.618]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.334]] [[ 0.078]
 [ 0.113]
 [ 0.078]
 [ 0.078]
 [ 0.078]
 [ 0.078]
 [-0.094]]
maxi score, test score, baseline:  -0.7768200000000001 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7768200000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07410777888782456, 0.2901238429581543, 0.265739757539427, 0.37002862061459413]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.595]
 [0.405]
 [0.584]
 [0.589]
 [0.513]
 [0.579]] [[ 0.51 ]
 [-0.072]
 [ 0.929]
 [-0.089]
 [-0.271]
 [ 0.38 ]
 [ 0.279]] [[0.478]
 [0.595]
 [0.405]
 [0.584]
 [0.589]
 [0.513]
 [0.579]]
line 256 mcts: sample exp_bonus 0.048251323937647646
actor:  1 policy actor:  1  step number:  47 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.7768200000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07431915362154089, 0.2894352786779515, 0.26515904313119, 0.3710865245693175]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.808]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.637]] [[1.857]
 [1.21 ]
 [1.857]
 [1.857]
 [1.857]
 [1.857]
 [1.741]] [[0.561]
 [0.808]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.637]]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.829]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]] [[1.127]
 [1.277]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]] [[0.727]
 [0.829]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  48 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7740066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07436032415306411, 0.28904110165493746, 0.26530614678686304, 0.3712924274051353]
maxi score, test score, baseline:  -0.7740066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07436032415306411, 0.28904110165493746, 0.26530614678686304, 0.3712924274051353]
siam score:  -0.848592
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.448]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.337]] [[1.126]
 [1.461]
 [1.126]
 [1.126]
 [1.126]
 [1.126]
 [1.378]] [[-0.13 ]
 [ 0.028]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.112]]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.007]] [[1.859]
 [1.859]
 [1.859]
 [1.859]
 [1.859]
 [1.859]
 [1.776]] [[0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.271]]
maxi score, test score, baseline:  -0.7740066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07436032415306411, 0.28904110165493746, 0.26530614678686304, 0.3712924274051353]
maxi score, test score, baseline:  -0.7740066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07436032415306411, 0.28904110165493746, 0.26530614678686304, 0.3712924274051353]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.02 ]
 [-0.08 ]
 [-0.081]
 [-0.062]
 [-0.055]
 [-0.088]
 [-0.095]] [[3.057]
 [2.172]
 [2.263]
 [0.637]
 [0.467]
 [1.522]
 [2.128]] [[ 0.609]
 [ 0.244]
 [ 0.274]
 [-0.259]
 [-0.311]
 [ 0.021]
 [ 0.219]]
maxi score, test score, baseline:  -0.7740066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07440133925052693, 0.2886484127967088, 0.2654526950715546, 0.37149755288120967]
actor:  1 policy actor:  1  step number:  67 total reward:  0.29333333333333234  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7740066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07443956445105308, 0.2882625429563533, 0.26560901879991167, 0.371688873792682]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  51 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7740066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07414586806619462, 0.28224565380622885, 0.27338644633460774, 0.3702220317929688]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]] [[2.486]
 [2.486]
 [2.486]
 [2.486]
 [2.486]
 [2.486]
 [2.486]] [[0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7740066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07414586806619462, 0.28224565380622885, 0.27338644633460774, 0.3702220317929688]
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.573]
 [0.525]
 [0.55 ]
 [0.512]
 [0.457]
 [0.509]] [[0.59 ]
 [0.588]
 [0.533]
 [0.567]
 [0.529]
 [0.635]
 [0.712]] [[-0.106]
 [-0.057]
 [-0.133]
 [-0.092]
 [-0.148]
 [-0.15 ]
 [-0.059]]
maxi score, test score, baseline:  -0.7740066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07414586806619462, 0.28224565380622885, 0.27338644633460774, 0.3702220317929688]
actor:  1 policy actor:  1  step number:  64 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  42 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.115]
 [-0.122]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.116]] [[-1.258]
 [-0.124]
 [-1.751]
 [-1.74 ]
 [-1.731]
 [-1.702]
 [-1.605]] [[ 0.087]
 [ 0.421]
 [-0.06 ]
 [-0.057]
 [-0.055]
 [-0.046]
 [-0.016]]
maxi score, test score, baseline:  -0.7740066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07570651895137827, 0.2771719187857202, 0.2690900241351369, 0.3780315381277645]
maxi score, test score, baseline:  -0.7740066666666667 0.3483333333333333 0.3483333333333333
first move QE:  0.11966243075231403
actor:  0 policy actor:  1  step number:  49 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.77142 0.3483333333333333 0.3483333333333333
probs:  [0.07574542678281083, 0.27679993047757057, 0.2692285146891311, 0.3782261280504875]
maxi score, test score, baseline:  -0.77142 0.3483333333333333 0.3483333333333333
probs:  [0.07574542678281083, 0.27679993047757057, 0.2692285146891311, 0.3782261280504875]
maxi score, test score, baseline:  -0.77142 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.77142 0.3483333333333333 0.3483333333333333
probs:  [0.07582279881359595, 0.27606019531963066, 0.2695039167100754, 0.378613089156698]
maxi score, test score, baseline:  -0.77142 0.3483333333333333 0.3483333333333333
probs:  [0.07582279881359595, 0.27606019531963066, 0.2695039167100754, 0.378613089156698]
actor:  0 policy actor:  1  step number:  69 total reward:  0.02666666666666595  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7693666666666668 0.3483333333333333 0.3483333333333333
probs:  [0.07589958493404919, 0.2753260619077932, 0.26977723321138347, 0.37899711994677415]
maxi score, test score, baseline:  -0.7693666666666668 0.3483333333333333 0.3483333333333333
probs:  [0.07589958493404919, 0.2753260619077932, 0.26977723321138347, 0.37899711994677415]
maxi score, test score, baseline:  -0.7693666666666668 0.3483333333333333 0.3483333333333333
probs:  [0.07589958493404919, 0.2753260619077932, 0.26977723321138347, 0.37899711994677415]
maxi score, test score, baseline:  -0.7693666666666668 0.3483333333333333 0.3483333333333333
probs:  [0.07589958493404919, 0.2753260619077932, 0.26977723321138347, 0.37899711994677415]
actor:  1 policy actor:  1  step number:  63 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.07589958493404919, 0.2753260619077932, 0.26977723321138347, 0.37899711994677415]
actor:  0 policy actor:  0  step number:  64 total reward:  0.059999999999999165  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7672466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07589681216777286, 0.2753337401052367, 0.2697860572185238, 0.3789833905084667]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  62 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  48 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.129]
 [-0.081]
 [-0.103]
 [-0.127]
 [-0.126]
 [-0.112]
 [-0.123]] [[0.239]
 [1.334]
 [0.821]
 [0.183]
 [0.156]
 [0.574]
 [0.259]] [[-0.369]
 [ 0.113]
 [-0.112]
 [-0.39 ]
 [-0.401]
 [-0.22 ]
 [-0.357]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
siam score:  -0.83624303
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07593222338666174, 0.274976515294647, 0.2699306317092342, 0.379160629609457]
Printing some Q and Qe and total Qs values:  [[-0.156]
 [-0.156]
 [-0.158]
 [-0.158]
 [-0.158]
 [-0.158]
 [-0.158]] [[0.233]
 [0.037]
 [0.109]
 [0.055]
 [0.067]
 [0.073]
 [0.226]] [[-0.068]
 [-0.166]
 [-0.132]
 [-0.159]
 [-0.153]
 [-0.15 ]
 [-0.074]]
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07593222338666175, 0.27497651529464706, 0.2699306317092342, 0.37916062960945707]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07593222338666175, 0.27497651529464706, 0.2699306317092342, 0.37916062960945707]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.001]
 [-0.015]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.024]] [[1.969]
 [2.221]
 [2.183]
 [1.969]
 [1.969]
 [1.969]
 [2.283]] [[0.158]
 [0.262]
 [0.236]
 [0.158]
 [0.158]
 [0.158]
 [0.26 ]]
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07600810624950334, 0.2742509801609641, 0.27020077038141754, 0.37954014320811497]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9404746876955032
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07780085158051135, 0.26855793610974366, 0.26513011317068247, 0.3885110991390625]
Printing some Q and Qe and total Qs values:  [[-0.128]
 [-0.128]
 [-0.111]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]] [[1.028]
 [1.028]
 [1.29 ]
 [1.028]
 [1.028]
 [1.028]
 [1.028]] [[0.285]
 [0.285]
 [0.409]
 [0.285]
 [0.285]
 [0.285]
 [0.285]]
siam score:  -0.83642495
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07783792850684877, 0.26820890043318846, 0.265256636446237, 0.3886965346137258]
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07783792850684877, 0.26820890043318846, 0.265256636446237, 0.3886965346137258]
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07783792850684877, 0.26820890043318846, 0.265256636446237, 0.3886965346137258]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07696044084360537, 0.2651811689790529, 0.2735504971641589, 0.3843078930131828]
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
Printing some Q and Qe and total Qs values:  [[-0.069]
 [-0.071]
 [-0.07 ]
 [-0.072]
 [-0.07 ]
 [-0.071]
 [-0.071]] [[ 0.276]
 [ 0.303]
 [-0.016]
 [-0.144]
 [ 0.002]
 [-0.118]
 [-0.021]] [[-0.614]
 [-0.607]
 [-0.712]
 [-0.756]
 [-0.706]
 [-0.746]
 [-0.715]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07701109594354456, 0.26479362348649405, 0.2736340036889532, 0.3845612768810082]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.333
siam score:  -0.84354156
actor:  1 policy actor:  1  step number:  53 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07698703519764313, 0.2650236328567977, 0.2735483918003403, 0.3844409401452189]
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07702295093961452, 0.2646802953215165, 0.27367618561495016, 0.38462056812391887]
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07702295093961452, 0.2646802953215165, 0.27367618561495016, 0.38462056812391887]
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07702295093961452, 0.2646802953215165, 0.27367618561495016, 0.38462056812391887]
maxi score, test score, baseline:  -0.7622066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07702295093961452, 0.2646802953215165, 0.27367618561495016, 0.38462056812391887]
actor:  0 policy actor:  1  step number:  53 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7596466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07702295093961452, 0.2646802953215165, 0.27367618561495016, 0.38462056812391887]
actor:  0 policy actor:  0  step number:  54 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7573666666666667 0.3483333333333333 0.3483333333333333
actor:  1 policy actor:  1  step number:  51 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.7573666666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07702295093961452, 0.2646802953215165, 0.27367618561495016, 0.38462056812391887]
maxi score, test score, baseline:  -0.7573666666666667 0.3483333333333333 0.3483333333333333
actor:  1 policy actor:  1  step number:  45 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7573666666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07795124276286972, 0.2617469475744961, 0.271036120277358, 0.38926568938527617]
Printing some Q and Qe and total Qs values:  [[-0.11]
 [-0.06]
 [-0.11]
 [-0.11]
 [-0.11]
 [-0.11]
 [-0.11]] [[0.693]
 [1.077]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[-0.374]
 [-0.203]
 [-0.374]
 [-0.374]
 [-0.374]
 [-0.374]
 [-0.374]]
siam score:  -0.8431127
maxi score, test score, baseline:  -0.7573666666666667 0.3483333333333333 0.3483333333333333
probs:  [0.07795124276286972, 0.2617469475744961, 0.271036120277358, 0.38926568938527617]
actor:  0 policy actor:  1  step number:  52 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7546866666666666 0.3483333333333333 0.3483333333333333
probs:  [0.07798663131123462, 0.26141135192793546, 0.2711593350015549, 0.3894426817592749]
maxi score, test score, baseline:  -0.7546866666666666 0.3483333333333333 0.3483333333333333
probs:  [0.07798663131123462, 0.26141135192793546, 0.2711593350015549, 0.3894426817592749]
actor:  1 policy actor:  1  step number:  44 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7546866666666666 0.3483333333333333 0.3483333333333333
probs:  [0.07773185316479948, 0.26382745659266316, 0.2702722565398815, 0.3881684337026558]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.084]
 [0.025]
 [0.011]
 [0.026]
 [0.011]
 [0.009]] [[-1.938]
 [-0.103]
 [-0.769]
 [-0.756]
 [-0.206]
 [-0.08 ]
 [-0.091]] [[0.031]
 [0.084]
 [0.025]
 [0.011]
 [0.026]
 [0.011]
 [0.009]]
siam score:  -0.84078985
maxi score, test score, baseline:  -0.7546866666666666 0.3483333333333333 0.3483333333333333
probs:  [0.07392090358409927, 0.2999673421668595, 0.25700341327814935, 0.3691083409708919]
maxi score, test score, baseline:  -0.7546866666666666 0.3483333333333333 0.3483333333333333
probs:  [0.07392090358409927, 0.2999673421668595, 0.25700341327814935, 0.3691083409708919]
line 256 mcts: sample exp_bonus 1.6006900819566523
maxi score, test score, baseline:  -0.7546866666666666 0.3483333333333333 0.3483333333333333
probs:  [0.07392090358409927, 0.2999673421668595, 0.25700341327814935, 0.3691083409708919]
maxi score, test score, baseline:  -0.7546866666666666 0.3483333333333333 0.3483333333333333
probs:  [0.07392090358409927, 0.2999673421668595, 0.25700341327814935, 0.3691083409708919]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.7546866666666666 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7546866666666666 0.3483333333333333 0.3483333333333333
probs:  [0.07229621997846161, 0.2933643611938673, 0.2733567744915641, 0.3609826443361071]
actor:  0 policy actor:  0  step number:  45 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7521000000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07229621997846161, 0.2933643611938673, 0.2733567744915641, 0.3609826443361071]
actor:  1 policy actor:  1  step number:  56 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7521000000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07282685399961411, 0.2909131762236585, 0.27262242903356626, 0.36363754074316107]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7521000000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07282685399961411, 0.2909131762236585, 0.27262242903356626, 0.36363754074316107]
maxi score, test score, baseline:  -0.7521000000000001 0.3483333333333333 0.3483333333333333
probs:  [0.07282685399961411, 0.2909131762236585, 0.27262242903356626, 0.36363754074316107]
siam score:  -0.84041286
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
line 256 mcts: sample exp_bonus 0.43617126612770457
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  44 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7492333333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07282685399961411, 0.2909131762236585, 0.27262242903356626, 0.36363754074316107]
maxi score, test score, baseline:  -0.7492333333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07282685399961411, 0.2909131762236585, 0.27262242903356626, 0.36363754074316107]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.8712297699166023
maxi score, test score, baseline:  -0.7492333333333334 0.3483333333333333 0.3483333333333333
probs:  [0.0728250396044469, 0.29091696029944236, 0.27262941800525425, 0.36362858209085647]
maxi score, test score, baseline:  -0.7492333333333334 0.3483333333333333 0.3483333333333333
probs:  [0.0728250396044469, 0.29091696029944236, 0.27262941800525425, 0.36362858209085647]
maxi score, test score, baseline:  -0.7492333333333334 0.3483333333333333 0.3483333333333333
probs:  [0.0728250396044469, 0.29091696029944236, 0.27262941800525425, 0.36362858209085647]
line 256 mcts: sample exp_bonus 1.26804739667132
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.655]
 [0.559]
 [0.531]
 [0.531]
 [0.491]
 [0.5  ]] [[0.394]
 [0.394]
 [0.271]
 [0.363]
 [0.454]
 [0.645]
 [0.738]] [[-0.169]
 [-0.015]
 [-0.132]
 [-0.144]
 [-0.129]
 [-0.136]
 [-0.112]]
siam score:  -0.8300136
maxi score, test score, baseline:  -0.7492333333333334 0.3483333333333333 0.3483333333333333
probs:  [0.0728250396044469, 0.29091696029944236, 0.27262941800525425, 0.36362858209085647]
actor:  1 policy actor:  1  step number:  71 total reward:  0.07999999999999918  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.7492333333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07282324263615725, 0.2909207080299877, 0.27263633984936403, 0.363619709484491]
maxi score, test score, baseline:  -0.7492333333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07282324263615725, 0.2909207080299877, 0.27263633984936403, 0.363619709484491]
maxi score, test score, baseline:  -0.7492333333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07282324263615725, 0.2909207080299877, 0.27263633984936403, 0.363619709484491]
actor:  1 policy actor:  1  step number:  66 total reward:  0.19333333333333202  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.7492333333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07282146284487294, 0.2909244199364241, 0.2726431955283923, 0.36361092169031073]
Printing some Q and Qe and total Qs values:  [[-0.128]
 [-0.151]
 [-0.139]
 [-0.136]
 [-0.131]
 [-0.141]
 [-0.148]] [[0.494]
 [0.039]
 [0.217]
 [0.244]
 [0.241]
 [0.111]
 [0.215]] [[-0.165]
 [-0.492]
 [-0.36 ]
 [-0.34 ]
 [-0.337]
 [-0.434]
 [-0.372]]
maxi score, test score, baseline:  -0.7492333333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07297347396402272, 0.28944214792811906, 0.2732131848528938, 0.36437119325496437]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.7492333333333334 0.3483333333333333 0.3483333333333333
probs:  [0.07297347396402272, 0.28944214792811906, 0.2732131848528938, 0.36437119325496437]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07297347396402272, 0.28944214792811906, 0.2732131848528938, 0.36437119325496437]
actor:  1 policy actor:  1  step number:  50 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7492333333333334 0.3483333333333333 0.3483333333333333
actor:  0 policy actor:  1  step number:  46 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07580057415624852, 0.280492932611118, 0.26518914947801747, 0.3785173437546159]
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07580057415624852, 0.280492932611118, 0.26518914947801747, 0.3785173437546159]
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07580057415624852, 0.280492932611118, 0.26518914947801747, 0.3785173437546159]
start point for exploration sampling:  11715
actor:  1 policy actor:  1  step number:  39 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07836981381927396, 0.27215882600936175, 0.25809821157336743, 0.3913731485979968]
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07840586905946248, 0.27182353701057943, 0.2582171121027698, 0.3915534818271882]
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07840586905946248, 0.27182353701057943, 0.2582171121027698, 0.3915534818271882]
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07840586905946248, 0.27182353701057943, 0.2582171121027698, 0.3915534818271882]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07840586905946248, 0.27182353701057943, 0.2582171121027698, 0.3915534818271882]
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07843213941586728, 0.2714560065468935, 0.2584269788764409, 0.3916848751607983]
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.0784679248193321, 0.2711231705100985, 0.2585450458909966, 0.3918638587795728]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  57 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999932  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
Printing some Q and Qe and total Qs values:  [[ 0.15 ]
 [ 0.193]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [ 0.031]] [[2.392]
 [2.374]
 [2.697]
 [2.697]
 [2.697]
 [2.697]
 [2.569]] [[0.295]
 [0.321]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.272]]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.005]
 [-0.005]
 [ 0.034]
 [-0.005]
 [-0.005]
 [-0.005]] [[1.916]
 [1.916]
 [1.916]
 [2.672]
 [1.916]
 [1.916]
 [1.916]] [[-0.439]
 [-0.439]
 [-0.439]
 [-0.021]
 [-0.439]
 [-0.439]
 [-0.439]]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.078449987982502, 0.2712735286516065, 0.25850223728494415, 0.39177424608094735]
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.078449987982502, 0.2712735286516065, 0.25850223728494415, 0.39177424608094735]
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.078449987982502, 0.2712735286516065, 0.25850223728494415, 0.39177424608094735]
Printing some Q and Qe and total Qs values:  [[ 0.277]
 [-0.041]
 [-0.013]
 [ 0.123]
 [ 0.267]
 [-0.008]
 [-0.039]] [[1.878]
 [1.447]
 [2.616]
 [1.383]
 [1.488]
 [2.194]
 [1.492]] [[0.405]
 [0.113]
 [0.447]
 [0.184]
 [0.292]
 [0.335]
 [0.126]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  56 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
siam score:  -0.83758044
actor:  1 policy actor:  1  step number:  77 total reward:  0.28000000000000014  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07832882034880832, 0.269966106323914, 0.2605367584520681, 0.39116831487520964]
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07832882034880832, 0.269966106323914, 0.2605367584520681, 0.39116831487520964]
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
actor:  1 policy actor:  1  step number:  54 total reward:  0.3666666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07834418566837331, 0.26992488206843235, 0.26048639719689326, 0.39124453506630114]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07834418566837331, 0.26992488206843235, 0.26048639719689326, 0.39124453506630114]
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07834418566837331, 0.26992488206843235, 0.26048639719689326, 0.39124453506630114]
using another actor
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07834418566837331, 0.26992488206843235, 0.2604863971968932, 0.3912445350663012]
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07834418566837331, 0.26992488206843235, 0.2604863971968932, 0.3912445350663012]
using another actor
actor:  1 policy actor:  1  step number:  54 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07835568785800424, 0.26988919124848737, 0.2604530314050711, 0.3913020894884372]
maxi score, test score, baseline:  -0.7465 0.3483333333333333 0.3483333333333333
probs:  [0.07835568785800424, 0.26988919124848737, 0.2604530314050711, 0.3913020894884372]
Printing some Q and Qe and total Qs values:  [[0.845]
 [0.887]
 [0.845]
 [0.845]
 [0.845]
 [0.845]
 [0.845]] [[0.746]
 [0.758]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]] [[0.845]
 [0.887]
 [0.845]
 [0.845]
 [0.845]
 [0.845]
 [0.845]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.806]
 [0.716]
 [0.703]
 [0.673]
 [0.698]
 [0.714]] [[1.434]
 [1.265]
 [1.39 ]
 [0.844]
 [1.226]
 [0.955]
 [1.224]] [[0.708]
 [0.806]
 [0.716]
 [0.703]
 [0.673]
 [0.698]
 [0.714]]
Printing some Q and Qe and total Qs values:  [[-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]] [[0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]] [[-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  1  step number:  57 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.7437266666666668 0.3483333333333333 0.3483333333333333
probs:  [0.0804238354680354, 0.2634708326228813, 0.25445458644646846, 0.4016507454626148]
maxi score, test score, baseline:  -0.7437266666666668 0.3483333333333333 0.3483333333333333
probs:  [0.08045564906122564, 0.26316224099353597, 0.2545721499382947, 0.40180996000694374]
maxi score, test score, baseline:  -0.7437266666666668 0.3483333333333333 0.3483333333333333
probs:  [0.08045564906122564, 0.26316224099353597, 0.2545721499382947, 0.40180996000694374]
maxi score, test score, baseline:  -0.7437266666666668 0.3483333333333333 0.3483333333333333
probs:  [0.08045564906122564, 0.26316224099353597, 0.2545721499382947, 0.40180996000694374]
maxi score, test score, baseline:  -0.7437266666666668 0.3483333333333333 0.3483333333333333
probs:  [0.08045564906122564, 0.26316224099353597, 0.2545721499382947, 0.40180996000694374]
maxi score, test score, baseline:  -0.7437266666666668 0.3483333333333333 0.3483333333333333
probs:  [0.08045564906122564, 0.26316224099353597, 0.2545721499382947, 0.40180996000694374]
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]] [[0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]] [[0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]]
maxi score, test score, baseline:  -0.7437266666666668 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7437266666666668 0.3483333333333333 0.3483333333333333
probs:  [0.08045564906122564, 0.26316224099353597, 0.2545721499382947, 0.40180996000694374]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7437266666666668 0.3483333333333333 0.3483333333333333
probs:  [0.08181220950553421, 0.25895930597419164, 0.25063059887750877, 0.40859788564276545]
maxi score, test score, baseline:  -0.7437266666666668 0.3483333333333333 0.3483333333333333
probs:  [0.08181220950553421, 0.25895930597419164, 0.25063059887750877, 0.40859788564276545]
maxi score, test score, baseline:  -0.7437266666666668 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7437266666666668 0.3483333333333333 0.3483333333333333
probs:  [0.08181220950553421, 0.25895930597419164, 0.25063059887750877, 0.40859788564276545]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.522]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[0.388]
 [1.204]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[0.478]
 [0.522]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]]
siam score:  -0.8423468
maxi score, test score, baseline:  -0.7437266666666668 0.3483333333333333 0.3483333333333333
probs:  [0.08181220950553421, 0.25895930597419164, 0.25063059887750866, 0.40859788564276545]
actor:  0 policy actor:  0  step number:  52 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7413666666666667 0.3483333333333333 0.3483333333333333
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.739]
 [0.714]
 [0.716]
 [0.717]
 [0.714]
 [0.725]] [[-0.347]
 [ 0.053]
 [-0.504]
 [-0.571]
 [-0.482]
 [-0.327]
 [-0.443]] [[0.729]
 [0.739]
 [0.714]
 [0.716]
 [0.717]
 [0.714]
 [0.725]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  51 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08181220950553421, 0.25895930597419164, 0.25063059887750866, 0.40859788564276545]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6533333333333337  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
siam score:  -0.8396945
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08440508006429648, 0.25073361351827067, 0.24328907303942232, 0.4215722333780104]
using explorer policy with actor:  0
from probs:  [0.08440508006429648, 0.25073361351827067, 0.24328907303942232, 0.4215722333780104]
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08440508006429648, 0.25073361351827067, 0.24328907303942232, 0.4215722333780104]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08440226279976785, 0.25074154845671326, 0.2432979642100128, 0.42155822453350605]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08440226279976785, 0.25074154845671326, 0.24329796421001282, 0.42155822453350605]
using explorer policy with actor:  1
siam score:  -0.8442161
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08440226279976785, 0.25074154845671326, 0.2432979642100128, 0.42155822453350605]
from probs:  [0.08440226279976785, 0.25074154845671326, 0.2432979642100128, 0.42155822453350605]
actor:  1 policy actor:  1  step number:  51 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.167
from probs:  [0.08440226279976785, 0.25074154845671326, 0.2432979642100128, 0.42155822453350605]
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08439947107106627, 0.2507494114724757, 0.24330677479058696, 0.4215443426658711]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.848]
 [0.871]
 [0.873]
 [0.848]
 [0.848]
 [0.848]
 [0.864]] [[0.734]
 [0.155]
 [0.425]
 [0.734]
 [0.734]
 [0.734]
 [0.294]] [[0.848]
 [0.871]
 [0.873]
 [0.848]
 [0.848]
 [0.848]
 [0.864]]
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08439947107106627, 0.2507494114724757, 0.24330677479058696, 0.4215443426658711]
Sims:  50 1 epoch:  79084 pick best:  False frame count:  79084
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08439947107106627, 0.2507494114724757, 0.24330677479058696, 0.4215443426658711]
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08443213938412564, 0.25045905908885613, 0.243401060440077, 0.4217077410869412]
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08443213938412564, 0.25045905908885613, 0.243401060440077, 0.4217077410869412]
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08443213938412564, 0.25045905908885613, 0.243401060440077, 0.4217077410869412]
siam score:  -0.84332967
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08442936764976311, 0.2504668908235139, 0.2434097830061603, 0.4216939585205627]
actor:  1 policy actor:  1  step number:  46 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7388066666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08445182528384819, 0.25026728535730636, 0.24347460356113523, 0.4218062857977103]
actor:  0 policy actor:  0  step number:  53 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.7362466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08444907490522163, 0.25027507378638925, 0.2434832419668105, 0.42179260934157864]
maxi score, test score, baseline:  -0.7362466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08444907490522163, 0.25027507378638925, 0.2434832419668105, 0.42179260934157864]
maxi score, test score, baseline:  -0.7362466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08444907490522163, 0.25027507378638925, 0.2434832419668105, 0.42179260934157864]
maxi score, test score, baseline:  -0.7362466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08444907490522163, 0.25027507378638925, 0.2434832419668105, 0.42179260934157864]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7362466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08444907490522163, 0.25027507378638925, 0.2434832419668105, 0.42179260934157864]
maxi score, test score, baseline:  -0.7362466666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08444907490522163, 0.25027507378638925, 0.2434832419668105, 0.42179260934157864]
maxi score, test score, baseline:  -0.7362466666666667 0.3483333333333333 0.3483333333333333
actor:  0 policy actor:  0  step number:  77 total reward:  0.24  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08444907490522163, 0.25027507378638925, 0.2434832419668105, 0.42179260934157864]
actor:  1 policy actor:  1  step number:  72 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[ 0.015]
 [-0.026]
 [-0.085]
 [-0.036]
 [ 0.003]
 [-0.016]
 [-0.022]] [[2.739]
 [2.785]
 [1.739]
 [2.697]
 [2.111]
 [3.15 ]
 [2.929]] [[ 0.104]
 [ 0.077]
 [-0.33 ]
 [ 0.038]
 [-0.117]
 [ 0.209]
 [ 0.129]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]] [[0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]] [[-0.34]
 [-0.34]
 [-0.34]
 [-0.34]
 [-0.34]
 [-0.34]
 [-0.34]]
maxi score, test score, baseline:  -0.7337666666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08447877673093238, 0.24999456031310724, 0.24358541319399296, 0.42194124976196745]
maxi score, test score, baseline:  -0.7337666666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08447877673093238, 0.24999456031310724, 0.24358541319399296, 0.42194124976196745]
siam score:  -0.83519167
siam score:  -0.83209616
maxi score, test score, baseline:  -0.7337666666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08451108532914893, 0.24970738588263924, 0.24367867971318768, 0.4221028490750242]
maxi score, test score, baseline:  -0.7337666666666667 0.3483333333333333 0.3483333333333333
probs:  [0.08451108532914893, 0.24970738588263924, 0.24367867971318768, 0.4221028490750242]
maxi score, test score, baseline:  -0.7337666666666667 0.3483333333333333 0.3483333333333333
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.02 ]] [[1.979]
 [1.921]
 [1.979]
 [1.979]
 [1.979]
 [1.979]
 [2.115]] [[-0.335]
 [-0.344]
 [-0.335]
 [-0.335]
 [-0.335]
 [-0.335]
 [-0.321]]
actor:  0 policy actor:  1  step number:  66 total reward:  0.08666666666666556  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.08451108532914893, 0.24970738588263924, 0.24367867971318768, 0.4221028490750242]
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.08454327557082014, 0.24942126346232882, 0.243771604567822, 0.422263856399029]
actor:  1 policy actor:  1  step number:  55 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.08454055901935226, 0.24942902835574873, 0.24378006482201792, 0.422250347802881]
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.142]
 [0.3  ]
 [0.142]
 [0.142]
 [0.291]
 [0.142]] [[2.043]
 [2.043]
 [2.047]
 [2.043]
 [2.043]
 [1.516]
 [2.043]] [[ 0.067]
 [ 0.067]
 [ 0.225]
 [ 0.067]
 [ 0.067]
 [-0.045]
 [ 0.067]]
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.08454055901935226, 0.24942902835574873, 0.24378006482201792, 0.422250347802881]
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.08454055901935226, 0.24942902835574873, 0.24378006482201792, 0.422250347802881]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  50 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.0845314880286093, 0.24940223381627963, 0.24386130112743276, 0.4222049770276783]
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.0845314880286093, 0.24940223381627963, 0.24386130112743276, 0.4222049770276783]
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.0845314880286093, 0.24940223381627963, 0.24386130112743276, 0.4222049770276783]
actor:  1 policy actor:  1  step number:  54 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.0845314880286093, 0.24940223381627963, 0.24386130112743276, 0.4222049770276783]
from probs:  [0.0845314880286093, 0.24940223381627963, 0.24386130112743276, 0.4222049770276783]
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
line 256 mcts: sample exp_bonus 0.28263819042846405
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.0845314880286093, 0.24940223381627963, 0.24386130112743276, 0.4222049770276783]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[-0.142]
 [-0.128]
 [-0.134]
 [-0.146]
 [-0.134]
 [-0.134]
 [-0.134]] [[0.019]
 [0.776]
 [0.264]
 [0.005]
 [0.264]
 [0.264]
 [0.264]] [[-0.587]
 [-0.195]
 [-0.456]
 [-0.598]
 [-0.456]
 [-0.456]
 [-0.456]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.609]
 [0.513]
 [0.41 ]
 [0.399]
 [0.396]
 [0.533]] [[1.058]
 [0.954]
 [2.044]
 [0.988]
 [0.787]
 [0.806]
 [0.992]] [[-0.11 ]
 [ 0.029]
 [ 0.296]
 [-0.158]
 [-0.237]
 [-0.233]
 [-0.034]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.0845314880286093, 0.24940223381627963, 0.24386130112743276, 0.4222049770276783]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]] [[0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.209]
 [ 0.124]
 [ 0.124]
 [ 0.124]
 [ 0.124]
 [ 0.124]] [[1.12 ]
 [0.934]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[-0.005]
 [ 0.209]
 [ 0.124]
 [ 0.124]
 [ 0.124]
 [ 0.124]
 [ 0.124]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  65 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.743]
 [0.586]
 [0.568]
 [0.559]
 [0.56 ]
 [0.548]] [[1.386]
 [1.756]
 [1.457]
 [1.378]
 [1.43 ]
 [1.421]
 [1.696]] [[0.548]
 [0.743]
 [0.586]
 [0.568]
 [0.559]
 [0.56 ]
 [0.548]]
Printing some Q and Qe and total Qs values:  [[0.938]
 [0.972]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]] [[1.221]
 [0.694]
 [1.221]
 [1.221]
 [1.221]
 [1.221]
 [1.221]] [[0.938]
 [0.972]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.08456354868695941, 0.24911721615635715, 0.24395389891752836, 0.42236533623915506]
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.08456354868695941, 0.24911721615635715, 0.24395389891752836, 0.42236533623915506]
Printing some Q and Qe and total Qs values:  [[0.886]
 [0.939]
 [0.886]
 [0.886]
 [0.886]
 [0.886]
 [0.886]] [[2.271]
 [1.287]
 [2.271]
 [2.271]
 [2.271]
 [2.271]
 [2.271]] [[0.886]
 [0.939]
 [0.886]
 [0.886]
 [0.886]
 [0.886]
 [0.886]]
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.027]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.053]] [[1.023]
 [2.073]
 [1.023]
 [1.023]
 [1.023]
 [1.023]
 [1.007]] [[-0.617]
 [-0.238]
 [-0.617]
 [-0.617]
 [-0.617]
 [-0.617]
 [-0.619]]
maxi score, test score, baseline:  -0.7315933333333334 0.3483333333333333 0.3483333333333333
probs:  [0.08456354868695941, 0.24911721615635715, 0.24395389891752836, 0.42236533623915506]
actor:  0 policy actor:  0  step number:  26 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.64  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.64  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  0.62  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  0.6333333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  29 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  29 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  29 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  29 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  29 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  29 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  29 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  30 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  35 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.474]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[1.962]
 [1.191]
 [2.896]
 [2.896]
 [2.896]
 [2.896]
 [2.896]] [[0.175]
 [0.001]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.5
siam score:  -0.83432734
maxi score, test score, baseline:  -0.6660333333333335 0.639 0.639
probs:  [0.10671598657632049, 0.5295081750806465, 0.10674705221106, 0.25702878613197294]
siam score:  -0.8338809
maxi score, test score, baseline:  -0.6660333333333335 0.639 0.639
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.545]
 [0.316]
 [0.487]
 [0.439]
 [0.379]
 [0.51 ]] [[1.28 ]
 [0.829]
 [0.869]
 [0.935]
 [0.785]
 [0.641]
 [0.889]] [[-0.017]
 [-0.075]
 [-0.291]
 [-0.099]
 [-0.196]
 [-0.303]
 [-0.091]]
maxi score, test score, baseline:  -0.6660333333333335 0.639 0.639
probs:  [0.10671598657632049, 0.5295081750806465, 0.10674705221106, 0.25702878613197294]
actor:  1 policy actor:  1  step number:  61 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6660333333333335 0.639 0.639
probs:  [0.10668186374972218, 0.5293305360811181, 0.1067129872425598, 0.2572746129265999]
siam score:  -0.8350264
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.319]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]] [[1.874]
 [1.745]
 [1.874]
 [1.874]
 [1.874]
 [1.874]
 [1.874]] [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]]
maxi score, test score, baseline:  -0.6660333333333335 0.639 0.639
actor:  0 policy actor:  1  step number:  51 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  0.333
from probs:  [0.10664776408114732, 0.5291530176392495, 0.10667894539281639, 0.25752027288678675]
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
probs:  [0.10664776408114732, 0.5291530176392495, 0.10667894539281639, 0.25752027288678675]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  55 total reward:  0.41333333333333344  reward:  1.0 rdn_beta:  0.667
from probs:  [0.10664776408114732, 0.5291530176392495, 0.10667894539281639, 0.25752027288678675]
Printing some Q and Qe and total Qs values:  [[ 0.065]
 [ 0.065]
 [ 0.065]
 [ 0.065]
 [ 0.065]
 [ 0.065]
 [-0.   ]] [[1.79 ]
 [1.79 ]
 [1.79 ]
 [1.79 ]
 [1.79 ]
 [1.79 ]
 [1.687]] [[0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.132]]
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
probs:  [0.10664776408114732, 0.5291530176392495, 0.10667894539281639, 0.25752027288678675]
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
siam score:  -0.8360484
Printing some Q and Qe and total Qs values:  [[-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]] [[-0.461]
 [-0.461]
 [-0.461]
 [-0.461]
 [-0.461]
 [-0.461]
 [-0.461]]
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
probs:  [0.106613687547029, 0.5289756196323545, 0.10664492663830329, 0.2577657661823133]
Printing some Q and Qe and total Qs values:  [[-0.105]
 [-0.089]
 [-0.104]
 [-0.105]
 [-0.087]
 [-0.086]
 [-0.102]] [[-2.788]
 [-0.957]
 [-1.948]
 [-2.258]
 [-1.923]
 [-1.794]
 [-1.61 ]] [[-0.369]
 [ 0.039]
 [-0.186]
 [-0.254]
 [-0.169]
 [-0.141]
 [-0.112]]
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
probs:  [0.106613687547029, 0.5289756196323545, 0.10664492663830329, 0.2577657661823133]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
probs:  [0.10644365107068036, 0.5280904318401046, 0.10647517847297949, 0.2589907386162356]
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.071]
 [-0.09 ]
 [-0.08 ]
 [-0.092]
 [-0.088]
 [-0.081]] [[-1.245]
 [-0.602]
 [-1.139]
 [-1.389]
 [-1.371]
 [-1.233]
 [-1.019]] [[-0.672]
 [-0.562]
 [-0.67 ]
 [-0.703]
 [-0.711]
 [-0.684]
 [-0.642]]
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
actor:  1 policy actor:  1  step number:  61 total reward:  0.4800000000000004  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.5
from probs:  [0.10644365107068036, 0.5280904318401046, 0.10647517847297949, 0.2589907386162356]
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
probs:  [0.10644365107068036, 0.5280904318401046, 0.10647517847297949, 0.2589907386162356]
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
probs:  [0.10644365107068036, 0.5280904318401046, 0.10647517847297949, 0.2589907386162356]
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
probs:  [0.10637579760006743, 0.5277371954925973, 0.1064074400535841, 0.2594795668537511]
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
probs:  [0.10627418943903305, 0.5272082366146436, 0.10630600417765948, 0.2602115697686638]
maxi score, test score, baseline:  -0.6634466666666667 0.639 0.639
probs:  [0.10627418943903305, 0.5272082366146436, 0.10630600417765948, 0.2602115697686638]
start point for exploration sampling:  11715
actor:  0 policy actor:  0  step number:  35 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6603800000000002 0.639 0.639
probs:  [0.10620656509035413, 0.5268561930461407, 0.10623849449170283, 0.2606987473718024]
maxi score, test score, baseline:  -0.6603800000000002 0.639 0.639
probs:  [0.10620656509035413, 0.5268561930461407, 0.10623849449170283, 0.2606987473718024]
start point for exploration sampling:  11715
first move QE:  0.07581144865093704
maxi score, test score, baseline:  -0.6603800000000002 0.639 0.639
probs:  [0.10620656509035413, 0.5268561930461407, 0.10623849449170283, 0.2606987473718024]
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]] [[2.203]
 [2.203]
 [2.203]
 [2.203]
 [2.203]
 [2.203]
 [2.203]] [[0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.23333333333333195  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6603800000000002 0.639 0.639
probs:  [0.10620656509035413, 0.5268561930461407, 0.10623849449170283, 0.2606987473718024]
actor:  1 policy actor:  1  step number:  56 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6603800000000002 0.639 0.639
probs:  [0.10620656509035413, 0.5268561930461407, 0.10623849449170283, 0.2606987473718024]
maxi score, test score, baseline:  -0.6603800000000002 0.639 0.639
probs:  [0.10620656509035413, 0.5268561930461407, 0.10623849449170283, 0.2606987473718024]
actor:  0 policy actor:  1  step number:  58 total reward:  0.3533333333333325  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6576733333333334 0.639 0.639
probs:  [0.10620656509035413, 0.5268561930461407, 0.10623849449170283, 0.2606987473718024]
siam score:  -0.8401419
maxi score, test score, baseline:  -0.6576733333333334 0.639 0.639
probs:  [0.10620656509035413, 0.5268561930461407, 0.10623849449170283, 0.2606987473718024]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6576733333333334 0.639 0.639
probs:  [0.10620656509035413, 0.5268561930461407, 0.10623849449170283, 0.2606987473718024]
maxi score, test score, baseline:  -0.6576733333333334 0.639 0.639
probs:  [0.10620656509035413, 0.5268561930461407, 0.10623849449170283, 0.2606987473718024]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  52 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.667
from probs:  [0.10620656509035413, 0.5268561930461407, 0.10623849449170283, 0.2606987473718024]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.10620656509035413, 0.5268561930461407, 0.10623849449170283, 0.2606987473718024]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.10617278717405736, 0.5266803496048061, 0.1062047738486799, 0.2609420893724566]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.661]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]] [[3.078]
 [1.944]
 [3.078]
 [3.078]
 [3.078]
 [3.078]
 [3.078]] [[0.483]
 [0.278]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]]
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.10613903206561294, 0.5265046248981861, 0.10617107597483659, 0.26118526706136436]
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.10613903206561294, 0.5265046248981861, 0.10617107597483659, 0.26118526706136436]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.15124683452844617
actor:  1 policy actor:  1  step number:  43 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.1061052997419279, 0.5263290188060615, 0.10613740084711921, 0.2614282806048915]
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.1061052997419279, 0.5263290188060615, 0.10613740084711921, 0.2614282806048915]
actor:  1 policy actor:  1  step number:  55 total reward:  0.5333333333333338  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.10607159017994033, 0.526153531208375, 0.10610374844250518, 0.2616711301691795]
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.10607159017994033, 0.526153531208375, 0.10610374844250518, 0.2616711301691795]
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.10607159017994033, 0.526153531208375, 0.10610374844250518, 0.2616711301691795]
actor:  1 policy actor:  1  step number:  48 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.333
using another actor
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [-0.039]
 [-0.073]
 [-0.056]
 [-0.039]
 [-0.128]
 [-0.103]] [[0.859]
 [0.82 ]
 [0.728]
 [0.671]
 [0.577]
 [0.858]
 [1.244]] [[0.18 ]
 [0.162]
 [0.095]
 [0.081]
 [0.048]
 [0.118]
 [0.315]]
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.10600423924896593, 0.5258029110168994, 0.10603651171065046, 0.2621563380234841]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.6048267234017815
Printing some Q and Qe and total Qs values:  [[-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.139]] [[1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.286]] [[-0.449]
 [-0.449]
 [-0.449]
 [-0.449]
 [-0.449]
 [-0.449]
 [-0.358]]
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.1059369790888163, 0.5254527633665379, 0.10596936559571098, 0.26264089194893475]
siam score:  -0.83664584
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.1059369790888163, 0.5254527633665379, 0.10596936559571098, 0.26264089194893475]
UNIT TEST: sample policy line 217 mcts : [0.347 0.02  0.    0.082 0.469 0.041 0.041]
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.1059369790888163, 0.5254527633665379, 0.10596936559571098, 0.26264089194893475]
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.10590338299047569, 0.525277866445848, 0.10593582646235677, 0.26288292410131947]
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.10590338299047569, 0.525277866445848, 0.10593582646235677, 0.26288292410131947]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.10586980951611316, 0.525103087302647, 0.1059023099146196, 0.2631247932666203]
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  49 total reward:  0.5333333333333337  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.10586980951611316, 0.525103087302647, 0.1059023099146196, 0.2631247932666203]
using explorer policy with actor:  1
siam score:  -0.8420808
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
actor:  1 policy actor:  1  step number:  47 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.105802730347972, 0.5247538818731537, 0.10583534448480152, 0.26360804329407267]
Printing some Q and Qe and total Qs values:  [[0.92 ]
 [0.966]
 [0.908]
 [0.906]
 [0.906]
 [0.906]
 [0.933]] [[1.559]
 [1.561]
 [1.488]
 [1.71 ]
 [1.71 ]
 [1.71 ]
 [1.406]] [[0.92 ]
 [0.966]
 [0.908]
 [0.906]
 [0.906]
 [0.906]
 [0.933]]
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.021]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]] [[1.132]
 [1.921]
 [1.132]
 [1.132]
 [1.132]
 [1.132]
 [1.132]] [[-0.428]
 [-0.159]
 [-0.428]
 [-0.428]
 [-0.428]
 [-0.428]
 [-0.428]]
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.105802730347972, 0.5247538818731537, 0.10583534448480152, 0.26360804329407267]
maxi score, test score, baseline:  -0.6551266666666667 0.639 0.639
probs:  [0.105802730347972, 0.5247538818731537, 0.10583534448480152, 0.26360804329407267]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.47 ]
 [0.422]
 [0.439]
 [0.439]
 [0.438]
 [0.426]] [[1.278]
 [1.148]
 [1.171]
 [1.84 ]
 [1.285]
 [1.361]
 [1.323]] [[-0.111]
 [-0.124]
 [-0.165]
 [ 0.074]
 [-0.11 ]
 [-0.086]
 [-0.11 ]]
actor:  0 policy actor:  1  step number:  64 total reward:  0.19333333333333236  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6527400000000001 0.639 0.639
probs:  [0.105802730347972, 0.5247538818731537, 0.10583534448480152, 0.26360804329407267]
start point for exploration sampling:  11715
actor:  1 policy actor:  1  step number:  47 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6527400000000001 0.639 0.639
probs:  [0.10576922460859534, 0.524579455349484, 0.1058018955572001, 0.2638494244847205]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6527400000000001 0.639 0.639
probs:  [0.10570228070546427, 0.52423095409205, 0.10573506516303918, 0.26433170003944656]
maxi score, test score, baseline:  -0.6527400000000001 0.639 0.639
probs:  [0.10570228070546427, 0.52423095409205, 0.10573506516303918, 0.26433170003944656]
maxi score, test score, baseline:  -0.6527400000000001 0.639 0.639
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.477]
 [0.538]
 [0.477]
 [0.477]
 [0.422]
 [0.477]] [[1.055]
 [1.055]
 [0.831]
 [1.055]
 [1.055]
 [0.534]
 [1.055]] [[0.477]
 [0.477]
 [0.538]
 [0.477]
 [0.477]
 [0.422]
 [0.477]]
maxi score, test score, baseline:  -0.6527400000000001 0.639 0.639
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6527400000000001 0.639 0.639
probs:  [0.10566884249629561, 0.5240568791218647, 0.10570168365114221, 0.26457259473069744]
from probs:  [0.10566884249629561, 0.5240568791218647, 0.10570168365114221, 0.26457259473069744]
maxi score, test score, baseline:  -0.6527400000000001 0.639 0.639
maxi score, test score, baseline:  -0.6527400000000001 0.639 0.639
probs:  [0.10566884249629561, 0.5240568791218647, 0.10570168365114221, 0.26457259473069744]
maxi score, test score, baseline:  -0.6527400000000001 0.639 0.639
probs:  [0.10566884249629561, 0.5240568791218647, 0.10570168365114221, 0.26457259473069744]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.963]
 [0.98 ]
 [0.961]
 [0.963]
 [0.963]
 [0.963]
 [0.957]] [[0.666]
 [0.39 ]
 [0.488]
 [0.666]
 [0.666]
 [0.666]
 [0.464]] [[0.963]
 [0.98 ]
 [0.961]
 [0.963]
 [0.963]
 [0.963]
 [0.957]]
actor:  0 policy actor:  0  step number:  47 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.6501266666666667 0.639 0.639
probs:  [0.10566884249629561, 0.5240568791218647, 0.10570168365114221, 0.26457259473069744]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.6501266666666667 0.639 0.639
probs:  [0.10566884249629561, 0.5240568791218647, 0.10570168365114221, 0.26457259473069744]
using another actor
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.0066286391298886
maxi score, test score, baseline:  -0.6501266666666667 0.639 0.639
probs:  [0.10566884249629561, 0.5240568791218647, 0.10570168365114221, 0.26457259473069744]
maxi score, test score, baseline:  -0.6501266666666667 0.639 0.639
siam score:  -0.84319884
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.452126894686278
actor:  1 policy actor:  1  step number:  53 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6501266666666667 0.639 0.639
probs:  [0.10566884249629561, 0.5240568791218647, 0.10570168365114221, 0.26457259473069744]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  0 policy actor:  1  step number:  39 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.64714 0.639 0.639
probs:  [0.10566884249629561, 0.5240568791218647, 0.10570168365114221, 0.26457259473069744]
maxi score, test score, baseline:  -0.64714 0.639 0.639
probs:  [0.10566884249629561, 0.5240568791218647, 0.10570168365114221, 0.26457259473069744]
line 256 mcts: sample exp_bonus 0.25622733171495965
actor:  1 policy actor:  1  step number:  79 total reward:  0.06666666666666576  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.64714 0.639 0.639
probs:  [0.10563542675183264, 0.5238829211000169, 0.1056683245658608, 0.2648133275822897]
maxi score, test score, baseline:  -0.64714 0.639 0.639
probs:  [0.10563542675183264, 0.5238829211000169, 0.1056683245658608, 0.2648133275822897]
actor:  0 policy actor:  0  step number:  41 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  0.5933333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6440733333333334 0.639 0.639
probs:  [0.1056020334494444, 0.523709079908693, 0.10563498788460117, 0.2650538987572614]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [ 0.161]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[2.039]
 [2.466]
 [2.039]
 [2.039]
 [2.039]
 [2.039]
 [2.039]] [[-0.031]
 [ 0.341]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]]
line 256 mcts: sample exp_bonus 2.013704410664464
maxi score, test score, baseline:  -0.6440733333333334 0.639 0.639
probs:  [0.1056020334494444, 0.523709079908693, 0.10563498788460117, 0.2650538987572614]
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  40 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6411266666666667 0.639 0.639
probs:  [0.1056020334494444, 0.523709079908693, 0.10563498788460117, 0.2650538987572614]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[1.513]
 [1.513]
 [1.513]
 [1.513]
 [1.513]
 [1.513]
 [1.513]] [[0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.123]
 [-0.115]
 [-0.123]
 [-0.128]
 [-0.119]
 [-0.123]
 [-0.125]] [[-0.818]
 [-0.658]
 [-0.897]
 [-0.536]
 [-0.987]
 [-0.972]
 [-0.881]] [[-0.532]
 [-0.471]
 [-0.558]
 [-0.443]
 [-0.584]
 [-0.583]
 [-0.554]]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.678]
 [0.716]
 [0.623]
 [0.678]
 [0.691]
 [0.467]] [[ 1.64 ]
 [ 0.   ]
 [ 0.117]
 [-0.559]
 [ 0.   ]
 [-0.389]
 [ 0.002]] [[0.263]
 [0.678]
 [0.716]
 [0.623]
 [0.678]
 [0.691]
 [0.467]]
maxi score, test score, baseline:  -0.6411266666666667 0.639 0.639
probs:  [0.10550198796887376, 0.5231882561420952, 0.10553511203948543, 0.2657746438495456]
maxi score, test score, baseline:  -0.6411266666666667 0.639 0.639
probs:  [0.10550198796887376, 0.5231882561420952, 0.10553511203948543, 0.2657746438495456]
maxi score, test score, baseline:  -0.6411266666666667 0.639 0.639
probs:  [0.10550198796887376, 0.5231882561420952, 0.10553511203948543, 0.2657746438495456]
maxi score, test score, baseline:  -0.6411266666666667 0.639 0.639
probs:  [0.10550198796887376, 0.5231882561420952, 0.10553511203948543, 0.2657746438495456]
maxi score, test score, baseline:  -0.6411266666666667 0.639 0.639
probs:  [0.10550198796887376, 0.5231882561420952, 0.10553511203948543, 0.2657746438495456]
line 256 mcts: sample exp_bonus 0.9942099250318508
maxi score, test score, baseline:  -0.6411266666666667 0.639 0.639
probs:  [0.10550198796887376, 0.5231882561420952, 0.10553511203948543, 0.2657746438495456]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6411266666666667 0.639 0.639
probs:  [0.10550198796887376, 0.5231882561420952, 0.10553511203948543, 0.2657746438495456]
siam score:  -0.8393599
Printing some Q and Qe and total Qs values:  [[0.795]
 [0.873]
 [0.845]
 [0.852]
 [0.852]
 [0.852]
 [0.794]] [[-0.271]
 [-0.198]
 [-0.417]
 [-0.414]
 [-0.414]
 [-0.414]
 [-0.44 ]] [[0.795]
 [0.873]
 [0.845]
 [0.852]
 [0.852]
 [0.852]
 [0.794]]
actor:  0 policy actor:  1  step number:  55 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6387266666666668 0.639 0.639
probs:  [0.10550198796887376, 0.5231882561420952, 0.10553511203948543, 0.2657746438495456]
UNIT TEST: sample policy line 217 mcts : [0.592 0.102 0.02  0.02  0.102 0.061 0.102]
first move QE:  0.06324670868014863
maxi score, test score, baseline:  -0.6387266666666668 0.639 0.639
probs:  [0.10550198796887376, 0.5231882561420952, 0.10553511203948543, 0.2657746438495456]
actor:  1 policy actor:  1  step number:  73 total reward:  0.13333333333333297  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6387266666666668 0.639 0.639
probs:  [0.10543540277866374, 0.5228416222974976, 0.10546863975001797, 0.2662543351738206]
siam score:  -0.84254295
maxi score, test score, baseline:  -0.6387266666666668 0.639 0.639
probs:  [0.10536890681618426, 0.5224954529608661, 0.10540225653698765, 0.2667333836859621]
maxi score, test score, baseline:  -0.6387266666666668 0.639 0.639
probs:  [0.10536890681618426, 0.5224954529608661, 0.10540225653698765, 0.2667333836859621]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.021]
 [-0.014]
 [-0.013]
 [-0.026]
 [-0.011]
 [-0.037]] [[1.47 ]
 [1.879]
 [2.121]
 [1.053]
 [1.266]
 [2.039]
 [2.251]] [[-0.011]
 [ 0.128]
 [ 0.167]
 [-0.13 ]
 [-0.082]
 [ 0.146]
 [ 0.184]]
maxi score, test score, baseline:  -0.6387266666666668 0.639 0.639
probs:  [0.10536890681618426, 0.5224954529608661, 0.10540225653698765, 0.2667333836859621]
maxi score, test score, baseline:  -0.6387266666666668 0.639 0.639
probs:  [0.10536890681618426, 0.5224954529608661, 0.10540225653698765, 0.2667333836859621]
maxi score, test score, baseline:  -0.6387266666666668 0.639 0.639
probs:  [0.10536890681618426, 0.5224954529608661, 0.10540225653698765, 0.2667333836859621]
maxi score, test score, baseline:  -0.6387266666666668 0.639 0.639
probs:  [0.10536890681618426, 0.5224954529608661, 0.10540225653698765, 0.2667333836859621]
siam score:  -0.8433257
actor:  0 policy actor:  0  step number:  52 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.6360733333333334 0.639 0.639
probs:  [0.10536890681618426, 0.5224954529608661, 0.10540225653698765, 0.2667333836859621]
actor:  1 policy actor:  1  step number:  50 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6360733333333334 0.639 0.639
probs:  [0.10533569223931366, 0.5223225421913539, 0.10536909827820207, 0.2669726672911303]
actor:  1 policy actor:  1  step number:  50 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  0.333
siam score:  -0.84519315
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6360733333333334 0.639 0.639
probs:  [0.10523618185795669, 0.5218045040817031, 0.10526975662499449, 0.2676895574353457]
maxi score, test score, baseline:  -0.6360733333333334 0.639 0.639
probs:  [0.10523618185795669, 0.5218045040817031, 0.10526975662499449, 0.2676895574353457]
maxi score, test score, baseline:  -0.6360733333333334 0.639 0.639
maxi score, test score, baseline:  -0.6360733333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
using another actor
actor:  1 policy actor:  1  step number:  48 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6360733333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6360733333333334 0.639 0.639
maxi score, test score, baseline:  -0.6360733333333334 0.639 0.639
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]] [[1.184]
 [1.184]
 [1.184]
 [1.184]
 [1.184]
 [1.184]
 [1.184]] [[-0.27]
 [-0.27]
 [-0.27]
 [-0.27]
 [-0.27]
 [-0.27]
 [-0.27]]
maxi score, test score, baseline:  -0.6360733333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.093]
 [-0.118]
 [-0.085]
 [-0.124]
 [-0.155]
 [-0.079]] [[ 0.517]
 [ 0.51 ]
 [ 1.913]
 [-0.453]
 [ 1.144]
 [ 0.082]
 [ 0.379]] [[-0.022]
 [-0.021]
 [ 0.308]
 [-0.253]
 [ 0.113]
 [-0.173]
 [-0.043]]
maxi score, test score, baseline:  -0.6360733333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6360733333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6360733333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
actor:  0 policy actor:  1  step number:  54 total reward:  0.25999999999999934  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
Printing some Q and Qe and total Qs values:  [[-0.013]
 [ 0.08 ]
 [ 0.012]
 [-0.013]
 [-0.013]
 [-0.036]
 [-0.077]] [[2.642]
 [2.499]
 [2.9  ]
 [2.642]
 [2.642]
 [2.507]
 [2.632]] [[0.455]
 [0.454]
 [0.545]
 [0.455]
 [0.455]
 [0.404]
 [0.423]]
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
siam score:  -0.8387411
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
actor:  1 policy actor:  1  step number:  58 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
actor:  1 policy actor:  1  step number:  60 total reward:  0.3933333333333333  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6133333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6335533333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.577]
 [0.475]
 [0.499]
 [0.447]
 [0.525]
 [0.45 ]] [[0.16 ]
 [0.115]
 [0.368]
 [0.063]
 [0.497]
 [0.214]
 [0.604]] [[-0.227]
 [-0.18 ]
 [-0.24 ]
 [-0.266]
 [-0.247]
 [-0.215]
 [-0.225]]
actor:  0 policy actor:  0  step number:  47 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.667
siam score:  -0.83939683
actor:  1 policy actor:  1  step number:  48 total reward:  0.4466666666666663  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8383896
maxi score, test score, baseline:  -0.6309133333333333 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6309133333333333 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6309133333333333 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
actor:  1 policy actor:  1  step number:  59 total reward:  0.4400000000000003  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6309133333333333 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6309133333333333 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8402211
maxi score, test score, baseline:  -0.6309133333333333 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
from probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6309133333333333 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
actor:  1 policy actor:  1  step number:  63 total reward:  0.30666666666666653  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6309133333333333 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
siam score:  -0.8360295
actor:  1 policy actor:  1  step number:  44 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.144]
 [0.156]
 [0.114]
 [0.116]
 [0.13 ]
 [0.1  ]] [[1.887]
 [1.905]
 [1.894]
 [1.961]
 [1.937]
 [1.972]
 [2.062]] [[0.052]
 [0.105]
 [0.11 ]
 [0.112]
 [0.098]
 [0.135]
 [0.166]]
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.092]
 [ 0.225]
 [ 0.092]
 [ 0.092]
 [ 0.092]
 [ 0.092]
 [-0.007]] [[2.623]
 [2.199]
 [2.623]
 [2.623]
 [2.623]
 [2.623]
 [2.264]] [[0.188]
 [0.183]
 [0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.053]]
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.412]
 [0.337]
 [0.31 ]
 [0.31 ]
 [0.315]
 [0.31 ]] [[0.966]
 [1.171]
 [1.243]
 [1.079]
 [1.099]
 [1.218]
 [1.181]] [[-0.331]
 [-0.177]
 [-0.228]
 [-0.31 ]
 [-0.303]
 [-0.258]
 [-0.275]]
siam score:  -0.8294656
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.3533333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.082]
 [-0.187]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.143]] [[ 0.628]
 [ 0.265]
 [ 0.297]
 [ 0.628]
 [ 0.628]
 [ 0.628]
 [-0.022]] [[-0.117]
 [-0.272]
 [-0.349]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.448]]
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6309133333333334 0.639 0.639
actor:  0 policy actor:  1  step number:  59 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
probs:  [0.10516995250517666, 0.5214597226805096, 0.10520363956960492, 0.26816668524470894]
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
actor:  1 policy actor:  1  step number:  34 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
probs:  [0.08868120399444339, 0.4396110945905903, 0.24562118581817, 0.2260865155967962]
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
probs:  [0.08868120399444339, 0.4396110945905903, 0.24562118581817, 0.2260865155967962]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.44064174935761835
siam score:  -0.83254105
first move QE:  0.047134295529407996
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
probs:  [0.08868120399444339, 0.4396110945905903, 0.24562118581817, 0.2260865155967962]
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
probs:  [0.08868120399444339, 0.4396110945905903, 0.24562118581817, 0.2260865155967962]
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
probs:  [0.08868120399444339, 0.4396110945905903, 0.24562118581817, 0.2260865155967962]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08868120399444339, 0.4396110945905903, 0.24562118581817, 0.2260865155967962]
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
actor:  1 policy actor:  1  step number:  43 total reward:  0.6133333333333337  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.479]
 [0.479]
 [0.479]
 [0.421]
 [0.479]
 [0.51 ]] [[0.599]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.369]
 [0.51 ]
 [1.032]] [[ 0.024]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.108]
 [-0.002]
 [ 0.202]]
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.157]
 [0.117]
 [0.044]
 [0.117]
 [0.045]
 [0.117]] [[1.258]
 [1.765]
 [1.795]
 [1.292]
 [1.795]
 [1.427]
 [1.795]] [[-0.67 ]
 [-0.39 ]
 [-0.419]
 [-0.66 ]
 [-0.419]
 [-0.614]
 [-0.419]]
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
siam score:  -0.83373505
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.18117127192883803
maxi score, test score, baseline:  -0.6283533333333334 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.039]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.05 ]] [[3.744]
 [3.161]
 [2.845]
 [2.845]
 [2.845]
 [2.845]
 [3.305]] [[0.655]
 [0.53 ]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.559]]
actor:  0 policy actor:  0  step number:  56 total reward:  0.25999999999999934  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6258333333333334 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
maxi score, test score, baseline:  -0.6258333333333334 0.639 0.639
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.745]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]] [[1.657]
 [1.051]
 [1.657]
 [1.657]
 [1.657]
 [1.657]
 [1.657]] [[0.29 ]
 [0.173]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]]
maxi score, test score, baseline:  -0.6258333333333334 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.1951641817074443
line 256 mcts: sample exp_bonus -0.5707833078488627
line 256 mcts: sample exp_bonus 0.0054419519834205945
line 256 mcts: sample exp_bonus -0.37412926751639214
Printing some Q and Qe and total Qs values:  [[-0.105]
 [-0.113]
 [-0.105]
 [-0.113]
 [-0.113]
 [-0.105]
 [-0.112]] [[ 0.   ]
 [ 0.117]
 [ 0.   ]
 [-1.854]
 [-1.885]
 [ 0.   ]
 [-1.419]] [[ 0.273]
 [ 0.303]
 [ 0.273]
 [-0.287]
 [-0.297]
 [ 0.273]
 [-0.156]]
maxi score, test score, baseline:  -0.6258333333333334 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
maxi score, test score, baseline:  -0.6258333333333334 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
maxi score, test score, baseline:  -0.6258333333333334 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]] [[2.34]
 [2.34]
 [2.34]
 [2.34]
 [2.34]
 [2.34]
 [2.34]] [[0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]]
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.735]
 [0.659]
 [0.644]
 [0.654]
 [0.624]
 [0.705]] [[0.401]
 [0.361]
 [0.228]
 [0.405]
 [0.301]
 [0.181]
 [0.308]] [[-0.032]
 [ 0.054]
 [-0.044]
 [-0.029]
 [-0.037]
 [-0.087]
 [ 0.015]]
actor:  1 policy actor:  1  step number:  67 total reward:  0.3866666666666668  reward:  1.0 rdn_beta:  0.167
from probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
actor:  0 policy actor:  1  step number:  55 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6234866666666667 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.198]] [[0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [1.015]] [[0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.7  ]]
maxi score, test score, baseline:  -0.6234866666666667 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
actor:  0 policy actor:  0  step number:  52 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.448]
 [0.39 ]
 [0.323]
 [0.338]
 [0.34 ]
 [0.371]] [[0.764]
 [1.028]
 [0.878]
 [1.173]
 [0.978]
 [0.984]
 [0.804]] [[-0.217]
 [-0.127]
 [-0.21 ]
 [-0.228]
 [-0.246]
 [-0.243]
 [-0.241]]
maxi score, test score, baseline:  -0.6210466666666667 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
Printing some Q and Qe and total Qs values:  [[ 0.029]
 [-0.001]
 [ 0.029]
 [ 0.029]
 [ 0.029]
 [ 0.029]
 [-0.016]] [[1.595]
 [1.861]
 [1.595]
 [1.595]
 [1.595]
 [1.595]
 [1.196]] [[-0.308]
 [-0.161]
 [-0.308]
 [-0.308]
 [-0.308]
 [-0.308]
 [-0.619]]
maxi score, test score, baseline:  -0.6210466666666667 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.6210466666666667 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
maxi score, test score, baseline:  -0.6210466666666667 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
using explorer policy with actor:  1
from probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
actor:  1 policy actor:  1  step number:  54 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.6210466666666667 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
maxi score, test score, baseline:  -0.6210466666666667 0.639 0.639
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6210466666666667 0.639 0.639
using another actor
maxi score, test score, baseline:  -0.6210466666666667 0.639 0.639
actor:  0 policy actor:  0  step number:  65 total reward:  0.01333333333333242  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6190200000000001 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
maxi score, test score, baseline:  -0.6190200000000001 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
siam score:  -0.8278181
actor:  1 policy actor:  1  step number:  63 total reward:  0.27999999999999914  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6190200000000001 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  0 policy actor:  1  step number:  57 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  -0.6162733333333335 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2999999999999994  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
siam score:  -0.8252723
actor:  1 policy actor:  1  step number:  48 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  35 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6162733333333335 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6162733333333335 0.639 0.639
maxi score, test score, baseline:  -0.6162733333333335 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
actor:  0 policy actor:  1  step number:  58 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6138333333333335 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
maxi score, test score, baseline:  -0.6138333333333335 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.636]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.475]] [[1.102]
 [0.778]
 [1.102]
 [1.102]
 [1.102]
 [1.102]
 [0.749]] [[ 0.011]
 [ 0.016]
 [ 0.011]
 [ 0.011]
 [ 0.011]
 [ 0.011]
 [-0.155]]
siam score:  -0.82639265
maxi score, test score, baseline:  -0.6138333333333335 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
actor:  1 policy actor:  1  step number:  48 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.855]
 [0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]] [[2.291]
 [1.844]
 [1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]] [[0.881]
 [0.855]
 [0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]]
siam score:  -0.8261793
maxi score, test score, baseline:  -0.6138333333333335 0.639 0.639
maxi score, test score, baseline:  -0.6138333333333335 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
maxi score, test score, baseline:  -0.6138333333333335 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
maxi score, test score, baseline:  -0.6138333333333335 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
maxi score, test score, baseline:  -0.6138333333333335 0.639 0.639
actor:  0 policy actor:  1  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  41 total reward:  0.5600000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  59 total reward:  0.013333333333332642  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07848882827802472, 0.3890170808106582, 0.21736072153692343, 0.31513336937439357]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
siam score:  -0.8337187
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
actor:  1 policy actor:  1  step number:  72 total reward:  0.2599999999999988  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.661]
 [0.47 ]
 [0.456]
 [0.396]
 [0.509]
 [0.63 ]] [[1.229]
 [1.138]
 [1.609]
 [1.536]
 [1.11 ]
 [1.269]
 [1.451]] [[-0.03 ]
 [ 0.106]
 [ 0.072]
 [ 0.034]
 [-0.168]
 [-0.002]
 [ 0.18 ]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.081]
 [-0.075]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.106]] [[0.556]
 [0.9  ]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.552]] [[-0.111]
 [ 0.01 ]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.137]]
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
line 256 mcts: sample exp_bonus 1.471266913862252
actor:  1 policy actor:  1  step number:  37 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  42 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
siam score:  -0.82976884
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.651]
 [0.629]
 [0.629]
 [0.502]
 [0.629]
 [0.605]] [[1.311]
 [0.974]
 [2.101]
 [2.101]
 [1.216]
 [2.101]
 [1.372]] [[0.494]
 [0.399]
 [0.718]
 [0.718]
 [0.383]
 [0.718]
 [0.489]]
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
actor:  1 policy actor:  1  step number:  69 total reward:  0.07999999999999996  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.058]
 [-0.061]
 [-0.058]
 [-0.058]
 [-0.058]
 [-0.058]
 [-0.072]] [[ 0.772]
 [ 0.089]
 [ 0.772]
 [ 0.772]
 [ 0.772]
 [ 0.772]
 [-0.375]] [[-0.175]
 [-0.52 ]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.762]]
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
actor:  1 policy actor:  1  step number:  44 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.641]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]] [[-0.45 ]
 [ 0.455]
 [-0.45 ]
 [-0.45 ]
 [-0.45 ]
 [-0.45 ]
 [-0.45 ]] [[0.597]
 [0.641]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]]
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.6090066666666668 0.639 0.639
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.739]
 [0.671]
 [0.752]
 [0.658]
 [0.752]
 [0.657]] [[1.282]
 [1.912]
 [2.04 ]
 [1.282]
 [1.756]
 [1.282]
 [1.822]] [[0.752]
 [0.739]
 [0.671]
 [0.752]
 [0.658]
 [0.752]
 [0.657]]
actor:  0 policy actor:  0  step number:  52 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  57 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6065133333333335 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
line 256 mcts: sample exp_bonus -0.642665692171893
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.069]
 [-0.078]
 [-0.098]
 [-0.099]
 [-0.097]
 [-0.095]] [[ 0.133]
 [ 1.068]
 [ 0.654]
 [-0.025]
 [-0.082]
 [ 0.018]
 [ 0.162]] [[-0.631]
 [-0.292]
 [-0.439]
 [-0.686]
 [-0.706]
 [-0.671]
 [-0.62 ]]
maxi score, test score, baseline:  -0.6065133333333335 0.639 0.639
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  57 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6041933333333335 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.6041933333333335 0.639 0.639
actor:  1 policy actor:  1  step number:  55 total reward:  0.30666666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6041933333333335 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.6041933333333335 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.6041933333333335 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.6041933333333335 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
line 256 mcts: sample exp_bonus 0.8918090662995881
using explorer policy with actor:  1
using explorer policy with actor:  0
actor:  0 policy actor:  0  step number:  57 total reward:  0.18666666666666554  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  57 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5991000000000001 0.639 0.639
maxi score, test score, baseline:  -0.5991000000000001 0.639 0.639
maxi score, test score, baseline:  -0.5991000000000001 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.5991000000000001 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
using another actor
actor:  1 policy actor:  1  step number:  96 total reward:  0.15333333333333243  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5991000000000001 0.639 0.639
maxi score, test score, baseline:  -0.5991000000000001 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.5991000000000001 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.5991000000000001 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
Printing some Q and Qe and total Qs values:  [[-0.134]
 [-0.098]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.136]] [[ 0.467]
 [ 0.316]
 [-0.76 ]
 [-0.76 ]
 [-0.76 ]
 [-0.76 ]
 [ 0.026]] [[0.415]
 [0.406]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.326]]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.455]
 [0.441]
 [0.302]
 [0.329]
 [0.302]
 [0.3  ]] [[ 0.031]
 [-0.047]
 [-0.496]
 [-0.132]
 [-0.131]
 [-0.072]
 [ 0.167]] [[-0.392]
 [-0.266]
 [-0.355]
 [-0.433]
 [-0.405]
 [-0.423]
 [-0.385]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  -0.5991000000000001 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.067]
 [ 0.067]
 [ 0.067]
 [ 0.067]
 [ 0.067]
 [ 0.067]] [[2.39 ]
 [1.679]
 [1.679]
 [1.679]
 [1.679]
 [1.679]
 [1.679]] [[-0.315]
 [-0.48 ]
 [-0.48 ]
 [-0.48 ]
 [-0.48 ]
 [-0.48 ]
 [-0.48 ]]
maxi score, test score, baseline:  -0.5991000000000001 0.639 0.639
maxi score, test score, baseline:  -0.5991000000000001 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.5991000000000001 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.5991000000000001 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
from probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  53 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5966733333333333 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.217]
 [-0.046]
 [-0.102]
 [-0.106]
 [-0.193]
 [-0.095]
 [-0.23 ]] [[2.257]
 [2.285]
 [2.213]
 [2.176]
 [1.052]
 [1.395]
 [1.779]] [[0.473]
 [0.564]
 [0.514]
 [0.5  ]
 [0.1  ]
 [0.256]
 [0.314]]
actor:  0 policy actor:  0  step number:  44 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.59386 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.59386 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
siam score:  -0.82223684
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.59386 0.639 0.639
maxi score, test score, baseline:  -0.59386 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.59386 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
actor:  0 policy actor:  0  step number:  55 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
siam score:  -0.8160806
actor:  0 policy actor:  0  step number:  45 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]] [[2.225]
 [2.003]
 [2.003]
 [2.003]
 [2.003]
 [2.003]
 [2.003]] [[0.313]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]]
maxi score, test score, baseline:  -0.5886333333333333 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.5886333333333333 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2277747698936232
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7484081988334657
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.46 ]
 [0.451]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]] [[2.859]
 [2.491]
 [2.108]
 [2.491]
 [2.491]
 [2.491]
 [2.491]] [[0.368]
 [0.214]
 [0.085]
 [0.214]
 [0.214]
 [0.214]
 [0.214]]
actor:  1 policy actor:  1  step number:  69 total reward:  0.2399999999999991  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.5886333333333333 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.5886333333333333 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.5886333333333333 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.5886333333333333 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.5886333333333333 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.5886333333333333 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.5886333333333333 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
maxi score, test score, baseline:  -0.5886333333333333 0.639 0.639
Printing some Q and Qe and total Qs values:  [[ 0.016]
 [-0.023]
 [-0.048]
 [ 0.049]
 [ 0.015]
 [-0.059]
 [-0.046]] [[5.078]
 [2.319]
 [1.933]
 [2.484]
 [2.196]
 [1.598]
 [2.237]] [[ 0.692]
 [-0.04 ]
 [-0.149]
 [ 0.031]
 [-0.057]
 [-0.241]
 [-0.07 ]]
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.505]
 [0.466]
 [0.461]
 [0.459]
 [0.461]
 [0.469]] [[0.325]
 [1.064]
 [0.659]
 [0.366]
 [0.297]
 [0.445]
 [1.036]] [[0.463]
 [0.505]
 [0.466]
 [0.461]
 [0.459]
 [0.461]
 [0.469]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5886333333333333 0.639 0.639
probs:  [0.07862596941971554, 0.3897119996950153, 0.21708871986342257, 0.31457331102184666]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.87 ]
 [0.745]
 [0.763]
 [0.698]
 [0.7  ]
 [0.756]] [[1.824]
 [2.472]
 [2.828]
 [1.956]
 [2.228]
 [2.051]
 [2.285]] [[0.725]
 [0.87 ]
 [0.745]
 [0.763]
 [0.698]
 [0.7  ]
 [0.756]]
maxi score, test score, baseline:  -0.5886333333333333 0.639 0.639
probs:  [0.0748284082385086, 0.35473974938277414, 0.19941546906191804, 0.3710163733167992]
maxi score, test score, baseline:  -0.5886333333333333 0.639 0.639
probs:  [0.0748284082385086, 0.35473974938277414, 0.19941546906191804, 0.3710163733167992]
line 256 mcts: sample exp_bonus 3.6319913425216344
actor:  0 policy actor:  0  step number:  63 total reward:  0.053333333333332233  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5865266666666668 0.639 0.639
probs:  [0.0748284082385086, 0.35473974938277414, 0.19941546906191804, 0.3710163733167992]
maxi score, test score, baseline:  -0.5865266666666668 0.639 0.639
probs:  [0.0748284082385086, 0.35473974938277414, 0.19941546906191804, 0.3710163733167992]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5865266666666668 0.639 0.639
probs:  [0.0748284082385086, 0.35473974938277414, 0.19941546906191804, 0.3710163733167992]
using another actor
maxi score, test score, baseline:  -0.5865266666666668 0.639 0.639
probs:  [0.0748284082385086, 0.35473974938277414, 0.19941546906191804, 0.3710163733167992]
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.864]
 [0.743]
 [0.734]
 [0.718]
 [0.696]
 [0.711]] [[1.257]
 [1.657]
 [1.276]
 [1.367]
 [1.338]
 [0.866]
 [1.193]] [[0.718]
 [0.864]
 [0.743]
 [0.734]
 [0.718]
 [0.696]
 [0.711]]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  33 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5835933333333334 0.639 0.639
probs:  [0.07373057526363483, 0.3655730486938456, 0.19608518047176612, 0.3646111955707534]
maxi score, test score, baseline:  -0.5835933333333334 0.639 0.639
probs:  [0.07373057526363483, 0.3655730486938456, 0.19608518047176612, 0.3646111955707534]
maxi score, test score, baseline:  -0.5835933333333334 0.639 0.639
probs:  [0.07373057526363483, 0.3655730486938456, 0.19608518047176612, 0.3646111955707534]
actor:  0 policy actor:  1  step number:  58 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5809933333333334 0.639 0.639
probs:  [0.07373057526363483, 0.3655730486938456, 0.19608518047176612, 0.3646111955707534]
maxi score, test score, baseline:  -0.5809933333333334 0.639 0.639
probs:  [0.07373057526363483, 0.3655730486938456, 0.19608518047176612, 0.3646111955707534]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  41 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  0.5
Starting evaluation
maxi score, test score, baseline:  -0.5780600000000001 0.639 0.639
probs:  [0.07373057526363483, 0.3655730486938456, 0.19608518047176612, 0.3646111955707534]
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.842]
 [0.754]
 [0.696]
 [0.744]
 [0.673]
 [0.752]] [[1.781]
 [1.302]
 [1.61 ]
 [1.406]
 [1.842]
 [1.494]
 [1.792]] [[0.759]
 [0.842]
 [0.754]
 [0.696]
 [0.744]
 [0.673]
 [0.752]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5780600000000001 0.639 0.639
probs:  [0.07373057526363483, 0.3655730486938456, 0.19608518047176612, 0.3646111955707534]
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.9  ]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.735]] [[1.553]
 [1.21 ]
 [1.553]
 [1.553]
 [1.553]
 [1.553]
 [1.675]] [[0.763]
 [0.9  ]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.735]]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]] [[1.314]
 [1.314]
 [1.314]
 [1.314]
 [1.314]
 [1.314]
 [1.314]] [[0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.5780600000000001 0.639 0.639
Printing some Q and Qe and total Qs values:  [[-0.048]
 [ 0.255]
 [ 0.046]
 [ 0.186]
 [ 0.079]
 [ 0.181]
 [ 0.247]] [[1.431]
 [1.477]
 [2.312]
 [1.728]
 [1.521]
 [2.161]
 [1.617]] [[-0.114]
 [ 0.104]
 [ 0.33 ]
 [ 0.167]
 [ 0.008]
 [ 0.352]
 [ 0.159]]
maxi score, test score, baseline:  -0.5780600000000001 0.639 0.639
probs:  [0.07373057526363483, 0.3655730486938456, 0.19608518047176612, 0.3646111955707534]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5611533333333334 0.639 0.639
probs:  [0.07373057526363483, 0.3655730486938456, 0.19608518047176612, 0.3646111955707534]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.5106600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
maxi score, test score, baseline:  -0.5106600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
actor:  1 policy actor:  1  step number:  50 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  65 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5106600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5106600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
maxi score, test score, baseline:  -0.5106600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
maxi score, test score, baseline:  -0.5106600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
maxi score, test score, baseline:  -0.5106600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
maxi score, test score, baseline:  -0.5106600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.871]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]] [[1.505]
 [1.409]
 [1.505]
 [1.505]
 [1.505]
 [1.505]
 [1.505]] [[0.83 ]
 [0.871]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]]
maxi score, test score, baseline:  -0.5106600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
actor:  0 policy actor:  0  step number:  44 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8105841
actor:  1 policy actor:  1  step number:  104 total reward:  0.17999999999999827  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
maxi score, test score, baseline:  -0.5078733333333334 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.088]] [[1.938]
 [1.938]
 [1.938]
 [1.938]
 [1.938]
 [1.938]
 [2.076]] [[-0.401]
 [-0.401]
 [-0.401]
 [-0.401]
 [-0.401]
 [-0.401]
 [-0.297]]
start point for exploration sampling:  11715
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.002]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.009]] [[1.572]
 [1.326]
 [1.572]
 [1.572]
 [1.572]
 [1.572]
 [1.183]] [[0.42 ]
 [0.302]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.226]]
maxi score, test score, baseline:  -0.5078733333333334 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
line 256 mcts: sample exp_bonus 2.5899055249106926
maxi score, test score, baseline:  -0.5078733333333334 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.5078733333333334 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.025]
 [-0.027]
 [-0.033]
 [-0.052]
 [-0.03 ]
 [-0.031]] [[ 0.84 ]
 [ 1.035]
 [ 1.601]
 [-0.305]
 [ 0.095]
 [ 0.534]
 [ 0.917]] [[ 0.035]
 [ 0.128]
 [ 0.378]
 [-0.472]
 [-0.307]
 [-0.097]
 [ 0.072]]
maxi score, test score, baseline:  -0.5078733333333334 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.035]
 [-0.069]
 [-0.057]
 [-0.074]
 [-0.053]
 [-0.029]] [[ 0.102]
 [ 0.441]
 [ 0.061]
 [-0.21 ]
 [-0.254]
 [ 0.33 ]
 [ 1.141]] [[-0.311]
 [-0.097]
 [-0.343]
 [-0.487]
 [-0.526]
 [-0.175]
 [ 0.307]]
maxi score, test score, baseline:  -0.5078733333333334 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.   ]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.104]] [[2.458]
 [3.374]
 [3.247]
 [3.247]
 [3.247]
 [3.247]
 [1.562]] [[ 0.159]
 [ 0.32 ]
 [ 0.293]
 [ 0.293]
 [ 0.293]
 [ 0.293]
 [-0.009]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.5078733333333334 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
Printing some Q and Qe and total Qs values:  [[-0.018]
 [ 0.103]
 [ 0.103]
 [ 0.103]
 [ 0.312]
 [ 0.103]
 [-0.029]] [[2.975]
 [3.297]
 [3.297]
 [3.297]
 [3.107]
 [3.297]
 [5.294]] [[0.25 ]
 [0.358]
 [0.358]
 [0.358]
 [0.384]
 [0.358]
 [0.744]]
maxi score, test score, baseline:  -0.5078733333333334 0.6850000000000002 0.6850000000000002
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.109]
 [ 0.042]
 [-0.01 ]
 [-0.01 ]
 [ 0.041]
 [ 0.003]] [[2.529]
 [4.172]
 [4.186]
 [2.529]
 [2.529]
 [3.221]
 [3.815]] [[0.146]
 [0.668]
 [0.643]
 [0.146]
 [0.146]
 [0.367]
 [0.52 ]]
maxi score, test score, baseline:  -0.5078733333333334 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.5078733333333334 0.6850000000000002 0.6850000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
line 256 mcts: sample exp_bonus 2.3334363968462064
actor:  1 policy actor:  1  step number:  49 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.5078733333333334 0.6850000000000002 0.6850000000000002
actor:  0 policy actor:  1  step number:  59 total reward:  0.21333333333333315  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.449]
 [0.406]
 [0.452]
 [0.459]
 [0.424]
 [0.49 ]] [[4.035]
 [3.393]
 [2.954]
 [2.52 ]
 [2.515]
 [1.666]
 [1.398]] [[0.596]
 [0.416]
 [0.301]
 [0.242]
 [0.244]
 [0.052]
 [0.038]]
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  0.6133333333333337  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
actor:  1 policy actor:  1  step number:  36 total reward:  0.54  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.019]
 [0.072]
 [0.146]
 [0.084]
 [0.083]
 [0.071]] [[0.59 ]
 [2.639]
 [0.505]
 [2.119]
 [0.487]
 [0.501]
 [0.763]] [[-0.466]
 [ 0.16 ]
 [-0.494]
 [ 0.074]
 [-0.492]
 [-0.488]
 [-0.411]]
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.079]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]] [[-0.254]
 [ 0.683]
 [-0.254]
 [-0.254]
 [-0.254]
 [-0.254]
 [-0.254]] [[-0.271]
 [-0.025]
 [-0.271]
 [-0.271]
 [-0.271]
 [-0.271]
 [-0.271]]
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.10558934816274065, 0.5146906049971078, 0.1919518486924212, 0.1877681981477303]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
siam score:  -0.8154245
actor:  1 policy actor:  1  step number:  35 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using explorer policy with actor:  1
using another actor
from probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
actor:  1 policy actor:  1  step number:  61 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
actor:  1 policy actor:  1  step number:  56 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  -0.5054466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  47 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.50246 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.068]
 [-0.008]
 [-0.007]
 [-0.041]
 [-0.016]
 [-0.032]] [[2.076]
 [2.09 ]
 [2.136]
 [1.987]
 [0.154]
 [1.672]
 [2.041]] [[ 0.209]
 [ 0.27 ]
 [ 0.222]
 [ 0.184]
 [-0.323]
 [ 0.094]
 [ 0.178]]
maxi score, test score, baseline:  -0.50246 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
maxi score, test score, baseline:  -0.50246 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.50246 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.626]
 [0.656]
 [0.655]
 [0.658]
 [0.657]
 [0.662]] [[-4.002]
 [-0.453]
 [-4.097]
 [-4.172]
 [-4.043]
 [-3.9  ]
 [-4.217]] [[0.626]
 [0.626]
 [0.656]
 [0.655]
 [0.658]
 [0.657]
 [0.662]]
maxi score, test score, baseline:  -0.50246 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
maxi score, test score, baseline:  -0.50246 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
maxi score, test score, baseline:  -0.50246 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
maxi score, test score, baseline:  -0.50246 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
maxi score, test score, baseline:  -0.50246 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.76 ]
 [0.693]
 [0.684]
 [0.645]
 [0.64 ]
 [0.677]] [[0.878]
 [1.003]
 [0.702]
 [0.56 ]
 [1.305]
 [1.286]
 [1.1  ]] [[0.744]
 [0.76 ]
 [0.693]
 [0.684]
 [0.645]
 [0.64 ]
 [0.677]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.50246 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.50246 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  69 total reward:  0.026666666666665728  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  69 total reward:  0.23999999999999877  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
maxi score, test score, baseline:  -0.5004066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
maxi score, test score, baseline:  -0.5004066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
actor:  1 policy actor:  1  step number:  61 total reward:  0.2266666666666659  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.086]
 [-0.049]
 [-0.086]
 [-0.085]
 [-0.086]
 [-0.087]
 [-0.087]] [[-0.91 ]
 [ 0.436]
 [-1.074]
 [-1.119]
 [-1.043]
 [-1.016]
 [-0.825]] [[-0.669]
 [-0.407]
 [-0.697]
 [-0.704]
 [-0.691]
 [-0.688]
 [-0.656]]
maxi score, test score, baseline:  -0.5004066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
Printing some Q and Qe and total Qs values:  [[-0.018]
 [ 0.016]
 [-0.024]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.004]] [[1.504]
 [1.982]
 [1.03 ]
 [1.504]
 [1.504]
 [1.504]
 [2.481]] [[-0.535]
 [-0.422]
 [-0.62 ]
 [-0.535]
 [-0.535]
 [-0.535]
 [-0.358]]
actor:  0 policy actor:  0  step number:  43 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  -0.4975000000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4975000000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4975000000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.736]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.717]] [[0.128]
 [0.387]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.406]] [[-0.008]
 [ 0.082]
 [ 0.031]
 [ 0.031]
 [ 0.031]
 [ 0.031]
 [ 0.067]]
maxi score, test score, baseline:  -0.4975000000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.534]
 [0.232]
 [0.234]
 [0.161]
 [0.148]
 [0.328]] [[ 0.042]
 [ 0.243]
 [-0.114]
 [-0.098]
 [-0.107]
 [-0.108]
 [-0.009]] [[ 0.036]
 [ 0.185]
 [-0.177]
 [-0.172]
 [-0.246]
 [-0.26 ]
 [-0.063]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.3695989342725415
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.64 ]
 [0.565]
 [0.474]
 [0.473]
 [0.453]
 [0.488]] [[ 0.023]
 [ 0.067]
 [-0.014]
 [-0.093]
 [-0.129]
 [ 0.005]
 [-0.107]] [[ 0.009]
 [ 0.195]
 [ 0.106]
 [ 0.003]
 [-0.004]
 [-0.002]
 [ 0.014]]
maxi score, test score, baseline:  -0.4975000000000001 0.6850000000000002 0.6850000000000002
actor:  1 policy actor:  1  step number:  53 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]] [[2.048]
 [2.048]
 [2.048]
 [2.048]
 [2.048]
 [2.048]
 [2.048]] [[0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]]
maxi score, test score, baseline:  -0.4975000000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
maxi score, test score, baseline:  -0.4975000000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4975000000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
actor:  0 policy actor:  1  step number:  49 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.49472666666666676 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.49472666666666676 0.6850000000000002 0.6850000000000002
probs:  [0.09330547664652264, 0.454745750397004, 0.16960660090794616, 0.28234217204852724]
actor:  0 policy actor:  1  step number:  44 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 1.397647326879395
Printing some Q and Qe and total Qs values:  [[-0.093]
 [-0.035]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.198]] [[1.48 ]
 [1.158]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.288]] [[0.312]
 [0.155]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.078]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  31 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.49175333333333343 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.029]
 [-0.033]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.03 ]] [[-0.263]
 [ 0.328]
 [-0.135]
 [-0.319]
 [-0.35 ]
 [-0.297]
 [-0.182]] [[-0.269]
 [-0.068]
 [-0.228]
 [-0.292]
 [-0.302]
 [-0.284]
 [-0.242]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  -0.49175333333333343 0.6850000000000002 0.6850000000000002
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]] [[2.597]
 [2.597]
 [2.597]
 [2.597]
 [2.597]
 [2.597]
 [2.597]] [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]]
maxi score, test score, baseline:  -0.49175333333333343 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.49175333333333343 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.49175333333333343 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
start point for exploration sampling:  11715
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  0 policy actor:  0  step number:  55 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
using another actor
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.425]
 [0.357]
 [0.344]
 [0.345]
 [0.352]
 [0.343]] [[0.952]
 [0.946]
 [0.941]
 [0.694]
 [0.719]
 [1.01 ]
 [0.926]] [[-0.135]
 [-0.046]
 [-0.115]
 [-0.29 ]
 [-0.273]
 [-0.075]
 [-0.139]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.4893533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4893533333333334 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4893533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
line 256 mcts: sample exp_bonus 0.3097933294703768
maxi score, test score, baseline:  -0.4893533333333334 0.6850000000000002 0.6850000000000002
actor:  1 policy actor:  1  step number:  56 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 2.4645026518648997e-07
maxi score, test score, baseline:  -0.4893533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
siam score:  -0.8068515
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.761]
 [0.723]
 [0.726]
 [0.698]
 [0.668]
 [0.714]] [[-0.038]
 [-0.163]
 [-0.121]
 [-0.124]
 [-0.209]
 [-0.111]
 [ 0.01 ]] [[0.696]
 [0.761]
 [0.723]
 [0.726]
 [0.698]
 [0.668]
 [0.714]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.665]
 [0.622]
 [0.571]
 [0.523]
 [0.536]
 [0.597]] [[-0.542]
 [-0.269]
 [-0.762]
 [-0.533]
 [-0.411]
 [-0.414]
 [-0.551]] [[-0.099]
 [-0.04 ]
 [-0.166]
 [-0.178]
 [-0.205]
 [-0.193]
 [-0.155]]
line 256 mcts: sample exp_bonus 0.4083664119243622
from probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48678000000000005 0.6850000000000002 0.6850000000000002
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.111576751480566
maxi score, test score, baseline:  -0.48678000000000005 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.48678000000000005 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48678000000000005 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
using another actor
from probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.677]
 [0.585]
 [0.661]
 [0.585]
 [0.579]
 [0.582]] [[-0.427]
 [ 0.036]
 [-0.58 ]
 [ 0.   ]
 [-0.771]
 [-0.729]
 [-0.649]] [[0.585]
 [0.677]
 [0.585]
 [0.661]
 [0.585]
 [0.579]
 [0.582]]
maxi score, test score, baseline:  -0.48678000000000005 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.298]
 [0.286]
 [0.298]
 [0.298]
 [0.181]
 [0.298]] [[2.384]
 [2.384]
 [1.616]
 [2.384]
 [2.384]
 [1.866]
 [2.384]] [[-0.001]
 [-0.001]
 [-0.269]
 [-0.001]
 [-0.001]
 [-0.291]
 [-0.001]]
Printing some Q and Qe and total Qs values:  [[-0.103]
 [-0.113]
 [-0.106]
 [-0.108]
 [-0.107]
 [-0.112]
 [-0.11 ]] [[-1.402]
 [-0.443]
 [-1.971]
 [-2.161]
 [-2.107]
 [-2.07 ]
 [-1.99 ]] [[-0.272]
 [-0.047]
 [-0.413]
 [-0.46 ]
 [-0.447]
 [-0.441]
 [-0.42 ]]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4867800000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4867800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.345]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[0.679]
 [1.044]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]] [[-0.128]
 [ 0.073]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]]
maxi score, test score, baseline:  -0.4867800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  43 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.48387333333333343 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]] [[-1.41]
 [-1.41]
 [-1.41]
 [-1.41]
 [-1.41]
 [-1.41]
 [-1.41]] [[0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]]
maxi score, test score, baseline:  -0.48387333333333343 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
siam score:  -0.79161155
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.48387333333333343 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48387333333333343 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.48387333333333343 0.6850000000000002 0.6850000000000002
actor:  0 policy actor:  0  step number:  47 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.   ]
 [ 0.013]
 [ 0.013]
 [ 0.013]
 [ 0.013]
 [-0.003]] [[1.829]
 [1.737]
 [1.072]
 [1.072]
 [1.072]
 [1.072]
 [1.641]] [[-0.332]
 [-0.362]
 [-0.57 ]
 [-0.57 ]
 [-0.57 ]
 [-0.57 ]
 [-0.397]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.245]
 [-0.092]
 [-0.083]
 [-0.113]
 [-0.113]
 [-0.108]
 [-0.08 ]] [[-0.163]
 [ 0.001]
 [ 0.266]
 [ 0.43 ]
 [ 0.43 ]
 [ 0.686]
 [ 0.19 ]] [[-0.218]
 [-0.067]
 [ 0.004]
 [ 0.022]
 [ 0.022]
 [ 0.088]
 [-0.012]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.5539991910530837
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
siam score:  -0.8064319
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
actor:  1 policy actor:  1  step number:  54 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
actor:  1 policy actor:  1  step number:  48 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.948]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]] [[0.129]
 [0.362]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]] [[0.888]
 [0.948]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  0.333
deleting a thread, now have 2 threads
Frames:  93737 train batches done:  10982 episodes:  2883
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
siam score:  -0.8017354
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
actor:  1 policy actor:  1  step number:  48 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.7861299546580645
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.169]
 [0.166]
 [0.166]
 [0.118]
 [0.166]
 [0.204]] [[1.898]
 [2.446]
 [2.501]
 [2.501]
 [1.756]
 [2.501]
 [2.47 ]] [[0.316]
 [0.449]
 [0.461]
 [0.461]
 [0.247]
 [0.461]
 [0.481]]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.006]
 [ 0.021]
 [-0.006]
 [-0.006]
 [-0.021]
 [-0.006]] [[ 0.725]
 [-0.086]
 [ 0.615]
 [-0.086]
 [-0.086]
 [-0.301]
 [-0.086]] [[-0.048]
 [-0.189]
 [-0.044]
 [-0.189]
 [-0.189]
 [-0.239]
 [-0.189]]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
start point for exploration sampling:  11715
actor:  1 policy actor:  1  step number:  44 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.469]
 [0.431]
 [0.39 ]
 [0.378]
 [0.34 ]
 [0.4  ]] [[1.204]
 [1.219]
 [1.488]
 [1.089]
 [1.305]
 [0.364]
 [1.243]] [[-0.183]
 [-0.094]
 [-0.043]
 [-0.217]
 [-0.157]
 [-0.509]
 [-0.156]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8063101132951678
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  0.333
siam score:  -0.80786455
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.319]] [[1.329]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[0.618]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.40000000000000013  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[2.481]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]] [[0.765]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]]
using explorer policy with actor:  1
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4811800000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
actor:  1 policy actor:  1  step number:  58 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
actor:  1 policy actor:  1  step number:  38 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
actor:  1 policy actor:  1  step number:  56 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  0.667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8029764737174446
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
actor:  1 policy actor:  1  step number:  67 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
from probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.042]
 [-0.12 ]
 [-0.12 ]
 [-0.049]
 [-0.12 ]
 [-0.247]] [[0.929]
 [0.692]
 [0.755]
 [0.755]
 [1.555]
 [0.755]
 [1.365]] [[0.22 ]
 [0.135]
 [0.101]
 [0.101]
 [0.449]
 [0.101]
 [0.232]]
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.023]
 [-0.044]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.044]] [[0.565]
 [0.711]
 [0.36 ]
 [0.566]
 [0.566]
 [0.566]
 [1.383]] [[0.087]
 [0.16 ]
 [0.01 ]
 [0.079]
 [0.079]
 [0.079]
 [0.4  ]]
Printing some Q and Qe and total Qs values:  [[0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]] [[0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]] [[0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]]
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.302]
 [0.295]
 [0.288]
 [0.066]
 [0.162]
 [0.088]] [[1.819]
 [1.599]
 [1.457]
 [1.353]
 [1.264]
 [2.529]
 [2.029]] [[0.342]
 [0.367]
 [0.334]
 [0.308]
 [0.192]
 [0.508]
 [0.368]]
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
actor:  1 policy actor:  1  step number:  36 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.48118000000000016 0.6850000000000002 0.6850000000000002
siam score:  -0.8140557
actor:  0 policy actor:  0  step number:  65 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  47 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.47595333333333345 0.6850000000000002 0.6850000000000002
probs:  [0.09695594504937015, 0.28746952652462576, 0.137173932665403, 0.478400595760601]
maxi score, test score, baseline:  -0.47595333333333345 0.6850000000000002 0.6850000000000002
actor:  1 policy actor:  1  step number:  34 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  37 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.47595333333333345 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
from probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.47595333333333345 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.47595333333333345 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.47595333333333345 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.2694901823377059
maxi score, test score, baseline:  -0.47595333333333345 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]] [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]]
maxi score, test score, baseline:  -0.47595333333333345 0.6850000000000002 0.6850000000000002
actor:  0 policy actor:  1  step number:  47 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  56 total reward:  0.5266666666666671  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4731266666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4731266666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4731266666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4731266666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4731266666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  74 total reward:  0.3533333333333326  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4731266666666668 0.6850000000000002 0.6850000000000002
actor:  1 policy actor:  1  step number:  62 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4731266666666668 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4731266666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
first move QE:  -0.009923389095829916
maxi score, test score, baseline:  -0.4731266666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  0 policy actor:  1  step number:  51 total reward:  0.4  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8143227
actor:  1 policy actor:  1  step number:  54 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  11715
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.006]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.03 ]] [[1.958]
 [2.176]
 [1.958]
 [1.958]
 [1.958]
 [1.958]
 [2.005]] [[0.253]
 [0.362]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.261]]
maxi score, test score, baseline:  -0.4703266666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4703266666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4703266666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4703266666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4703266666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4703266666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.4703266666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4703266666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
siam score:  -0.8200716
maxi score, test score, baseline:  -0.4703266666666667 0.6850000000000002 0.6850000000000002
actor:  1 policy actor:  1  step number:  50 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  50 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  -0.4676466666666667 0.6850000000000002 0.6850000000000002
siam score:  -0.8219419
maxi score, test score, baseline:  -0.4676466666666667 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4676466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4676466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4676466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4676466666666667 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  0 policy actor:  1  step number:  50 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.167
from probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4649400000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4649400000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4649400000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4649400000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4649400000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  0 policy actor:  0  step number:  52 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.4625533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.718]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[0.411]
 [0.448]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[0.146]
 [0.283]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.146]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.01480773329529661
maxi score, test score, baseline:  -0.4625533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4625533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4625533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4625533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.558]
 [0.482]
 [0.383]
 [0.486]
 [0.488]
 [0.423]] [[1.235]
 [1.174]
 [1.156]
 [0.902]
 [1.268]
 [1.257]
 [1.341]] [[ 0.24 ]
 [ 0.309]
 [ 0.22 ]
 [-0.049]
 [ 0.298]
 [ 0.293]
 [ 0.285]]
using another actor
Printing some Q and Qe and total Qs values:  [[ 1.174]
 [-0.002]
 [ 1.174]
 [ 1.174]
 [ 1.174]
 [ 1.174]
 [ 1.174]] [[0.   ]
 [0.655]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[ 1.174]
 [-0.002]
 [ 1.174]
 [ 1.174]
 [ 1.174]
 [ 1.174]
 [ 1.174]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.25999999999999934  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4625533333333334 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4625533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4625533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.927]] [[1.697]
 [1.697]
 [1.697]
 [1.697]
 [1.697]
 [1.697]
 [1.399]] [[0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.927]]
maxi score, test score, baseline:  -0.4625533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.844]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.783]] [[1.229]
 [1.478]
 [1.229]
 [1.229]
 [1.229]
 [1.229]
 [1.407]] [[0.807]
 [0.844]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.783]]
maxi score, test score, baseline:  -0.4625533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  0 policy actor:  1  step number:  48 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  39 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.063]
 [ 0.016]
 [-0.063]
 [-0.063]
 [-0.012]
 [-0.011]
 [-0.022]] [[0.559]
 [0.84 ]
 [0.559]
 [0.559]
 [0.681]
 [0.481]
 [0.5  ]] [[-0.736]
 [-0.517]
 [-0.736]
 [-0.736]
 [-0.624]
 [-0.723]
 [-0.725]]
siam score:  -0.81650424
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  41 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
Printing some Q and Qe and total Qs values:  [[-0.081]
 [-0.087]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]] [[0.006]
 [1.098]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[-0.289]
 [ 0.142]
 [-0.289]
 [-0.289]
 [-0.289]
 [-0.289]
 [-0.289]]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
siam score:  -0.81901443
siam score:  -0.81895536
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  47 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  0.667
first move QE:  -0.016926621997470383
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[-0.129]
 [-0.031]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.128]
 [-0.129]] [[1.735]
 [1.618]
 [1.735]
 [1.735]
 [1.735]
 [0.534]
 [1.735]] [[ 0.118]
 [ 0.155]
 [ 0.118]
 [ 0.118]
 [ 0.118]
 [-0.438]
 [ 0.118]]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
using another actor
from probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5400000000000003  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
siam score:  -0.80603653
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.027]] [[1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]
 [2.23 ]] [[0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.598]]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
siam score:  -0.8048819
actor:  1 policy actor:  1  step number:  77 total reward:  0.173333333333332  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.115]
 [-0.085]
 [-0.084]
 [-0.115]
 [-0.115]
 [-0.085]
 [-0.086]] [[1.142]
 [0.778]
 [0.382]
 [1.142]
 [1.142]
 [0.56 ]
 [0.941]] [[-0.419]
 [-0.572]
 [-0.769]
 [-0.419]
 [-0.419]
 [-0.681]
 [-0.491]]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
line 256 mcts: sample exp_bonus 2.1688974445066993
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
start point for exploration sampling:  11715
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
siam score:  -0.80587274
actor:  1 policy actor:  1  step number:  48 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  49 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]] [[2.976]
 [2.976]
 [2.976]
 [2.976]
 [2.976]
 [2.976]
 [2.976]] [[-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4568600000000001 0.6850000000000002 0.6850000000000002
actor:  1 policy actor:  1  step number:  47 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]] [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]]
actor:  0 policy actor:  0  step number:  49 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4544066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  0 policy actor:  1  step number:  45 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4515000000000001 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  0 policy actor:  0  step number:  56 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]] [[2.395]
 [2.395]
 [2.395]
 [2.395]
 [2.395]
 [2.395]
 [2.395]] [[0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.656]
 [0.813]
 [0.656]
 [0.656]
 [0.618]
 [0.651]] [[ 0.266]
 [ 0.266]
 [ 0.781]
 [ 0.266]
 [ 0.266]
 [-0.07 ]
 [ 0.706]] [[0.656]
 [0.656]
 [0.813]
 [0.656]
 [0.656]
 [0.618]
 [0.651]]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.489]
 [0.423]
 [0.429]
 [0.418]
 [0.279]
 [0.216]] [[0.667]
 [0.562]
 [0.713]
 [0.512]
 [0.557]
 [0.182]
 [0.351]] [[ 0.264]
 [ 0.231]
 [ 0.215]
 [ 0.155]
 [ 0.158]
 [-0.106]
 [-0.112]]
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[-0.03 ]
 [-0.019]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]] [[0.763]
 [1.764]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]] [[0.177]
 [0.463]
 [0.177]
 [0.177]
 [0.177]
 [0.177]
 [0.177]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.586]
 [0.506]
 [0.467]
 [0.541]
 [0.559]
 [0.469]] [[0.528]
 [0.466]
 [0.361]
 [0.485]
 [0.437]
 [0.459]
 [0.586]] [[-0.046]
 [-0.018]
 [-0.151]
 [-0.128]
 [-0.078]
 [-0.049]
 [-0.076]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3199999999999992  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.553]
 [0.489]
 [0.516]
 [0.479]
 [0.479]
 [0.489]] [[1.043]
 [1.221]
 [0.985]
 [1.192]
 [1.126]
 [1.127]
 [0.985]] [[0.041]
 [0.226]
 [0.044]
 [0.174]
 [0.104]
 [0.104]
 [0.044]]
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  60 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.5
from probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[0.165]
 [0.217]
 [0.093]
 [0.165]
 [0.165]
 [0.165]
 [0.172]] [[0.723]
 [0.577]
 [1.474]
 [0.723]
 [0.723]
 [0.723]
 [0.366]] [[0.165]
 [0.217]
 [0.093]
 [0.165]
 [0.165]
 [0.165]
 [0.172]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4491933333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44919333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.44919333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44919333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
siam score:  -0.80070764
maxi score, test score, baseline:  -0.44919333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
start point for exploration sampling:  11715
siam score:  -0.80013597
maxi score, test score, baseline:  -0.44919333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
first move QE:  -0.020538104321366914
actor:  1 policy actor:  1  step number:  41 total reward:  0.5866666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.44919333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  50 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.44919333333333344 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.44919333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44919333333333344 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.44919333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44919333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44919333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44919333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44919333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  0 policy actor:  1  step number:  48 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.777092173432419
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[ 0.038]
 [ 0.029]
 [-0.019]
 [-0.002]
 [ 0.037]
 [-0.019]
 [-0.001]] [[1.785]
 [1.336]
 [1.635]
 [1.193]
 [1.481]
 [0.809]
 [1.894]] [[-0.599]
 [-0.683]
 [-0.681]
 [-0.737]
 [-0.65 ]
 [-0.819]
 [-0.619]]
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  53 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  41 total reward:  0.6133333333333337  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.01 ]
 [-0.034]
 [-0.045]
 [-0.081]
 [-0.061]
 [-0.056]] [[2.69 ]
 [1.664]
 [1.494]
 [1.115]
 [2.001]
 [1.888]
 [1.887]] [[ 0.409]
 [ 0.05 ]
 [-0.027]
 [-0.175]
 [ 0.136]
 [ 0.105]
 [ 0.108]]
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.631]
 [0.575]
 [0.539]
 [0.39 ]
 [0.525]
 [0.578]] [[1.376]
 [1.778]
 [1.287]
 [1.586]
 [0.702]
 [1.569]
 [2.957]] [[ 0.134]
 [ 0.352]
 [ 0.118]
 [ 0.218]
 [-0.23 ]
 [ 0.203]
 [ 0.797]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  47 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
from probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
start point for exploration sampling:  11715
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.03 ]
 [-0.046]
 [-0.042]
 [-0.042]
 [-0.062]
 [-0.048]] [[0.94 ]
 [1.026]
 [0.834]
 [0.94 ]
 [0.94 ]
 [0.751]
 [1.012]] [[-0.323]
 [-0.276]
 [-0.375]
 [-0.323]
 [-0.323]
 [-0.425]
 [-0.295]]
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.628]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.612]] [[1.688]
 [1.352]
 [1.688]
 [1.688]
 [1.688]
 [1.688]
 [1.539]] [[0.616]
 [0.628]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.612]]
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
siam score:  -0.79812044
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.867]
 [0.748]
 [0.749]
 [0.832]
 [0.72 ]
 [0.699]] [[0.708]
 [0.558]
 [0.545]
 [0.539]
 [0.189]
 [0.659]
 [0.808]] [[0.724]
 [0.867]
 [0.748]
 [0.749]
 [0.832]
 [0.72 ]
 [0.699]]
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
siam score:  -0.8062341
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  64 total reward:  0.16666666666666652  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.029]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]] [[1.166]
 [1.312]
 [1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]] [[-0.577]
 [-0.526]
 [-0.577]
 [-0.577]
 [-0.577]
 [-0.577]
 [-0.577]]
Printing some Q and Qe and total Qs values:  [[-0.015]
 [ 0.036]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.017]] [[1.69 ]
 [1.71 ]
 [1.697]
 [1.697]
 [1.697]
 [1.697]
 [1.809]] [[-0.516]
 [-0.459]
 [-0.522]
 [-0.522]
 [-0.522]
 [-0.522]
 [-0.479]]
Printing some Q and Qe and total Qs values:  [[-0.091]
 [-0.093]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]] [[-1.916]
 [-0.189]
 [-1.916]
 [-1.916]
 [-1.916]
 [-1.916]
 [-1.916]] [[0.179]
 [0.473]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]]
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6000000000000002  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  40 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.851]
 [0.653]
 [0.705]
 [0.652]
 [0.584]
 [0.772]] [[2.088]
 [0.984]
 [2.12 ]
 [2.062]
 [1.418]
 [2.55 ]
 [1.66 ]] [[0.598]
 [0.851]
 [0.653]
 [0.705]
 [0.652]
 [0.584]
 [0.772]]
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
siam score:  -0.81988525
maxi score, test score, baseline:  -0.4463533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4463533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4463533333333334 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4463533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4463533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6466666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.44635333333333344 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
first move QE:  -0.024534222634948973
maxi score, test score, baseline:  -0.4463533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  0 policy actor:  1  step number:  63 total reward:  0.19999999999999973  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.4439533333333334 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4439533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
using another actor
from probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  34 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.4439533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4439533333333334 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4439533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4439533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4439533333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4439533333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
first move QE:  -0.023457466023047106
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.372]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.251]] [[2.263]
 [1.84 ]
 [2.263]
 [2.263]
 [2.263]
 [2.263]
 [2.521]] [[-0.257]
 [-0.285]
 [-0.257]
 [-0.257]
 [-0.257]
 [-0.257]
 [-0.293]]
maxi score, test score, baseline:  -0.4439533333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.4439533333333335 0.6850000000000002 0.6850000000000002
Printing some Q and Qe and total Qs values:  [[0.202]
 [0.264]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]] [[0.467]
 [0.524]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]] [[0.202]
 [0.264]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]]
maxi score, test score, baseline:  -0.4439533333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.502]
 [0.429]
 [0.389]
 [0.396]
 [0.347]
 [0.381]] [[0.805]
 [0.439]
 [0.729]
 [0.629]
 [0.582]
 [1.117]
 [0.623]] [[ 0.115]
 [-0.007]
 [ 0.114]
 [ 0.007]
 [-0.018]
 [ 0.29 ]
 [-0.005]]
actor:  1 policy actor:  1  step number:  81 total reward:  0.3333333333333326  reward:  1.0 rdn_beta:  0.667
first move QE:  -0.023444862959078838
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]] [[-0.081]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]] [[-0.567]
 [-0.567]
 [-0.567]
 [-0.567]
 [-0.567]
 [-0.567]
 [-0.567]]
maxi score, test score, baseline:  -0.4439533333333335 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  0 policy actor:  0  step number:  67 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.009]
 [-0.026]
 [-0.077]
 [-0.084]
 [-0.087]
 [-0.026]] [[ 1.019]
 [ 1.263]
 [ 1.019]
 [-0.063]
 [ 0.221]
 [-0.03 ]
 [ 1.019]] [[ 0.058]
 [ 0.155]
 [ 0.058]
 [-0.356]
 [-0.258]
 [-0.35 ]
 [ 0.058]]
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  60 total reward:  0.12666666666666637  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
siam score:  -0.81517255
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.372]
 [0.4  ]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[1.367]
 [1.413]
 [1.941]
 [1.413]
 [1.413]
 [1.413]
 [1.413]] [[ 0.086]
 [-0.061]
 [ 0.142]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]]
siam score:  -0.8162029
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6600000000000004  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.872]
 [0.885]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]] [[-0.271]
 [-0.14 ]
 [-0.271]
 [-0.271]
 [-0.271]
 [-0.271]
 [-0.271]] [[0.872]
 [0.885]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5333333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[0.892]
 [0.982]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]] [[1.912]
 [1.247]
 [1.912]
 [1.912]
 [1.912]
 [1.912]
 [1.912]] [[0.892]
 [0.982]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]]
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
maxi score, test score, baseline:  -0.4416066666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  0 policy actor:  1  step number:  54 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4391133333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4391133333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4391133333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4391133333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  0 policy actor:  1  step number:  37 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  52 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.43320666666666674 0.6850000000000002 0.6850000000000002
from probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.43320666666666674 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.43320666666666674 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.826]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.747]] [[0.517]
 [0.909]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.59 ]] [[0.746]
 [0.826]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.747]]
maxi score, test score, baseline:  -0.43320666666666674 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
UNIT TEST: sample policy line 217 mcts : [0.061 0.429 0.102 0.02  0.041 0.082 0.265]
actor:  0 policy actor:  0  step number:  52 total reward:  0.36666666666666603  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4304733333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4304733333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  0 policy actor:  0  step number:  63 total reward:  0.15999999999999892  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.4281533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.4281533333333334 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  42 total reward:  0.47333333333333305  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8069075
maxi score, test score, baseline:  -0.4281533333333334 0.6850000000000002 0.6850000000000002
actor:  0 policy actor:  0  step number:  34 total reward:  0.5533333333333332  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4250466666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4250466666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
maxi score, test score, baseline:  -0.4250466666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.668]
 [0.477]
 [0.493]
 [0.464]
 [0.457]
 [0.51 ]] [[ 0.154]
 [-0.011]
 [ 0.05 ]
 [-0.135]
 [-0.073]
 [-0.052]
 [ 0.051]] [[ 0.133]
 [ 0.213]
 [ 0.063]
 [-0.044]
 [-0.032]
 [-0.025]
 [ 0.097]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6066666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4250466666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
siam score:  -0.80651414
maxi score, test score, baseline:  -0.4250466666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  0.333
Starting evaluation
actor:  1 policy actor:  1  step number:  72 total reward:  0.04666666666666586  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]] [[2.12]
 [2.12]
 [2.12]
 [2.12]
 [2.12]
 [2.12]
 [2.12]] [[0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]]
Printing some Q and Qe and total Qs values:  [[0.916]
 [0.965]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]] [[2.293]
 [2.143]
 [2.293]
 [2.293]
 [2.293]
 [2.293]
 [2.293]] [[0.916]
 [0.965]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]]
maxi score, test score, baseline:  -0.4250466666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]] [[0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]] [[0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  30 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  30 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  32 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  34 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  35 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  35 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  38 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8107046
actor:  1 policy actor:  1  step number:  44 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3687666666666668 0.6850000000000002 0.6850000000000002
probs:  [0.09587625404437211, 0.29541146086192605, 0.13564568492042256, 0.4730666001732793]
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.858]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.84 ]] [[1.631]
 [1.397]
 [1.631]
 [1.631]
 [1.631]
 [1.631]
 [2.36 ]] [[0.808]
 [0.858]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.84 ]]
actor:  0 policy actor:  0  step number:  59 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.728]
 [0.834]
 [0.841]
 [0.728]
 [0.798]
 [0.655]] [[ 1.677]
 [ 1.356]
 [ 1.099]
 [ 0.328]
 [ 1.356]
 [-0.212]
 [ 0.735]] [[0.511]
 [0.728]
 [0.834]
 [0.841]
 [0.728]
 [0.798]
 [0.655]]
actor:  0 policy actor:  1  step number:  60 total reward:  0.15333333333333266  reward:  1.0 rdn_beta:  0.667
using another actor
actor:  0 policy actor:  0  step number:  67 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3608066666666668 0.6850000000000002 0.6850000000000002
actor:  0 policy actor:  0  step number:  69 total reward:  0.4  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08837360816158232, 0.2891500439057629, 0.18182039783682283, 0.440655950095832]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08837360816158232, 0.2891500439057629, 0.18182039783682283, 0.440655950095832]
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08837360816158232, 0.2891500439057629, 0.18182039783682283, 0.440655950095832]
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08837360816158232, 0.2891500439057629, 0.18182039783682283, 0.440655950095832]
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08837360816158232, 0.2891500439057629, 0.18182039783682283, 0.440655950095832]
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08837360816158232, 0.2891500439057629, 0.18182039783682283, 0.440655950095832]
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08837360816158232, 0.2891500439057629, 0.18182039783682283, 0.440655950095832]
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08837360816158232, 0.2891500439057629, 0.18182039783682283, 0.440655950095832]
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08837360816158231, 0.28915004390576293, 0.18182039783682286, 0.44065595009583197]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08837360816158231, 0.28915004390576293, 0.18182039783682286, 0.44065595009583197]
actor:  1 policy actor:  1  step number:  44 total reward:  0.54  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.2891545722664962, 0.18184573275762267, 0.44063110803699335]
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.2891545722664962, 0.18184573275762267, 0.44063110803699335]
siam score:  -0.809911
actor:  1 policy actor:  1  step number:  49 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.35800666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.2891545722664962, 0.18184573275762267, 0.44063110803699335]
actor:  1 policy actor:  1  step number:  61 total reward:  0.4400000000000004  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  54 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3554600000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.2891545722664962, 0.18184573275762267, 0.44063110803699335]
maxi score, test score, baseline:  -0.3554600000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.2891545722664962, 0.18184573275762267, 0.44063110803699335]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5800000000000003  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3554600000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.2891545722664962, 0.18184573275762267, 0.44063110803699335]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.018]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.064]] [[2.147]
 [1.914]
 [3.004]
 [3.004]
 [3.004]
 [3.004]
 [2.775]] [[-0.037]
 [-0.095]
 [ 0.259]
 [ 0.259]
 [ 0.259]
 [ 0.259]
 [ 0.164]]
maxi score, test score, baseline:  -0.3554600000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.2891545722664962, 0.18184573275762267, 0.44063110803699335]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
from probs:  [0.08836858693888774, 0.2891545722664962, 0.18184573275762267, 0.44063110803699335]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.3554600000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.2891545722664962, 0.18184573275762267, 0.44063110803699335]
actor:  1 policy actor:  1  step number:  63 total reward:  0.36  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08836858693888774, 0.28915457226649627, 0.1818457327576227, 0.44063110803699335]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3554600000000001 0.6183333333333334 0.6183333333333334
actor:  0 policy actor:  0  step number:  40 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.35256666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.2891545722664962, 0.18184573275762267, 0.44063110803699335]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.473]
 [0.381]
 [0.382]
 [0.441]
 [0.448]
 [0.435]] [[1.043]
 [1.12 ]
 [1.135]
 [0.961]
 [0.839]
 [1.11 ]
 [1.037]] [[ 0.056]
 [ 0.15 ]
 [ 0.069]
 [-0.047]
 [-0.069]
 [ 0.119]
 [ 0.057]]
maxi score, test score, baseline:  -0.35256666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.2891545722664962, 0.18184573275762267, 0.44063110803699335]
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.631]
 [0.514]
 [0.514]
 [0.514]
 [0.432]
 [0.425]] [[ 0.232]
 [-0.436]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.325]
 [ 0.192]] [[ 0.139]
 [-0.136]
 [ 0.037]
 [ 0.037]
 [ 0.037]
 [ 0.172]
 [ 0.076]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.4600000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.35256666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.2891545722664962, 0.18184573275762267, 0.44063110803699335]
maxi score, test score, baseline:  -0.35256666666666675 0.6183333333333334 0.6183333333333334
actor:  1 policy actor:  1  step number:  49 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8157237
maxi score, test score, baseline:  -0.35256666666666675 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.35256666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.2891545722664962, 0.18184573275762267, 0.44063110803699335]
line 256 mcts: sample exp_bonus 1.896543608656401
actor:  1 policy actor:  1  step number:  42 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.35256666666666675 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.2891545722664962, 0.18184573275762267, 0.44063110803699335]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.35256666666666675 0.6183333333333334 0.6183333333333334
actor:  1 policy actor:  1  step number:  51 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  45 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.28915457226649627, 0.1818457327576227, 0.44063110803699335]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08836858693888774, 0.28915457226649627, 0.1818457327576227, 0.44063110803699335]
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
actor:  1 policy actor:  1  step number:  40 total reward:  0.6600000000000003  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08918871573210983, 0.2854839647765393, 0.1805751717564785, 0.44475214773487237]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084417, 0.3025618228563575, 0.1762600584518339, 0.4341188860009644]
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084417, 0.3025618228563575, 0.1762600584518339, 0.4341188860009644]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.131]
 [0.047]
 [0.057]
 [0.059]
 [0.067]
 [0.072]] [[1.177]
 [1.542]
 [1.155]
 [1.206]
 [1.19 ]
 [1.196]
 [1.29 ]] [[-0.456]
 [-0.118]
 [-0.46 ]
 [-0.416]
 [-0.426]
 [-0.414]
 [-0.345]]
Printing some Q and Qe and total Qs values:  [[-0.066]
 [-0.061]
 [-0.067]
 [-0.063]
 [-0.065]
 [-0.065]
 [-0.063]] [[-3.181]
 [ 0.209]
 [ 0.076]
 [-2.279]
 [-2.323]
 [-2.43 ]
 [-0.099]] [[-0.253]
 [ 0.49 ]
 [ 0.458]
 [-0.055]
 [-0.065]
 [-0.089]
 [ 0.422]]
UNIT TEST: sample policy line 217 mcts : [0.061 0.327 0.082 0.02  0.082 0.082 0.347]
siam score:  -0.8198656
actor:  1 policy actor:  1  step number:  47 total reward:  0.6000000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084417, 0.3025618228563575, 0.1762600584518339, 0.4341188860009644]
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.145]
 [0.275]] [[1.702]
 [2.052]
 [2.052]
 [2.052]
 [2.052]
 [2.503]
 [1.774]] [[-0.172]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.031]
 [-0.144]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084417, 0.3025618228563575, 0.1762600584518339, 0.4341188860009644]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084417, 0.3025618228563575, 0.1762600584518339, 0.4341188860009644]
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084417, 0.3025618228563575, 0.1762600584518339, 0.4341188860009644]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  0.667
siam score:  -0.82019424
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  38 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 1.628205427531674
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084417, 0.3025618228563575, 0.1762600584518339, 0.4341188860009644]
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084417, 0.3025618228563575, 0.1762600584518339, 0.4341188860009644]
actor:  1 policy actor:  1  step number:  47 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]] [[1.107]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[-0.762]
 [-1.064]
 [-1.064]
 [-1.064]
 [-1.064]
 [-1.064]
 [-1.064]]
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084416, 0.30256182285635747, 0.17626005845183396, 0.4341188860009643]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084416, 0.30256182285635747, 0.17626005845183396, 0.4341188860009643]
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084416, 0.30256182285635747, 0.17626005845183396, 0.4341188860009643]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084416, 0.30256182285635747, 0.17626005845183396, 0.4341188860009643]
from probs:  [0.08705923269084416, 0.30256182285635747, 0.17626005845183396, 0.4341188860009643]
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084416, 0.30256182285635747, 0.17626005845183396, 0.4341188860009643]
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
actor:  1 policy actor:  1  step number:  42 total reward:  0.6066666666666669  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.056]
 [-0.193]
 [-0.048]
 [-0.039]
 [-0.048]
 [-0.066]] [[ 0.755]
 [ 0.785]
 [-0.013]
 [ 0.853]
 [ 0.64 ]
 [ 0.853]
 [ 0.972]] [[0.494]
 [0.508]
 [0.217]
 [0.531]
 [0.473]
 [0.531]
 [0.558]]
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084416, 0.30256182285635747, 0.17626005845183396, 0.4341188860009643]
maxi score, test score, baseline:  -0.34971333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084416, 0.30256182285635747, 0.17626005845183396, 0.4341188860009643]
actor:  0 policy actor:  0  step number:  64 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.34727333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084416, 0.30256182285635747, 0.17626005845183396, 0.4341188860009643]
siam score:  -0.82982135
maxi score, test score, baseline:  -0.34727333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084416, 0.30256182285635747, 0.17626005845183396, 0.4341188860009643]
maxi score, test score, baseline:  -0.34727333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.08705923269084416, 0.30256182285635747, 0.17626005845183396, 0.4341188860009643]
actor:  0 policy actor:  0  step number:  56 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.34488666666666673 0.6183333333333334 0.6183333333333334
probs:  [0.08705471651686214, 0.30256365000978264, 0.1762850845456077, 0.43409654892774757]
maxi score, test score, baseline:  -0.34488666666666673 0.6183333333333334 0.6183333333333334
probs:  [0.08705471651686214, 0.30256365000978264, 0.1762850845456077, 0.43409654892774757]
maxi score, test score, baseline:  -0.34488666666666673 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.34488666666666673 0.6183333333333334 0.6183333333333334
probs:  [0.08705471651686214, 0.30256365000978264, 0.1762850845456077, 0.43409654892774757]
maxi score, test score, baseline:  -0.34488666666666673 0.6183333333333334 0.6183333333333334
probs:  [0.08705471651686214, 0.30256365000978264, 0.1762850845456077, 0.43409654892774757]
maxi score, test score, baseline:  -0.34488666666666673 0.6183333333333334 0.6183333333333334
probs:  [0.08705471651686214, 0.30256365000978264, 0.1762850845456077, 0.43409654892774757]
maxi score, test score, baseline:  -0.34488666666666673 0.6183333333333334 0.6183333333333334
probs:  [0.08705471651686214, 0.30256365000978264, 0.1762850845456077, 0.43409654892774757]
actor:  0 policy actor:  0  step number:  54 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3422866666666668 0.6183333333333334 0.6183333333333334
probs:  [0.08705471651686214, 0.30256365000978264, 0.1762850845456077, 0.43409654892774757]
maxi score, test score, baseline:  -0.3422866666666668 0.6183333333333334 0.6183333333333334
probs:  [0.08705471651686214, 0.30256365000978264, 0.1762850845456077, 0.43409654892774757]
using another actor
from probs:  [0.08705471651686214, 0.30256365000978264, 0.1762850845456077, 0.43409654892774757]
actor:  1 policy actor:  1  step number:  48 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3422866666666668 0.6183333333333334 0.6183333333333334
probs:  [0.08705471651686214, 0.30256365000978264, 0.1762850845456077, 0.43409654892774757]
maxi score, test score, baseline:  -0.3422866666666668 0.6183333333333334 0.6183333333333334
probs:  [0.08705471651686214, 0.30256365000978264, 0.1762850845456077, 0.43409654892774757]
actor:  0 policy actor:  0  step number:  38 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3393933333333334 0.6183333333333334 0.6183333333333334
probs:  [0.08705471651686214, 0.30256365000978264, 0.1762850845456077, 0.43409654892774757]
maxi score, test score, baseline:  -0.3393933333333334 0.6183333333333334 0.6183333333333334
actor:  1 policy actor:  1  step number:  38 total reward:  0.6866666666666669  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3393933333333334 0.6183333333333334 0.6183333333333334
probs:  [0.08916087275769387, 0.29271977052376486, 0.17344338981534052, 0.44467596690320066]
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.117]
 [0.07 ]
 [0.033]
 [0.033]
 [0.033]
 [0.107]] [[2.781]
 [4.728]
 [1.956]
 [2.781]
 [2.781]
 [2.781]
 [3.175]] [[ 0.071]
 [ 0.391]
 [-0.032]
 [ 0.071]
 [ 0.071]
 [ 0.071]
 [ 0.161]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3393933333333334 0.6183333333333334 0.6183333333333334
start point for exploration sampling:  11715
actor:  1 policy actor:  1  step number:  52 total reward:  0.32666666666666666  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.3393933333333334 0.6183333333333334 0.6183333333333334
probs:  [0.08916087275769387, 0.29271977052376486, 0.17344338981534052, 0.44467596690320066]
maxi score, test score, baseline:  -0.3393933333333334 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.3393933333333334 0.6183333333333334 0.6183333333333334
probs:  [0.08916087275769387, 0.29271977052376486, 0.17344338981534052, 0.44467596690320066]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6600000000000004  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.743]
 [0.528]
 [0.651]
 [0.617]
 [0.598]
 [0.736]] [[0.413]
 [0.652]
 [0.219]
 [0.305]
 [0.245]
 [0.12 ]
 [0.499]] [[ 0.013]
 [ 0.115]
 [-0.172]
 [-0.035]
 [-0.079]
 [-0.119]
 [ 0.082]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.3393933333333334 0.6183333333333334 0.6183333333333334
probs:  [0.08824801910182, 0.2999359209028181, 0.17169874774290866, 0.44011731225245315]
maxi score, test score, baseline:  -0.3393933333333334 0.6183333333333334 0.6183333333333334
probs:  [0.08824801910182, 0.2999359209028181, 0.17169874774290866, 0.44011731225245315]
using another actor
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.462]
 [0.565]
 [0.524]
 [0.565]
 [0.561]
 [0.56 ]] [[-0.331]
 [ 0.267]
 [-0.382]
 [-0.316]
 [-0.375]
 [-0.382]
 [-0.079]] [[0.524]
 [0.462]
 [0.565]
 [0.524]
 [0.565]
 [0.561]
 [0.56 ]]
maxi score, test score, baseline:  -0.3393933333333334 0.6183333333333334 0.6183333333333334
probs:  [0.08824801910182, 0.2999359209028181, 0.17169874774290866, 0.44011731225245315]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.3393933333333334 0.6183333333333334 0.6183333333333334
probs:  [0.08824801910182, 0.2999359209028181, 0.17169874774290866, 0.44011731225245315]
actor:  0 policy actor:  1  step number:  62 total reward:  0.20666666666666578  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.33698000000000006 0.6183333333333334 0.6183333333333334
probs:  [0.08824801910182, 0.2999359209028181, 0.17169874774290866, 0.44011731225245315]
maxi score, test score, baseline:  -0.33698000000000006 0.6183333333333334 0.6183333333333334
probs:  [0.08824801910182, 0.2999359209028181, 0.17169874774290866, 0.44011731225245315]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824801910182, 0.2999359209028181, 0.17169874774290866, 0.44011731225245315]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824801910182, 0.2999359209028181, 0.17169874774290866, 0.44011731225245315]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824801910182, 0.2999359209028181, 0.17169874774290866, 0.44011731225245315]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
siam score:  -0.81177336
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
actor:  1 policy actor:  1  step number:  45 total reward:  0.46666666666666634  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
siam score:  -0.80535907
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
actor:  1 policy actor:  1  step number:  63 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
Printing some Q and Qe and total Qs values:  [[0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]] [[0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]] [[-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]]
line 256 mcts: sample exp_bonus 0.2910297017033621
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
actor:  1 policy actor:  1  step number:  61 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
Printing some Q and Qe and total Qs values:  [[ 0.188]
 [ 0.202]
 [ 0.175]
 [ 0.157]
 [ 0.162]
 [-0.08 ]
 [ 0.151]] [[2.082]
 [2.221]
 [2.431]
 [2.391]
 [2.002]
 [0.511]
 [2.148]] [[ 0.38 ]
 [ 0.442]
 [ 0.507]
 [ 0.481]
 [ 0.335]
 [-0.377]
 [ 0.385]]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
siam score:  -0.80559427
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3369800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
actor:  0 policy actor:  0  step number:  45 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3341000000000001 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.3341000000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08824365476032613, 0.29993804980326094, 0.17172258938370763, 0.4400957060527053]
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.675]
 [0.704]
 [0.704]
 [0.704]
 [0.573]
 [0.712]] [[0.566]
 [0.302]
 [0.566]
 [0.566]
 [0.566]
 [0.496]
 [0.352]] [[ 0.082]
 [-0.035]
 [ 0.082]
 [ 0.082]
 [ 0.082]
 [-0.073]
 [ 0.019]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3341000000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08705062046387683, 0.3094127449192282, 0.16939901815335093, 0.43413761646354404]
maxi score, test score, baseline:  -0.3341000000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08705062046387683, 0.3094127449192282, 0.16939901815335093, 0.43413761646354404]
maxi score, test score, baseline:  -0.3341000000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08705062046387683, 0.3094127449192282, 0.16939901815335093, 0.43413761646354404]
maxi score, test score, baseline:  -0.3341000000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08705062046387683, 0.3094127449192282, 0.16939901815335093, 0.43413761646354404]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6400000000000002  reward:  1.0 rdn_beta:  0.667
first move QE:  -0.027282644707986817
using another actor
from probs:  [0.08725619774870258, 0.3084149431456582, 0.16915894248574748, 0.4351699166198917]
line 256 mcts: sample exp_bonus 0.09454276315590493
maxi score, test score, baseline:  -0.3341000000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08725619774870258, 0.3084149431456582, 0.16915894248574748, 0.4351699166198917]
maxi score, test score, baseline:  -0.3341000000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08725619774870258, 0.3084149431456582, 0.16915894248574748, 0.4351699166198917]
actor:  0 policy actor:  1  step number:  43 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.3312200000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08725619774870258, 0.3084149431456582, 0.16915894248574748, 0.4351699166198917]
maxi score, test score, baseline:  -0.3312200000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08725619774870258, 0.3084149431456582, 0.16915894248574748, 0.4351699166198917]
maxi score, test score, baseline:  -0.3312200000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08725619774870258, 0.3084149431456582, 0.16915894248574748, 0.4351699166198917]
maxi score, test score, baseline:  -0.3312200000000001 0.6183333333333334 0.6183333333333334
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.3312200000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08725619774870258, 0.3084149431456582, 0.16915894248574748, 0.4351699166198917]
Printing some Q and Qe and total Qs values:  [[-0.089]
 [-0.057]
 [-0.099]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.06 ]] [[1.075]
 [1.003]
 [1.166]
 [1.003]
 [1.003]
 [1.003]
 [1.175]] [[0.336]
 [0.331]
 [0.371]
 [0.331]
 [0.331]
 [0.331]
 [0.415]]
actor:  0 policy actor:  1  step number:  55 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08725619774870258, 0.3084149431456582, 0.16915894248574748, 0.4351699166198917]
maxi score, test score, baseline:  -0.32892666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08725619774870258, 0.3084149431456582, 0.16915894248574748, 0.4351699166198917]
maxi score, test score, baseline:  -0.32892666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08725619774870258, 0.3084149431456582, 0.16915894248574748, 0.4351699166198917]
maxi score, test score, baseline:  -0.32892666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08725619774870258, 0.3084149431456582, 0.16915894248574748, 0.4351699166198917]
maxi score, test score, baseline:  -0.32892666666666676 0.6183333333333334 0.6183333333333334
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.3289266666666667 0.6183333333333334 0.6183333333333334
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  61 total reward:  0.15999999999999914  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.3266066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.08725619774870258, 0.3084149431456582, 0.16915894248574748, 0.4351699166198917]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.3266066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.08725619774870258, 0.3084149431456582, 0.16915894248574748, 0.4351699166198917]
maxi score, test score, baseline:  -0.3266066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.08725619774870258, 0.3084149431456582, 0.16915894248574748, 0.4351699166198917]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.3266066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.0872521093512103, 0.30841561855922484, 0.1691825894282412, 0.43514968266132364]
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.064]] [[1.3  ]
 [1.3  ]
 [1.3  ]
 [1.3  ]
 [1.3  ]
 [1.3  ]
 [0.897]] [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.352]]
maxi score, test score, baseline:  -0.3266066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.0872521093512103, 0.30841561855922484, 0.1691825894282412, 0.43514968266132364]
actor:  0 policy actor:  0  step number:  52 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.3240600000000001 0.6183333333333334 0.6183333333333334
probs:  [0.0872521093512103, 0.30841561855922484, 0.1691825894282412, 0.43514968266132364]
siam score:  -0.80318785
maxi score, test score, baseline:  -0.3240600000000001 0.6183333333333334 0.6183333333333334
probs:  [0.0872521093512103, 0.30841561855922484, 0.1691825894282412, 0.43514968266132364]
maxi score, test score, baseline:  -0.3240600000000001 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.3240600000000001 0.6183333333333334 0.6183333333333334
probs:  [0.0872521093512103, 0.30841561855922484, 0.1691825894282412, 0.43514968266132364]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6800000000000004  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3240600000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08538358745198948, 0.3232418828974871, 0.16555648967825706, 0.42581803997226636]
maxi score, test score, baseline:  -0.3240600000000001 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.3240600000000001 0.6183333333333334 0.6183333333333334
probs:  [0.08538358745198948, 0.3232418828974871, 0.16555648967825706, 0.42581803997226636]
Printing some Q and Qe and total Qs values:  [[-0.083]
 [ 0.125]
 [ 0.079]
 [ 0.304]
 [ 0.125]
 [ 0.225]
 [ 0.32 ]] [[1.259]
 [1.483]
 [2.61 ]
 [0.691]
 [1.483]
 [0.735]
 [0.862]] [[-0.121]
 [ 0.052]
 [ 0.331]
 [-0.066]
 [ 0.052]
 [-0.097]
 [-0.011]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7333333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3240600000000001 0.6183333333333334 0.6183333333333334
actor:  1 policy actor:  1  step number:  48 total reward:  0.6333333333333336  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.3240600000000001 0.6183333333333334 0.6183333333333334
probs:  [0.07937267498384334, 0.3697579623981434, 0.15507060548708226, 0.395798757130931]
actor:  0 policy actor:  0  step number:  52 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3213533333333335 0.6183333333333334 0.6183333333333334
probs:  [0.07937267498384334, 0.3697579623981434, 0.15507060548708226, 0.395798757130931]
maxi score, test score, baseline:  -0.3213533333333335 0.6183333333333334 0.6183333333333334
probs:  [0.07937015687187324, 0.369749432724175, 0.15509406285921937, 0.3957863475447324]
Printing some Q and Qe and total Qs values:  [[ 0.169]
 [-0.01 ]
 [-0.01 ]
 [-0.024]
 [-0.01 ]
 [-0.01 ]
 [-0.064]] [[2.475]
 [3.008]
 [3.008]
 [1.888]
 [3.008]
 [3.008]
 [2.045]] [[0.34 ]
 [0.425]
 [0.425]
 [0.061]
 [0.425]
 [0.425]
 [0.092]]
maxi score, test score, baseline:  -0.3213533333333335 0.6183333333333334 0.6183333333333334
probs:  [0.07936765657449045, 0.3697409633940766, 0.15511735428027537, 0.39577402575115767]
maxi score, test score, baseline:  -0.3213533333333335 0.6183333333333334 0.6183333333333334
probs:  [0.07936765657449045, 0.3697409633940766, 0.15511735428027537, 0.39577402575115767]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3213533333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08170852491553449, 0.40755885073795484, 0.14879673800173368, 0.36193588634477697]
siam score:  -0.7982811
maxi score, test score, baseline:  -0.3213533333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08170852491553449, 0.40755885073795484, 0.14879673800173368, 0.36193588634477697]
maxi score, test score, baseline:  -0.3213533333333335 0.6183333333333334 0.6183333333333334
siam score:  -0.79576516
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3213533333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08170852491553449, 0.40755885073795484, 0.14879673800173368, 0.36193588634477697]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.847]
 [0.742]
 [0.804]
 [0.804]
 [0.804]
 [0.741]] [[ 0.166]
 [ 0.443]
 [-0.089]
 [ 0.385]
 [ 0.385]
 [ 0.385]
 [-0.021]] [[0.759]
 [0.847]
 [0.742]
 [0.804]
 [0.804]
 [0.804]
 [0.741]]
maxi score, test score, baseline:  -0.3213533333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08170852491553449, 0.40755885073795484, 0.14879673800173368, 0.36193588634477697]
maxi score, test score, baseline:  -0.3213533333333335 0.6183333333333334 0.6183333333333334
actor:  0 policy actor:  0  step number:  62 total reward:  0.15333333333333266  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.165]
 [0.043]
 [0.043]
 [0.028]
 [0.043]
 [0.058]] [[-0.155]
 [-0.657]
 [-0.812]
 [-0.812]
 [-0.444]
 [-0.812]
 [-0.724]] [[0.019]
 [0.165]
 [0.043]
 [0.043]
 [0.028]
 [0.043]
 [0.058]]
siam score:  -0.79990846
actor:  1 policy actor:  1  step number:  29 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.667
first move QE:  -0.03151140978219308
maxi score, test score, baseline:  -0.31904666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08339168408493863, 0.36014808400803294, 0.14039406402024168, 0.4160661678867868]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.31904666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08339168408493863, 0.36014808400803294, 0.14039406402024168, 0.4160661678867868]
maxi score, test score, baseline:  -0.31904666666666676 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.31904666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08339168408493863, 0.36014808400803294, 0.14039406402024168, 0.4160661678867868]
maxi score, test score, baseline:  -0.31904666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08339168408493863, 0.36014808400803294, 0.14039406402024168, 0.4160661678867868]
maxi score, test score, baseline:  -0.31904666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08339168408493863, 0.36014808400803294, 0.14039406402024168, 0.4160661678867868]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8239070144090468
maxi score, test score, baseline:  -0.31904666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08339168408493863, 0.36014808400803294, 0.14039406402024168, 0.4160661678867868]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  41 total reward:  0.52  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.31904666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08338938750387738, 0.3601428542217022, 0.1404129588717886, 0.41605479940263174]
maxi score, test score, baseline:  -0.31904666666666676 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.31904666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08338938750387738, 0.3601428542217022, 0.1404129588717886, 0.41605479940263174]
maxi score, test score, baseline:  -0.31904666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08338938750387738, 0.3601428542217022, 0.1404129588717886, 0.41605479940263174]
maxi score, test score, baseline:  -0.31904666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08338938750387738, 0.3601428542217022, 0.1404129588717886, 0.41605479940263174]
maxi score, test score, baseline:  -0.31904666666666676 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.31904666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08338938750387738, 0.3601428542217022, 0.1404129588717886, 0.41605479940263174]
first move QE:  -0.032364845023331736
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  51 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08338710683523078, 0.36013766067120484, 0.140431722805809, 0.41604350968775544]
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08338710683523078, 0.36013766067120484, 0.140431722805809, 0.41604350968775544]
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08338710683523078, 0.36013766067120484, 0.140431722805809, 0.41604350968775544]
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08338710683523078, 0.36013766067120484, 0.140431722805809, 0.41604350968775544]
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.129]] [[1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.263]] [[-0.338]
 [-0.338]
 [-0.338]
 [-0.338]
 [-0.338]
 [-0.338]
 [-0.252]]
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08338710683523078, 0.36013766067120484, 0.140431722805809, 0.41604350968775544]
siam score:  -0.80219287
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08338710683523078, 0.36013766067120484, 0.140431722805809, 0.41604350968775544]
actor:  1 policy actor:  1  step number:  61 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08338710683523078, 0.36013766067120484, 0.140431722805809, 0.41604350968775544]
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08338710683523078, 0.36013766067120484, 0.140431722805809, 0.41604350968775544]
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
actor:  1 policy actor:  1  step number:  32 total reward:  0.7133333333333335  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  42 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08052775299703627, 0.38210268314806567, 0.13561277940992622, 0.4017567844449719]
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08052775299703627, 0.38210268314806567, 0.13561277940992622, 0.4017567844449719]
siam score:  -0.8036554
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08052775299703627, 0.38210268314806567, 0.13561277940992622, 0.4017567844449719]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.3470872097929067
actor:  1 policy actor:  1  step number:  51 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08052775299703627, 0.38210268314806567, 0.13561277940992622, 0.4017567844449719]
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08052586487732713, 0.38209540571390277, 0.13563127764486493, 0.40174745176390514]
Printing some Q and Qe and total Qs values:  [[0.876]
 [0.958]
 [0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.919]] [[1.128]
 [0.708]
 [1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.625]] [[0.876]
 [0.958]
 [0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.919]]
maxi score, test score, baseline:  -0.3165133333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08052586487732713, 0.38209540571390277, 0.13563127764486493, 0.40174745176390514]
actor:  0 policy actor:  0  step number:  46 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.333
from probs:  [0.08052586487732713, 0.38209540571390277, 0.13563127764486493, 0.40174745176390514]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.498]
 [ 0.396]
 [ 0.438]
 [ 0.408]
 [ 0.474]
 [-0.015]
 [ 0.356]] [[1.318]
 [1.653]
 [3.249]
 [1.09 ]
 [1.324]
 [0.569]
 [1.382]] [[ 0.176]
 [ 0.184]
 [ 0.608]
 [ 0.054]
 [ 0.16 ]
 [-0.388]
 [ 0.087]]
line 256 mcts: sample exp_bonus 0.9235782795695212
actor:  1 policy actor:  1  step number:  66 total reward:  0.4733333333333336  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]] [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]] [[0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]]
maxi score, test score, baseline:  -0.31388666666666687 0.6183333333333334 0.6183333333333334
probs:  [0.08052586487732713, 0.38209540571390277, 0.13563127764486493, 0.40174745176390514]
first move QE:  -0.03691485117435313
actor:  0 policy actor:  0  step number:  46 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.08052586487732713, 0.38209540571390277, 0.13563127764486493, 0.40174745176390514]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.073]
 [-0.055]
 [-0.055]
 [-0.067]
 [-0.055]
 [-0.055]] [[-1.09 ]
 [-0.338]
 [-1.322]
 [-1.416]
 [-1.026]
 [-1.341]
 [-1.042]] [[-0.753]
 [-0.649]
 [-0.795]
 [-0.81 ]
 [-0.757]
 [-0.798]
 [-0.748]]
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.08052586487732713, 0.38209540571390277, 0.13563127764486493, 0.40174745176390514]
Printing some Q and Qe and total Qs values:  [[ 0.041]
 [ 0.127]
 [ 0.099]
 [ 0.099]
 [ 0.099]
 [-0.126]
 [ 0.102]] [[1.456]
 [1.527]
 [1.356]
 [1.356]
 [1.356]
 [0.18 ]
 [2.23 ]] [[-0.047]
 [ 0.051]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.427]
 [ 0.144]]
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.08052586487732713, 0.38209540571390277, 0.13563127764486493, 0.40174745176390514]
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.08052586487732713, 0.38209540571390277, 0.13563127764486493, 0.40174745176390514]
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.08052586487732713, 0.38209540571390277, 0.13563127764486493, 0.40174745176390514]
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.08052586487732713, 0.38209540571390277, 0.13563127764486493, 0.40174745176390514]
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.0805239897359868, 0.38208817830264047, 0.13564964872848295, 0.4017381832328898]
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.0805239897359868, 0.38208817830264047, 0.13564964872848295, 0.4017381832328898]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.64 ]
 [0.549]
 [0.536]
 [0.422]
 [0.56 ]
 [0.403]] [[1.391]
 [1.724]
 [1.623]
 [1.396]
 [1.346]
 [2.132]
 [1.75 ]] [[0.025]
 [0.406]
 [0.295]
 [0.179]
 [0.075]
 [0.542]
 [0.251]]
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.0805239897359868, 0.38208817830264047, 0.13564964872848295, 0.4017381832328898]
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.0805239897359868, 0.38208817830264047, 0.13564964872848295, 0.4017381832328898]
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.0805239897359868, 0.38208817830264047, 0.13564964872848295, 0.4017381832328898]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  58 total reward:  0.5533333333333339  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.0805239897359868, 0.38208817830264047, 0.13564964872848295, 0.4017381832328898]
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.0805239897359868, 0.38208817830264047, 0.13564964872848295, 0.4017381832328898]
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.0805239897359868, 0.38208817830264047, 0.13564964872848295, 0.4017381832328898]
siam score:  -0.7956391
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.0805239897359868, 0.38208817830264047, 0.13564964872848295, 0.4017381832328898]
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.0805239897359868, 0.38208817830264047, 0.13564964872848295, 0.4017381832328898]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.748]
 [0.707]
 [0.632]
 [0.633]
 [0.637]
 [0.718]] [[ 0.36 ]
 [ 0.587]
 [ 0.471]
 [-0.014]
 [-0.066]
 [ 0.297]
 [ 0.669]] [[0.739]
 [0.748]
 [0.707]
 [0.632]
 [0.633]
 [0.637]
 [0.718]]
first move QE:  -0.03760021381177199
maxi score, test score, baseline:  -0.31126000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.0805239897359868, 0.38208817830264047, 0.13564964872848295, 0.4017381832328898]
actor:  0 policy actor:  0  step number:  48 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.076]
 [-0.006]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.006]] [[0.869]
 [1.389]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [2.868]] [[-0.46 ]
 [-0.219]
 [-0.46 ]
 [-0.46 ]
 [-0.46 ]
 [-0.46 ]
 [ 0.27 ]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.30871333333333345 0.6183333333333334 0.6183333333333334
probs:  [0.0805239897359868, 0.38208817830264047, 0.13564964872848295, 0.4017381832328898]
actor:  0 policy actor:  0  step number:  53 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3060733333333335 0.6183333333333334 0.6183333333333334
actor:  1 policy actor:  1  step number:  41 total reward:  0.6266666666666668  reward:  1.0 rdn_beta:  0.333
using another actor
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
first move QE:  -0.03810851839902764
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.3060733333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08050275477422349, 0.3822513127852487, 0.13561384947851238, 0.4016320829620154]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.3060733333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08050275477422349, 0.3822513127852487, 0.13561384947851238, 0.4016320829620154]
maxi score, test score, baseline:  -0.3060733333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08050275477422349, 0.3822513127852487, 0.13561384947851238, 0.4016320829620154]
actor:  0 policy actor:  1  step number:  46 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.30328666666666676 0.6183333333333334 0.6183333333333334
actor:  0 policy actor:  0  step number:  49 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3006733333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08050275477422349, 0.3822513127852487, 0.13561384947851238, 0.4016320829620154]
siam score:  -0.78227174
maxi score, test score, baseline:  -0.3006733333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08050275477422349, 0.3822513127852487, 0.13561384947851238, 0.4016320829620154]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3006733333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08050275477422349, 0.3822513127852487, 0.13561384947851238, 0.4016320829620154]
actor:  0 policy actor:  0  step number:  63 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  42 total reward:  0.6066666666666669  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  11715
siam score:  -0.78826165
maxi score, test score, baseline:  -0.29840666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08050275477422349, 0.3822513127852487, 0.13561384947851238, 0.4016320829620154]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.29840666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08050275477422349, 0.3822513127852487, 0.13561384947851238, 0.4016320829620154]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.29840666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08050275477422349, 0.3822513127852487, 0.13561384947851238, 0.4016320829620154]
maxi score, test score, baseline:  -0.29840666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08050275477422349, 0.3822513127852487, 0.13561384947851238, 0.4016320829620154]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.29840666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08415267956775006, 0.36117088791931734, 0.1347470446772524, 0.4199293878356801]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.29840666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08415267956775006, 0.36117088791931734, 0.1347470446772524, 0.4199293878356801]
maxi score, test score, baseline:  -0.29840666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08415267956775006, 0.36117088791931734, 0.1347470446772524, 0.4199293878356801]
maxi score, test score, baseline:  -0.29840666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08415267956775006, 0.36117088791931734, 0.1347470446772524, 0.4199293878356801]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5800000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.29840666666666676 0.6183333333333334 0.6183333333333334
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.107]
 [0.026]
 [0.107]
 [0.107]
 [0.209]
 [0.107]] [[3.499]
 [3.499]
 [1.647]
 [3.499]
 [3.499]
 [3.485]
 [3.499]] [[-0.029]
 [-0.029]
 [-0.698]
 [-0.029]
 [-0.029]
 [ 0.064]
 [-0.029]]
maxi score, test score, baseline:  -0.29840666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08415267956775006, 0.36117088791931734, 0.1347470446772524, 0.4199293878356801]
maxi score, test score, baseline:  -0.29840666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.08415267956775006, 0.36117088791931734, 0.1347470446772524, 0.4199293878356801]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  64 total reward:  0.20666666666666578  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08415267956775006, 0.36117088791931734, 0.1347470446772524, 0.4199293878356801]
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08415267956775006, 0.36117088791931734, 0.1347470446772524, 0.4199293878356801]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  40 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08415267956775009, 0.3611708879193173, 0.13474704467725243, 0.41992938783568023]
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08415267956775009, 0.3611708879193173, 0.13474704467725243, 0.41992938783568023]
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08415267956775009, 0.3611708879193173, 0.13474704467725243, 0.41992938783568023]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.809374509469013
using explorer policy with actor:  1
start point for exploration sampling:  11715
actor:  1 policy actor:  1  step number:  70 total reward:  0.4733333333333337  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08415267956775009, 0.3611708879193173, 0.13474704467725243, 0.41992938783568023]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08626534115847943, 0.34896904638836396, 0.1342453178434476, 0.43052029460970903]
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08626534115847943, 0.34896904638836396, 0.1342453178434476, 0.43052029460970903]
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08626534115847943, 0.34896904638836396, 0.1342453178434476, 0.43052029460970903]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
actor:  1 policy actor:  1  step number:  54 total reward:  0.526666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08626534115847943, 0.34896904638836396, 0.1342453178434476, 0.43052029460970903]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08626534115847943, 0.34896904638836396, 0.1342453178434476, 0.43052029460970903]
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08626534115847943, 0.34896904638836396, 0.1342453178434476, 0.43052029460970903]
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08626534115847943, 0.34896904638836396, 0.1342453178434476, 0.43052029460970903]
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08626534115847943, 0.34896904638836396, 0.1342453178434476, 0.43052029460970903]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08626534115847943, 0.34896904638836396, 0.1342453178434476, 0.43052029460970903]
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08626534115847943, 0.34896904638836396, 0.1342453178434476, 0.43052029460970903]
line 256 mcts: sample exp_bonus 1.815974713085878
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08626534115847943, 0.34896904638836396, 0.1342453178434476, 0.43052029460970903]
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08626534115847943, 0.34896904638836396, 0.1342453178434476, 0.43052029460970903]
actor:  1 policy actor:  1  step number:  65 total reward:  0.5200000000000006  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.08626534115847943, 0.34896904638836396, 0.1342453178434476, 0.43052029460970903]
Printing some Q and Qe and total Qs values:  [[0.889]
 [0.973]
 [0.951]
 [0.889]
 [0.889]
 [0.889]
 [0.889]] [[1.533]
 [1.802]
 [1.597]
 [1.533]
 [1.533]
 [1.533]
 [1.533]] [[0.889]
 [0.973]
 [0.951]
 [0.889]
 [0.889]
 [0.889]
 [0.889]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018395, 0.3269471473565879, 0.13333980061905545, 0.44963477655417267]
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018395, 0.3269471473565879, 0.13333980061905545, 0.44963477655417267]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.083]
 [-0.066]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.07 ]] [[-4.113]
 [-0.626]
 [-3.179]
 [-3.521]
 [-3.538]
 [-3.542]
 [-1.442]] [[-0.211]
 [ 0.424]
 [-0.039]
 [-0.102]
 [-0.105]
 [-0.106]
 [ 0.278]]
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
maxi score, test score, baseline:  -0.2959933333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
actor:  0 policy actor:  1  step number:  44 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.845]
 [0.836]
 [0.659]
 [0.824]
 [0.682]
 [0.371]] [[-0.013]
 [ 0.994]
 [ 0.958]
 [-0.05 ]
 [ 1.875]
 [-0.086]
 [ 0.17 ]] [[0.734]
 [0.845]
 [0.836]
 [0.659]
 [0.824]
 [0.682]
 [0.371]]
siam score:  -0.78453875
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.29315333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
maxi score, test score, baseline:  -0.29315333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  43 total reward:  0.56  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
Printing some Q and Qe and total Qs values:  [[-0.047]
 [-0.047]
 [-0.047]
 [-0.047]
 [-0.047]
 [-0.047]
 [-0.047]] [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
line 256 mcts: sample exp_bonus 0.4759597585108859
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2866666666666666  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018395, 0.3269471473565879, 0.13333980061905545, 0.44963477655417267]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018395, 0.3269471473565879, 0.13333980061905545, 0.44963477655417267]
Printing some Q and Qe and total Qs values:  [[ 0.166]
 [ 0.337]
 [-0.019]
 [ 0.166]
 [ 0.183]
 [ 0.166]
 [ 0.206]] [[0.93 ]
 [0.854]
 [1.782]
 [0.93 ]
 [1.133]
 [0.93 ]
 [1.767]] [[-0.501]
 [-0.356]
 [-0.402]
 [-0.501]
 [-0.417]
 [-0.501]
 [-0.183]]
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018395, 0.3269471473565879, 0.13333980061905545, 0.44963477655417267]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3933333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018395, 0.3269471473565879, 0.13333980061905545, 0.44963477655417267]
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018395, 0.3269471473565879, 0.13333980061905545, 0.44963477655417267]
siam score:  -0.7933641
actor:  1 policy actor:  1  step number:  50 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
maxi score, test score, baseline:  -0.2900333333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09007827547018393, 0.32694714735658803, 0.13333980061905543, 0.4496347765541726]
actor:  0 policy actor:  0  step number:  55 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  36 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.911]
 [0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.837]] [[1.263]
 [2.265]
 [1.263]
 [1.263]
 [1.263]
 [1.263]
 [1.263]] [[0.837]
 [0.911]
 [0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.837]]
maxi score, test score, baseline:  -0.2876066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09043696663992859, 0.3248754985743209, 0.13325461660960525, 0.45143291817614517]
Printing some Q and Qe and total Qs values:  [[0.887]
 [0.891]
 [0.87 ]
 [0.874]
 [0.874]
 [0.874]
 [0.867]] [[0.001]
 [0.127]
 [0.383]
 [0.275]
 [0.275]
 [0.275]
 [0.192]] [[0.887]
 [0.891]
 [0.87 ]
 [0.874]
 [0.874]
 [0.874]
 [0.867]]
actor:  0 policy actor:  0  step number:  35 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.28462000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.09043696663992859, 0.3248754985743209, 0.13325461660960525, 0.45143291817614517]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.208]
 [-0.055]
 [-0.047]
 [-0.082]
 [-0.082]
 [-0.071]
 [-0.039]] [[0.396]
 [1.079]
 [0.83 ]
 [1.132]
 [1.132]
 [1.548]
 [1.19 ]] [[-0.163]
 [ 0.178]
 [ 0.09 ]
 [ 0.183]
 [ 0.183]
 [ 0.345]
 [ 0.229]]
maxi score, test score, baseline:  -0.28462000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.09218900670742096, 0.3147564547325467, 0.1328385322165252, 0.46021600634350707]
Printing some Q and Qe and total Qs values:  [[0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]] [[0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]] [[0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]]
maxi score, test score, baseline:  -0.28462000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.09218900670742096, 0.3147564547325467, 0.1328385322165252, 0.46021600634350707]
maxi score, test score, baseline:  -0.28462000000000015 0.6183333333333334 0.6183333333333334
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.28462000000000015 0.6183333333333334 0.6183333333333334
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6133333333333334  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.28462000000000015 0.6183333333333334 0.6183333333333334
probs:  [0.09218900670742096, 0.3147564547325467, 0.1328385322165252, 0.46021600634350707]
actor:  0 policy actor:  0  step number:  64 total reward:  0.059999999999999165  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.28250000000000014 0.6183333333333334 0.6183333333333334
probs:  [0.09218900670742096, 0.3147564547325467, 0.1328385322165252, 0.46021600634350707]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.28250000000000014 0.6183333333333334 0.6183333333333334
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  0 policy actor:  1  step number:  54 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2799800000000002 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.2799800000000002 0.6183333333333334 0.6183333333333334
probs:  [0.08959266403291508, 0.33405737909246785, 0.12911195211059628, 0.44723800476402076]
maxi score, test score, baseline:  -0.2799800000000002 0.6183333333333334 0.6183333333333334
probs:  [0.08959266403291508, 0.33405737909246785, 0.12911195211059628, 0.44723800476402076]
maxi score, test score, baseline:  -0.2799800000000002 0.6183333333333334 0.6183333333333334
probs:  [0.08959266403291508, 0.33405737909246785, 0.12911195211059628, 0.44723800476402076]
actor:  0 policy actor:  0  step number:  66 total reward:  0.11333333333333262  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.27775333333333346 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
maxi score, test score, baseline:  -0.27775333333333346 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
maxi score, test score, baseline:  -0.27775333333333346 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.601]
 [0.515]
 [0.549]
 [0.495]
 [0.549]
 [0.499]] [[0.208]
 [0.731]
 [0.431]
 [0.741]
 [0.408]
 [0.741]
 [0.567]] [[-0.126]
 [ 0.087]
 [-0.099]
 [ 0.038]
 [-0.126]
 [ 0.038]
 [-0.07 ]]
maxi score, test score, baseline:  -0.27775333333333346 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
maxi score, test score, baseline:  -0.27775333333333346 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5533333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.27775333333333346 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
maxi score, test score, baseline:  -0.27775333333333346 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
maxi score, test score, baseline:  -0.27775333333333346 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
actor:  0 policy actor:  1  step number:  55 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  0.667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5861996710705375
actor:  1 policy actor:  1  step number:  48 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2750066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
maxi score, test score, baseline:  -0.2750066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2750066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
maxi score, test score, baseline:  -0.2750066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
maxi score, test score, baseline:  -0.2750066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.046]
 [-0.097]
 [-0.046]
 [-0.089]
 [-0.089]
 [-0.04 ]] [[ 0.646]
 [ 0.351]
 [ 0.405]
 [-0.306]
 [ 0.087]
 [ 0.087]
 [ 0.264]] [[-0.041]
 [-0.088]
 [-0.13 ]
 [-0.197]
 [-0.175]
 [-0.175]
 [-0.096]]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.2750066666666668 0.6183333333333334 0.6183333333333334
maxi score, test score, baseline:  -0.2750066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
maxi score, test score, baseline:  -0.2750066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09197649879504685, 0.31999978720149436, 0.12883792487348678, 0.4591857891299721]
actor:  1 policy actor:  1  step number:  47 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.0420097437799599
siam score:  -0.79447883
maxi score, test score, baseline:  -0.2750066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09197447784647378, 0.31999927413842505, 0.12885050563057032, 0.45917574238453085]
maxi score, test score, baseline:  -0.2750066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09197447784647378, 0.31999927413842505, 0.12885050563057032, 0.45917574238453085]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2750066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09530151439993921, 0.30038082534283816, 0.12846681260983678, 0.4758508476473857]
maxi score, test score, baseline:  -0.2750066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09530151439993921, 0.30038082534283816, 0.12846681260983678, 0.4758508476473857]
maxi score, test score, baseline:  -0.2750066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09530151439993921, 0.30038082534283816, 0.12846681260983678, 0.4758508476473857]
maxi score, test score, baseline:  -0.2750066666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09530151439993921, 0.30038082534283816, 0.12846681260983678, 0.4758508476473857]
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.739]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[0.088]
 [0.189]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[0.692]
 [0.739]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]]
actor:  0 policy actor:  0  step number:  51 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.27234000000000014 0.6183333333333334 0.6183333333333334
probs:  [0.09530151439993921, 0.30038082534283816, 0.12846681260983678, 0.4758508476473857]
maxi score, test score, baseline:  -0.27234000000000014 0.6183333333333334 0.6183333333333334
probs:  [0.09530151439993921, 0.30038082534283816, 0.12846681260983678, 0.4758508476473857]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.27234000000000014 0.6183333333333334 0.6183333333333334
probs:  [0.09530151439993921, 0.30038082534283816, 0.12846681260983678, 0.4758508476473857]
maxi score, test score, baseline:  -0.27234000000000014 0.6183333333333334 0.6183333333333334
probs:  [0.09530151439993921, 0.30038082534283816, 0.12846681260983678, 0.4758508476473857]
maxi score, test score, baseline:  -0.27234000000000014 0.6183333333333334 0.6183333333333334
probs:  [0.09530151439993921, 0.30038082534283816, 0.12846681260983678, 0.4758508476473857]
maxi score, test score, baseline:  -0.27234000000000014 0.6183333333333334 0.6183333333333334
probs:  [0.09530151439993921, 0.30038082534283816, 0.12846681260983678, 0.4758508476473857]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  1 policy actor:  1  step number:  36 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.27234000000000014 0.6183333333333334 0.6183333333333334
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.192]] [[1.501]
 [1.501]
 [1.501]
 [1.501]
 [1.501]
 [1.501]
 [2.201]] [[-0.319]
 [-0.319]
 [-0.319]
 [-0.319]
 [-0.319]
 [-0.319]
 [ 0.102]]
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.761]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[-0.342]
 [ 0.966]
 [-0.342]
 [-0.342]
 [-0.342]
 [-0.342]
 [-0.342]] [[0.717]
 [0.761]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[3.595]
 [3.595]
 [3.595]
 [3.595]
 [3.595]
 [3.595]
 [3.595]] [[0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 0.024060719474767327
actor:  0 policy actor:  1  step number:  54 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.2699800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.09525256886600329, 0.3007404747886645, 0.1284008070038171, 0.47560614934151507]
maxi score, test score, baseline:  -0.2699800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.09525256886600329, 0.3007404747886645, 0.1284008070038171, 0.47560614934151507]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6466666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2699800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.09507834561926984, 0.30202065888512897, 0.1281658578663041, 0.4747351376292972]
first move QE:  -0.04263535364814933
maxi score, test score, baseline:  -0.2699800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.09507834561926984, 0.30202065888512897, 0.1281658578663041, 0.4747351376292972]
maxi score, test score, baseline:  -0.2699800000000001 0.6183333333333334 0.6183333333333334
actor:  1 policy actor:  1  step number:  52 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 1.6621757327854754
maxi score, test score, baseline:  -0.2699800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.09507834561926984, 0.30202065888512897, 0.1281658578663041, 0.4747351376292972]
maxi score, test score, baseline:  -0.2699800000000001 0.6183333333333334 0.6183333333333334
probs:  [0.09507834561926984, 0.30202065888512897, 0.1281658578663041, 0.4747351376292972]
actor:  1 policy actor:  1  step number:  46 total reward:  0.7000000000000003  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  43 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.141]
 [-0.019]
 [-0.038]
 [-0.086]
 [-0.046]
 [-0.156]
 [-0.242]] [[0.202]
 [2.061]
 [1.992]
 [0.183]
 [1.764]
 [0.884]
 [1.18 ]] [[-0.157]
 [ 0.413]
 [ 0.386]
 [-0.14 ]
 [ 0.319]
 [ 0.027]
 [ 0.073]]
siam score:  -0.7896984
maxi score, test score, baseline:  -0.26723333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09596309090283325, 0.2967940399413474, 0.12807347171875907, 0.4791693974370604]
Printing some Q and Qe and total Qs values:  [[ 0.168]
 [ 0.211]
 [ 0.189]
 [ 0.177]
 [ 0.189]
 [-0.061]
 [-0.093]] [[ 2.554]
 [ 2.461]
 [ 2.436]
 [ 2.412]
 [ 2.27 ]
 [-0.006]
 [ 1.538]] [[ 0.535]
 [ 0.528]
 [ 0.513]
 [ 0.502]
 [ 0.47 ]
 [-0.215]
 [ 0.171]]
maxi score, test score, baseline:  -0.26723333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09596309090283325, 0.2967940399413474, 0.12807347171875907, 0.4791693974370604]
maxi score, test score, baseline:  -0.26723333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09596309090283325, 0.2967940399413474, 0.12807347171875907, 0.4791693974370604]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.675]
 [0.64 ]
 [0.625]
 [0.538]
 [0.551]
 [0.635]] [[0.997]
 [0.473]
 [1.074]
 [0.93 ]
 [0.797]
 [0.909]
 [1.073]] [[0.27 ]
 [0.092]
 [0.457]
 [0.347]
 [0.171]
 [0.258]
 [0.452]]
maxi score, test score, baseline:  -0.26723333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09596309090283325, 0.2967940399413474, 0.12807347171875907, 0.4791693974370604]
actor:  1 policy actor:  1  step number:  41 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.26723333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09757147763268156, 0.28729252045622033, 0.1279055220766488, 0.48723047983444934]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5866666666666671  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.412]
 [0.412]
 [0.412]
 [0.375]
 [0.412]
 [0.412]] [[2.463]
 [2.82 ]
 [2.82 ]
 [2.82 ]
 [3.765]
 [2.82 ]
 [2.82 ]] [[0.37 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.696]
 [0.52 ]
 [0.52 ]]
maxi score, test score, baseline:  -0.26723333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09757147763268156, 0.28729252045622033, 0.1279055220766488, 0.48723047983444934]
maxi score, test score, baseline:  -0.26723333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09757147763268156, 0.28729252045622033, 0.1279055220766488, 0.48723047983444934]
actor:  1 policy actor:  1  step number:  36 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.26723333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09546548053205048, 0.3026896526558053, 0.12514378301623252, 0.47670108379591175]
maxi score, test score, baseline:  -0.26723333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09546548053205048, 0.3026896526558053, 0.12514378301623252, 0.47670108379591175]
maxi score, test score, baseline:  -0.26723333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09546367381576218, 0.30269019037977607, 0.12515404307786257, 0.4766920927265992]
Printing some Q and Qe and total Qs values:  [[ 0.012]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[2.106]
 [2.236]
 [2.236]
 [2.236]
 [2.236]
 [2.236]
 [2.236]] [[0.004]
 [0.052]
 [0.052]
 [0.052]
 [0.052]
 [0.052]
 [0.052]]
siam score:  -0.78038025
line 256 mcts: sample exp_bonus 1.3506660776870634
maxi score, test score, baseline:  -0.26723333333333343 0.6183333333333334 0.6183333333333334
actor:  1 policy actor:  1  step number:  47 total reward:  0.4933333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.26723333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09546367381576218, 0.30269019037977607, 0.12515404307786257, 0.4766920927265992]
maxi score, test score, baseline:  -0.26723333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09546367381576218, 0.30269019037977607, 0.12515404307786257, 0.4766920927265992]
maxi score, test score, baseline:  -0.26723333333333343 0.6183333333333334 0.6183333333333334
probs:  [0.09546367381576218, 0.30269019037977607, 0.12515404307786257, 0.4766920927265992]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  57 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[2.372]
 [2.065]
 [2.065]
 [2.065]
 [2.065]
 [2.065]
 [2.065]] [[0.585]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  56 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2648866666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09546367381576218, 0.30269019037977607, 0.12515404307786257, 0.4766920927265992]
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.2648866666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09546187918167624, 0.3026907245077812, 0.12516423452653888, 0.4766831617840038]
maxi score, test score, baseline:  -0.2648866666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09546187918167624, 0.3026907245077812, 0.12516423452653888, 0.4766831617840038]
maxi score, test score, baseline:  -0.2648866666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09546187918167624, 0.3026907245077812, 0.12516423452653888, 0.4766831617840038]
actor:  0 policy actor:  0  step number:  52 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7813527
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2621266666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09546187918167624, 0.3026907245077812, 0.12516423452653888, 0.4766831617840038]
maxi score, test score, baseline:  -0.2621266666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09546187918167624, 0.3026907245077812, 0.12516423452653888, 0.4766831617840038]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.333
siam score:  -0.78048617
maxi score, test score, baseline:  -0.2621266666666668 0.6183333333333334 0.6183333333333334
actor:  1 policy actor:  1  step number:  52 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2621266666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.2621266666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.2621266666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
first move QE:  -0.04470851458508824
actor:  1 policy actor:  1  step number:  44 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
siam score:  -0.77766633
maxi score, test score, baseline:  -0.2621266666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.2621266666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2621266666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
siam score:  -0.7783073
maxi score, test score, baseline:  -0.2621266666666668 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.26212666666666673 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
from probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
using another actor
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.461]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]] [[0.889]
 [1.237]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]] [[0.39 ]
 [0.461]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]]
maxi score, test score, baseline:  -0.26212666666666673 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.26212666666666673 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.26212666666666673 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.26212666666666673 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
actor:  0 policy actor:  0  step number:  54 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.333
from probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
siam score:  -0.7810985
maxi score, test score, baseline:  -0.25960666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.25960666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.25960666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.25960666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.892]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.769]] [[0.415]
 [0.078]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.339]] [[0.815]
 [0.892]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.769]]
maxi score, test score, baseline:  -0.25960666666666676 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
actor:  0 policy actor:  0  step number:  37 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.25946000000000013 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.25946000000000013 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.25946000000000013 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
actor:  0 policy actor:  0  step number:  49 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2569533333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.2569533333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.2569533333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.2569533333333335 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
actor:  0 policy actor:  0  step number:  62 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.25796666666666684 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.25796666666666684 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
maxi score, test score, baseline:  -0.25796666666666684 0.6183333333333334 0.6183333333333334
probs:  [0.09373329084750069, 0.31532904571382864, 0.12289696101550089, 0.46804070242316986]
actor:  1 policy actor:  1  step number:  44 total reward:  0.6600000000000003  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.25796666666666684 0.6183333333333334 0.6183333333333334
probs:  [0.09340793347803325, 0.31770784861373674, 0.12247021155131087, 0.4664140063569191]
maxi score, test score, baseline:  -0.25796666666666684 0.6183333333333334 0.6183333333333334
probs:  [0.09340793347803325, 0.31770784861373674, 0.12247021155131087, 0.4664140063569191]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.25796666666666684 0.6183333333333334 0.6183333333333334
probs:  [0.09126649285305624, 0.31041591334159846, 0.14261019306938214, 0.4557074007359632]
maxi score, test score, baseline:  -0.25796666666666684 0.6183333333333334 0.6183333333333334
probs:  [0.09126649285305624, 0.31041591334159846, 0.14261019306938214, 0.4557074007359632]
maxi score, test score, baseline:  -0.25796666666666684 0.6183333333333334 0.6183333333333334
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  0.333
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.39 ]
 [0.301]
 [0.325]
 [0.306]
 [0.299]
 [0.391]] [[ 1.047]
 [ 0.423]
 [ 0.982]
 [ 0.625]
 [-0.314]
 [ 0.719]
 [ 1.006]] [[0.324]
 [0.39 ]
 [0.301]
 [0.325]
 [0.306]
 [0.299]
 [0.391]]
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.951]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.81 ]] [[1.374]
 [1.034]
 [1.374]
 [1.374]
 [1.374]
 [1.374]
 [1.333]] [[0.852]
 [0.951]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.81 ]]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.605]
 [0.469]
 [0.432]
 [0.515]
 [0.428]
 [0.563]] [[1.68 ]
 [1.712]
 [3.499]
 [3.393]
 [1.68 ]
 [3.49 ]
 [3.15 ]] [[0.515]
 [0.605]
 [0.469]
 [0.432]
 [0.515]
 [0.428]
 [0.563]]
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]] [[1.387]
 [1.387]
 [1.387]
 [1.387]
 [1.387]
 [1.387]
 [1.387]] [[0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]]
maxi score, test score, baseline:  -0.25796666666666684 0.6183333333333334 0.6183333333333334
probs:  [0.09126500537153422, 0.3104160214863631, 0.14261896977411487, 0.45570000336798794]
maxi score, test score, baseline:  -0.25796666666666684 0.6183333333333334 0.6183333333333334
probs:  [0.09126500537153422, 0.3104160214863631, 0.14261896977411487, 0.45570000336798794]
maxi score, test score, baseline:  -0.25796666666666684 0.6183333333333334 0.6183333333333334
probs:  [0.09126352776428401, 0.3104161289132359, 0.1426276882169016, 0.4556926551055786]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.20267333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10186742091699649, 0.2596335689399395, 0.1310629160911863, 0.5074360940518776]
maxi score, test score, baseline:  -0.20267333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10186742091699649, 0.2596335689399395, 0.1310629160911863, 0.5074360940518776]
maxi score, test score, baseline:  -0.20267333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10186742091699649, 0.2596335689399395, 0.1310629160911863, 0.5074360940518776]
Printing some Q and Qe and total Qs values:  [[0.797]
 [0.854]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]] [[1.492]
 [1.671]
 [1.492]
 [1.492]
 [1.492]
 [1.492]
 [1.492]] [[0.797]
 [0.854]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]]
maxi score, test score, baseline:  -0.20267333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10186742091699649, 0.2596335689399395, 0.1310629160911863, 0.5074360940518776]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.20267333333333346 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.20267333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
maxi score, test score, baseline:  -0.20267333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
Printing some Q and Qe and total Qs values:  [[ 0.107]
 [ 0.269]
 [ 0.143]
 [ 0.107]
 [ 0.107]
 [-0.009]
 [ 0.155]] [[3.329]
 [2.598]
 [4.327]
 [3.329]
 [3.329]
 [4.323]
 [3.623]] [[0.373]
 [0.253]
 [0.634]
 [0.373]
 [0.373]
 [0.576]
 [0.464]]
maxi score, test score, baseline:  -0.20267333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.855]
 [0.782]
 [0.781]
 [0.807]
 [0.783]
 [0.796]] [[0.602]
 [0.405]
 [0.4  ]
 [0.384]
 [0.326]
 [0.368]
 [0.545]] [[0.777]
 [0.855]
 [0.782]
 [0.781]
 [0.807]
 [0.783]
 [0.796]]
actor:  0 policy actor:  0  step number:  57 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[ 0.148]
 [ 0.122]
 [ 0.115]
 [ 0.115]
 [ 0.116]
 [-0.057]
 [ 0.13 ]] [[2.229]
 [2.543]
 [2.03 ]
 [2.03 ]
 [1.831]
 [0.316]
 [2.442]] [[ 0.399]
 [ 0.492]
 [ 0.315]
 [ 0.315]
 [ 0.248]
 [-0.352]
 [ 0.461]]
maxi score, test score, baseline:  -0.20030000000000012 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
siam score:  -0.75970656
actor:  1 policy actor:  1  step number:  39 total reward:  0.6400000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.20030000000000012 0.6870000000000002 0.6870000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.20030000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
actor:  1 policy actor:  1  step number:  34 total reward:  0.6866666666666669  reward:  1.0 rdn_beta:  0.667
from probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
maxi score, test score, baseline:  -0.20030000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1983550049762766
maxi score, test score, baseline:  -0.20030000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.673]
 [0.673]
 [0.673]
 [0.683]
 [0.673]
 [0.673]] [[1.34 ]
 [1.931]
 [1.931]
 [1.931]
 [1.886]
 [1.931]
 [1.931]] [[0.706]
 [0.673]
 [0.673]
 [0.673]
 [0.683]
 [0.673]
 [0.673]]
maxi score, test score, baseline:  -0.20030000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
siam score:  -0.770791
maxi score, test score, baseline:  -0.20030000000000017 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  50 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.015]
 [ 0.038]
 [-0.015]
 [-0.016]
 [-0.015]
 [-0.015]] [[3.478]
 [3.478]
 [2.611]
 [3.478]
 [1.151]
 [3.478]
 [3.478]] [[ 0.132]
 [ 0.132]
 [-0.104]
 [ 0.132]
 [-0.644]
 [ 0.132]
 [ 0.132]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.20030000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
actor:  0 policy actor:  0  step number:  56 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.19751333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
maxi score, test score, baseline:  -0.19751333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
maxi score, test score, baseline:  -0.19751333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
maxi score, test score, baseline:  -0.19751333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
maxi score, test score, baseline:  -0.19751333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
maxi score, test score, baseline:  -0.19751333333333346 0.6870000000000002 0.6870000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.19751333333333346 0.6870000000000002 0.6870000000000002
actor:  0 policy actor:  0  step number:  50 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.167
siam score:  -0.78122926
maxi score, test score, baseline:  -0.1950733333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
maxi score, test score, baseline:  -0.1950733333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
maxi score, test score, baseline:  -0.1950733333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
actor:  0 policy actor:  0  step number:  61 total reward:  0.039999999999999036  reward:  1.0 rdn_beta:  0.667
from probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
actor:  0 policy actor:  1  step number:  33 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.667
using another actor
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.128]
 [0.102]
 [0.11 ]
 [0.106]
 [0.091]
 [0.1  ]] [[-0.663]
 [ 0.49 ]
 [-0.427]
 [-0.025]
 [-0.028]
 [-0.092]
 [ 0.173]] [[0.11 ]
 [0.128]
 [0.102]
 [0.11 ]
 [0.106]
 [0.091]
 [0.1  ]]
siam score:  -0.77396643
maxi score, test score, baseline:  -0.1895533333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
maxi score, test score, baseline:  -0.18955333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
maxi score, test score, baseline:  -0.18955333333333346 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.18955333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.047726368184183916
maxi score, test score, baseline:  -0.18955333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.18955333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
siam score:  -0.77323943
maxi score, test score, baseline:  -0.18955333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.18955333333333346 0.6870000000000002 0.6870000000000002
probs:  [0.10422679145442408, 0.2459278677616786, 0.13044935656962436, 0.5193959842142729]
actor:  0 policy actor:  0  step number:  48 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  32 total reward:  0.7400000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.18692666666666682 0.6870000000000002 0.6870000000000002
probs:  [0.10094949359535102, 0.23818851619998493, 0.1578166500938587, 0.5030453401108054]
maxi score, test score, baseline:  -0.18692666666666682 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.18692666666666682 0.6870000000000002 0.6870000000000002
probs:  [0.10094949359535102, 0.23818851619998493, 0.1578166500938587, 0.5030453401108054]
maxi score, test score, baseline:  -0.18692666666666682 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  51 total reward:  0.6533333333333338  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.5133333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.18692666666666682 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.18692666666666682 0.6870000000000002 0.6870000000000002
probs:  [0.10094949359535102, 0.23818851619998493, 0.1578166500938587, 0.5030453401108054]
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]] [[3.053]
 [3.053]
 [3.053]
 [3.053]
 [3.053]
 [3.053]
 [3.053]] [[0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]]
Printing some Q and Qe and total Qs values:  [[ 0.579]
 [ 0.701]
 [ 0.629]
 [ 0.589]
 [ 0.606]
 [-0.008]
 [ 0.62 ]] [[1.059]
 [0.623]
 [0.   ]
 [0.915]
 [0.944]
 [0.116]
 [1.32 ]] [[ 0.579]
 [ 0.701]
 [ 0.629]
 [ 0.589]
 [ 0.606]
 [-0.008]
 [ 0.62 ]]
maxi score, test score, baseline:  -0.18692666666666682 0.6870000000000002 0.6870000000000002
probs:  [0.10094949359535102, 0.23818851619998493, 0.1578166500938587, 0.5030453401108054]
maxi score, test score, baseline:  -0.18692666666666682 0.6870000000000002 0.6870000000000002
probs:  [0.10094949359535102, 0.23818851619998493, 0.1578166500938587, 0.5030453401108054]
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.258]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.265]] [[2.025]
 [2.701]
 [2.384]
 [2.384]
 [2.384]
 [2.384]
 [2.322]] [[0.172]
 [0.403]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.253]]
actor:  0 policy actor:  0  step number:  49 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.18666000000000013 0.6870000000000002 0.6870000000000002
probs:  [0.10094949359535102, 0.23818851619998493, 0.1578166500938587, 0.5030453401108054]
maxi score, test score, baseline:  -0.18666000000000013 0.6870000000000002 0.6870000000000002
probs:  [0.10094949359535102, 0.23818851619998493, 0.1578166500938587, 0.5030453401108054]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  49 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.18666000000000013 0.6870000000000002 0.6870000000000002
probs:  [0.10094949359535102, 0.23818851619998493, 0.1578166500938587, 0.5030453401108054]
maxi score, test score, baseline:  -0.18666000000000013 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  39 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]] [[1.217]
 [1.217]
 [1.217]
 [1.217]
 [1.217]
 [1.217]
 [1.217]] [[0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]]
maxi score, test score, baseline:  -0.18666000000000013 0.6870000000000002 0.6870000000000002
probs:  [0.10094949359535102, 0.23818851619998493, 0.1578166500938587, 0.5030453401108054]
maxi score, test score, baseline:  -0.18666000000000013 0.6870000000000002 0.6870000000000002
probs:  [0.10094949359535102, 0.23818851619998493, 0.1578166500938587, 0.5030453401108054]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.18666000000000013 0.6870000000000002 0.6870000000000002
probs:  [0.10094949359535102, 0.23818851619998493, 0.1578166500938587, 0.5030453401108054]
actor:  0 policy actor:  0  step number:  47 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.18399333333333348 0.6870000000000002 0.6870000000000002
probs:  [0.10094949359535102, 0.23818851619998493, 0.1578166500938587, 0.5030453401108054]
maxi score, test score, baseline:  -0.18399333333333348 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  29 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.18399333333333348 0.6870000000000002 0.6870000000000002
probs:  [0.1035196208545631, 0.22611559198439157, 0.15431919770968913, 0.516045589451356]
maxi score, test score, baseline:  -0.18399333333333348 0.6870000000000002 0.6870000000000002
probs:  [0.1035196208545631, 0.22611559198439157, 0.15431919770968913, 0.516045589451356]
maxi score, test score, baseline:  -0.18399333333333348 0.6870000000000002 0.6870000000000002
probs:  [0.1035196208545631, 0.22611559198439157, 0.15431919770968913, 0.516045589451356]
maxi score, test score, baseline:  -0.18399333333333348 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  35 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  39 total reward:  0.6000000000000002  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  50 total reward:  0.40666666666666595  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.18118000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.1035196208545631, 0.22611559198439157, 0.15431919770968913, 0.516045589451356]
siam score:  -0.78727114
actor:  1 policy actor:  1  step number:  31 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.18118000000000015 0.6870000000000002 0.6870000000000002
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.18118000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.1055941727671578, 0.21637058502712983, 0.15149612873254756, 0.5265391134731648]
maxi score, test score, baseline:  -0.18118000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.1055941727671578, 0.21637058502712983, 0.15149612873254756, 0.5265391134731648]
maxi score, test score, baseline:  -0.18118000000000015 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.18118000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.1055941727671578, 0.21637058502712983, 0.15149612873254756, 0.5265391134731648]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.18118000000000015 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.18118000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
siam score:  -0.787798
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.591]
 [0.591]
 [0.591]
 [0.656]
 [0.591]
 [0.591]] [[1.704]
 [2.211]
 [2.211]
 [2.211]
 [2.139]
 [2.211]
 [2.211]] [[0.766]
 [0.591]
 [0.591]
 [0.591]
 [0.656]
 [0.591]
 [0.591]]
maxi score, test score, baseline:  -0.18118000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.731]
 [0.602]
 [0.602]
 [0.677]
 [0.597]
 [0.6  ]] [[1.072]
 [0.975]
 [0.941]
 [1.147]
 [1.066]
 [1.003]
 [1.031]] [[0.609]
 [0.731]
 [0.602]
 [0.602]
 [0.677]
 [0.597]
 [0.6  ]]
maxi score, test score, baseline:  -0.18118000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
actor:  0 policy actor:  0  step number:  62 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  64 total reward:  0.046666666666665746  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
siam score:  -0.77974075
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
Printing some Q and Qe and total Qs values:  [[-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]] [[2.725]
 [2.725]
 [2.725]
 [2.725]
 [2.725]
 [2.725]
 [2.564]] [[0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.41 ]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.757]
 [0.731]
 [0.726]
 [0.723]
 [0.717]
 [0.724]] [[1.295]
 [0.676]
 [0.713]
 [0.685]
 [0.719]
 [0.796]
 [0.818]] [[0.386]
 [0.151]
 [0.15 ]
 [0.131]
 [0.147]
 [0.181]
 [0.198]]
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
siam score:  -0.7742929
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[1.356]
 [1.143]
 [1.143]
 [1.143]
 [1.143]
 [1.143]
 [1.143]] [[0.404]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]]
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
siam score:  -0.77326643
using explorer policy with actor:  1
siam score:  -0.7726241
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
actor:  1 policy actor:  1  step number:  54 total reward:  0.6066666666666671  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.578]
 [0.583]] [[1.235]
 [1.235]
 [1.235]
 [1.235]
 [1.235]
 [1.253]
 [1.349]] [[0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.485]
 [0.553]]
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
from probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
maxi score, test score, baseline:  -0.17696666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
actor:  0 policy actor:  0  step number:  43 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.017]
 [ 0.005]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]] [[1.509]
 [1.713]
 [1.509]
 [1.509]
 [1.509]
 [1.509]
 [1.509]] [[-0.25]
 [-0.11]
 [-0.25]
 [-0.25]
 [-0.25]
 [-0.25]
 [-0.25]]
maxi score, test score, baseline:  -0.17411333333333348 0.6870000000000002 0.6870000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.17411333333333348 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
maxi score, test score, baseline:  -0.17411333333333348 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
maxi score, test score, baseline:  -0.17411333333333348 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
maxi score, test score, baseline:  -0.17411333333333348 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  79 total reward:  0.23999999999999866  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  34 total reward:  0.5133333333333332  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.285]
 [0.146]
 [0.152]
 [0.176]
 [0.167]
 [0.15 ]] [[2.135]
 [2.461]
 [2.486]
 [2.462]
 [2.332]
 [2.417]
 [2.548]] [[0.075]
 [0.353]
 [0.275]
 [0.268]
 [0.228]
 [0.259]
 [0.304]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.17108666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10358492154200655, 0.23129415413112397, 0.14861221103539335, 0.5165087132914762]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]] [[1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]] [[0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9098992977991354
actor:  1 policy actor:  1  step number:  42 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.17108666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10356963964780527, 0.23136984684903894, 0.14862900466238393, 0.5164315088407719]
actor:  1 policy actor:  1  step number:  55 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.17108666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10356963964780527, 0.23136984684903894, 0.14862900466238393, 0.5164315088407719]
maxi score, test score, baseline:  -0.17108666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10356963964780527, 0.23136984684903894, 0.14862900466238393, 0.5164315088407719]
maxi score, test score, baseline:  -0.17108666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10356963964780527, 0.23136984684903894, 0.14862900466238393, 0.5164315088407719]
actor:  0 policy actor:  1  step number:  55 total reward:  0.3333333333333326  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.727]
 [0.707]
 [0.666]
 [0.673]
 [0.108]
 [0.73 ]] [[1.316]
 [0.635]
 [0.566]
 [0.627]
 [0.772]
 [0.55 ]
 [0.896]] [[0.708]
 [0.461]
 [0.423]
 [0.423]
 [0.482]
 [0.072]
 [0.563]]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.045]
 [-0.056]
 [-0.055]
 [-0.057]
 [-0.062]
 [-0.057]] [[-0.949]
 [-0.388]
 [-1.159]
 [-1.155]
 [-0.925]
 [-0.821]
 [-0.769]] [[0.17 ]
 [0.27 ]
 [0.131]
 [0.132]
 [0.169]
 [0.18 ]
 [0.195]]
maxi score, test score, baseline:  -0.16842000000000013 0.6870000000000002 0.6870000000000002
probs:  [0.10356963964780527, 0.23136984684903894, 0.14862900466238393, 0.5164315088407719]
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]] [[1.437]
 [1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]] [[-0.308]
 [-0.42 ]
 [-0.42 ]
 [-0.42 ]
 [-0.42 ]
 [-0.42 ]
 [-0.42 ]]
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.096]
 [-0.096]
 [-0.096]
 [-0.096]
 [-0.096]
 [-0.129]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.522]] [[-0.623]
 [-0.623]
 [-0.623]
 [-0.623]
 [-0.623]
 [-0.623]
 [-0.307]]
maxi score, test score, baseline:  -0.16842000000000013 0.6870000000000002 0.6870000000000002
probs:  [0.10356963964780527, 0.23136984684903894, 0.14862900466238393, 0.5164315088407719]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.16842000000000013 0.6870000000000002 0.6870000000000002
probs:  [0.10715959077477638, 0.21358846749883625, 0.14468392433747437, 0.5345680173889129]
maxi score, test score, baseline:  -0.16842000000000013 0.6870000000000002 0.6870000000000002
probs:  [0.10715959077477638, 0.21358846749883625, 0.14468392433747437, 0.5345680173889129]
maxi score, test score, baseline:  -0.16842000000000013 0.6870000000000002 0.6870000000000002
probs:  [0.10715959077477638, 0.21358846749883625, 0.14468392433747437, 0.5345680173889129]
maxi score, test score, baseline:  -0.16842000000000013 0.6870000000000002 0.6870000000000002
probs:  [0.10715959077477638, 0.21358846749883625, 0.14468392433747437, 0.5345680173889129]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.16842000000000013 0.6870000000000002 0.6870000000000002
probs:  [0.10715959077477638, 0.21358846749883625, 0.14468392433747437, 0.5345680173889129]
actor:  1 policy actor:  1  step number:  43 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.16842000000000013 0.6870000000000002 0.6870000000000002
probs:  [0.10715959077477638, 0.21358846749883625, 0.14468392433747437, 0.5345680173889129]
maxi score, test score, baseline:  -0.16842000000000013 0.6870000000000002 0.6870000000000002
siam score:  -0.77702147
line 256 mcts: sample exp_bonus 1.553240527512183
actor:  0 policy actor:  1  step number:  63 total reward:  0.29333333333333256  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.047]] [[2.054]
 [2.054]
 [2.054]
 [2.054]
 [2.054]
 [2.054]
 [2.162]] [[-0.255]
 [-0.255]
 [-0.255]
 [-0.255]
 [-0.255]
 [-0.255]
 [-0.201]]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]] [[1.866]
 [1.866]
 [1.866]
 [1.866]
 [1.866]
 [1.866]
 [1.866]] [[0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.6066666666666671  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1658333333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10715959077477638, 0.21358846749883625, 0.14468392433747437, 0.5345680173889129]
actor:  1 policy actor:  1  step number:  42 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1658333333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.021]
 [-0.102]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.048]] [[1.121]
 [1.22 ]
 [0.465]
 [1.429]
 [1.429]
 [1.429]
 [1.034]] [[-0.257]
 [-0.219]
 [-0.552]
 [-0.16 ]
 [-0.16 ]
 [-0.16 ]
 [-0.308]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.01 ]
 [-0.041]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]] [[1.349]
 [0.741]
 [1.147]
 [0.741]
 [0.741]
 [0.741]
 [0.741]] [[0.444]
 [0.246]
 [0.366]
 [0.246]
 [0.246]
 [0.246]
 [0.246]]
from probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
maxi score, test score, baseline:  -0.1658333333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
actor:  1 policy actor:  1  step number:  43 total reward:  0.6000000000000003  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1658333333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
maxi score, test score, baseline:  -0.1658333333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
maxi score, test score, baseline:  -0.1658333333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
maxi score, test score, baseline:  -0.1658333333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
Printing some Q and Qe and total Qs values:  [[ 0.37 ]
 [ 0.389]
 [ 0.198]
 [ 0.302]
 [ 0.302]
 [ 0.302]
 [-0.013]] [[3.077]
 [2.546]
 [2.25 ]
 [2.385]
 [2.385]
 [2.385]
 [2.402]] [[ 0.21 ]
 [ 0.052]
 [-0.238]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.398]]
maxi score, test score, baseline:  -0.1658333333333335 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.1658333333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
maxi score, test score, baseline:  -0.1658333333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
actor:  0 policy actor:  0  step number:  47 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.1631400000000002 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1631400000000002 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
from probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
maxi score, test score, baseline:  -0.16314000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
Printing some Q and Qe and total Qs values:  [[0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]] [[1.409]
 [1.409]
 [1.409]
 [1.409]
 [1.409]
 [1.409]
 [1.409]] [[0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]]
maxi score, test score, baseline:  -0.16314000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
maxi score, test score, baseline:  -0.16314000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
maxi score, test score, baseline:  -0.16314000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
maxi score, test score, baseline:  -0.16314000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
maxi score, test score, baseline:  -0.16314000000000017 0.6870000000000002 0.6870000000000002
siam score:  -0.7843078
maxi score, test score, baseline:  -0.16314000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.9  ]
 [0.726]
 [0.787]
 [0.756]
 [0.787]
 [0.752]] [[1.629]
 [0.687]
 [1.967]
 [0.58 ]
 [1.159]
 [0.58 ]
 [1.803]] [[0.739]
 [0.9  ]
 [0.726]
 [0.787]
 [0.756]
 [0.787]
 [0.752]]
maxi score, test score, baseline:  -0.16314000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.16314000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
actor:  0 policy actor:  0  step number:  65 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.16095333333333353 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]] [[0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]] [[0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]]
Printing some Q and Qe and total Qs values:  [[-0.053]
 [-0.062]
 [ 1.5  ]
 [-0.055]
 [ 1.5  ]
 [ 1.5  ]
 [-0.057]] [[-1.294]
 [-0.06 ]
 [ 0.   ]
 [-1.047]
 [ 0.   ]
 [ 0.   ]
 [-1.068]] [[-0.054]
 [ 0.143]
 [ 1.715]
 [-0.014]
 [ 1.715]
 [ 1.715]
 [-0.02 ]]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.064]
 [-0.056]
 [-0.066]
 [-0.055]
 [-0.057]
 [-0.071]] [[-0.795]
 [-0.23 ]
 [-0.963]
 [-0.897]
 [-0.932]
 [-1.168]
 [-0.48 ]] [[ 0.014]
 [ 0.113]
 [-0.001]
 [ 0.   ]
 [ 0.005]
 [-0.037]
 [ 0.064]]
maxi score, test score, baseline:  -0.16095333333333353 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
maxi score, test score, baseline:  -0.16095333333333353 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.16095333333333353 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
maxi score, test score, baseline:  -0.16095333333333353 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  39 total reward:  0.6533333333333337  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.16095333333333353 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.16095333333333353 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
maxi score, test score, baseline:  -0.16095333333333353 0.6870000000000002 0.6870000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.16095333333333353 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
maxi score, test score, baseline:  -0.16095333333333353 0.6870000000000002 0.6870000000000002
probs:  [0.10661348834150938, 0.21759946584796905, 0.1439463239974079, 0.5318407218131137]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.16095333333333353 0.6870000000000002 0.6870000000000002
probs:  [0.10690405584741179, 0.21613965786791417, 0.14364811009306816, 0.5333081761916058]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  49 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.167
from probs:  [0.10690405584741179, 0.21613965786791417, 0.14364811009306816, 0.5333081761916058]
maxi score, test score, baseline:  -0.15512666666666683 0.6870000000000002 0.6870000000000002
probs:  [0.10690405584741179, 0.21613965786791417, 0.14364811009306816, 0.5333081761916058]
maxi score, test score, baseline:  -0.15512666666666683 0.6870000000000002 0.6870000000000002
probs:  [0.10690405584741179, 0.21613965786791417, 0.14364811009306816, 0.5333081761916058]
line 256 mcts: sample exp_bonus 1.597578986592381
maxi score, test score, baseline:  -0.15746000000000016 0.6870000000000002 0.6870000000000002
probs:  [0.10690405584741179, 0.21613965786791417, 0.14364811009306816, 0.5333081761916058]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.667
siam score:  -0.78062147
maxi score, test score, baseline:  -0.15746000000000016 0.6870000000000002 0.6870000000000002
probs:  [0.10690405584741179, 0.21613965786791417, 0.14364811009306816, 0.5333081761916058]
maxi score, test score, baseline:  -0.15746000000000016 0.6870000000000002 0.6870000000000002
from probs:  [0.10690405584741179, 0.21613965786791417, 0.14364811009306816, 0.5333081761916058]
maxi score, test score, baseline:  -0.15746000000000018 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.15746000000000018 0.6870000000000002 0.6870000000000002
probs:  [0.10690405584741179, 0.21613965786791417, 0.14364811009306816, 0.5333081761916058]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  51 total reward:  0.30666666666666653  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 1.871948293732554
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.486666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
Printing some Q and Qe and total Qs values:  [[-0.058]
 [-0.043]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.048]] [[0.248]
 [0.473]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.771]] [[-0.801]
 [-0.748]
 [-0.852]
 [-0.852]
 [-0.852]
 [-0.852]
 [-0.704]]
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6200000000000002  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
maxi score, test score, baseline:  -0.15734000000000012 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
Printing some Q and Qe and total Qs values:  [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]]
actor:  0 policy actor:  1  step number:  54 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.15468666666666683 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
line 256 mcts: sample exp_bonus 1.1345094627964831
maxi score, test score, baseline:  -0.15468666666666683 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949125, 0.23583353945832702, 0.14004075972427463, 0.5199052995879071]
Printing some Q and Qe and total Qs values:  [[-0.153]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.153]] [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[-0.53]
 [-0.53]
 [-0.53]
 [-0.53]
 [-0.53]
 [-0.53]
 [-0.53]]
maxi score, test score, baseline:  -0.15468666666666683 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.15468666666666683 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.15468666666666683 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
actor:  1 policy actor:  1  step number:  52 total reward:  0.5266666666666671  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.15468666666666683 0.6870000000000002 0.6870000000000002
using another actor
maxi score, test score, baseline:  -0.15468666666666683 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.15468666666666683 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
first move QE:  -0.03993497458959648
maxi score, test score, baseline:  -0.15468666666666683 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
actor:  0 policy actor:  1  step number:  61 total reward:  0.23999999999999988  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.15220666666666682 0.6870000000000002 0.6870000000000002
from probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
maxi score, test score, baseline:  -0.15220666666666682 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.15220666666666682 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
maxi score, test score, baseline:  -0.15220666666666682 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
using explorer policy with actor:  0
siam score:  -0.7625793
siam score:  -0.76313215
maxi score, test score, baseline:  -0.15426000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.066]
 [-0.014]
 [-0.025]
 [-0.025]
 [-0.029]
 [-0.036]] [[1.554]
 [1.744]
 [1.7  ]
 [1.332]
 [1.332]
 [1.24 ]
 [1.397]] [[-0.17 ]
 [-0.032]
 [-0.106]
 [-0.281]
 [-0.281]
 [-0.326]
 [-0.259]]
actor:  0 policy actor:  0  step number:  48 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7628967
siam score:  -0.76338714
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
actor:  1 policy actor:  1  step number:  55 total reward:  0.506666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.138]] [[2.373]
 [2.244]
 [2.244]
 [2.244]
 [2.244]
 [2.244]
 [2.185]] [[ 0.138]
 [ 0.033]
 [ 0.033]
 [ 0.033]
 [ 0.033]
 [ 0.033]
 [-0.037]]
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.025]
 [ 0.108]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]] [[1.792]
 [1.792]
 [1.634]
 [1.792]
 [1.792]
 [1.792]
 [1.792]] [[0.288]
 [0.288]
 [0.339]
 [0.288]
 [0.288]
 [0.288]
 [0.288]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
actor:  1 policy actor:  1  step number:  44 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  40 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.166]
 [0.074]
 [0.099]
 [0.022]
 [0.076]
 [0.137]] [[2.019]
 [2.488]
 [2.052]
 [2.05 ]
 [1.496]
 [1.924]
 [2.286]] [[-0.263]
 [ 0.088]
 [-0.228]
 [-0.208]
 [-0.577]
 [-0.297]
 [-0.048]]
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
actor:  1 policy actor:  1  step number:  56 total reward:  0.5400000000000004  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.06 ]
 [-0.052]
 [-0.055]
 [-0.052]
 [-0.057]
 [-0.118]
 [-0.055]] [[1.398]
 [1.068]
 [0.862]
 [1.068]
 [1.127]
 [0.628]
 [1.3  ]] [[ 0.136]
 [-0.018]
 [-0.12 ]
 [-0.018]
 [ 0.007]
 [-0.278]
 [ 0.092]]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.037]
 [-0.044]
 [-0.044]
 [-0.064]
 [-0.044]
 [-0.062]] [[0.388]
 [0.502]
 [0.769]
 [0.769]
 [0.408]
 [0.769]
 [0.306]] [[0.615]
 [0.669]
 [0.733]
 [0.733]
 [0.623]
 [0.733]
 [0.598]]
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
siam score:  -0.76613444
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.1514466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
actor:  0 policy actor:  0  step number:  51 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.169]
 [0.144]
 [0.101]
 [0.04 ]
 [0.04 ]
 [0.008]] [[2.048]
 [2.17 ]
 [2.206]
 [2.033]
 [2.535]
 [2.535]
 [2.067]] [[ 0.053]
 [ 0.132]
 [ 0.126]
 [ 0.024]
 [ 0.174]
 [ 0.174]
 [-0.036]]
Printing some Q and Qe and total Qs values:  [[-0.093]
 [-0.093]
 [-0.079]
 [-0.093]
 [-0.093]
 [-0.047]
 [-0.078]] [[1.103]
 [1.103]
 [1.64 ]
 [1.103]
 [1.103]
 [0.664]
 [0.768]] [[0.478]
 [0.478]
 [0.663]
 [0.478]
 [0.478]
 [0.365]
 [0.378]]
maxi score, test score, baseline:  -0.15074000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.546]
 [0.417]
 [0.475]
 [0.46 ]
 [0.451]
 [0.45 ]] [[1.599]
 [1.656]
 [1.49 ]
 [1.843]
 [1.723]
 [1.813]
 [1.712]] [[0.234]
 [0.322]
 [0.109]
 [0.344]
 [0.269]
 [0.305]
 [0.253]]
maxi score, test score, baseline:  -0.15074000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
maxi score, test score, baseline:  -0.15074000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
first move QE:  -0.039001023050779376
maxi score, test score, baseline:  -0.15074000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
maxi score, test score, baseline:  -0.15074000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
maxi score, test score, baseline:  -0.15074000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
maxi score, test score, baseline:  -0.15074000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
actor:  0 policy actor:  0  step number:  46 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.1503266666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
actor:  0 policy actor:  1  step number:  42 total reward:  0.5  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.03921723767403912
Printing some Q and Qe and total Qs values:  [[-0.065]
 [-0.065]
 [-0.019]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.133]] [[2.85 ]
 [2.85 ]
 [2.336]
 [2.85 ]
 [2.85 ]
 [2.85 ]
 [0.14 ]] [[ 0.62 ]
 [ 0.62 ]
 [ 0.486]
 [ 0.62 ]
 [ 0.62 ]
 [ 0.62 ]
 [-0.228]]
maxi score, test score, baseline:  -0.1473266666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
maxi score, test score, baseline:  -0.1473266666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10422040122949124, 0.23583353945832708, 0.1400407597242746, 0.519905299587907]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.742]
 [0.598]
 [0.656]
 [0.667]
 [0.582]
 [0.706]] [[0.365]
 [0.197]
 [0.232]
 [0.147]
 [0.231]
 [0.307]
 [0.27 ]] [[-0.135]
 [ 0.008]
 [-0.131]
 [-0.087]
 [-0.061]
 [-0.134]
 [-0.016]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  0.8000000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1473266666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10739999629102766, 0.21891058732801374, 0.1377491650796713, 0.5359402513012873]
maxi score, test score, baseline:  -0.1473266666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10739999629102766, 0.21891058732801374, 0.1377491650796713, 0.5359402513012873]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.269]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.236]] [[2.023]
 [2.662]
 [2.483]
 [2.483]
 [2.483]
 [2.483]
 [2.15 ]] [[0.38 ]
 [0.556]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.402]]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.674]
 [0.529]
 [0.492]
 [0.1  ]
 [0.484]
 [0.548]] [[1.743]
 [1.494]
 [1.248]
 [1.421]
 [0.174]
 [1.896]
 [1.609]] [[ 0.616]
 [ 0.599]
 [ 0.41 ]
 [ 0.458]
 [-0.29 ]
 [ 0.647]
 [ 0.568]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.104]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]] [[1.502]
 [1.502]
 [1.502]
 [1.502]
 [1.502]
 [1.502]
 [1.502]] [[-0.21]
 [-0.21]
 [-0.21]
 [-0.21]
 [-0.21]
 [-0.21]
 [-0.21]]
siam score:  -0.76344544
maxi score, test score, baseline:  -0.1473266666666668 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.1473266666666668 0.6870000000000002 0.6870000000000002
using explorer policy with actor:  1
from probs:  [0.1099478707086315, 0.2053498803924822, 0.13591286352003765, 0.5487893853788488]
actor:  1 policy actor:  1  step number:  36 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1473266666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.015]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.026]] [[0.965]
 [1.198]
 [1.08 ]
 [1.08 ]
 [1.08 ]
 [1.08 ]
 [1.092]] [[0.188]
 [0.262]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.229]]
maxi score, test score, baseline:  -0.1473266666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.14732666666666683 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
maxi score, test score, baseline:  -0.14732666666666683 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
maxi score, test score, baseline:  -0.14732666666666683 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
maxi score, test score, baseline:  -0.14732666666666683 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.14732666666666683 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
actor:  0 policy actor:  1  step number:  55 total reward:  0.346666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1446333333333335 0.6870000000000002 0.6870000000000002
Printing some Q and Qe and total Qs values:  [[-0.132]
 [-0.164]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.279]] [[1.514]
 [1.316]
 [1.166]
 [1.166]
 [1.166]
 [1.166]
 [2.565]] [[ 0.118]
 [-0.021]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [ 0.589]]
maxi score, test score, baseline:  -0.1446333333333335 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.1446333333333335 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.1446333333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
maxi score, test score, baseline:  -0.1446333333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
maxi score, test score, baseline:  -0.1446333333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1446333333333335 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.1446333333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
maxi score, test score, baseline:  -0.1446333333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
actor:  0 policy actor:  1  step number:  55 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  36 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.02 ]
 [ 0.02 ]
 [ 0.02 ]
 [ 0.02 ]
 [ 0.02 ]
 [-0.027]] [[2.072]
 [1.979]
 [1.979]
 [1.979]
 [1.979]
 [1.979]
 [1.32 ]] [[0.389]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.11 ]]
maxi score, test score, baseline:  -0.14159333333333352 0.6870000000000002 0.6870000000000002
probs:  [0.1099739232952175, 0.20521121912677598, 0.13589408692530058, 0.548920770652706]
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.157]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]] [[1.261]
 [2.003]
 [1.261]
 [1.261]
 [1.261]
 [1.261]
 [1.261]] [[-0.378]
 [ 0.084]
 [-0.378]
 [-0.378]
 [-0.378]
 [-0.378]
 [-0.378]]
maxi score, test score, baseline:  -0.14159333333333352 0.6870000000000002 0.6870000000000002
probs:  [0.1099739232952175, 0.20521121912677598, 0.13589408692530058, 0.548920770652706]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.088]
 [0.269]
 [0.088]
 [0.212]
 [0.088]
 [0.088]] [[1.12 ]
 [1.998]
 [2.091]
 [1.998]
 [0.806]
 [1.998]
 [1.998]] [[-0.028]
 [ 0.168]
 [ 0.305]
 [ 0.168]
 [-0.213]
 [ 0.168]
 [ 0.168]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.14159333333333352 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.14159333333333352 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.1415933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
siam score:  -0.7561359
maxi score, test score, baseline:  -0.1415933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1415933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.019]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.032]] [[0.924]
 [1.102]
 [1.099]
 [1.099]
 [1.099]
 [1.099]
 [0.88 ]] [[0.27 ]
 [0.334]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.253]]
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.3568792165505816
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10997392329521749, 0.20521121912677595, 0.13589408692530056, 0.5489207706527058]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  37 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.000518348336112
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8647439287469024
first move QE:  -0.040441588041140716
Printing some Q and Qe and total Qs values:  [[0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]] [[1.819]
 [1.819]
 [1.819]
 [1.819]
 [1.819]
 [1.819]
 [1.819]] [[0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]]
siam score:  -0.7598332
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
actor:  1 policy actor:  1  step number:  62 total reward:  0.486666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
from probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
maxi score, test score, baseline:  -0.1384466666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
actor:  0 policy actor:  0  step number:  51 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  0.667
first move QE:  -0.040471339773965956
Printing some Q and Qe and total Qs values:  [[ 0.348]
 [ 0.427]
 [-0.005]
 [ 0.371]
 [ 0.315]
 [ 0.344]
 [ 0.385]] [[ 0.677]
 [ 0.053]
 [-0.02 ]
 [ 0.81 ]
 [ 0.976]
 [ 0.   ]
 [ 0.535]] [[ 0.348]
 [ 0.427]
 [-0.005]
 [ 0.371]
 [ 0.315]
 [ 0.344]
 [ 0.385]]
maxi score, test score, baseline:  -0.13575333333333348 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0745992488367662
Printing some Q and Qe and total Qs values:  [[ 0.107]
 [ 0.107]
 [-0.009]
 [ 0.107]
 [ 0.107]
 [ 0.107]
 [ 0.107]] [[1.984]
 [1.984]
 [2.101]
 [1.984]
 [1.984]
 [1.984]
 [1.984]] [[0.093]
 [0.093]
 [0.035]
 [0.093]
 [0.093]
 [0.093]
 [0.093]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  0 policy actor:  0  step number:  46 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.1329133333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
maxi score, test score, baseline:  -0.1329133333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10653160795124021, 0.19874082520650535, 0.16300715096683296, 0.5317204158754215]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7333333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1329133333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10691639866872436, 0.19720831994409677, 0.1622176520112327, 0.5336576293759462]
maxi score, test score, baseline:  -0.1329133333333335 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.1329133333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10691639866872436, 0.19720831994409677, 0.1622176520112327, 0.5336576293759462]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.1329133333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10691639866872436, 0.19720831994409677, 0.1622176520112327, 0.5336576293759462]
actor:  1 policy actor:  1  step number:  46 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1329133333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10691639866872436, 0.19720831994409677, 0.1622176520112327, 0.5336576293759462]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.10691639866872436, 0.19720831994409677, 0.1622176520112327, 0.5336576293759462]
maxi score, test score, baseline:  -0.1329133333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
maxi score, test score, baseline:  -0.1329133333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
maxi score, test score, baseline:  -0.1329133333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
maxi score, test score, baseline:  -0.1329133333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
maxi score, test score, baseline:  -0.1329133333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
maxi score, test score, baseline:  -0.1329133333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
actor:  0 policy actor:  1  step number:  58 total reward:  0.25999999999999934  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1303933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
siam score:  -0.759328
maxi score, test score, baseline:  -0.1303933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
maxi score, test score, baseline:  -0.1303933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1303933333333335 0.6870000000000002 0.6870000000000002
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.675]
 [0.731]
 [0.81 ]
 [0.536]
 [0.627]
 [0.636]] [[1.356]
 [1.265]
 [1.304]
 [1.398]
 [1.771]
 [2.485]
 [2.11 ]] [[0.494]
 [0.675]
 [0.731]
 [0.81 ]
 [0.536]
 [0.627]
 [0.636]]
maxi score, test score, baseline:  -0.1303933333333335 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.1303933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
actor:  0 policy actor:  0  step number:  62 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
siam score:  -0.75271225
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
line 256 mcts: sample exp_bonus 2.900993651047037
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
actor:  1 policy actor:  1  step number:  54 total reward:  0.446666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  0.5866666666666668  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.161]
 [0.231]
 [0.301]
 [0.123]
 [0.218]
 [0.276]] [[2.478]
 [2.428]
 [2.384]
 [1.851]
 [0.893]
 [2.592]
 [2.522]] [[ 0.306]
 [ 0.212]
 [ 0.233]
 [ 0.088]
 [-0.328]
 [ 0.297]
 [ 0.302]]
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10703729595847773, 0.19672682253696247, 0.16196959955139364, 0.5342662819531662]
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.331]
 [0.326]
 [0.255]
 [0.255]
 [0.278]
 [0.303]] [[1.523]
 [1.189]
 [2.283]
 [1.76 ]
 [1.76 ]
 [2.524]
 [1.529]] [[-0.243]
 [-0.195]
 [ 0.162]
 [-0.081]
 [-0.081]
 [ 0.194]
 [-0.11 ]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  32 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10594441960991438, 0.2049353379777958, 0.16031506464171222, 0.5288051777705775]
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10594441960991438, 0.2049353379777958, 0.16031506464171222, 0.5288051777705775]
actor:  1 policy actor:  1  step number:  57 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10594441960991438, 0.2049353379777958, 0.16031506464171222, 0.5288051777705775]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.1281933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.10594441960991438, 0.2049353379777958, 0.16031506464171222, 0.5288051777705775]
actor:  1 policy actor:  1  step number:  30 total reward:  0.8066666666666668  reward:  1.0 rdn_beta:  0.667
from probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
maxi score, test score, baseline:  -0.13030000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.798833083556783
maxi score, test score, baseline:  -0.13030000000000017 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.13030000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
maxi score, test score, baseline:  -0.13030000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
maxi score, test score, baseline:  -0.13030000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
actor:  0 policy actor:  0  step number:  52 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.13103333333333347 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
maxi score, test score, baseline:  -0.13103333333333347 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.13103333333333347 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
maxi score, test score, baseline:  -0.13434000000000015 0.6870000000000002 0.6870000000000002
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.13434000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.839]
 [0.754]
 [0.753]
 [0.751]
 [0.805]
 [0.823]] [[4.43 ]
 [4.749]
 [1.978]
 [2.497]
 [2.22 ]
 [4.128]
 [4.013]] [[0.822]
 [0.839]
 [0.754]
 [0.753]
 [0.751]
 [0.805]
 [0.823]]
maxi score, test score, baseline:  -0.13434000000000015 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.13434000000000015 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
start point for exploration sampling:  11715
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.711]
 [0.716]
 [0.722]
 [0.721]
 [0.725]
 [0.696]] [[1.501]
 [1.51 ]
 [1.504]
 [1.517]
 [1.52 ]
 [1.507]
 [1.512]] [[0.82 ]
 [0.812]
 [0.813]
 [0.823]
 [0.824]
 [0.821]
 [0.8  ]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5533333333333332  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.13434000000000015 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
using explorer policy with actor:  1
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
actor:  1 policy actor:  1  step number:  42 total reward:  0.54  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[ 0.356]
 [ 0.384]
 [-0.01 ]
 [ 0.356]
 [ 0.356]
 [ 0.356]
 [ 0.306]] [[3.196]
 [2.888]
 [2.924]
 [3.196]
 [3.196]
 [3.196]
 [3.245]] [[0.461]
 [0.415]
 [0.317]
 [0.461]
 [0.461]
 [0.461]
 [0.457]]
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.584]
 [0.379]
 [0.485]
 [0.489]
 [0.485]
 [0.543]] [[0.962]
 [0.889]
 [1.019]
 [0.978]
 [1.01 ]
 [0.845]
 [0.953]] [[-0.056]
 [ 0.082]
 [-0.037]
 [ 0.042]
 [ 0.067]
 [-0.046]
 [ 0.084]]
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10823561139897991, 0.1953474977656476, 0.15608171213006622, 0.5403351787053062]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  39 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  48 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10828989308048458, 0.19512034782071608, 0.1559814178230462, 0.5406083412757531]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10828989308048458, 0.19512034782071608, 0.1559814178230462, 0.5406083412757531]
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10828989308048458, 0.19512034782071608, 0.1559814178230462, 0.5406083412757531]
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  46 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10828989308048458, 0.19512034782071608, 0.1559814178230462, 0.5406083412757531]
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10828989308048458, 0.19512034782071608, 0.1559814178230462, 0.5406083412757531]
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10828989308048458, 0.19512034782071608, 0.1559814178230462, 0.5406083412757531]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10828989308048458, 0.19512034782071608, 0.1559814178230462, 0.5406083412757531]
maxi score, test score, baseline:  -0.13764666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.10828989308048458, 0.19512034782071608, 0.1559814178230462, 0.5406083412757531]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
main train batch thing paused
add a thread
Adding thread: now have 4 threads
actor:  1 policy actor:  1  step number:  54 total reward:  0.32666666666666666  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
add a thread
Adding thread: now have 5 threads
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0550823520427803
Printing some Q and Qe and total Qs values:  [[ 0.027]
 [ 0.079]
 [ 0.01 ]
 [ 0.014]
 [ 0.013]
 [-0.001]
 [ 0.039]] [[-0.862]
 [ 0.105]
 [-1.298]
 [-1.392]
 [-1.479]
 [-1.411]
 [-0.218]] [[ 0.027]
 [ 0.079]
 [ 0.01 ]
 [ 0.014]
 [ 0.013]
 [-0.001]
 [ 0.039]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.1409266666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10828989308048458, 0.19512034782071608, 0.1559814178230462, 0.5406083412757531]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.1409266666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10828989308048458, 0.19512034782071608, 0.1559814178230462, 0.5406083412757531]
line 256 mcts: sample exp_bonus 1.1368709865669622
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.303]
 [0.229]
 [0.229]
 [0.229]
 [0.17 ]
 [0.187]] [[0.487]
 [0.867]
 [0.487]
 [0.487]
 [0.487]
 [0.75 ]
 [0.819]] [[-0.361]
 [-0.033]
 [-0.361]
 [-0.361]
 [-0.361]
 [-0.244]
 [-0.181]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
line 256 mcts: sample exp_bonus 0.4782457948550517
Printing some Q and Qe and total Qs values:  [[0.894]
 [1.019]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]] [[0.914]
 [0.936]
 [0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]] [[0.894]
 [1.019]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]]
actor:  1 policy actor:  1  step number:  90 total reward:  0.23333333333333173  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1409266666666668 0.6870000000000002 0.6870000000000002
probs:  [0.10828989308048458, 0.19512034782071608, 0.1559814178230462, 0.5406083412757531]
actor:  0 policy actor:  1  step number:  44 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  45 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  0.667
from probs:  [0.10828989308048458, 0.19512034782071608, 0.1559814178230462, 0.5406083412757531]
actor:  0 policy actor:  0  step number:  57 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.14200666666666684 0.6870000000000002 0.6870000000000002
probs:  [0.10828989308048458, 0.19512034782071608, 0.1559814178230462, 0.5406083412757531]
maxi score, test score, baseline:  -0.14527333333333345 0.6870000000000002 0.6870000000000002
probs:  [0.10828989308048458, 0.19512034782071608, 0.1559814178230462, 0.5406083412757531]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.814]
 [0.798]
 [0.762]
 [0.745]
 [0.766]
 [0.809]] [[1.367]
 [1.649]
 [1.79 ]
 [1.995]
 [1.69 ]
 [1.892]
 [2.   ]] [[0.732]
 [0.814]
 [0.798]
 [0.762]
 [0.745]
 [0.766]
 [0.809]]
using explorer policy with actor:  0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  0.6066666666666669  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  30 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  11715
actor:  1 policy actor:  1  step number:  57 total reward:  0.2133333333333326  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.14527333333333345 0.6870000000000002 0.6870000000000002
probs:  [0.10549665085298422, 0.19008428581439255, 0.1777706956951786, 0.5266483676374446]
actor:  0 policy actor:  1  step number:  59 total reward:  0.31999999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.14595333333333352 0.6870000000000002 0.6870000000000002
probs:  [0.10549665085298422, 0.19008428581439255, 0.1777706956951786, 0.5266483676374446]
maxi score, test score, baseline:  -0.14595333333333352 0.6870000000000002 0.6870000000000002
probs:  [0.10549665085298422, 0.19008428581439255, 0.1777706956951786, 0.5266483676374446]
maxi score, test score, baseline:  -0.14918000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10549665085298422, 0.19008428581439255, 0.1777706956951786, 0.5266483676374446]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.028]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]] [[0.979]
 [1.52 ]
 [0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]] [[0.267]
 [0.422]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.333
siam score:  -0.74901277
maxi score, test score, baseline:  -0.14918000000000017 0.6870000000000002 0.6870000000000002
probs:  [0.10140330472882303, 0.22153684356048553, 0.17086907397474943, 0.5061907777359421]
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.631]
 [0.665]
 [0.631]
 [0.631]
 [0.631]
 [0.631]] [[1.982]
 [1.982]
 [1.966]
 [1.982]
 [1.982]
 [1.982]
 [1.982]] [[0.335]
 [0.335]
 [0.357]
 [0.335]
 [0.335]
 [0.335]
 [0.335]]
actor:  0 policy actor:  0  step number:  45 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.8266666666666667  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7484035
actor:  1 policy actor:  1  step number:  40 total reward:  0.5400000000000003  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511736, 0.15876614016760834, 0.4703156071657234]
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511736, 0.15876614016760834, 0.4703156071657234]
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511736, 0.15876614016760834, 0.4703156071657234]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511736, 0.15876614016760834, 0.4703156071657234]
actor:  1 policy actor:  1  step number:  36 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  40 total reward:  0.5933333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511735, 0.15876614016760834, 0.4703156071657234]
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]] [[2.056]
 [2.056]
 [2.056]
 [2.056]
 [2.056]
 [2.056]
 [2.056]] [[0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]]
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511735, 0.15876614016760834, 0.4703156071657234]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511735, 0.15876614016760834, 0.4703156071657234]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511735, 0.15876614016760834, 0.4703156071657234]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.766]
 [0.   ]
 [0.766]
 [0.766]
 [0.766]
 [0.   ]] [[-0.   ]
 [ 1.381]
 [ 0.   ]
 [ 1.381]
 [ 1.381]
 [ 1.381]
 [-0.   ]] [[-0.332]
 [ 0.801]
 [-0.332]
 [ 0.801]
 [ 0.801]
 [ 0.801]
 [-0.332]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5466666666666671  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  50 total reward:  0.5266666666666671  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511735, 0.15876614016760834, 0.4703156071657234]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9201625757688817
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511735, 0.15876614016760834, 0.4703156071657234]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511735, 0.15876614016760834, 0.4703156071657234]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.128]
 [0.165]
 [0.262]
 [0.329]
 [0.355]
 [0.261]] [[2.349]
 [1.429]
 [0.831]
 [1.604]
 [1.854]
 [1.857]
 [1.898]] [[ 0.04 ]
 [-0.404]
 [-0.467]
 [-0.241]
 [-0.133]
 [-0.106]
 [-0.192]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.27333333333333254  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.566666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511735, 0.15876614016760834, 0.4703156071657234]
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  40 total reward:  0.54  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511735, 0.15876614016760834, 0.4703156071657234]
Printing some Q and Qe and total Qs values:  [[0.65]
 [0.65]
 [0.65]
 [0.65]
 [0.65]
 [0.65]
 [0.65]] [[1.714]
 [1.714]
 [1.714]
 [1.714]
 [1.714]
 [1.714]
 [1.714]] [[0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511735, 0.15876614016760834, 0.4703156071657234]
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511735, 0.15876614016760834, 0.4703156071657234]
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09422506471549473, 0.2766931879511735, 0.15876614016760834, 0.4703156071657234]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09274434997809025, 0.27234047964486446, 0.171999830361987, 0.4629153400150583]
maxi score, test score, baseline:  -0.1497933333333335 0.6870000000000002 0.6870000000000002
probs:  [0.09274434997809025, 0.27234047964486446, 0.171999830361987, 0.4629153400150583]
actor:  0 policy actor:  1  step number:  41 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.15619333333333352 0.6870000000000002 0.6870000000000002
probs:  [0.09274434997809025, 0.27234047964486446, 0.171999830361987, 0.4629153400150583]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.667
using another actor
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.287]
 [0.095]
 [0.287]
 [0.38 ]
 [0.064]
 [0.331]] [[1.825]
 [1.825]
 [2.358]
 [1.825]
 [0.995]
 [2.377]
 [2.648]] [[-0.076]
 [-0.076]
 [ 0.044]
 [-0.076]
 [-0.415]
 [ 0.03 ]
 [ 0.36 ]]
siam score:  -0.7522128
maxi score, test score, baseline:  -0.15619333333333352 0.6870000000000002 0.6870000000000002
probs:  [0.09274434997809025, 0.27234047964486446, 0.171999830361987, 0.4629153400150583]
maxi score, test score, baseline:  -0.15619333333333352 0.6870000000000002 0.6870000000000002
probs:  [0.09274434997809025, 0.27234047964486446, 0.171999830361987, 0.4629153400150583]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5333333333333335  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.5000000000000004  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.15619333333333352 0.6870000000000002 0.6870000000000002
probs:  [0.09274434997809025, 0.27234047964486446, 0.171999830361987, 0.4629153400150583]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.134]
 [0.072]
 [0.065]
 [0.066]
 [0.091]
 [0.075]] [[-0.732]
 [-0.08 ]
 [-0.611]
 [-1.22 ]
 [-1.323]
 [-1.239]
 [-0.829]] [[0.087]
 [0.134]
 [0.072]
 [0.065]
 [0.066]
 [0.091]
 [0.075]]
maxi score, test score, baseline:  -0.15619333333333352 0.6870000000000002 0.6870000000000002
probs:  [0.09274434997809025, 0.27234047964486446, 0.171999830361987, 0.4629153400150583]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.15619333333333352 0.6870000000000002 0.6870000000000002
line 256 mcts: sample exp_bonus 1.7593836132076381
siam score:  -0.75824493
maxi score, test score, baseline:  -0.15940666666666686 0.6870000000000002 0.6870000000000002
probs:  [0.09274434997809025, 0.27234047964486446, 0.171999830361987, 0.4629153400150583]
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.931]
 [0.831]
 [0.897]
 [0.77 ]
 [0.647]
 [0.845]] [[1.512]
 [0.492]
 [1.808]
 [0.665]
 [1.08 ]
 [1.255]
 [1.151]] [[0.77 ]
 [0.931]
 [0.831]
 [0.897]
 [0.77 ]
 [0.647]
 [0.845]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.13600594272613525
maxi score, test score, baseline:  -0.15940666666666686 0.6870000000000002 0.6870000000000002
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.868]
 [0.983]
 [0.875]
 [0.881]
 [0.86 ]
 [0.854]
 [0.949]] [[0.557]
 [0.782]
 [0.279]
 [0.731]
 [0.46 ]
 [0.436]
 [0.727]] [[0.868]
 [0.983]
 [0.875]
 [0.881]
 [0.86 ]
 [0.854]
 [0.949]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]]
actor:  0 policy actor:  1  step number:  78 total reward:  0.019999999999999685  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.10454000000000015 0.6963333333333334 0.6963333333333334
probs:  [0.0952127525939554, 0.26303450890746416, 0.16658726448549133, 0.4751654740130892]
maxi score, test score, baseline:  -0.10454000000000015 0.6963333333333334 0.6963333333333334
probs:  [0.0952127525939554, 0.26303450890746416, 0.16658726448549133, 0.4751654740130892]
maxi score, test score, baseline:  -0.10454000000000015 0.6963333333333334 0.6963333333333334
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.203]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]] [[1.278]
 [1.418]
 [1.278]
 [1.278]
 [1.278]
 [1.278]
 [1.278]] [[-0.588]
 [-0.5  ]
 [-0.588]
 [-0.588]
 [-0.588]
 [-0.588]
 [-0.588]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  53 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7595064
actor:  0 policy actor:  1  step number:  40 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  46 total reward:  0.6200000000000003  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.09567333333333349 0.6963333333333334 0.6963333333333334
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[-0.027]
 [ 0.054]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]] [[2.26 ]
 [2.303]
 [2.26 ]
 [2.26 ]
 [2.26 ]
 [2.26 ]
 [2.26 ]] [[-0.343]
 [-0.248]
 [-0.343]
 [-0.343]
 [-0.343]
 [-0.343]
 [-0.343]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
siam score:  -0.7611209
actor:  1 policy actor:  1  step number:  32 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  0.7133333333333335  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09143972105972974, 0.2922667211703842, 0.15998138809925683, 0.45631216967062915]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5800000000000002  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.012]
 [ 0.007]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]] [[1.998]
 [1.955]
 [1.998]
 [1.998]
 [1.998]
 [1.998]
 [1.998]] [[0.379]
 [0.374]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.09567333333333349 0.6963333333333334 0.6963333333333334
probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
line 256 mcts: sample exp_bonus 1.444637569423238
maxi score, test score, baseline:  -0.09567333333333349 0.6963333333333334 0.6963333333333334
probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
maxi score, test score, baseline:  -0.09567333333333347 0.6963333333333334 0.6963333333333334
maxi score, test score, baseline:  -0.09567333333333347 0.6963333333333334 0.6963333333333334
probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
using explorer policy with actor:  0
from probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.106]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.192]] [[0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.106]]
actor:  0 policy actor:  1  step number:  40 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]] [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]]
Printing some Q and Qe and total Qs values:  [[0.953]
 [1.02 ]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]] [[0.692]
 [0.813]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[0.953]
 [1.02 ]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]]
maxi score, test score, baseline:  -0.09550000000000015 0.6963333333333334 0.6963333333333334
probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6000000000000003  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.09550000000000015 0.6963333333333334 0.6963333333333334
probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
actor:  0 policy actor:  1  step number:  31 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [-0.025]
 [-0.04 ]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.035]] [[0.601]
 [0.804]
 [0.992]
 [1.011]
 [1.011]
 [1.011]
 [1.036]] [[0.213]
 [0.305]
 [0.373]
 [0.384]
 [0.384]
 [0.384]
 [0.395]]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.853]
 [0.757]
 [0.757]
 [0.748]
 [0.757]
 [0.787]] [[2.542]
 [3.029]
 [2.738]
 [2.738]
 [2.242]
 [2.738]
 [2.642]] [[0.729]
 [0.853]
 [0.757]
 [0.757]
 [0.748]
 [0.757]
 [0.787]]
maxi score, test score, baseline:  -0.09206000000000017 0.6963333333333334 0.6963333333333334
probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
start point for exploration sampling:  11715
actor:  0 policy actor:  0  step number:  58 total reward:  0.13999999999999924  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[ 0.325]
 [ 0.283]
 [ 0.279]
 [-0.007]
 [ 0.328]
 [ 0.298]
 [-0.108]] [[2.225]
 [1.194]
 [3.962]
 [3.058]
 [2.271]
 [3.765]
 [1.993]] [[0.37 ]
 [0.112]
 [0.763]
 [0.449]
 [0.382]
 [0.723]
 [0.162]]
line 256 mcts: sample exp_bonus 4.617491420155999
maxi score, test score, baseline:  -0.08978000000000017 0.6963333333333334 0.6963333333333334
probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
maxi score, test score, baseline:  -0.08978000000000017 0.6963333333333334 0.6963333333333334
probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
maxi score, test score, baseline:  -0.08978000000000017 0.6963333333333334 0.6963333333333334
probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
maxi score, test score, baseline:  -0.08978000000000017 0.6963333333333334 0.6963333333333334
probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
maxi score, test score, baseline:  -0.08978000000000017 0.6963333333333334 0.6963333333333334
probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
actor:  1 policy actor:  1  step number:  45 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[5.68 ]
 [1.592]
 [1.592]
 [1.592]
 [1.592]
 [1.592]
 [1.592]] [[0.775]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]]
siam score:  -0.7611752
actor:  1 policy actor:  1  step number:  63 total reward:  0.5733333333333339  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  57 total reward:  0.5466666666666673  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.08978000000000017 0.6963333333333334 0.6963333333333334
probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.024]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[-0.13 ]
 [ 0.747]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]] [[-0.234]
 [ 0.238]
 [-0.234]
 [-0.234]
 [-0.234]
 [-0.234]
 [-0.234]]
Printing some Q and Qe and total Qs values:  [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]] [[1.263]
 [1.263]
 [1.263]
 [1.263]
 [1.263]
 [1.263]
 [1.263]] [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]]
maxi score, test score, baseline:  -0.08978000000000017 0.6963333333333334 0.6963333333333334
Printing some Q and Qe and total Qs values:  [[0.848]
 [0.855]
 [0.864]
 [0.848]
 [0.844]
 [0.873]
 [0.871]] [[1.291]
 [1.311]
 [1.227]
 [1.117]
 [1.121]
 [1.162]
 [1.22 ]] [[0.848]
 [0.855]
 [0.864]
 [0.848]
 [0.844]
 [0.873]
 [0.871]]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.033]] [[0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [1.037]] [[0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.835]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.76560104
maxi score, test score, baseline:  -0.08978000000000017 0.6963333333333334 0.6963333333333334
actor:  0 policy actor:  0  step number:  38 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08683333333333347 0.6963333333333334 0.6963333333333334
probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
maxi score, test score, baseline:  -0.08683333333333347 0.6963333333333334 0.6963333333333334
probs:  [0.09160271152831959, 0.2914554538504211, 0.15981186723761284, 0.45712996738364653]
actor:  0 policy actor:  1  step number:  50 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.68]
 [0.68]
 [0.68]
 [0.68]
 [0.68]
 [0.68]
 [0.63]] [[0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [1.714]] [[0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.686]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08386000000000016 0.6963333333333334 0.6963333333333334
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.556]
 [0.516]
 [0.499]
 [0.504]
 [0.459]
 [0.522]] [[0.644]
 [0.371]
 [0.628]
 [0.589]
 [0.657]
 [0.762]
 [0.552]] [[0.077]
 [0.028]
 [0.073]
 [0.044]
 [0.071]
 [0.061]
 [0.055]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.08386000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09064034997211974, 0.28839003374367655, 0.168648468663694, 0.45232114762050973]
actor:  1 policy actor:  1  step number:  43 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.08386000000000014 0.6963333333333334 0.6963333333333334
maxi score, test score, baseline:  -0.08386000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09064034997211974, 0.28839003374367655, 0.168648468663694, 0.45232114762050973]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7731009
maxi score, test score, baseline:  -0.08386000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09012869940422925, 0.28676026795727905, 0.17334654924343956, 0.44976448339505215]
maxi score, test score, baseline:  -0.08386000000000014 0.6963333333333334 0.6963333333333334
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.216]
 [0.216]
 [0.303]
 [0.368]
 [0.216]
 [0.054]] [[1.973]
 [2.824]
 [2.824]
 [2.973]
 [2.325]
 [2.824]
 [2.495]] [[0.231]
 [0.449]
 [0.449]
 [0.553]
 [0.348]
 [0.449]
 [0.236]]
actor:  0 policy actor:  0  step number:  47 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  45 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.08103333333333347 0.6963333333333334 0.6963333333333334
maxi score, test score, baseline:  -0.08103333333333347 0.6963333333333334 0.6963333333333334
probs:  [0.09012869940422925, 0.28676026795727905, 0.17334654924343956, 0.44976448339505215]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09012869940422925, 0.28676026795727905, 0.17334654924343956, 0.44976448339505215]
using another actor
maxi score, test score, baseline:  -0.08103333333333347 0.6963333333333334 0.6963333333333334
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [-0.034]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.046]
 [-0.046]] [[-1.513]
 [-0.152]
 [-1.06 ]
 [-1.079]
 [-1.15 ]
 [-1.136]
 [-1.034]] [[-0.032]
 [ 0.408]
 [ 0.115]
 [ 0.108]
 [ 0.086]
 [ 0.09 ]
 [ 0.122]]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.525]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]] [[1.112]
 [0.999]
 [1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.112]] [[0.36 ]
 [0.331]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]]
maxi score, test score, baseline:  -0.08368666666666681 0.6963333333333334 0.6963333333333334
probs:  [0.09012869940422925, 0.28676026795727905, 0.17334654924343956, 0.44976448339505215]
actor:  0 policy actor:  1  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09012869940422925, 0.28676026795727905, 0.17334654924343956, 0.44976448339505215]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5933333333333336  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.43333333333333357  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.08062000000000014 0.6963333333333334 0.6963333333333334
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[ 0.215]
 [ 0.169]
 [ 0.16 ]
 [ 0.144]
 [ 0.201]
 [ 0.16 ]
 [-0.113]] [[1.917]
 [2.262]
 [1.976]
 [1.797]
 [1.628]
 [1.976]
 [0.468]] [[ 0.363]
 [ 0.433]
 [ 0.357]
 [ 0.305]
 [ 0.283]
 [ 0.357]
 [-0.134]]
maxi score, test score, baseline:  -0.08062000000000014 0.6963333333333334 0.6963333333333334
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.08062000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09203635874461938, 0.2779226091311147, 0.1707066057739705, 0.4593344263502954]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.08062000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09203635874461938, 0.2779226091311147, 0.1707066057739705, 0.4593344263502954]
using another actor
maxi score, test score, baseline:  -0.08062000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09203635874461938, 0.2779226091311147, 0.1707066057739705, 0.4593344263502954]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.08347333333333347 0.6963333333333334 0.6963333333333334
probs:  [0.09203635874461938, 0.2779226091311147, 0.1707066057739705, 0.4593344263502954]
actor:  0 policy actor:  0  step number:  55 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  52 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07836666666666682 0.6963333333333334 0.6963333333333334
line 256 mcts: sample exp_bonus 0.49664472844156127
using another actor
actor:  0 policy actor:  1  step number:  47 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0780333333333335 0.6963333333333334 0.6963333333333334
probs:  [0.09203635874461938, 0.2779226091311147, 0.1707066057739705, 0.4593344263502954]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.165]
 [0.137]
 [0.122]
 [0.12 ]
 [0.135]
 [0.121]] [[0.513]
 [1.3  ]
 [0.487]
 [0.542]
 [0.489]
 [0.494]
 [0.571]] [[-0.502]
 [ 0.059]
 [-0.44 ]
 [-0.421]
 [-0.454]
 [-0.438]
 [-0.406]]
maxi score, test score, baseline:  -0.08062000000000014 0.6963333333333334 0.6963333333333334
maxi score, test score, baseline:  -0.08062000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.0937463257877613, 0.270000804115245, 0.16834024180480053, 0.46791262829219327]
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.844]
 [0.871]
 [0.844]
 [0.844]
 [0.865]
 [0.898]] [[1.19 ]
 [1.19 ]
 [2.443]
 [1.19 ]
 [1.19 ]
 [1.58 ]
 [1.65 ]] [[0.844]
 [0.844]
 [0.871]
 [0.844]
 [0.844]
 [0.865]
 [0.898]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.08062000000000014 0.6963333333333334 0.6963333333333334
actor:  1 policy actor:  1  step number:  39 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08062000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.0937463257877613, 0.270000804115245, 0.16834024180480053, 0.46791262829219327]
actor:  1 policy actor:  1  step number:  28 total reward:  0.8200000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08062000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09692837214638074, 0.25525926222089085, 0.16393671842611002, 0.48387564720661835]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  65 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  31 total reward:  0.7333333333333335  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  32 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.769]
 [0.71 ]
 [0.716]
 [0.71 ]
 [0.718]
 [0.715]] [[-0.349]
 [ 0.146]
 [-0.322]
 [-0.146]
 [-0.447]
 [-0.13 ]
 [-0.059]] [[0.708]
 [0.769]
 [0.71 ]
 [0.716]
 [0.71 ]
 [0.718]
 [0.715]]
maxi score, test score, baseline:  -0.07816666666666681 0.6963333333333334 0.6963333333333334
actor:  0 policy actor:  0  step number:  35 total reward:  0.5600000000000002  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.49 ]
 [0.458]
 [0.569]
 [0.415]
 [0.572]
 [0.344]] [[ 1.084]
 [ 1.595]
 [ 2.408]
 [ 0.921]
 [-0.183]
 [ 0.867]
 [ 2.316]] [[-0.045]
 [ 0.083]
 [ 0.186]
 [ 0.049]
 [-0.289]
 [ 0.043]
 [ 0.058]]
from probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07980666666666682 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
maxi score, test score, baseline:  -0.07980666666666682 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
Printing some Q and Qe and total Qs values:  [[1.005]
 [1.004]
 [1.004]
 [1.004]
 [1.005]
 [1.004]
 [1.003]] [[0.046]
 [0.467]
 [0.317]
 [0.048]
 [0.09 ]
 [0.282]
 [0.159]] [[1.005]
 [1.004]
 [1.004]
 [1.004]
 [1.005]
 [1.004]
 [1.003]]
actor:  0 policy actor:  0  step number:  45 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0770866666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
maxi score, test score, baseline:  -0.0770866666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
maxi score, test score, baseline:  -0.0770866666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
siam score:  -0.77079314
maxi score, test score, baseline:  -0.0770866666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.097]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]] [[2.143]
 [1.768]
 [2.143]
 [2.143]
 [2.143]
 [2.143]
 [2.143]] [[-0.108]
 [-0.174]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]]
maxi score, test score, baseline:  -0.0770866666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
using another actor
actor:  0 policy actor:  0  step number:  46 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.873]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.772]] [[0.402]
 [0.324]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.394]] [[0.792]
 [0.873]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.772]]
actor:  0 policy actor:  0  step number:  57 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07458000000000015 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.823]
 [0.722]
 [0.801]
 [0.801]
 [0.722]
 [0.738]] [[1.194]
 [1.355]
 [1.101]
 [1.194]
 [1.194]
 [1.002]
 [1.029]] [[0.801]
 [0.823]
 [0.722]
 [0.801]
 [0.801]
 [0.722]
 [0.738]]
maxi score, test score, baseline:  -0.07458000000000015 0.6963333333333334 0.6963333333333334
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.5933333333333337  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.07458000000000015 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
Printing some Q and Qe and total Qs values:  [[0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]] [[0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]] [[0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]]
siam score:  -0.76644254
siam score:  -0.7641277
actor:  1 policy actor:  1  step number:  52 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.07458000000000015 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
actor:  0 policy actor:  1  step number:  62 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  57 total reward:  0.15999999999999925  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.07236666666666682 0.6963333333333334 0.6963333333333334
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.07236666666666682 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
actor:  1 policy actor:  1  step number:  44 total reward:  0.54  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.872]
 [0.849]
 [0.862]
 [0.862]
 [0.835]
 [0.834]] [[0.81 ]
 [0.926]
 [0.674]
 [1.206]
 [1.206]
 [0.94 ]
 [1.028]] [[0.852]
 [0.872]
 [0.849]
 [0.862]
 [0.862]
 [0.835]
 [0.834]]
maxi score, test score, baseline:  -0.0723666666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.161]
 [0.05 ]
 [0.05 ]
 [0.05 ]
 [0.05 ]
 [0.05 ]] [[0.383]
 [1.097]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]] [[-0.285]
 [ 0.044]
 [-0.285]
 [-0.285]
 [-0.285]
 [-0.285]
 [-0.285]]
Printing some Q and Qe and total Qs values:  [[-0.086]
 [-0.137]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.095]] [[0.313]
 [0.459]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.413]] [[-0.313]
 [-0.315]
 [-0.313]
 [-0.313]
 [-0.313]
 [-0.313]
 [-0.288]]
maxi score, test score, baseline:  -0.0723666666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.0723666666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
actor:  1 policy actor:  1  step number:  44 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  50 total reward:  0.5933333333333337  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0723666666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
line 256 mcts: sample exp_bonus 1.9626268038462307
actor:  1 policy actor:  1  step number:  36 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.0723666666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.629]
 [0.626]
 [0.627]
 [0.628]
 [0.621]
 [0.628]] [[1.228]
 [0.92 ]
 [1.147]
 [1.068]
 [0.714]
 [1.255]
 [1.425]] [[0.763]
 [0.615]
 [0.723]
 [0.686]
 [0.513]
 [0.773]
 [0.86 ]]
maxi score, test score, baseline:  -0.0723666666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09357947188222498, 0.24643151523680856, 0.19285210292957083, 0.4671369099513955]
actor:  1 policy actor:  1  step number:  49 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.7266666666666669  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.272]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]] [[0.764]
 [0.668]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]] [[-0.58 ]
 [-0.514]
 [-0.58 ]
 [-0.58 ]
 [-0.58 ]
 [-0.58 ]
 [-0.58 ]]
maxi score, test score, baseline:  -0.0723666666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09376418558493504, 0.24571871950621682, 0.19245391227013908, 0.46806318263870905]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.4216],
        [ 0.0795],
        [ 0.2986],
        [ 0.7331],
        [-0.0740],
        [-0.1536],
        [ 0.0026],
        [ 0.9242],
        [-0.0535],
        [-0.0000]], dtype=torch.float64)
-0.09703970119800001 0.32459552315558127
-0.04528388706599999 0.034221963015589454
-0.045026434398 0.2535604249362098
-0.032346567066 0.7007454583170248
-0.083839701198 -0.1578622542510819
-0.032346567066 -0.18590489928215978
-0.083839701198 -0.08125347962868741
-0.032346567066 0.8918361218634627
-0.032346567066 -0.08584678525735309
-0.9187464 -0.9187464
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.0723666666666668 0.6963333333333334 0.6963333333333334
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.174]
 [0.066]
 [0.06 ]
 [0.071]
 [0.024]
 [0.181]] [[2.853]
 [1.707]
 [2.781]
 [2.344]
 [2.853]
 [2.766]
 [2.478]] [[0.46 ]
 [0.103]
 [0.432]
 [0.271]
 [0.46 ]
 [0.404]
 [0.385]]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07170000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09376418558493504, 0.24571871950621682, 0.19245391227013908, 0.46806318263870905]
maxi score, test score, baseline:  -0.07170000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09376418558493504, 0.24571871950621682, 0.19245391227013908, 0.46806318263870905]
line 256 mcts: sample exp_bonus 1.4051149936153071
line 256 mcts: sample exp_bonus 1.9468008619911712
maxi score, test score, baseline:  -0.07170000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09376418558493504, 0.24571871950621682, 0.19245391227013908, 0.46806318263870905]
actor:  1 policy actor:  1  step number:  57 total reward:  0.5333333333333337  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.07170000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09376418558493504, 0.24571871950621682, 0.19245391227013908, 0.46806318263870905]
actor:  1 policy actor:  1  step number:  30 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07170000000000014 0.6963333333333334 0.6963333333333334
actor:  1 policy actor:  1  step number:  27 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07170000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09565198527004708, 0.23843384818363578, 0.1883843484507736, 0.4775298180955436]
actor:  1 policy actor:  1  step number:  29 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.747]] [[0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]]
maxi score, test score, baseline:  -0.07170000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09526148267534533, 0.24154601026863431, 0.1876146684575392, 0.4755778385984811]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.709]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[0.672]
 [0.666]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[0.056]
 [0.17 ]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]]
maxi score, test score, baseline:  -0.07170000000000015 0.6963333333333334 0.6963333333333334
probs:  [0.09526148267534534, 0.24154601026863426, 0.1876146684575392, 0.47557783859848113]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07170000000000015 0.6963333333333334 0.6963333333333334
probs:  [0.09451348630897728, 0.2481610687928982, 0.1854840205514832, 0.47184142434664134]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  65 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.039459436209054496
maxi score, test score, baseline:  -0.07170000000000015 0.6963333333333334 0.6963333333333334
probs:  [0.09451348630897728, 0.2481610687928982, 0.1854840205514832, 0.47184142434664134]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6400000000000003  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.07170000000000015 0.6963333333333334 0.6963333333333334
probs:  [0.09451348630897728, 0.2481610687928982, 0.1854840205514832, 0.47184142434664134]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07170000000000015 0.6963333333333334 0.6963333333333334
using another actor
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]] [[0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]] [[0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.661]
 [0.589]
 [0.611]
 [0.592]
 [0.588]
 [0.592]] [[-2.004]
 [ 0.165]
 [-1.454]
 [-1.397]
 [-1.556]
 [-1.474]
 [-1.554]] [[0.587]
 [0.661]
 [0.589]
 [0.611]
 [0.592]
 [0.588]
 [0.592]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.296]
 [0.213]
 [0.296]
 [0.296]
 [0.296]
 [0.296]] [[2.376]
 [2.376]
 [1.904]
 [2.376]
 [2.376]
 [2.376]
 [2.376]] [[0.672]
 [0.672]
 [0.526]
 [0.672]
 [0.672]
 [0.672]
 [0.672]]
siam score:  -0.7698082
actor:  1 policy actor:  1  step number:  33 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.8133333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07170000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09664884302239768, 0.24516201588152825, 0.1756192645238832, 0.4825698765721909]
maxi score, test score, baseline:  -0.07471333333333348 0.6963333333333334 0.6963333333333334
probs:  [0.09664884302239768, 0.24516201588152825, 0.1756192645238832, 0.4825698765721909]
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.292]
 [0.349]
 [0.292]
 [0.292]
 [0.292]
 [0.292]] [[2.314]
 [2.314]
 [2.23 ]
 [2.314]
 [2.314]
 [2.314]
 [2.314]] [[-0.09]
 [-0.09]
 [-0.06]
 [-0.09]
 [-0.09]
 [-0.09]
 [-0.09]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  40 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  36 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.716]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.646]] [[0.51 ]
 [1.099]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.581]] [[0.707]
 [0.716]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.646]]
maxi score, test score, baseline:  -0.07471333333333348 0.6963333333333334 0.6963333333333334
probs:  [0.09585143204254698, 0.2513958410469543, 0.17416928797411765, 0.4785834389363811]
maxi score, test score, baseline:  -0.07471333333333348 0.6963333333333334 0.6963333333333334
probs:  [0.09585143204254698, 0.2513958410469543, 0.17416928797411765, 0.4785834389363811]
Printing some Q and Qe and total Qs values:  [[0.872]
 [0.972]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]] [[2.383]
 [1.655]
 [2.383]
 [2.383]
 [2.383]
 [2.383]
 [2.383]] [[0.872]
 [0.972]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]]
maxi score, test score, baseline:  -0.07471333333333348 0.6963333333333334 0.6963333333333334
probs:  [0.09585143204254698, 0.2513958410469543, 0.17416928797411765, 0.4785834389363811]
Printing some Q and Qe and total Qs values:  [[ 0.178]
 [ 0.235]
 [-0.099]
 [ 0.173]
 [ 0.192]
 [-0.164]
 [ 0.218]] [[ 1.817]
 [ 1.906]
 [ 0.545]
 [ 1.095]
 [ 1.756]
 [-0.018]
 [ 1.928]] [[ 0.403]
 [ 0.46 ]
 [-0.052]
 [ 0.244]
 [ 0.399]
 [-0.216]
 [ 0.453]]
maxi score, test score, baseline:  -0.0770466666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09585143204254698, 0.2513958410469543, 0.17416928797411765, 0.4785834389363811]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.003]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[1.087]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]] [[0.371]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]]
actor:  0 policy actor:  0  step number:  58 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.07471333333333349 0.6963333333333334 0.6963333333333334
maxi score, test score, baseline:  -0.07471333333333349 0.6963333333333334 0.6963333333333334
probs:  [0.09585143204254698, 0.25139584104695417, 0.17416928797411765, 0.4785834389363811]
using another actor
actor:  1 policy actor:  1  step number:  38 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  48 total reward:  0.6333333333333337  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  41 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.07718000000000014 0.6963333333333334 0.6963333333333334
maxi score, test score, baseline:  -0.07718000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09585143204254698, 0.25139584104695417, 0.17416928797411763, 0.4785834389363811]
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.179]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.1  ]] [[1.386]
 [1.85 ]
 [1.386]
 [1.386]
 [1.386]
 [1.386]
 [1.143]] [[-0.523]
 [-0.324]
 [-0.523]
 [-0.523]
 [-0.523]
 [-0.523]
 [-0.521]]
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.107]
 [0.287]
 [0.193]
 [0.193]
 [0.316]
 [0.39 ]] [[1.693]
 [0.703]
 [1.669]
 [1.693]
 [1.693]
 [1.772]
 [1.399]] [[-0.069]
 [-0.32 ]
 [ 0.021]
 [-0.069]
 [-0.069]
 [ 0.068]
 [ 0.079]]
maxi score, test score, baseline:  -0.07718000000000014 0.6963333333333334 0.6963333333333334
maxi score, test score, baseline:  -0.07718000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09585143204254698, 0.25139584104695417, 0.17416928797411763, 0.4785834389363811]
line 256 mcts: sample exp_bonus 0.25490675101777244
actor:  1 policy actor:  1  step number:  34 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  40 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  33 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07718000000000014 0.6963333333333334 0.6963333333333334
actor:  1 policy actor:  1  step number:  37 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07718000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09621571498754905, 0.2505159217436704, 0.17286930426039337, 0.4803990590083872]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  51 total reward:  0.346666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0767266666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09621571498754905, 0.2505159217436704, 0.17286930426039337, 0.4803990590083872]
actor:  0 policy actor:  0  step number:  52 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  0.333
siam score:  -0.75713784
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.267]
 [0.183]
 [0.185]
 [0.185]
 [0.149]
 [0.236]] [[2.022]
 [2.361]
 [3.075]
 [2.022]
 [2.022]
 [3.424]
 [3.034]] [[0.099]
 [0.258]
 [0.43 ]
 [0.099]
 [0.099]
 [0.519]
 [0.451]]
maxi score, test score, baseline:  -0.07434000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09621571498754905, 0.2505159217436704, 0.17286930426039337, 0.4803990590083872]
maxi score, test score, baseline:  -0.07434000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09621571498754905, 0.2505159217436704, 0.17286930426039337, 0.4803990590083872]
Printing some Q and Qe and total Qs values:  [[0.889]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.833]] [[1.169]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [0.911]] [[0.889]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.833]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.6066666666666669  reward:  1.0 rdn_beta:  0.667
using another actor
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  48 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  39 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07140666666666681 0.6963333333333334 0.6963333333333334
probs:  [0.09621571498754905, 0.2505159217436704, 0.17286930426039337, 0.4803990590083872]
maxi score, test score, baseline:  -0.07140666666666681 0.6963333333333334 0.6963333333333334
maxi score, test score, baseline:  -0.07140666666666681 0.6963333333333334 0.6963333333333334
probs:  [0.09621571498754905, 0.2505159217436704, 0.17286930426039337, 0.4803990590083872]
actor:  0 policy actor:  1  step number:  53 total reward:  0.44000000000000017  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  32 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0685266666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09620003628931507, 0.25058418050184766, 0.17289532419266432, 0.48032045901617293]
actor:  0 policy actor:  1  step number:  33 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  42 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.8000000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.06791333333333349 0.6963333333333334 0.6963333333333334
Printing some Q and Qe and total Qs values:  [[-0.038]
 [ 0.031]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]] [[0.993]
 [1.764]
 [0.993]
 [0.993]
 [0.993]
 [0.993]
 [0.993]] [[0.118]
 [0.35 ]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]]
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.05 ]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[3.505]
 [2.378]
 [2.004]
 [2.004]
 [2.004]
 [2.004]
 [2.004]] [[0.327]
 [0.171]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.06791333333333348 0.6963333333333334 0.6963333333333334
probs:  [0.09363996341146928, 0.2705467547002182, 0.1682910527893648, 0.46752222909894764]
UNIT TEST: sample policy line 217 mcts : [0.061 0.531 0.041 0.02  0.061 0.224 0.061]
maxi score, test score, baseline:  -0.06791333333333348 0.6963333333333334 0.6963333333333334
probs:  [0.09363996341146928, 0.2705467547002182, 0.1682910527893648, 0.46752222909894764]
maxi score, test score, baseline:  -0.06791333333333348 0.6963333333333334 0.6963333333333334
probs:  [0.09363996341146928, 0.2705467547002182, 0.1682910527893648, 0.46752222909894764]
maxi score, test score, baseline:  -0.06791333333333348 0.6963333333333334 0.6963333333333334
actor:  1 policy actor:  1  step number:  56 total reward:  0.5533333333333338  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.09363996341146928, 0.2705467547002183, 0.16829105278936485, 0.46752222909894764]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.06791333333333348 0.6963333333333334 0.6963333333333334
probs:  [0.09363996341146928, 0.2705467547002182, 0.1682910527893648, 0.46752222909894764]
UNIT TEST: sample policy line 217 mcts : [0.184 0.327 0.061 0.163 0.082 0.061 0.122]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.06791333333333348 0.6963333333333334 0.6963333333333334
probs:  [0.09351027151751959, 0.2701716529900279, 0.1694441977320671, 0.46687387776038547]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6066666666666668  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.06791333333333348 0.6963333333333334 0.6963333333333334
probs:  [0.09350953368074046, 0.2701726594313631, 0.16944759864023673, 0.46687020824765973]
actor:  0 policy actor:  1  step number:  43 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.06490000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09350953368074046, 0.2701726594313631, 0.16944759864023673, 0.46687020824765973]
actor:  0 policy actor:  1  step number:  34 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.333
from probs:  [0.09350953368074046, 0.2701726594313631, 0.16944759864023673, 0.46687020824765973]
actor:  0 policy actor:  0  step number:  46 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.0651666666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09350953368074046, 0.2701726594313631, 0.16944759864023673, 0.46687020824765973]
actor:  1 policy actor:  1  step number:  28 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  38 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
siam score:  -0.76445633
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.708]
 [0.507]
 [0.688]
 [0.693]
 [0.708]
 [0.685]] [[1.684]
 [1.429]
 [1.07 ]
 [1.472]
 [1.435]
 [1.429]
 [1.986]] [[0.769]
 [0.708]
 [0.507]
 [0.688]
 [0.693]
 [0.708]
 [0.685]]
maxi score, test score, baseline:  -0.0651666666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09491234106669187, 0.2637127331151848, 0.16747385481698412, 0.4739010710011392]
actor:  0 policy actor:  1  step number:  54 total reward:  0.3400000000000001  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.06584666666666682 0.6963333333333334 0.6963333333333334
probs:  [0.09491234106669187, 0.2637127331151848, 0.16747385481698412, 0.4739010710011392]
start point for exploration sampling:  11715
actor:  0 policy actor:  0  step number:  47 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.06651333333333348 0.6963333333333334 0.6963333333333334
probs:  [0.09491234106669187, 0.2637127331151848, 0.16747385481698412, 0.4739010710011392]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.06651333333333348 0.6963333333333334 0.6963333333333334
probs:  [0.09491234106669187, 0.2637127331151848, 0.16747385481698412, 0.4739010710011392]
actor:  0 policy actor:  0  step number:  54 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  48 total reward:  0.4733333333333335  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  33 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  38 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [-0.043]
 [-0.046]
 [-0.044]
 [-0.039]
 [-0.044]
 [-0.047]] [[-1.207]
 [-0.421]
 [-0.958]
 [-1.258]
 [-1.227]
 [-1.022]
 [-0.906]] [[0.079]
 [0.207]
 [0.114]
 [0.066]
 [0.076]
 [0.105]
 [0.122]]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.06778000000000015 0.6963333333333334 0.6963333333333334
probs:  [0.09401203426589318, 0.2707036843974037, 0.16588416485444632, 0.4694001164822567]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.783]
 [0.643]
 [0.783]
 [0.598]
 [0.783]
 [0.827]] [[1.279]
 [0.918]
 [0.275]
 [0.918]
 [1.016]
 [0.918]
 [1.594]] [[0.655]
 [0.783]
 [0.643]
 [0.783]
 [0.598]
 [0.783]
 [0.827]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  38 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [ 0.155]] [[2.011]
 [2.011]
 [2.011]
 [2.011]
 [2.011]
 [2.011]
 [2.214]] [[0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.397]]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.759]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]] [[0.454]
 [0.215]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[0.581]
 [0.759]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]]
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.294]
 [0.506]
 [0.159]
 [0.671]
 [0.425]
 [0.625]] [[1.362]
 [1.06 ]
 [1.65 ]
 [0.626]
 [0.067]
 [1.866]
 [1.609]] [[0.379]
 [0.294]
 [0.506]
 [0.159]
 [0.671]
 [0.425]
 [0.625]]
maxi score, test score, baseline:  -0.06778000000000015 0.6963333333333334 0.6963333333333334
probs:  [0.0940113329553484, 0.2707046139668605, 0.16588742513580296, 0.46939662794198816]
maxi score, test score, baseline:  -0.06778000000000015 0.6963333333333334 0.6963333333333334
probs:  [0.0940113329553484, 0.2707046139668605, 0.16588742513580298, 0.46939662794198816]
actor:  0 policy actor:  0  step number:  33 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.07116666666666682 0.6963333333333334 0.6963333333333334
probs:  [0.0940113329553484, 0.2707046139668605, 0.16588742513580298, 0.46939662794198816]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7333333333333335  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.384]] [[1.886]
 [1.911]
 [1.911]
 [1.911]
 [1.911]
 [1.911]
 [0.918]] [[ 0.176]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.069]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.54  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  35 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  44 total reward:  0.5800000000000003  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.07116666666666682 0.6963333333333334 0.6963333333333334
maxi score, test score, baseline:  -0.07116666666666682 0.6963333333333334 0.6963333333333334
probs:  [0.09422373220078083, 0.26970507100900587, 0.16561008143908454, 0.47046111535112883]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  0 policy actor:  0  step number:  51 total reward:  0.3333333333333327  reward:  1.0 rdn_beta:  0.333
from probs:  [0.09422373220078081, 0.26970507100900587, 0.1656100814390845, 0.4704611153511288]
maxi score, test score, baseline:  -0.07183333333333347 0.6963333333333334 0.6963333333333334
probs:  [0.09422373220078081, 0.26970507100900587, 0.1656100814390845, 0.4704611153511288]
actor:  1 policy actor:  1  step number:  36 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07183333333333347 0.6963333333333334 0.6963333333333334
probs:  [0.09422373220078081, 0.26970507100900587, 0.1656100814390845, 0.4704611153511288]
maxi score, test score, baseline:  -0.07183333333333347 0.6963333333333334 0.6963333333333334
actor:  1 policy actor:  1  step number:  35 total reward:  0.6400000000000003  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  39 total reward:  0.6133333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07183333333333347 0.6963333333333334 0.6963333333333334
maxi score, test score, baseline:  -0.07183333333333347 0.6963333333333334 0.6963333333333334
probs:  [0.09422373220078083, 0.26970507100900587, 0.16561008143908454, 0.47046111535112883]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.07183333333333347 0.6963333333333334 0.6963333333333334
probs:  [0.09422373220078083, 0.26970507100900587, 0.16561008143908454, 0.47046111535112883]
maxi score, test score, baseline:  -0.07183333333333347 0.6963333333333334 0.6963333333333334
probs:  [0.09422303915675087, 0.26970600529339067, 0.1656132877921872, 0.47045766775767134]
maxi score, test score, baseline:  -0.07183333333333347 0.6963333333333334 0.6963333333333334
probs:  [0.09422303915675087, 0.26970600529339067, 0.1656132877921872, 0.47045766775767134]
maxi score, test score, baseline:  -0.07183333333333347 0.6963333333333334 0.6963333333333334
probs:  [0.09422303915675087, 0.26970600529339067, 0.1656132877921872, 0.47045766775767134]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  61 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  47 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  52 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07330000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09422303915675087, 0.26970600529339067, 0.1656132877921872, 0.47045766775767134]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.448]
 [0.437]
 [0.399]
 [0.446]
 [0.403]
 [0.444]] [[ 0.115]
 [ 0.549]
 [ 0.111]
 [-0.05 ]
 [-0.065]
 [-0.066]
 [ 0.151]] [[-0.045]
 [ 0.122]
 [-0.035]
 [-0.126]
 [-0.084]
 [-0.128]
 [-0.014]]
maxi score, test score, baseline:  -0.07330000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09422303915675087, 0.26970600529339067, 0.1656132877921872, 0.47045766775767134]
actor:  1 policy actor:  1  step number:  26 total reward:  0.78  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  40 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.667
siam score:  -0.73472154
actor:  1 policy actor:  1  step number:  33 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07330000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.0951952151692801, 0.265141806893306, 0.16433314749985292, 0.475329830437561]
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.16 ]
 [0.389]
 [0.396]
 [0.483]
 [0.389]
 [0.425]] [[1.126]
 [1.217]
 [0.893]
 [0.918]
 [0.592]
 [0.893]
 [1.249]] [[ 0.089]
 [-0.21 ]
 [-0.089]
 [-0.073]
 [-0.095]
 [-0.089]
 [ 0.066]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.546666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.07330000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09519452745616755, 0.26514280236914356, 0.16433626156834105, 0.47532640860634773]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.014]
 [ 0.094]
 [ 0.029]
 [ 0.102]
 [ 0.076]
 [-0.004]
 [-0.009]] [[0.092]
 [0.221]
 [0.436]
 [0.29 ]
 [0.141]
 [0.236]
 [0.345]] [[-0.119]
 [ 0.074]
 [ 0.153]
 [ 0.129]
 [ 0.003]
 [-0.013]
 [ 0.055]]
Printing some Q and Qe and total Qs values:  [[ 0.418]
 [ 0.178]
 [ 0.172]
 [ 0.286]
 [ 0.3  ]
 [ 0.232]
 [-0.001]] [[4.121]
 [1.615]
 [2.366]
 [2.063]
 [1.808]
 [2.154]
 [2.775]] [[0.836]
 [0.06 ]
 [0.262]
 [0.226]
 [0.162]
 [0.228]
 [0.302]]
first move QE:  -0.06717893421081578
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.6000000000000004  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.15670145308253
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8944659858785335
maxi score, test score, baseline:  -0.07652666666666681 0.6963333333333334 0.6963333333333334
probs:  [0.09519452745616755, 0.26514280236914356, 0.16433626156834105, 0.47532640860634773]
maxi score, test score, baseline:  -0.07652666666666681 0.6963333333333334 0.6963333333333334
probs:  [0.09519452745616755, 0.26514280236914356, 0.16433626156834102, 0.47532640860634773]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.6800000000000004  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.07652666666666681 0.6963333333333334 0.6963333333333334
probs:  [0.09574041451438596, 0.2609247462297439, 0.16527926293840828, 0.478055576317462]
maxi score, test score, baseline:  -0.07652666666666681 0.6963333333333334 0.6963333333333334
probs:  [0.09574041451438596, 0.2609247462297439, 0.16527926293840828, 0.478055576317462]
actor:  1 policy actor:  1  step number:  77 total reward:  0.4266666666666671  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.812]
 [0.905]
 [0.812]
 [0.812]
 [0.812]
 [0.812]] [[2.03 ]
 [2.03 ]
 [1.319]
 [2.03 ]
 [2.03 ]
 [2.03 ]
 [2.03 ]] [[0.812]
 [0.812]
 [0.905]
 [0.812]
 [0.812]
 [0.812]
 [0.812]]
maxi score, test score, baseline:  -0.07652666666666681 0.6963333333333334 0.6963333333333334
probs:  [0.09574041451438596, 0.2609247462297439, 0.16527926293840828, 0.478055576317462]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]] [[1.22]
 [1.22]
 [1.22]
 [1.22]
 [1.22]
 [1.22]
 [1.22]] [[-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.41333333333333344  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  41 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07704666666666682 0.6963333333333334 0.6963333333333334
probs:  [0.09574041451438596, 0.2609247462297439, 0.16527926293840828, 0.478055576317462]
maxi score, test score, baseline:  -0.07704666666666682 0.6963333333333334 0.6963333333333334
probs:  [0.09574041451438596, 0.2609247462297439, 0.16527926293840828, 0.478055576317462]
maxi score, test score, baseline:  -0.07704666666666682 0.6963333333333334 0.6963333333333334
probs:  [0.09574041451438596, 0.2609247462297439, 0.16527926293840828, 0.478055576317462]
actor:  1 policy actor:  1  step number:  29 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.004]
 [-0.002]
 [-0.004]
 [-0.004]
 [-0.005]
 [-0.004]] [[0.944]
 [1.038]
 [0.969]
 [0.944]
 [0.944]
 [1.061]
 [0.944]] [[-0.655]
 [-0.584]
 [-0.637]
 [-0.655]
 [-0.655]
 [-0.578]
 [-0.655]]
maxi score, test score, baseline:  -0.0770466666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09678373967987507, 0.2560855396168964, 0.16384617595611495, 0.48328454474711363]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  0.6133333333333336  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09678373967987505, 0.2560855396168964, 0.16384617595611495, 0.4832845447471136]
maxi score, test score, baseline:  -0.0770466666666668 0.6963333333333334 0.6963333333333334
maxi score, test score, baseline:  -0.0770466666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09678373967987507, 0.2560855396168964, 0.16384617595611495, 0.48328454474711363]
maxi score, test score, baseline:  -0.0792866666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09678373967987507, 0.2560855396168964, 0.16384617595611495, 0.48328454474711363]
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.592]
 [0.506]
 [0.592]
 [0.647]
 [0.592]
 [0.056]] [[0.826]
 [0.761]
 [0.637]
 [0.761]
 [0.613]
 [0.761]
 [0.581]] [[ 0.589]
 [ 0.492]
 [ 0.361]
 [ 0.492]
 [ 0.476]
 [ 0.492]
 [-0.063]]
maxi score, test score, baseline:  -0.0792866666666668 0.6963333333333334 0.6963333333333334
probs:  [0.09678373967987507, 0.2560855396168964, 0.16384617595611495, 0.48328454474711363]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.5
from probs:  [0.09678373967987507, 0.2560855396168964, 0.16384617595611495, 0.48328454474711363]
maxi score, test score, baseline:  -0.07928666666666682 0.6963333333333334 0.6963333333333334
probs:  [0.09678373967987507, 0.2560855396168964, 0.16384617595611495, 0.48328454474711363]
from probs:  [0.09678373967987507, 0.2560855396168964, 0.16384617595611495, 0.48328454474711363]
maxi score, test score, baseline:  -0.07928666666666682 0.6963333333333334 0.6963333333333334
probs:  [0.09671439224447813, 0.256407190602665, 0.16394142996724206, 0.48293698718561473]
actor:  0 policy actor:  1  step number:  51 total reward:  0.4933333333333335  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  41 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.768]
 [0.637]
 [0.643]
 [0.698]
 [0.64 ]
 [0.653]] [[ 0.312]
 [-0.258]
 [ 0.117]
 [-0.009]
 [-0.802]
 [ 0.056]
 [-0.046]] [[0.639]
 [0.768]
 [0.637]
 [0.643]
 [0.698]
 [0.64 ]
 [0.653]]
actor:  0 policy actor:  1  step number:  51 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07606000000000014 0.6963333333333334 0.6963333333333334
probs:  [0.09671369656835532, 0.25640833038295663, 0.1639444482955816, 0.4829335247531064]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.014]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[-0.337]
 [ 0.92 ]
 [-0.337]
 [-0.337]
 [-0.337]
 [-0.337]
 [-0.337]] [[-0.882]
 [-0.447]
 [-0.882]
 [-0.882]
 [-0.882]
 [-0.882]
 [-0.882]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.8066666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  61 total reward:  0.10666666666666602  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07596666666666681 0.6963333333333334 0.6963333333333334
probs:  [0.09814808244759483, 0.24975539072903027, 0.16197410502350373, 0.49012242179987126]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.015]
 [-0.024]
 [-0.03 ]
 [-0.031]
 [-0.024]
 [-0.034]] [[ 0.712]
 [ 0.396]
 [ 0.6  ]
 [-0.013]
 [ 0.692]
 [ 0.6  ]
 [ 0.532]] [[0.684]
 [0.611]
 [0.659]
 [0.491]
 [0.677]
 [0.659]
 [0.632]]
maxi score, test score, baseline:  -0.07596666666666681 0.6963333333333334 0.6963333333333334
probs:  [0.09890761661013994, 0.24622871763102785, 0.16093454423565487, 0.4939291215231773]
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.824]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]] [[3.536]
 [2.32 ]
 [3.536]
 [3.536]
 [3.536]
 [3.536]
 [3.536]] [[0.775]
 [0.824]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]]
actor:  0 policy actor:  0  step number:  31 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.508]] [[6.363]
 [2.288]
 [2.288]
 [2.288]
 [2.288]
 [2.288]
 [2.3  ]] [[0.696]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.158]]
maxi score, test score, baseline:  -0.07252666666666682 0.6963333333333334 0.6963333333333334
probs:  [0.09890761661013994, 0.24622871763102785, 0.16093454423565487, 0.4939291215231773]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.07252666666666682 0.6963333333333334 0.6963333333333334
probs:  [0.09890761661013994, 0.24622871763102785, 0.16093454423565487, 0.4939291215231773]
actor:  1 policy actor:  1  step number:  62 total reward:  0.5666666666666671  reward:  1.0 rdn_beta:  0.667
using another actor
actor:  1 policy actor:  1  step number:  71 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07252666666666681 0.6963333333333334 0.6963333333333334
probs:  [0.09890761661013994, 0.24622871763102785, 0.16093454423565487, 0.4939291215231773]
line 256 mcts: sample exp_bonus 5.9536986317331255
from probs:  [0.09890761661013994, 0.24622871763102785, 0.16093454423565487, 0.4939291215231773]
siam score:  -0.75778383
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.379]
 [0.432]
 [0.379]
 [0.379]
 [0.379]
 [0.448]] [[2.429]
 [3.39 ]
 [2.876]
 [3.39 ]
 [3.39 ]
 [3.39 ]
 [2.814]] [[0.18 ]
 [0.371]
 [0.284]
 [0.371]
 [0.371]
 [0.371]
 [0.277]]
maxi score, test score, baseline:  -0.07252666666666681 0.6963333333333334 0.6963333333333334
probs:  [0.09847167628473885, 0.2482487000399038, 0.16153533758584024, 0.49174428608951715]
Starting evaluation
maxi score, test score, baseline:  -0.07252666666666681 0.6963333333333334 0.6963333333333334
probs:  [0.09847167628473885, 0.2482487000399038, 0.16153533758584024, 0.49174428608951715]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.029]
 [0.029]
 [0.021]
 [0.008]
 [0.026]
 [0.021]] [[-0.905]
 [ 0.761]
 [ 0.402]
 [-0.258]
 [ 0.117]
 [-0.218]
 [ 0.404]] [[0.007]
 [0.029]
 [0.029]
 [0.021]
 [0.008]
 [0.026]
 [0.021]]
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.8  ]
 [0.731]
 [0.766]
 [0.77 ]
 [0.56 ]
 [0.819]] [[0.562]
 [1.041]
 [1.159]
 [0.596]
 [0.187]
 [0.147]
 [1.104]] [[0.816]
 [0.8  ]
 [0.731]
 [0.766]
 [0.77 ]
 [0.56 ]
 [0.819]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07252666666666681 0.6963333333333334 0.6963333333333334
probs:  [0.09847167628473885, 0.2482487000399038, 0.16153533758584024, 0.49174428608951715]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.07252666666666681 0.6963333333333334 0.6963333333333334
actor:  0 policy actor:  1  step number:  33 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.5800000000000005  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  29 total reward:  0.6133333333333334  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  45 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.020860000000000135 0.688 0.688
probs:  [0.09790329717578404, 0.2490508868391159, 0.16410000274530182, 0.4889458132397983]
maxi score, test score, baseline:  -0.020860000000000135 0.688 0.688
probs:  [0.09790329717578404, 0.2490508868391159, 0.16410000274530182, 0.4889458132397983]
actor:  1 policy actor:  1  step number:  38 total reward:  0.54  reward:  1.0 rdn_beta:  0.667
siam score:  -0.757115
maxi score, test score, baseline:  -0.020860000000000135 0.688 0.688
probs:  [0.09790329717578404, 0.2490508868391159, 0.16410000274530182, 0.4889458132397983]
actor:  1 policy actor:  1  step number:  43 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.434]
 [0.678]
 [0.434]
 [0.434]
 [0.649]
 [0.434]] [[0.953]
 [0.953]
 [1.101]
 [0.953]
 [0.953]
 [1.286]
 [0.953]] [[-0.159]
 [-0.159]
 [ 0.11 ]
 [-0.159]
 [-0.159]
 [ 0.112]
 [-0.159]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6400000000000002  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  34 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.020860000000000135 0.688 0.688
probs:  [0.09691177075635804, 0.2426952412413624, 0.176399838579452, 0.4839931494228276]
line 256 mcts: sample exp_bonus 0.6973373398538661
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.4662135239889738
Printing some Q and Qe and total Qs values:  [[-0.06 ]
 [-0.056]
 [-0.144]
 [-0.062]
 [-0.062]
 [-0.067]
 [-0.066]] [[-0.367]
 [-0.26 ]
 [-0.77 ]
 [-0.547]
 [-0.167]
 [ 0.318]
 [-0.132]] [[0.343]
 [0.377]
 [0.159]
 [0.291]
 [0.398]
 [0.531]
 [0.405]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  36 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]] [[0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]] [[0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.020860000000000135 0.688 0.688
probs:  [0.096493174958455, 0.24596942729248433, 0.175637374123664, 0.4819000236253966]
maxi score, test score, baseline:  -0.020860000000000135 0.688 0.688
probs:  [0.096493174958455, 0.24596942729248433, 0.175637374123664, 0.4819000236253966]
maxi score, test score, baseline:  -0.020860000000000135 0.688 0.688
probs:  [0.096493174958455, 0.2459694272924844, 0.175637374123664, 0.4819000236253966]
siam score:  -0.74791425
maxi score, test score, baseline:  -0.020860000000000135 0.688 0.688
probs:  [0.096493174958455, 0.2459694272924844, 0.175637374123664, 0.4819000236253966]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [ 0.004]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]] [[-0.762]
 [ 0.214]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.375]
 [-0.042]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.44721544506400823
maxi score, test score, baseline:  -0.023793333333333475 0.688 0.688
maxi score, test score, baseline:  -0.023793333333333475 0.688 0.688
probs:  [0.096493174958455, 0.24596942729248433, 0.175637374123664, 0.4819000236253966]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.023793333333333475 0.688 0.688
probs:  [0.096493174958455, 0.24596942729248433, 0.175637374123664, 0.4819000236253966]
using another actor
actor:  1 policy actor:  1  step number:  43 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.025926666666666806 0.688 0.688
maxi score, test score, baseline:  -0.025926666666666806 0.688 0.688
probs:  [0.096493174958455, 0.2459694272924844, 0.175637374123664, 0.4819000236253966]
from probs:  [0.096493174958455, 0.2459694272924844, 0.175637374123664, 0.4819000236253966]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.667
from probs:  [0.096493174958455, 0.2459694272924844, 0.175637374123664, 0.4819000236253966]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.028020000000000135 0.688 0.688
maxi score, test score, baseline:  -0.028020000000000135 0.688 0.688
probs:  [0.09711591765943643, 0.24333109159515987, 0.17453345165731085, 0.48501953908809287]
maxi score, test score, baseline:  -0.028020000000000135 0.688 0.688
probs:  [0.09711591765943643, 0.24333109159515987, 0.17453345165731085, 0.48501953908809287]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.51 ]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[1.291]
 [1.161]
 [1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.409]] [[0.53 ]
 [0.468]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.601]]
maxi score, test score, baseline:  -0.028020000000000135 0.688 0.688
probs:  [0.09711591765943643, 0.24333109159515987, 0.17453345165731085, 0.48501953908809287]
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]] [[0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]]
maxi score, test score, baseline:  -0.030833333333333476 0.688 0.688
actor:  0 policy actor:  0  step number:  30 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.027620000000000134 0.688 0.688
probs:  [0.09711591765943643, 0.24333109159515987, 0.17453345165731085, 0.48501953908809287]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.012]
 [-0.129]
 [-0.061]
 [-0.003]
 [-0.003]
 [ 0.04 ]] [[1.161]
 [0.864]
 [0.724]
 [0.604]
 [1.161]
 [1.161]
 [1.328]] [[0.252]
 [0.173]
 [0.071]
 [0.066]
 [0.252]
 [0.252]
 [0.319]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.806]
 [0.745]
 [0.742]
 [0.755]
 [0.75 ]
 [0.75 ]] [[-0.237]
 [-0.4  ]
 [ 0.09 ]
 [-0.002]
 [-0.533]
 [ 0.039]
 [ 0.174]] [[0.748]
 [0.806]
 [0.745]
 [0.742]
 [0.755]
 [0.75 ]
 [0.75 ]]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.458]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.367]] [[1.481]
 [1.082]
 [1.481]
 [1.481]
 [1.481]
 [1.481]
 [2.118]] [[0.216]
 [0.048]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.53 ]]
actor:  0 policy actor:  0  step number:  59 total reward:  0.2666666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.027673333333333463 0.688 0.688
probs:  [0.09711591765943643, 0.24333109159515987, 0.17453345165731085, 0.48501953908809287]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.037]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]] [[0.396]
 [1.363]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]] [[-0.947]
 [-0.626]
 [-0.947]
 [-0.947]
 [-0.947]
 [-0.947]
 [-0.947]]
actor:  0 policy actor:  0  step number:  54 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.025180000000000143 0.688 0.688
probs:  [0.09711591765943643, 0.24333109159515987, 0.17453345165731085, 0.48501953908809287]
maxi score, test score, baseline:  -0.025180000000000143 0.688 0.688
probs:  [0.09711591765943643, 0.24333109159515987, 0.17453345165731085, 0.48501953908809287]
actor:  1 policy actor:  1  step number:  55 total reward:  0.5066666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.025180000000000143 0.688 0.688
probs:  [0.09711591765943645, 0.2433310915951598, 0.17453345165731085, 0.485019539088093]
actor:  1 policy actor:  1  step number:  36 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.025180000000000143 0.688 0.688
probs:  [0.09717187502553613, 0.24309402046405487, 0.17443425724925432, 0.48529984726115477]
maxi score, test score, baseline:  -0.02723333333333347 0.688 0.688
probs:  [0.09717187502553613, 0.24309402046405487, 0.17443425724925432, 0.48529984726115477]
maxi score, test score, baseline:  -0.02723333333333347 0.688 0.688
probs:  [0.09717187502553613, 0.24309402046405487, 0.17443425724925432, 0.48529984726115477]
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.044]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]] [[1.356]
 [1.467]
 [1.356]
 [1.356]
 [1.356]
 [1.356]
 [1.356]] [[-0.02 ]
 [ 0.013]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[1.068]
 [1.387]
 [1.387]
 [1.387]
 [1.387]
 [1.387]
 [1.387]] [[0.571]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]]
actor:  0 policy actor:  1  step number:  46 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  52 total reward:  0.4866666666666669  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  35 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.02654000000000013 0.688 0.688
probs:  [0.09746818168631961, 0.24183867625966982, 0.1739090008644689, 0.4867841411895416]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]] [[1.76]
 [1.76]
 [1.76]
 [1.76]
 [1.76]
 [1.76]
 [1.76]] [[0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]]
maxi score, test score, baseline:  -0.02654000000000013 0.688 0.688
probs:  [0.09746818168631961, 0.24183867625966982, 0.1739090008644689, 0.4867841411895416]
from probs:  [0.09746818168631961, 0.24183867625966982, 0.1739090008644689, 0.4867841411895416]
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.113]
 [0.099]
 [0.122]
 [0.104]
 [0.098]
 [0.104]] [[-3.959]
 [ 0.079]
 [-2.12 ]
 [-0.982]
 [-2.284]
 [-2.153]
 [-2.257]] [[0.086]
 [0.113]
 [0.099]
 [0.122]
 [0.104]
 [0.098]
 [0.104]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5333333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.02892666666666679 0.688 0.688
probs:  [0.09746818168631961, 0.24183867625966982, 0.1739090008644689, 0.4867841411895416]
maxi score, test score, baseline:  -0.03158000000000013 0.688 0.688
probs:  [0.09746729610627246, 0.24184046457850544, 0.1739125081406341, 0.486779731174588]
Printing some Q and Qe and total Qs values:  [[ 0.043]
 [ 0.065]
 [ 0.031]
 [-0.001]
 [-0.006]
 [-0.018]
 [ 0.015]] [[0.951]
 [0.982]
 [0.333]
 [0.368]
 [0.405]
 [0.275]
 [1.205]] [[-0.436]
 [-0.404]
 [-0.653]
 [-0.674]
 [-0.667]
 [-0.722]
 [-0.379]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.373]] [[2.199]
 [2.199]
 [2.199]
 [2.199]
 [2.199]
 [2.199]
 [3.754]] [[0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.794]]
maxi score, test score, baseline:  -0.03158000000000012 0.688 0.688
probs:  [0.09746729610627246, 0.24184046457850544, 0.1739125081406341, 0.486779731174588]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using explorer policy with actor:  1
siam score:  -0.7553032
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  53 total reward:  0.5733333333333338  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.034140000000000136 0.688 0.688
probs:  [0.09746641555984863, 0.24184224273256433, 0.17391599548149567, 0.4867753462260913]
actor:  1 policy actor:  1  step number:  49 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]] [[ 0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[0.674]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]]
maxi score, test score, baseline:  -0.03642000000000012 0.688 0.688
probs:  [0.09746641555984863, 0.24184224273256433, 0.17391599548149567, 0.4867753462260913]
actor:  1 policy actor:  1  step number:  56 total reward:  0.5666666666666671  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.03642000000000012 0.688 0.688
probs:  [0.09746641555984863, 0.24184224273256433, 0.17391599548149567, 0.4867753462260913]
actor:  0 policy actor:  0  step number:  43 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.333
using another actor
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.0066678181414672
maxi score, test score, baseline:  -0.036246666666666795 0.688 0.688
Printing some Q and Qe and total Qs values:  [[ 0.233]
 [ 0.192]
 [-0.029]
 [ 0.223]
 [ 0.238]
 [ 0.226]
 [ 0.223]] [[2.407]
 [2.163]
 [1.887]
 [1.422]
 [1.285]
 [1.938]
 [2.146]] [[0.606]
 [0.529]
 [0.36 ]
 [0.369]
 [0.344]
 [0.493]
 [0.54 ]]
maxi score, test score, baseline:  -0.036246666666666795 0.688 0.688
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.5333333333333337  reward:  1.0 rdn_beta:  0.667
siam score:  -0.76022154
actor:  1 policy actor:  1  step number:  48 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  53 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.333
from probs:  [0.0974655400042532, 0.24184401080826562, 0.17391946305653982, 0.4867709861309412]
maxi score, test score, baseline:  -0.03358000000000012 0.688 0.688
probs:  [0.0974655400042532, 0.24184401080826562, 0.17391946305653982, 0.4867709861309412]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.641]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.572]] [[1.477]
 [1.979]
 [1.425]
 [1.425]
 [1.425]
 [1.425]
 [1.034]] [[0.574]
 [0.641]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.572]]
siam score:  -0.766835
maxi score, test score, baseline:  -0.03358000000000012 0.688 0.688
probs:  [0.0974655400042532, 0.24184401080826562, 0.17391946305653982, 0.4867709861309412]
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.793]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]] [[1.072]
 [0.986]
 [1.072]
 [1.072]
 [1.072]
 [1.072]
 [1.072]] [[0.813]
 [0.793]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]]
maxi score, test score, baseline:  -0.03358000000000012 0.688 0.688
probs:  [0.0974655400042532, 0.24184401080826562, 0.17391946305653982, 0.4867709861309412]
actor:  1 policy actor:  1  step number:  42 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.03358000000000012 0.688 0.688
probs:  [0.0974655400042532, 0.24184401080826562, 0.17391946305653982, 0.4867709861309412]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]] [[1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]] [[-0.415]
 [-0.415]
 [-0.415]
 [-0.415]
 [-0.415]
 [-0.415]
 [-0.415]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.6400000000000002  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  45 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  41 total reward:  0.626666666666667  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.0211],
        [ 0.8950],
        [-0.1827],
        [-0.1724],
        [ 0.9027],
        [ 0.1494],
        [-0.0642],
        [ 0.7096],
        [-0.6374],
        [ 0.0579]], dtype=torch.float64)
-0.057834381198 -0.03677568081462351
-0.071422513866 0.8235814166072378
-0.058351887066 -0.2410388425215088
-0.032346567066 -0.20472300519859496
-0.058351887066 0.8443605407231477
-0.032346567066 0.11700880501746974
-0.032346567066 -0.0965587904743444
-0.070771701198 0.6388489701831123
-0.084359833866 -0.7217844981456305
-0.08410238119800001 -0.026166406067914212
maxi score, test score, baseline:  -0.0332866666666668 0.688 0.688
probs:  [0.0974655400042532, 0.24184401080826562, 0.17391946305653982, 0.4867709861309412]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0332866666666668 0.688 0.688
using another actor
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.677]] [[1.42 ]
 [1.42 ]
 [1.42 ]
 [1.42 ]
 [1.42 ]
 [1.42 ]
 [1.396]] [[0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.351]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.5
from probs:  [0.0974655400042532, 0.24184401080826562, 0.17391946305653982, 0.4867709861309412]
actor:  1 policy actor:  1  step number:  47 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.003]
 [-0.005]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.401]
 [0.44 ]
 [0.212]
 [0.483]
 [0.483]
 [0.483]
 [0.476]] [[-0.501]
 [-0.496]
 [-0.536]
 [-0.488]
 [-0.488]
 [-0.488]
 [-0.489]]
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.111]
 [0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]] [[0.505]
 [0.634]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[-0.207]
 [-0.18 ]
 [-0.238]
 [-0.238]
 [-0.238]
 [-0.238]
 [-0.238]]
maxi score, test score, baseline:  -0.03615333333333346 0.688 0.688
probs:  [0.0974655400042532, 0.24184401080826562, 0.17391946305653982, 0.4867709861309412]
maxi score, test score, baseline:  -0.03615333333333346 0.688 0.688
probs:  [0.0974655400042532, 0.24184401080826562, 0.17391946305653982, 0.4867709861309412]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.03615333333333346 0.688 0.688
probs:  [0.0974655400042532, 0.24184401080826562, 0.17391946305653982, 0.4867709861309412]
maxi score, test score, baseline:  -0.03615333333333346 0.688 0.688
probs:  [0.0974655400042532, 0.24184401080826562, 0.17391946305653982, 0.4867709861309412]
siam score:  -0.75689805
actor:  1 policy actor:  1  step number:  39 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 1.8440113814599992
Printing some Q and Qe and total Qs values:  [[-0.029]
 [ 0.185]
 [ 0.156]
 [ 0.157]
 [ 0.09 ]
 [ 0.233]
 [ 0.146]] [[1.228]
 [1.516]
 [1.873]
 [1.442]
 [1.408]
 [2.629]
 [2.   ]] [[-0.457]
 [-0.162]
 [-0.077]
 [-0.212]
 [-0.287]
 [ 0.237]
 [-0.046]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03615333333333346 0.688 0.688
probs:  [0.09746466939717498, 0.24184576889105144, 0.1739229110333371, 0.4867666506784365]
line 256 mcts: sample exp_bonus 2.078955575672559
actor:  1 policy actor:  1  step number:  37 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.03615333333333346 0.688 0.688
probs:  [0.09669140920660106, 0.24786646637574308, 0.17254211592311122, 0.4829000084945446]
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.54 ]] [[0.731]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [0.4  ]] [[-0.087]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]
 [-0.085]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03615333333333346 0.688 0.688
probs:  [0.0966897342387683, 0.2478696304977592, 0.17254896699912597, 0.48289166826434643]
maxi score, test score, baseline:  -0.03615333333333346 0.688 0.688
probs:  [0.0966897342387683, 0.2478696304977592, 0.17254896699912597, 0.48289166826434643]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.03615333333333346 0.688 0.688
probs:  [0.0966889037946907, 0.24787119926005896, 0.17255236374226973, 0.4828875332029806]
actor:  1 policy actor:  1  step number:  62 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.03615333333333346 0.688 0.688
maxi score, test score, baseline:  -0.03615333333333346 0.688 0.688
probs:  [0.09668807799138456, 0.24787275925564264, 0.17255574150339018, 0.48288342124958256]
using explorer policy with actor:  0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.5933333333333337  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.104]
 [-0.106]
 [-0.095]
 [-0.095]
 [-0.1  ]
 [-0.104]
 [-0.097]] [[0.16 ]
 [0.005]
 [0.019]
 [0.013]
 [0.004]
 [0.319]
 [0.602]] [[-0.231]
 [-0.259]
 [-0.246]
 [-0.246]
 [-0.253]
 [-0.205]
 [-0.151]]
actor:  0 policy actor:  1  step number:  42 total reward:  0.4999999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.03315333333333346 0.688 0.688
probs:  [0.09638415103968107, 0.24709651115849354, 0.17515566837501706, 0.48136366942680825]
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.766]] [[1.42 ]
 [1.146]
 [1.146]
 [1.146]
 [1.146]
 [1.146]
 [1.2  ]] [[0.816]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.766]]
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]] [[1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.155]] [[0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]]
maxi score, test score, baseline:  -0.03315333333333346 0.688 0.688
probs:  [0.09638415103968107, 0.24709651115849354, 0.17515566837501706, 0.48136366942680825]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.376510236673179
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.222]] [[1.375]
 [1.547]
 [1.547]
 [1.547]
 [1.547]
 [1.547]
 [1.529]] [[0.253]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.096]]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[1.145]
 [1.145]
 [1.145]
 [1.145]
 [1.145]
 [1.145]
 [1.145]] [[-0.268]
 [-0.268]
 [-0.268]
 [-0.268]
 [-0.268]
 [-0.268]
 [-0.268]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  39 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  38 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.9283400167073115
actor:  0 policy actor:  1  step number:  48 total reward:  0.4866666666666669  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  53 total reward:  0.5466666666666671  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5647212249796139
maxi score, test score, baseline:  -0.03291333333333346 0.688 0.688
probs:  [0.09721002132346532, 0.2435747819014755, 0.1737147167842092, 0.48550047999084994]
siam score:  -0.76238805
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.732]
 [0.674]
 [0.719]
 [0.704]
 [0.632]
 [0.756]] [[1.491]
 [1.174]
 [1.633]
 [1.115]
 [1.491]
 [1.046]
 [1.392]] [[0.624]
 [0.471]
 [0.678]
 [0.428]
 [0.624]
 [0.316]
 [0.612]]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.494]
 [0.412]
 [0.449]
 [0.398]
 [0.395]
 [0.368]] [[ 0.082]
 [ 0.048]
 [ 0.074]
 [-0.028]
 [ 0.133]
 [ 0.055]
 [ 0.49 ]] [[-0.247]
 [-0.148]
 [-0.226]
 [-0.206]
 [-0.23 ]
 [-0.247]
 [-0.201]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  54 total reward:  0.5933333333333337  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  60 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7674953
maxi score, test score, baseline:  -0.03291333333333346 0.688 0.688
probs:  [0.09720922750024225, 0.2435763571690455, 0.17371788846785757, 0.48549652686285477]
Printing some Q and Qe and total Qs values:  [[ 0.506]
 [ 0.069]
 [-0.102]
 [ 0.297]
 [ 0.453]
 [-0.034]
 [ 0.095]] [[1.373]
 [1.32 ]
 [1.332]
 [1.763]
 [1.896]
 [1.679]
 [3.957]] [[0.411]
 [0.258]
 [0.206]
 [0.427]
 [0.506]
 [0.302]
 [0.834]]
maxi score, test score, baseline:  -0.03291333333333346 0.688 0.688
maxi score, test score, baseline:  -0.03291333333333346 0.688 0.688
probs:  [0.09720922750024225, 0.2435763571690455, 0.17371788846785757, 0.48549652686285477]
actor:  1 policy actor:  1  step number:  30 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.03291333333333346 0.688 0.688
probs:  [0.09809589690818246, 0.23979944896339167, 0.17216682421700436, 0.4899378299114214]
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.619]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[0.099]
 [0.111]
 [0.564]
 [0.462]
 [0.117]
 [0.246]
 [0.506]] [[0.239]
 [0.237]
 [0.318]
 [0.301]
 [0.243]
 [0.265]
 [0.309]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.271]
 [0.271]
 [0.3  ]
 [0.441]
 [0.271]
 [0.271]] [[1.74 ]
 [2.432]
 [2.432]
 [1.41 ]
 [1.946]
 [2.432]
 [2.432]] [[ 0.209]
 [ 0.563]
 [ 0.563]
 [-0.027]
 [ 0.424]
 [ 0.563]
 [ 0.563]]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.564]
 [0.516]
 [0.524]
 [0.524]
 [0.512]
 [0.512]] [[1.464]
 [1.309]
 [1.18 ]
 [2.04 ]
 [2.04 ]
 [1.181]
 [1.09 ]] [[0.288]
 [0.245]
 [0.144]
 [0.585]
 [0.585]
 [0.141]
 [0.095]]
maxi score, test score, baseline:  -0.03291333333333346 0.688 0.688
probs:  [0.09810306161329427, 0.23978482554079025, 0.1721385985190591, 0.48997351432685643]
maxi score, test score, baseline:  -0.03291333333333346 0.688 0.688
probs:  [0.09810306161329427, 0.23978482554079025, 0.17213859851905908, 0.48997351432685643]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03291333333333346 0.688 0.688
probs:  [0.09810306161329427, 0.23978482554079025, 0.17213859851905908, 0.48997351432685643]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  40 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using another actor
siam score:  -0.77118754
actor:  1 policy actor:  1  step number:  38 total reward:  0.6600000000000003  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]] [[1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.766]] [[0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]]
siam score:  -0.77476865
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.037]
 [ 0.177]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[1.099]
 [1.099]
 [1.704]
 [1.099]
 [1.099]
 [1.099]
 [1.099]] [[-0.723]
 [-0.723]
 [-0.308]
 [-0.723]
 [-0.723]
 [-0.723]
 [-0.723]]
maxi score, test score, baseline:  -0.03291333333333346 0.688 0.688
probs:  [0.09810306161329427, 0.23978482554079025, 0.17213859851905905, 0.48997351432685643]
actor:  0 policy actor:  0  step number:  55 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.030033333333333457 0.688 0.688
probs:  [0.09810306161329427, 0.23978482554079023, 0.17213859851905905, 0.48997351432685643]
maxi score, test score, baseline:  -0.030033333333333457 0.688 0.688
probs:  [0.0981022601040906, 0.23978646145010138, 0.17214175610284368, 0.48996952234296437]
maxi score, test score, baseline:  -0.030033333333333457 0.688 0.688
probs:  [0.0981022601040906, 0.23978646145010138, 0.17214175610284368, 0.48996952234296437]
maxi score, test score, baseline:  -0.030033333333333457 0.688 0.688
probs:  [0.0981022601040906, 0.23978646145010138, 0.17214175610284368, 0.48996952234296437]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
from probs:  [0.0981022601040906, 0.23978646145010138, 0.17214175610284368, 0.48996952234296437]
siam score:  -0.7678046
actor:  0 policy actor:  0  step number:  52 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  31 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.72 ]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[0.014]
 [0.071]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]] [[-0.053]
 [ 0.046]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5800000000000002  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09675128907159168, 0.25026521868408325, 0.16976962310430768, 0.4832138691400175]
maxi score, test score, baseline:  -0.030233333333333463 0.688 0.688
probs:  [0.09675128907159168, 0.25026521868408325, 0.16976962310430768, 0.4832138691400175]
maxi score, test score, baseline:  -0.030233333333333463 0.688 0.688
probs:  [0.09675128907159168, 0.25026521868408325, 0.16976962310430768, 0.4832138691400175]
actor:  0 policy actor:  1  step number:  35 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.026820000000000118 0.688 0.688
probs:  [0.09675128907159168, 0.25026521868408325, 0.16976962310430768, 0.4832138691400175]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.02682000000000012 0.688 0.688
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [ 0.226]
 [-0.019]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]] [[0.766]
 [2.049]
 [1.186]
 [0.766]
 [0.766]
 [0.766]
 [0.766]] [[0.175]
 [0.537]
 [0.263]
 [0.175]
 [0.175]
 [0.175]
 [0.175]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.02682000000000012 0.688 0.688
probs:  [0.09675128907159168, 0.2502652186840832, 0.16976962310430768, 0.4832138691400175]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  53 total reward:  0.27999999999999925  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.024260000000000122 0.688 0.688
maxi score, test score, baseline:  -0.024260000000000122 0.688 0.688
probs:  [0.09675128907159168, 0.2502652186840832, 0.16976962310430768, 0.4832138691400175]
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.283]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]] [[1.407]
 [3.126]
 [1.407]
 [1.407]
 [1.407]
 [1.407]
 [1.407]] [[0.273]
 [0.78 ]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]]
maxi score, test score, baseline:  -0.024260000000000122 0.688 0.688
probs:  [0.09675128907159168, 0.2502652186840832, 0.16976962310430768, 0.4832138691400175]
actor:  0 policy actor:  0  step number:  50 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.02386000000000012 0.688 0.688
probs:  [0.09675128907159168, 0.2502652186840832, 0.16976962310430768, 0.4832138691400175]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.02386000000000012 0.688 0.688
probs:  [0.09604698432597553, 0.24844166915910892, 0.17581941714719024, 0.4796919293677253]
maxi score, test score, baseline:  -0.02386000000000012 0.688 0.688
probs:  [0.09604698432597553, 0.24844166915910892, 0.17581941714719024, 0.4796919293677253]
maxi score, test score, baseline:  -0.02386000000000012 0.688 0.688
actor:  0 policy actor:  0  step number:  50 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.023660000000000125 0.688 0.688
probs:  [0.09604698432597553, 0.24844166915910892, 0.17581941714719024, 0.4796919293677253]
maxi score, test score, baseline:  -0.023660000000000125 0.688 0.688
probs:  [0.09604698432597553, 0.248441669159109, 0.17581941714719024, 0.4796919293677253]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.023660000000000125 0.688 0.688
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.404]] [[1.612]
 [1.612]
 [1.612]
 [1.612]
 [1.612]
 [1.612]
 [2.289]] [[0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.489]]
Printing some Q and Qe and total Qs values:  [[0.851]
 [0.899]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.843]] [[ 0.285]
 [-0.611]
 [ 0.285]
 [ 0.285]
 [ 0.285]
 [ 0.285]
 [ 0.04 ]] [[0.851]
 [0.899]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.843]]
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.356]
 [0.403]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[1.053]
 [1.053]
 [2.224]
 [1.053]
 [1.053]
 [1.053]
 [1.053]] [[-0.426]
 [-0.426]
 [ 0.207]
 [-0.426]
 [-0.426]
 [-0.426]
 [-0.426]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.405]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]] [[2.069]
 [2.198]
 [2.069]
 [2.069]
 [2.069]
 [2.069]
 [2.069]] [[0.29 ]
 [0.387]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]]
maxi score, test score, baseline:  -0.02366000000000011 0.688 0.688
probs:  [0.09604698432597553, 0.248441669159109, 0.17581941714719024, 0.4796919293677253]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[1.988]
 [1.988]
 [1.988]
 [1.988]
 [1.988]
 [1.988]
 [1.988]] [[0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]]
maxi score, test score, baseline:  -0.02366000000000011 0.688 0.688
probs:  [0.09672093500152586, 0.24557285633943338, 0.17463887491024951, 0.48306733374879124]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [-0.026]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]] [[1.697]
 [2.202]
 [1.697]
 [1.697]
 [1.697]
 [1.697]
 [1.697]] [[0.723]
 [0.877]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]]
maxi score, test score, baseline:  -0.02366000000000011 0.688 0.688
probs:  [0.09672093500152586, 0.24557285633943338, 0.17463887491024951, 0.48306733374879124]
maxi score, test score, baseline:  -0.02366000000000011 0.688 0.688
probs:  [0.09672093500152586, 0.24557285633943338, 0.17463887491024951, 0.48306733374879124]
maxi score, test score, baseline:  -0.02366000000000011 0.688 0.688
probs:  [0.09672093500152586, 0.24557285633943338, 0.17463887491024951, 0.48306733374879124]
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]] [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[-0.24]
 [-0.24]
 [-0.24]
 [-0.24]
 [-0.24]
 [-0.24]
 [-0.24]]
maxi score, test score, baseline:  -0.02366000000000011 0.688 0.688
actor:  0 policy actor:  0  step number:  43 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.020913333333333464 0.688 0.688
probs:  [0.09704906412152295, 0.24417610571641807, 0.17406409948620166, 0.48471073067585735]
maxi score, test score, baseline:  -0.020913333333333464 0.688 0.688
probs:  [0.09704906412152295, 0.24417610571641807, 0.17406409948620166, 0.48471073067585735]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.020913333333333464 0.688 0.688
probs:  [0.09704906412152295, 0.24417610571641807, 0.17406409948620166, 0.48471073067585735]
maxi score, test score, baseline:  -0.020913333333333464 0.688 0.688
probs:  [0.09704906412152295, 0.24417610571641807, 0.17406409948620166, 0.48471073067585735]
maxi score, test score, baseline:  -0.020913333333333464 0.688 0.688
probs:  [0.09704906412152295, 0.24417610571641807, 0.17406409948620166, 0.48471073067585735]
actor:  1 policy actor:  1  step number:  66 total reward:  0.3266666666666659  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.020913333333333464 0.688 0.688
probs:  [0.09704906412152295, 0.24417610571641807, 0.17406409948620166, 0.48471073067585735]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6333333333333337  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  40 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.02 ]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]] [[1.024]
 [0.804]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]] [[0.412]
 [0.271]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]]
Printing some Q and Qe and total Qs values:  [[ 0.118]
 [ 0.088]
 [-0.002]
 [-0.011]
 [-0.041]
 [-0.013]
 [ 0.029]] [[0.657]
 [0.885]
 [1.21 ]
 [0.627]
 [0.942]
 [0.567]
 [1.235]] [[0.222]
 [0.309]
 [0.4  ]
 [0.116]
 [0.245]
 [0.086]
 [0.434]]
siam score:  -0.76911056
maxi score, test score, baseline:  -0.01802000000000013 0.688 0.688
maxi score, test score, baseline:  -0.01802000000000013 0.688 0.688
probs:  [0.09704906412152295, 0.24417610571641807, 0.17406409948620166, 0.48471073067585735]
actor:  0 policy actor:  0  step number:  61 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.015593333333333452 0.688 0.688
probs:  [0.09704906412152295, 0.24417610571641807, 0.17406409948620166, 0.48471073067585735]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.667
using another actor
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.537]
 [0.444]
 [0.445]
 [0.457]
 [0.444]
 [0.466]] [[0.423]
 [0.437]
 [0.297]
 [0.328]
 [0.353]
 [0.417]
 [0.503]] [[-0.209]
 [-0.084]
 [-0.201]
 [-0.195]
 [-0.179]
 [-0.181]
 [-0.144]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5933333333333337  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.015593333333333452 0.688 0.688
probs:  [0.09759089750092015, 0.24186804292908895, 0.17311659857551084, 0.4874244609944801]
from probs:  [0.09759089750092015, 0.24186804292908895, 0.17311659857551084, 0.4874244609944801]
maxi score, test score, baseline:  -0.015593333333333452 0.688 0.688
deleting a thread, now have 4 threads
Frames:  136258 train batches done:  15963 episodes:  4320
actor:  0 policy actor:  1  step number:  41 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.015086666666666792 0.688 0.688
actor:  0 policy actor:  0  step number:  42 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  35 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 1.5330259573440805
deleting a thread, now have 3 threads
Frames:  136376 train batches done:  15976 episodes:  4323
actor:  0 policy actor:  0  step number:  53 total reward:  0.36  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.012060000000000128 0.688 0.688
probs:  [0.0974582270126301, 0.24289966292439572, 0.17288109629227952, 0.4867610137706947]
maxi score, test score, baseline:  -0.012060000000000128 0.688 0.688
probs:  [0.0974582270126301, 0.24289966292439572, 0.17288109629227952, 0.4867610137706947]
maxi score, test score, baseline:  -0.012060000000000128 0.688 0.688
probs:  [0.0974582270126301, 0.24289966292439572, 0.17288109629227952, 0.4867610137706947]
actor:  1 policy actor:  1  step number:  44 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.012060000000000128 0.688 0.688
probs:  [0.09745750648306227, 0.24290109650771718, 0.1728839718790843, 0.4867574251301363]
maxi score, test score, baseline:  -0.012060000000000128 0.688 0.688
probs:  [0.09745750648306227, 0.24290109650771718, 0.1728839718790843, 0.4867574251301363]
deleting a thread, now have 2 threads
Frames:  136473 train batches done:  15990 episodes:  4325
siam score:  -0.7628087
maxi score, test score, baseline:  -0.012060000000000128 0.688 0.688
probs:  [0.09745750648306227, 0.24290109650771718, 0.1728839718790843, 0.4867574251301363]
maxi score, test score, baseline:  -0.012060000000000128 0.688 0.688
probs:  [0.09745750648306227, 0.24290109650771718, 0.1728839718790843, 0.4867574251301363]
maxi score, test score, baseline:  -0.012060000000000128 0.688 0.688
maxi score, test score, baseline:  -0.012060000000000128 0.688 0.688
probs:  [0.09745750648306227, 0.24290109650771718, 0.1728839718790843, 0.4867574251301363]
from probs:  [0.09745750648306227, 0.24290109650771718, 0.1728839718790843, 0.4867574251301363]
maxi score, test score, baseline:  -0.012060000000000128 0.688 0.688
probs:  [0.09745678987768606, 0.24290252228337053, 0.17288683180469455, 0.48675385603424876]
maxi score, test score, baseline:  -0.012060000000000128 0.688 0.688
maxi score, test score, baseline:  -0.012060000000000128 0.688 0.688
probs:  [0.09745678987768606, 0.24290252228337053, 0.17288683180469455, 0.48675385603424876]
actor:  0 policy actor:  0  step number:  47 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  54 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.009046666666666786 0.688 0.688
maxi score, test score, baseline:  -0.009046666666666786 0.688 0.688
maxi score, test score, baseline:  -0.009046666666666786 0.688 0.688
probs:  [0.09745678987768606, 0.24290252228337053, 0.17288683180469455, 0.48675385603424876]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.374]
 [0.384]
 [0.374]
 [0.374]
 [0.374]
 [0.374]] [[0.978]
 [0.978]
 [1.207]
 [0.978]
 [0.978]
 [0.978]
 [0.978]] [[-0.295]
 [-0.295]
 [-0.209]
 [-0.295]
 [-0.295]
 [-0.295]
 [-0.295]]
maxi score, test score, baseline:  -0.009046666666666786 0.688 0.688
maxi score, test score, baseline:  -0.009046666666666786 0.688 0.688
probs:  [0.09745678987768606, 0.24290252228337053, 0.17288683180469455, 0.48675385603424876]
maxi score, test score, baseline:  -0.009046666666666786 0.688 0.688
probs:  [0.09745678987768606, 0.24290252228337053, 0.17288683180469455, 0.48675385603424876]
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.607]
 [0.668]
 [0.63 ]
 [0.607]
 [0.607]
 [0.677]] [[0.217]
 [0.493]
 [0.795]
 [0.489]
 [0.493]
 [0.493]
 [0.814]] [[-0.234]
 [-0.095]
 [ 0.066]
 [-0.073]
 [-0.095]
 [-0.095]
 [ 0.082]]
maxi score, test score, baseline:  -0.009046666666666786 0.688 0.688
probs:  [0.09745678987768606, 0.24290252228337053, 0.17288683180469455, 0.48675385603424876]
actor:  1 policy actor:  1  step number:  41 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.751]
 [0.756]
 [0.714]
 [0.698]
 [0.668]
 [0.729]] [[0.391]
 [0.26 ]
 [1.232]
 [0.314]
 [0.232]
 [0.101]
 [0.289]] [[0.376]
 [0.322]
 [0.812]
 [0.32 ]
 [0.268]
 [0.179]
 [0.32 ]]
maxi score, test score, baseline:  -0.009046666666666786 0.688 0.688
probs:  [0.09745678987768606, 0.24290252228337053, 0.17288683180469455, 0.48675385603424876]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.009046666666666786 0.688 0.688
probs:  [0.09745887525432877, 0.2428936105081823, 0.17288321391586522, 0.48676430032162366]
maxi score, test score, baseline:  -0.009046666666666786 0.688 0.688
probs:  [0.09745887525432877, 0.2428936105081823, 0.17288321391586522, 0.48676430032162366]
actor:  1 policy actor:  1  step number:  30 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
first move QE:  -0.0701659934388902
actor:  1 policy actor:  1  step number:  46 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.009046666666666786 0.688 0.688
probs:  [0.097568045705567, 0.24242707487202325, 0.1726938157285346, 0.4873110636938751]
maxi score, test score, baseline:  -0.009046666666666786 0.688 0.688
first move QE:  -0.07031949392366806
maxi score, test score, baseline:  -0.009046666666666786 0.688 0.688
probs:  [0.097568045705567, 0.24242707487202325, 0.1726938157285346, 0.4873110636938751]
using explorer policy with actor:  0
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  42 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
probs:  [0.097568045705567, 0.24242707487202325, 0.1726938157285346, 0.4873110636938751]
actor:  1 policy actor:  1  step number:  38 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
probs:  [0.09776259668483633, 0.24159405927147984, 0.17235788218415174, 0.48828546185953203]
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
probs:  [0.09776259668483633, 0.24159405927147984, 0.17235788218415174, 0.48828546185953203]
siam score:  -0.76938033
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
probs:  [0.09776259668483633, 0.24159405927147984, 0.17235788218415174, 0.48828546185953203]
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
probs:  [0.09776188956808328, 0.24159548019461224, 0.17236069038565496, 0.48828193985164964]
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
probs:  [0.09776188956808328, 0.24159548019461224, 0.17236069038565496, 0.48828193985164964]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.57 ]] [[2.099]
 [1.738]
 [1.738]
 [1.738]
 [1.738]
 [1.738]
 [2.859]] [[0.422]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.741]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
probs:  [0.09776188956808328, 0.24159548019461224, 0.17236069038565494, 0.48828193985164964]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8632653012603289
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
probs:  [0.09776188956808328, 0.24159548019461224, 0.17236069038565494, 0.48828193985164964]
actor:  1 policy actor:  1  step number:  36 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
probs:  [0.09777221043195576, 0.24155137560300483, 0.17234278367281836, 0.4883336302922212]
siam score:  -0.7676917
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
probs:  [0.09777221043195576, 0.24155137560300483, 0.17234278367281836, 0.4883336302922212]
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
probs:  [0.09777221043195576, 0.24155137560300483, 0.17234278367281836, 0.4883336302922212]
actor:  1 policy actor:  1  step number:  39 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
probs:  [0.09784537859059411, 0.2412387029628766, 0.17221583681820352, 0.4887000816283257]
from probs:  [0.09784537859059411, 0.2412387029628766, 0.17221583681820352, 0.4887000816283257]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.005753333333333452 0.688 0.688
probs:  [0.09784467578740401, 0.2412401189733251, 0.17221862417951656, 0.4886965810597544]
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.199]
 [0.256]
 [0.102]
 [0.187]
 [0.178]
 [0.126]] [[2.479]
 [0.983]
 [1.45 ]
 [0.562]
 [1.035]
 [0.763]
 [0.909]] [[-0.149]
 [-0.497]
 [-0.362]
 [-0.664]
 [-0.499]
 [-0.554]
 [-0.581]]
maxi score, test score, baseline:  -0.00904666666666679 0.688 0.688
probs:  [0.09784467578740401, 0.2412401189733251, 0.17221862417951656, 0.4886965810597544]
maxi score, test score, baseline:  -0.00904666666666679 0.688 0.688
probs:  [0.09784467578740401, 0.2412401189733251, 0.17221862417951656, 0.4886965810597544]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.38 ]
 [0.397]
 [0.38 ]
 [0.38 ]
 [0.305]
 [0.38 ]] [[1.064]
 [1.064]
 [1.468]
 [1.064]
 [1.064]
 [1.079]
 [1.064]] [[-0.308]
 [-0.308]
 [-0.224]
 [-0.308]
 [-0.308]
 [-0.381]
 [-0.308]]
using explorer policy with actor:  1
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.00904666666666679 0.688 0.688
probs:  [0.09784397677022223, 0.24124152735570994, 0.17222139652528415, 0.48869309934878363]
actor:  1 policy actor:  1  step number:  44 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.00904666666666679 0.688 0.688
probs:  [0.0978432815085381, 0.24124292817150447, 0.17222415397651378, 0.4886896363434436]
actor:  1 policy actor:  1  step number:  67 total reward:  0.5466666666666673  reward:  1.0 rdn_beta:  0.667
from probs:  [0.0978432815085381, 0.24124292817150447, 0.17222415397651378, 0.4886896363434436]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]] [[1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]] [[-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]]
maxi score, test score, baseline:  -0.00904666666666679 0.688 0.688
probs:  [0.0978432815085381, 0.24124292817150447, 0.17222415397651378, 0.4886896363434436]
siam score:  -0.7673242
actor:  1 policy actor:  1  step number:  33 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.012366666666666793 0.688 0.688
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.142]
 [-0.041]
 [ 0.062]
 [-0.039]
 [-0.032]
 [-0.042]] [[-0.102]
 [ 0.09 ]
 [-0.085]
 [-0.219]
 [-0.099]
 [-0.088]
 [-0.041]] [[-0.234]
 [-0.208]
 [-0.223]
 [-0.21 ]
 [-0.23 ]
 [-0.217]
 [-0.195]]
maxi score, test score, baseline:  -0.012366666666666793 0.688 0.688
probs:  [0.0978432815085381, 0.24124292817150447, 0.17222415397651378, 0.4886896363434436]
maxi score, test score, baseline:  -0.012366666666666793 0.688 0.688
probs:  [0.0978432815085381, 0.24124292817150447, 0.17222415397651378, 0.4886896363434436]
maxi score, test score, baseline:  -0.012366666666666793 0.688 0.688
probs:  [0.0978432815085381, 0.24124292817150447, 0.17222415397651378, 0.4886896363434436]
first move QE:  -0.07270960217157667
actor:  1 policy actor:  1  step number:  36 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.012366666666666793 0.688 0.688
probs:  [0.09765221916939867, 0.2407714141342073, 0.17384218456112882, 0.4877341821352652]
maxi score, test score, baseline:  -0.015673333333333463 0.688 0.688
probs:  [0.09765221916939867, 0.2407714141342073, 0.17384218456112882, 0.4877341821352652]
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.136]
 [0.083]
 [0.097]
 [0.113]
 [0.058]
 [0.109]] [[1.629]
 [3.059]
 [1.205]
 [0.627]
 [1.912]
 [0.976]
 [1.502]] [[ 0.143]
 [ 0.539]
 [ 0.015]
 [-0.133]
 [ 0.22 ]
 [-0.06 ]
 [ 0.108]]
maxi score, test score, baseline:  -0.015673333333333463 0.688 0.688
maxi score, test score, baseline:  -0.015673333333333463 0.688 0.688
probs:  [0.09765221916939867, 0.2407714141342073, 0.17384218456112882, 0.4877341821352652]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.015673333333333463 0.688 0.688
probs:  [0.09765221916939867, 0.2407714141342073, 0.17384218456112882, 0.4877341821352652]
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.876]
 [0.836]
 [0.843]
 [0.843]
 [0.843]
 [0.864]] [[0.508]
 [0.454]
 [0.887]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [1.059]] [[0.821]
 [0.876]
 [0.836]
 [0.843]
 [0.843]
 [0.843]
 [0.864]]
maxi score, test score, baseline:  -0.015673333333333463 0.688 0.688
probs:  [0.09765221916939867, 0.2407714141342073, 0.17384218456112882, 0.4877341821352652]
actor:  0 policy actor:  0  step number:  71 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.667
siam score:  -0.77571434
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.016473333333333458 0.688 0.688
probs:  [0.09765221916939867, 0.2407714141342073, 0.17384218456112882, 0.4877341821352652]
actor:  0 policy actor:  0  step number:  53 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[-1.99]
 [-1.99]
 [-1.99]
 [-1.99]
 [-1.99]
 [-1.99]
 [-1.99]] [[-0.261]
 [-0.261]
 [-0.261]
 [-0.261]
 [-0.261]
 [-0.261]
 [-0.261]]
maxi score, test score, baseline:  -0.01690000000000013 0.688 0.688
probs:  [0.09765153271356389, 0.24077281391974195, 0.17384489030189876, 0.4877307630647954]
maxi score, test score, baseline:  -0.01690000000000013 0.688 0.688
probs:  [0.09765153271356389, 0.24077281391974195, 0.17384489030189876, 0.4877307630647954]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.667
siam score:  -0.76481265
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.243699072242633
maxi score, test score, baseline:  -0.01690000000000013 0.688 0.688
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.459]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]] [[1.094]
 [1.431]
 [1.415]
 [1.415]
 [1.415]
 [1.415]
 [1.415]] [[0.074]
 [0.18 ]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]]
using another actor
actor:  1 policy actor:  1  step number:  41 total reward:  0.6800000000000003  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
probs:  [0.09765017077732265, 0.24077559111028063, 0.17385025852258718, 0.48772397958980956]
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
probs:  [0.09765017077732265, 0.24077559111028063, 0.17385025852258718, 0.48772397958980956]
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
probs:  [0.09765017077732265, 0.24077559111028063, 0.17385025852258718, 0.48772397958980956]
siam score:  -0.75896573
actor:  1 policy actor:  1  step number:  42 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.443]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.552]] [[3.255]
 [2.202]
 [2.042]
 [2.042]
 [2.042]
 [2.042]
 [1.96 ]] [[0.34 ]
 [0.186]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.174]]
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
probs:  [0.09765017077732265, 0.24077559111028063, 0.17385025852258718, 0.48772397958980956]
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
probs:  [0.09765017077732265, 0.24077559111028063, 0.17385025852258718, 0.48772397958980956]
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
probs:  [0.09765017077732265, 0.24077559111028063, 0.17385025852258718, 0.48772397958980956]
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
probs:  [0.09765017077732265, 0.24077559111028063, 0.17385025852258718, 0.48772397958980956]
actor:  1 policy actor:  1  step number:  84 total reward:  0.40666666666666684  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
probs:  [0.09764949523857795, 0.24077696863424494, 0.17385292123245216, 0.4877206148947249]
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.743]
 [0.56 ]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[1.511]
 [1.363]
 [1.946]
 [1.511]
 [1.511]
 [1.511]
 [1.511]] [[0.634]
 [0.743]
 [0.56 ]
 [0.634]
 [0.634]
 [0.634]
 [0.634]]
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
probs:  [0.09764949523857795, 0.24077696863424494, 0.17385292123245216, 0.4877206148947249]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.542]
 [0.523]
 [0.508]
 [0.502]
 [0.51 ]
 [0.506]] [[5.503]
 [0.432]
 [0.216]
 [0.145]
 [0.173]
 [0.183]
 [0.234]] [[0.698]
 [0.048]
 [0.016]
 [0.003]
 [0.006]
 [0.009]
 [0.014]]
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
probs:  [0.09764949523857795, 0.24077696863424494, 0.17385292123245216, 0.4877206148947249]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
probs:  [0.09764949523857795, 0.24077696863424494, 0.17385292123245216, 0.4877206148947249]
maxi score, test score, baseline:  -0.0202066666666668 0.688 0.688
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[5.11 ]
 [1.406]
 [1.406]
 [1.406]
 [1.406]
 [1.406]
 [1.406]] [[0.809]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]]
maxi score, test score, baseline:  -0.02354000000000013 0.688 0.688
probs:  [0.09764949523857795, 0.24077696863424494, 0.17385292123245216, 0.4877206148947249]
actor:  1 policy actor:  1  step number:  109 total reward:  0.11999999999999778  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7465834
start point for exploration sampling:  11715
actor:  1 policy actor:  1  step number:  32 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.02354000000000013 0.688 0.688
probs:  [0.09745853178061342, 0.2403086972642532, 0.17546710249049405, 0.48676566846463937]
maxi score, test score, baseline:  -0.02354000000000013 0.688 0.688
probs:  [0.09745853178061342, 0.2403086972642532, 0.17546710249049405, 0.48676566846463937]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.02354000000000013 0.688 0.688
line 256 mcts: sample exp_bonus 0.8474813794720895
actor:  1 policy actor:  1  step number:  33 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  44 total reward:  0.5933333333333337  reward:  1.0 rdn_beta:  0.167
from probs:  [0.09782670480032007, 0.23876985194227077, 0.17479387822060685, 0.48860956503680225]
Printing some Q and Qe and total Qs values:  [[ 0.535]
 [ 0.445]
 [ 0.445]
 [-0.001]
 [ 0.368]
 [ 0.445]
 [ 0.445]] [[1.178]
 [1.758]
 [1.758]
 [0.955]
 [1.247]
 [1.758]
 [1.758]] [[ 0.048]
 [ 0.248]
 [ 0.248]
 [-0.6  ]
 [-0.085]
 [ 0.248]
 [ 0.248]]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.561]
 [0.587]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[1.17 ]
 [1.17 ]
 [1.349]
 [1.17 ]
 [1.17 ]
 [1.17 ]
 [1.17 ]] [[0.155]
 [0.155]
 [0.271]
 [0.155]
 [0.155]
 [0.155]
 [0.155]]
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.752]
 [0.719]
 [0.723]
 [0.657]
 [0.703]
 [0.751]] [[ 0.383]
 [ 0.616]
 [ 0.119]
 [ 0.339]
 [-0.085]
 [ 0.369]
 [ 0.842]] [[0.442]
 [0.563]
 [0.282]
 [0.396]
 [0.118]
 [0.391]
 [0.676]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.026846666666666793 0.688 0.688
probs:  [0.09705367684109527, 0.23688435211488063, 0.18131813173583533, 0.4847438393081887]
Printing some Q and Qe and total Qs values:  [[0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]] [[2.128]
 [2.128]
 [2.128]
 [2.128]
 [2.128]
 [2.128]
 [2.128]] [[0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.026846666666666793 0.688 0.688
probs:  [0.09679987751925875, 0.23626432419602392, 0.1834611522668263, 0.4834746460178909]
maxi score, test score, baseline:  -0.026846666666666793 0.688 0.688
maxi score, test score, baseline:  -0.026846666666666793 0.688 0.688
probs:  [0.09679987751925875, 0.23626432419602392, 0.1834611522668263, 0.4834746460178909]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.026846666666666793 0.688 0.688
probs:  [0.09679987751925875, 0.23626432419602392, 0.1834611522668263, 0.4834746460178909]
siam score:  -0.7438963
maxi score, test score, baseline:  -0.026846666666666793 0.688 0.688
probs:  [0.09679987751925875, 0.23626432419602392, 0.1834611522668263, 0.4834746460178909]
maxi score, test score, baseline:  -0.026846666666666793 0.688 0.688
probs:  [0.09679987751925875, 0.23626432419602392, 0.1834611522668263, 0.4834746460178909]
maxi score, test score, baseline:  -0.026846666666666793 0.688 0.688
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.026846666666666793 0.688 0.688
probs:  [0.09679987751925875, 0.23626432419602392, 0.1834611522668263, 0.4834746460178909]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.026846666666666793 0.688 0.688
probs:  [0.09679987751925875, 0.23626432419602392, 0.1834611522668263, 0.4834746460178909]
actor:  1 policy actor:  1  step number:  33 total reward:  0.6800000000000003  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.026846666666666793 0.688 0.688
probs:  [0.09679987751925875, 0.23626432419602392, 0.1834611522668263, 0.4834746460178909]
maxi score, test score, baseline:  -0.026846666666666793 0.688 0.688
maxi score, test score, baseline:  -0.026846666666666793 0.688 0.688
probs:  [0.09679987751925875, 0.23626432419602392, 0.1834611522668263, 0.4834746460178909]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.709]
 [0.585]
 [0.581]
 [0.608]
 [0.626]
 [0.592]] [[0.473]
 [0.089]
 [0.445]
 [0.099]
 [0.184]
 [0.013]
 [0.254]] [[0.593]
 [0.709]
 [0.585]
 [0.581]
 [0.608]
 [0.626]
 [0.592]]
siam score:  -0.74633044
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.904]
 [0.762]
 [0.791]
 [0.791]
 [0.791]
 [0.821]] [[0.934]
 [0.529]
 [1.038]
 [0.176]
 [0.176]
 [0.176]
 [0.731]] [[0.815]
 [0.904]
 [0.762]
 [0.791]
 [0.791]
 [0.791]
 [0.821]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.2923951796671754
maxi score, test score, baseline:  -0.026846666666666793 0.688 0.688
probs:  [0.09679987751925875, 0.23626432419602392, 0.1834611522668263, 0.4834746460178909]
actor:  0 policy actor:  0  step number:  59 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0277266666666668 0.688 0.688
probs:  [0.09679987751925875, 0.23626432419602392, 0.1834611522668263, 0.4834746460178909]
actor:  0 policy actor:  0  step number:  65 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.028646666666666806 0.688 0.688
probs:  [0.09679924529629165, 0.23626572841018356, 0.18346352896325924, 0.48347149733026545]
siam score:  -0.7472584
maxi score, test score, baseline:  -0.028646666666666806 0.688 0.688
probs:  [0.09679924529629165, 0.23626572841018356, 0.18346352896325924, 0.48347149733026545]
maxi score, test score, baseline:  -0.028646666666666806 0.688 0.688
actor:  1 policy actor:  1  step number:  44 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  38 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09679924529629165, 0.23626572841018362, 0.18346352896325924, 0.48347149733026545]
maxi score, test score, baseline:  -0.028646666666666806 0.688 0.688
probs:  [0.09679924529629165, 0.23626572841018356, 0.18346352896325924, 0.48347149733026545]
maxi score, test score, baseline:  -0.028646666666666806 0.688 0.688
probs:  [0.09679924529629165, 0.23626572841018356, 0.18346352896325924, 0.48347149733026545]
maxi score, test score, baseline:  -0.028646666666666806 0.688 0.688
probs:  [0.09679924529629165, 0.23626572841018356, 0.18346352896325924, 0.48347149733026545]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.028646666666666806 0.688 0.688
siam score:  -0.74862075
maxi score, test score, baseline:  -0.028646666666666806 0.688 0.688
probs:  [0.09679924529629165, 0.23626572841018356, 0.18346352896325924, 0.48347149733026545]
actor:  1 policy actor:  1  step number:  42 total reward:  0.7266666666666669  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.028646666666666806 0.688 0.688
probs:  [0.09661544249452317, 0.23581669532737012, 0.1850155213187455, 0.4825523408593612]
line 256 mcts: sample exp_bonus 0.6204630617514746
actor:  0 policy actor:  1  step number:  55 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7478597
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.028766666666666798 0.688 0.688
maxi score, test score, baseline:  -0.028766666666666798 0.688 0.688
probs:  [0.09661544249452317, 0.23581669532737012, 0.1850155213187455, 0.4825523408593612]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.028766666666666798 0.688 0.688
maxi score, test score, baseline:  -0.028766666666666798 0.688 0.688
actor:  1 policy actor:  1  step number:  44 total reward:  0.566666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.028766666666666798 0.688 0.688
maxi score, test score, baseline:  -0.028766666666666798 0.688 0.688
probs:  [0.09657270657483612, 0.2361550426715133, 0.1849336226165247, 0.48233862813712586]
maxi score, test score, baseline:  -0.028766666666666798 0.688 0.688
probs:  [0.09657270657483612, 0.2361550426715133, 0.1849336226165247, 0.48233862813712586]
maxi score, test score, baseline:  -0.028766666666666798 0.688 0.688
probs:  [0.09657270657483612, 0.2361550426715133, 0.1849336226165247, 0.48233862813712586]
maxi score, test score, baseline:  -0.03202000000000013 0.688 0.688
probs:  [0.09657270657483612, 0.2361550426715133, 0.1849336226165247, 0.48233862813712586]
actor:  1 policy actor:  1  step number:  34 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  11715
UNIT TEST: sample policy line 217 mcts : [0.143 0.102 0.02  0.041 0.469 0.204 0.02 ]
maxi score, test score, baseline:  -0.03202000000000013 0.688 0.688
probs:  [0.09657208329106234, 0.23615643825071722, 0.18493595440386681, 0.48233552405435354]
maxi score, test score, baseline:  -0.03202000000000013 0.688 0.688
probs:  [0.09657208329106234, 0.23615643825071722, 0.18493595440386681, 0.48233552405435354]
maxi score, test score, baseline:  -0.03202000000000013 0.688 0.688
probs:  [0.09657208329106234, 0.23615643825071722, 0.18493595440386681, 0.48233552405435354]
maxi score, test score, baseline:  -0.03202000000000013 0.688 0.688
actor:  1 policy actor:  1  step number:  38 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[2.318]
 [2.318]
 [2.318]
 [2.318]
 [2.318]
 [2.318]
 [1.398]] [[0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.163]]
maxi score, test score, baseline:  -0.03202000000000013 0.688 0.688
maxi score, test score, baseline:  -0.03202000000000013 0.688 0.688
probs:  [0.09657208329106234, 0.23615643825071722, 0.18493595440386681, 0.48233552405435354]
maxi score, test score, baseline:  -0.03530000000000012 0.688 0.688
probs:  [0.09657208329106234, 0.23615643825071722, 0.18493595440386681, 0.48233552405435354]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.333
from probs:  [0.09657208329106234, 0.23615643825071722, 0.18493595440386681, 0.48233552405435354]
maxi score, test score, baseline:  -0.03530000000000012 0.688 0.688
probs:  [0.09657208329106234, 0.23615643825071722, 0.18493595440386681, 0.48233552405435354]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.788]
 [0.7  ]
 [0.713]
 [0.535]
 [0.661]
 [0.775]] [[2.568]
 [1.216]
 [3.108]
 [2.477]
 [2.357]
 [2.857]
 [2.834]] [[0.668]
 [0.788]
 [0.7  ]
 [0.713]
 [0.535]
 [0.661]
 [0.775]]
siam score:  -0.7416075
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.786]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]] [[-0.025]
 [-0.102]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]] [[0.728]
 [0.786]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]]
maxi score, test score, baseline:  -0.03530000000000012 0.688 0.688
probs:  [0.09657208329106234, 0.23615643825071722, 0.18493595440386681, 0.48233552405435354]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.03560666666666679 0.688 0.688
actor:  0 policy actor:  1  step number:  49 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.03571333333333346 0.688 0.688
probs:  [0.09657208329106234, 0.23615643825071722, 0.18493595440386684, 0.48233552405435354]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03571333333333346 0.688 0.688
probs:  [0.09657208329106234, 0.23615643825071722, 0.18493595440386684, 0.48233552405435354]
maxi score, test score, baseline:  -0.03571333333333346 0.688 0.688
probs:  [0.09657208329106234, 0.23615643825071722, 0.18493595440386684, 0.48233552405435354]
actor:  1 policy actor:  1  step number:  30 total reward:  0.8066666666666668  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  1 policy actor:  1  step number:  52 total reward:  0.6333333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.03571333333333346 0.688 0.688
probs:  [0.09487615603791817, 0.24958343704577388, 0.18168583383416007, 0.473854573082148]
maxi score, test score, baseline:  -0.03571333333333346 0.688 0.688
probs:  [0.09487615603791817, 0.24958343704577388, 0.18168583383416007, 0.473854573082148]
siam score:  -0.74266744
maxi score, test score, baseline:  -0.03571333333333346 0.688 0.688
probs:  [0.09487615603791817, 0.24958343704577388, 0.18168583383416007, 0.473854573082148]
maxi score, test score, baseline:  -0.03571333333333346 0.688 0.688
probs:  [0.09487615603791817, 0.24958343704577388, 0.18168583383416007, 0.473854573082148]
from probs:  [0.09487615603791817, 0.24958343704577388, 0.18168583383416007, 0.473854573082148]
maxi score, test score, baseline:  -0.042220000000000125 0.688 0.688
maxi score, test score, baseline:  -0.042220000000000125 0.688 0.688
probs:  [0.09487615603791817, 0.24958343704577388, 0.18168583383416007, 0.473854573082148]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.042220000000000125 0.688 0.688
probs:  [0.09487615603791817, 0.24958343704577388, 0.18168583383416007, 0.473854573082148]
siam score:  -0.7512621
maxi score, test score, baseline:  -0.0487666666666668 0.688 0.688
probs:  [0.09487615603791817, 0.24958343704577388, 0.18168583383416007, 0.473854573082148]
maxi score, test score, baseline:  -0.0487666666666668 0.688 0.688
probs:  [0.09487615603791817, 0.24958343704577388, 0.18168583383416007, 0.473854573082148]
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[1.457]
 [1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]] [[0.258]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]]
maxi score, test score, baseline:  -0.0487666666666668 0.688 0.688
probs:  [0.09487615603791816, 0.24958343704577382, 0.1816858338341601, 0.4738545730821479]
actor:  1 policy actor:  1  step number:  33 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  0.667
Starting evaluation
maxi score, test score, baseline:  -0.0487666666666668 0.688 0.688
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]] [[1.144]
 [1.144]
 [1.144]
 [1.144]
 [1.144]
 [1.144]
 [1.144]] [[0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.0487666666666668 0.688 0.688
probs:  [0.0943175176091378, 0.25189023673269134, 0.18273505612110238, 0.4710571895370685]
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.742]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.689]] [[1.537]
 [0.865]
 [1.537]
 [1.537]
 [1.537]
 [1.537]
 [1.264]] [[0.677]
 [0.742]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.689]]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 3.460005126459107
Printing some Q and Qe and total Qs values:  [[0.911]
 [0.968]
 [0.944]
 [0.889]
 [0.902]
 [0.863]
 [0.956]] [[1.088]
 [0.591]
 [0.499]
 [0.528]
 [0.509]
 [0.412]
 [0.675]] [[0.911]
 [0.968]
 [0.944]
 [0.889]
 [0.902]
 [0.863]
 [0.956]]
maxi score, test score, baseline:  -0.0487666666666668 0.688 0.688
probs:  [0.0943175176091378, 0.25189023673269134, 0.18273505612110238, 0.4710571895370685]
line 256 mcts: sample exp_bonus 2.2323334270889794
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0068733333333334465 0.688 0.688
probs:  [0.0943175176091378, 0.25189023673269134, 0.18273505612110238, 0.4710571895370685]
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  34 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0032333333333334495 0.6893333333333335 0.6893333333333335
probs:  [0.09438585519863042, 0.2520043420529505, 0.18221573345313594, 0.471394069295283]
maxi score, test score, baseline:  -0.0032333333333334495 0.6893333333333335 0.6893333333333335
probs:  [0.09438585519863042, 0.2520043420529505, 0.1822157334531359, 0.471394069295283]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0032333333333334495 0.6893333333333335 0.6893333333333335
probs:  [0.09438585519863042, 0.2520043420529505, 0.18221573345313594, 0.471394069295283]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using another actor
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.025]
 [-0.024]
 [-0.025]
 [-0.025]
 [-0.024]
 [-0.023]] [[-1.745]
 [-0.291]
 [-1.604]
 [-1.685]
 [-1.761]
 [-1.788]
 [-1.149]] [[-0.011]
 [ 0.461]
 [ 0.035]
 [ 0.009]
 [-0.016]
 [-0.024]
 [ 0.184]]
maxi score, test score, baseline:  -0.0032333333333334495 0.6893333333333335 0.6893333333333335
probs:  [0.09438530947502788, 0.2520053766785519, 0.18221796145184926, 0.47139135239457086]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2475585136526934
first move QE:  -0.08047228633374978
siam score:  -0.7484343
maxi score, test score, baseline:  -0.006300000000000115 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.006300000000000115 0.6893333333333335 0.6893333333333335
probs:  [0.09438530947502788, 0.2520053766785519, 0.18221796145184926, 0.47139135239457086]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  46 total reward:  0.6200000000000002  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.032]
 [-0.029]
 [-0.032]
 [-0.032]
 [-0.028]
 [-0.03 ]] [[-0.811]
 [-0.196]
 [-0.786]
 [-0.652]
 [-0.667]
 [-0.835]
 [-0.695]] [[0.038]
 [0.223]
 [0.049]
 [0.086]
 [0.082]
 [0.035]
 [0.076]]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.228]
 [0.05 ]
 [0.122]
 [0.122]
 [0.122]
 [0.168]] [[2.244]
 [1.915]
 [2.036]
 [2.375]
 [2.375]
 [2.375]
 [3.452]] [[-0.132]
 [-0.1  ]
 [-0.229]
 [-0.055]
 [-0.055]
 [-0.055]
 [ 0.323]]
using explorer policy with actor:  1
siam score:  -0.7465603
maxi score, test score, baseline:  -0.006300000000000115 0.6893333333333335 0.6893333333333335
probs:  [0.09337699726992368, 0.2493105407413145, 0.19096336017923743, 0.4663491018095244]
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[1.905]
 [1.905]
 [1.905]
 [1.905]
 [1.905]
 [1.905]
 [1.905]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  -0.006300000000000115 0.6893333333333335 0.6893333333333335
probs:  [0.09337699726992368, 0.2493105407413145, 0.19096336017923743, 0.4663491018095244]
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.123]
 [0.123]
 [0.123]
 [0.123]
 [0.123]
 [0.185]] [[1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.417]] [[-0.122]
 [-0.122]
 [-0.122]
 [-0.122]
 [-0.122]
 [-0.122]
 [ 0.064]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.006300000000000115 0.6893333333333335 0.6893333333333335
probs:  [0.09314266068489135, 0.2511963511996511, 0.1904837294880306, 0.465177258627427]
using another actor
maxi score, test score, baseline:  -0.006300000000000115 0.6893333333333335 0.6893333333333335
probs:  [0.09314266068489135, 0.2511963511996511, 0.1904837294880306, 0.465177258627427]
maxi score, test score, baseline:  -0.006300000000000115 0.6893333333333335 0.6893333333333335
probs:  [0.09314266068489135, 0.2511963511996511, 0.1904837294880306, 0.465177258627427]
actor:  1 policy actor:  1  step number:  36 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.006300000000000115 0.6893333333333335 0.6893333333333335
probs:  [0.09314266068489135, 0.2511963511996511, 0.1904837294880306, 0.465177258627427]
maxi score, test score, baseline:  -0.006300000000000115 0.6893333333333335 0.6893333333333335
probs:  [0.09314266068489135, 0.2511963511996511, 0.1904837294880306, 0.465177258627427]
actor:  0 policy actor:  1  step number:  38 total reward:  0.54  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
probs:  [0.09314266068489135, 0.2511963511996511, 0.1904837294880306, 0.465177258627427]
start point for exploration sampling:  11715
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]] [[1.623]
 [1.419]
 [1.419]
 [1.419]
 [1.419]
 [1.419]
 [1.419]] [[0.344]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]]
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
probs:  [0.09314266068489134, 0.2511963511996511, 0.19048372948803063, 0.46517725862742687]
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
probs:  [0.09314266068489134, 0.2511963511996511, 0.19048372948803063, 0.46517725862742687]
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
probs:  [0.09314266068489134, 0.2511963511996511, 0.19048372948803063, 0.46517725862742687]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
probs:  [0.09314266068489134, 0.2511963511996511, 0.19048372948803063, 0.46517725862742687]
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
probs:  [0.09314266068489134, 0.2511963511996511, 0.19048372948803063, 0.46517725862742687]
actor:  1 policy actor:  1  step number:  47 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
probs:  [0.09314072892705297, 0.25121189691196105, 0.19047977564383195, 0.46516759851715406]
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
probs:  [0.09314072892705297, 0.25121189691196105, 0.19047977564383195, 0.46516759851715406]
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
probs:  [0.09314072892705297, 0.25121189691196105, 0.19047977564383195, 0.46516759851715406]
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
probs:  [0.09314072892705297, 0.25121189691196105, 0.19047977564383195, 0.46516759851715406]
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
probs:  [0.09314072892705297, 0.25121189691196105, 0.19047977564383195, 0.46516759851715406]
siam score:  -0.7444262
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]] [[0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]] [[-0.168]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
probs:  [0.09314072892705297, 0.25121189691196105, 0.19047977564383195, 0.46516759851715406]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
probs:  [0.09314072892705297, 0.25121189691196105, 0.19047977564383195, 0.46516759851715406]
maxi score, test score, baseline:  -0.006166666666666788 0.6893333333333335 0.6893333333333335
probs:  [0.09314072892705297, 0.25121189691196105, 0.19047977564383195, 0.46516759851715406]
actor:  0 policy actor:  1  step number:  36 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.767]
 [0.615]
 [0.604]
 [0.681]
 [0.419]
 [0.633]] [[0.569]
 [0.901]
 [0.716]
 [0.733]
 [2.464]
 [1.129]
 [0.786]] [[0.104]
 [0.23 ]
 [0.119]
 [0.118]
 [0.617]
 [0.15 ]
 [0.145]]
maxi score, test score, baseline:  -0.002793333333333452 0.6893333333333335 0.6893333333333335
probs:  [0.09314072892705297, 0.25121189691196105, 0.19047977564383195, 0.46516759851715406]
actor:  1 policy actor:  1  step number:  36 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  -0.002793333333333452 0.6893333333333335 0.6893333333333335
probs:  [0.09314072892705297, 0.25121189691196105, 0.19047977564383195, 0.46516759851715406]
maxi score, test score, baseline:  -0.002793333333333452 0.6893333333333335 0.6893333333333335
probs:  [0.09314072892705297, 0.25121189691196105, 0.19047977564383195, 0.46516759851715406]
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.056]
 [0.082]
 [0.006]
 [0.056]
 [0.056]
 [0.056]] [[1.804]
 [1.804]
 [1.967]
 [1.793]
 [1.804]
 [1.804]
 [1.804]] [[-0.155]
 [-0.155]
 [-0.074]
 [-0.209]
 [-0.155]
 [-0.155]
 [-0.155]]
maxi score, test score, baseline:  -0.002793333333333452 0.6893333333333335 0.6893333333333335
probs:  [0.09314072892705297, 0.25121189691196105, 0.19047977564383195, 0.46516759851715406]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.002793333333333452 0.6893333333333335 0.6893333333333335
probs:  [0.09314072892705297, 0.25121189691196105, 0.19047977564383195, 0.46516759851715406]
actor:  1 policy actor:  1  step number:  29 total reward:  0.8133333333333335  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.023]
 [ 0.211]
 [-0.023]
 [-0.023]
 [ 0.109]
 [-0.023]] [[2.854]
 [2.854]
 [2.51 ]
 [2.854]
 [2.854]
 [1.945]
 [2.854]] [[0.757]
 [0.757]
 [0.759]
 [0.757]
 [0.757]
 [0.603]
 [0.757]]
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.048]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]] [[2.438]
 [2.88 ]
 [2.438]
 [2.438]
 [2.438]
 [2.438]
 [2.438]] [[0.434]
 [0.524]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]]
maxi score, test score, baseline:  -0.00519333333333345 0.6893333333333335 0.6893333333333335
probs:  [0.09120065400739095, 0.2663599295286764, 0.18697428227763302, 0.4554651341862996]
line 256 mcts: sample exp_bonus 4.92101838323468
maxi score, test score, baseline:  -0.00519333333333345 0.6893333333333335 0.6893333333333335
probs:  [0.09120065400739095, 0.2663599295286764, 0.18697428227763302, 0.4554651341862996]
maxi score, test score, baseline:  -0.00519333333333345 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.00519333333333345 0.6893333333333335 0.6893333333333335
probs:  [0.09120065400739095, 0.2663599295286764, 0.18697428227763302, 0.4554651341862996]
maxi score, test score, baseline:  -0.00519333333333345 0.6893333333333335 0.6893333333333335
probs:  [0.09120065400739095, 0.2663599295286764, 0.18697428227763302, 0.4554651341862996]
line 256 mcts: sample exp_bonus 1.0136952572292057
maxi score, test score, baseline:  -0.00519333333333345 0.6893333333333335 0.6893333333333335
siam score:  -0.7384976
maxi score, test score, baseline:  -0.00519333333333345 0.6893333333333335 0.6893333333333335
probs:  [0.09120065400739095, 0.2663599295286764, 0.18697428227763302, 0.4554651341862996]
actor:  0 policy actor:  0  step number:  40 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.004873333333333457 0.6893333333333335 0.6893333333333335
probs:  [0.09120065400739095, 0.2663599295286764, 0.18697428227763302, 0.4554651341862996]
maxi score, test score, baseline:  -0.004873333333333457 0.6893333333333335 0.6893333333333335
probs:  [0.09120065400739095, 0.2663599295286764, 0.18697428227763302, 0.4554651341862996]
actor:  1 policy actor:  1  step number:  62 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.004873333333333457 0.6893333333333335 0.6893333333333335
probs:  [0.09117466860022584, 0.2664684598091439, 0.18702184738856348, 0.45533502420206673]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.004873333333333457 0.6893333333333335 0.6893333333333335
probs:  [0.09117466860022584, 0.2664684598091439, 0.18702184738856348, 0.45533502420206673]
actor:  1 policy actor:  1  step number:  40 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[ 0.099]
 [ 0.225]
 [ 0.099]
 [ 0.115]
 [-0.105]
 [ 0.099]
 [ 0.168]] [[1.903]
 [2.086]
 [1.903]
 [2.464]
 [1.376]
 [1.903]
 [2.882]] [[0.448]
 [0.533]
 [0.448]
 [0.582]
 [0.256]
 [0.448]
 [0.697]]
maxi score, test score, baseline:  -0.004873333333333457 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.004873333333333457 0.6893333333333335 0.6893333333333335
probs:  [0.09117466860022584, 0.2664684598091439, 0.18702184738856348, 0.45533502420206673]
maxi score, test score, baseline:  -0.004873333333333457 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.004873333333333457 0.6893333333333335 0.6893333333333335
probs:  [0.09117466860022584, 0.2664684598091439, 0.18702184738856348, 0.45533502420206673]
maxi score, test score, baseline:  -0.004873333333333457 0.6893333333333335 0.6893333333333335
probs:  [0.09117466860022584, 0.2664684598091439, 0.18702184738856348, 0.45533502420206673]
actor:  1 policy actor:  1  step number:  50 total reward:  0.6733333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.004873333333333457 0.6893333333333335 0.6893333333333335
probs:  [0.09117466860022584, 0.2664684598091439, 0.18702184738856348, 0.45533502420206673]
maxi score, test score, baseline:  -0.004873333333333457 0.6893333333333335 0.6893333333333335
UNIT TEST: sample policy line 217 mcts : [0.082 0.592 0.061 0.041 0.102 0.041 0.082]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.046]
 [ 0.049]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]] [[1.474]
 [1.474]
 [1.033]
 [1.474]
 [1.474]
 [1.474]
 [1.474]] [[0.413]
 [0.413]
 [0.327]
 [0.413]
 [0.413]
 [0.413]
 [0.413]]
maxi score, test score, baseline:  -0.004873333333333457 0.6893333333333335 0.6893333333333335
probs:  [0.09117466860022584, 0.2664684598091439, 0.18702184738856348, 0.45533502420206673]
actor:  0 policy actor:  0  step number:  56 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.165]
 [0.33 ]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.221]] [[0.639]
 [1.007]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.986]] [[-0.459]
 [-0.232]
 [-0.39 ]
 [-0.39 ]
 [-0.39 ]
 [-0.39 ]
 [-0.344]]
maxi score, test score, baseline:  -0.002300000000000123 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.002300000000000123 0.6893333333333335 0.6893333333333335
probs:  [0.09117466860022584, 0.2664684598091439, 0.18702184738856348, 0.45533502420206673]
maxi score, test score, baseline:  -0.002300000000000123 0.6893333333333335 0.6893333333333335
probs:  [0.09117466860022584, 0.2664684598091439, 0.18702184738856348, 0.45533502420206673]
maxi score, test score, baseline:  -0.002300000000000123 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.002300000000000123 0.6893333333333335 0.6893333333333335
probs:  [0.09117419417797619, 0.26646922080935603, 0.18702392154440703, 0.4553326634682608]
Printing some Q and Qe and total Qs values:  [[0.862]
 [0.862]
 [0.875]
 [0.893]
 [0.88 ]
 [0.893]
 [0.804]] [[0.842]
 [0.885]
 [0.655]
 [0.68 ]
 [0.707]
 [0.68 ]
 [0.542]] [[0.862]
 [0.862]
 [0.875]
 [0.893]
 [0.88 ]
 [0.893]
 [0.804]]
actor:  0 policy actor:  1  step number:  34 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  73 total reward:  0.48000000000000054  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0008333333333332144 0.6893333333333335 0.6893333333333335
probs:  [0.09117419417797619, 0.26646922080935603, 0.18702392154440703, 0.4553326634682608]
maxi score, test score, baseline:  0.0008333333333332144 0.6893333333333335 0.6893333333333335
probs:  [0.09117419417797619, 0.26646922080935603, 0.18702392154440703, 0.4553326634682608]
actor:  1 policy actor:  1  step number:  36 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.633]
 [0.739]
 [0.73 ]
 [0.696]
 [0.719]
 [0.73 ]] [[ 1.108]
 [ 0.146]
 [-0.534]
 [ 0.   ]
 [ 0.752]
 [-0.803]
 [ 0.   ]] [[0.789]
 [0.633]
 [0.739]
 [0.73 ]
 [0.696]
 [0.719]
 [0.73 ]]
maxi score, test score, baseline:  0.0008333333333332144 0.6893333333333335 0.6893333333333335
probs:  [0.09088679270228411, 0.2676683429788352, 0.18755121676909398, 0.4538936475497867]
first move QE:  -0.08208227860562807
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  0 policy actor:  0  step number:  53 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0008999999999998764 0.6893333333333335 0.6893333333333335
probs:  [0.09088679270228411, 0.2676683429788352, 0.18755121676909398, 0.4538936475497867]
maxi score, test score, baseline:  0.0008999999999998764 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.0008999999999998764 0.6893333333333335 0.6893333333333335
probs:  [0.09088679270228411, 0.2676683429788352, 0.18755121676909398, 0.4538936475497867]
maxi score, test score, baseline:  0.0008999999999998764 0.6893333333333335 0.6893333333333335
probs:  [0.09088679270228411, 0.2676683429788352, 0.18755121676909398, 0.4538936475497867]
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]] [[2.137]
 [2.137]
 [2.137]
 [2.137]
 [2.137]
 [2.137]
 [2.137]] [[0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]]
line 256 mcts: sample exp_bonus 2.43895055713079
siam score:  -0.742264
maxi score, test score, baseline:  0.0008999999999998764 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  36 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  28 total reward:  0.8200000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0008999999999998764 0.6893333333333335 0.6893333333333335
probs:  [0.08893252793485368, 0.2619066082482121, 0.20503981781295064, 0.4441210460039836]
maxi score, test score, baseline:  -0.0017400000000001243 0.6893333333333335 0.6893333333333335
probs:  [0.08893252793485368, 0.2619066082482121, 0.20503981781295064, 0.4441210460039836]
maxi score, test score, baseline:  -0.0017400000000001243 0.6893333333333335 0.6893333333333335
probs:  [0.08893252793485368, 0.2619066082482121, 0.20503981781295064, 0.4441210460039836]
siam score:  -0.73598504
actor:  0 policy actor:  0  step number:  44 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08893252793485368, 0.2619066082482121, 0.20503981781295064, 0.4441210460039836]
maxi score, test score, baseline:  0.001179999999999881 0.6893333333333335 0.6893333333333335
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.388]] [[0.919]
 [1.386]
 [1.386]
 [1.386]
 [1.386]
 [1.386]
 [1.273]] [[0.232]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.31 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.001179999999999883 0.6893333333333335 0.6893333333333335
probs:  [0.08893252793485368, 0.2619066082482121, 0.20503981781295064, 0.4441210460039836]
maxi score, test score, baseline:  0.001179999999999883 0.6893333333333335 0.6893333333333335
probs:  [0.08893252793485368, 0.2619066082482121, 0.20503981781295064, 0.4441210460039836]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.9850006103515626e-08
siam score:  -0.73293185
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.001179999999999883 0.6893333333333335 0.6893333333333335
probs:  [0.08893252793485368, 0.2619066082482121, 0.20503981781295064, 0.4441210460039836]
actor:  1 policy actor:  1  step number:  44 total reward:  0.6600000000000003  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.001179999999999883 0.6893333333333335 0.6893333333333335
probs:  [0.08893252793485368, 0.2619066082482121, 0.20503981781295064, 0.4441210460039836]
maxi score, test score, baseline:  -0.001380000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08893252793485368, 0.2619066082482121, 0.20503981781295064, 0.4441210460039836]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.001380000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08893210083505547, 0.2619074250243013, 0.20504155247231756, 0.4441189216683256]
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.184]
 [0.222]
 [0.242]
 [0.228]
 [0.233]
 [0.231]] [[1.607]
 [1.94 ]
 [3.12 ]
 [1.636]
 [1.593]
 [1.6  ]
 [1.558]] [[-0.299]
 [-0.235]
 [ 0.196]
 [-0.278]
 [-0.306]
 [-0.299]
 [-0.315]]
maxi score, test score, baseline:  -0.001380000000000116 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.001380000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08893210083505547, 0.2619074250243013, 0.20504155247231756, 0.4441189216683256]
maxi score, test score, baseline:  -0.001380000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08893210083505547, 0.2619074250243013, 0.20504155247231756, 0.4441189216683256]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.965]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [1.006]] [[1.552]
 [1.716]
 [1.716]
 [1.716]
 [1.716]
 [1.716]
 [2.526]] [[0.965]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [1.006]]
siam score:  -0.7397595
actor:  0 policy actor:  1  step number:  43 total reward:  0.6000000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0007000000000001144 0.6893333333333335 0.6893333333333335
probs:  [0.08882690294824357, 0.26278145694234295, 0.20479877666764096, 0.4435928634417725]
maxi score, test score, baseline:  -0.0030466666666667786 0.6893333333333335 0.6893333333333335
probs:  [0.08882648025431998, 0.2627822545012022, 0.20480050417894613, 0.44359076106553175]
actor:  1 policy actor:  1  step number:  27 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0030466666666667786 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.0030466666666667786 0.6893333333333335 0.6893333333333335
probs:  [0.0892763444539078, 0.26107099482192037, 0.2038095755476136, 0.4458430851765582]
maxi score, test score, baseline:  -0.0030466666666667786 0.6893333333333335 0.6893333333333335
probs:  [0.0892763444539078, 0.26107099482192037, 0.2038095755476136, 0.4458430851765582]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  30 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.0030466666666667786 0.6893333333333335 0.6893333333333335
probs:  [0.0883739548197179, 0.25842921425680054, 0.2118663115638734, 0.4413305193596082]
maxi score, test score, baseline:  -0.0030466666666667786 0.6893333333333335 0.6893333333333335
probs:  [0.0883739548197179, 0.25842921425680054, 0.2118663115638734, 0.4413305193596082]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.239]
 [0.345]
 [0.239]
 [0.239]
 [0.239]
 [0.173]] [[2.513]
 [2.513]
 [2.867]
 [2.513]
 [2.513]
 [2.513]
 [2.501]] [[0.146]
 [0.146]
 [0.378]
 [0.146]
 [0.146]
 [0.146]
 [0.087]]
actor:  0 policy actor:  1  step number:  41 total reward:  0.6800000000000004  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.002126666666666777 0.6893333333333335 0.6893333333333335
probs:  [0.0883739548197179, 0.25842921425680054, 0.2118663115638734, 0.4413305193596082]
using another actor
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.0883739548197179, 0.25842921425680054, 0.2118663115638734, 0.4413305193596082]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.0883739548197179, 0.25842921425680054, 0.2118663115638734, 0.4413305193596082]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.0883739548197179, 0.25842921425680054, 0.2118663115638734, 0.4413305193596082]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.0883739548197179, 0.25842921425680054, 0.2118663115638734, 0.4413305193596082]
first move QE:  -0.08565549486994259
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.0883739548197179, 0.25842921425680054, 0.2118663115638734, 0.4413305193596082]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.0883739548197179, 0.25842921425680054, 0.2118663115638734, 0.4413305193596082]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.0883739548197179, 0.25842921425680054, 0.2118663115638734, 0.4413305193596082]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.0883739548197179, 0.25842921425680054, 0.2118663115638734, 0.4413305193596082]
actor:  1 policy actor:  1  step number:  28 total reward:  0.8066666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08952747256910808, 0.25423240869876523, 0.20913448029927667, 0.44710563843285006]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08952747256910808, 0.25423240869876523, 0.20913448029927667, 0.44710563843285006]
actor:  1 policy actor:  1  step number:  32 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.043]
 [ 0.02 ]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[1.406]
 [1.847]
 [1.406]
 [1.406]
 [1.406]
 [1.406]
 [1.406]] [[-0.141]
 [ 0.125]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.141]]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08952747256910808, 0.25423240869876523, 0.20913448029927667, 0.44710563843285006]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08936274175899571, 0.2548317433685825, 0.2095246058760571, 0.4462809089963647]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08936274175899571, 0.2548317433685825, 0.2095246058760571, 0.4462809089963647]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08936233362770507, 0.25483261876034036, 0.2095261689086323, 0.44627887870332233]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08936233362770507, 0.25483261876034036, 0.2095261689086323, 0.44627887870332233]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.295]
 [0.371]
 [0.295]
 [0.295]
 [0.321]
 [0.295]] [[2.145]
 [2.145]
 [2.792]
 [2.145]
 [2.145]
 [2.529]
 [2.145]] [[0.531]
 [0.531]
 [0.728]
 [0.531]
 [0.531]
 [0.641]
 [0.531]]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08936233362770507, 0.25483261876034036, 0.2095261689086323, 0.44627887870332233]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08936233362770507, 0.25483261876034036, 0.2095261689086323, 0.44627887870332233]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08936233362770507, 0.25483261876034036, 0.2095261689086323, 0.44627887870332233]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08936233362770507, 0.25483261876034036, 0.2095261689086323, 0.44627887870332233]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08936233362770507, 0.25483261876034036, 0.2095261689086323, 0.44627887870332233]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08936233362770507, 0.25483261876034036, 0.2095261689086323, 0.44627887870332233]
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08936233362770507, 0.25483261876034036, 0.2095261689086323, 0.44627887870332233]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.004153333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08892655000464825, 0.25847040532795645, 0.20850341063113353, 0.4440996340362618]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
probs:  [0.08892655000464825, 0.25847040532795645, 0.20850341063113353, 0.4440996340362618]
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.777]
 [0.777]
 [0.752]
 [0.752]
 [0.777]
 [0.777]] [[0.006]
 [0.018]
 [0.153]
 [0.207]
 [0.207]
 [0.048]
 [0.014]] [[0.096]
 [0.103]
 [0.17 ]
 [0.172]
 [0.172]
 [0.118]
 [0.101]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.40666666666666684  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
probs:  [0.08892655000464825, 0.25847040532795645, 0.2085034106311335, 0.4440996340362618]
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
probs:  [0.08892655000464825, 0.25847040532795645, 0.2085034106311335, 0.4440996340362618]
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
probs:  [0.08892655000464825, 0.25847040532795645, 0.2085034106311335, 0.4440996340362618]
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  50 total reward:  0.566666666666667  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.074]
 [0.068]
 [0.074]
 [0.074]
 [0.074]
 [0.074]] [[1.376]
 [1.376]
 [1.085]
 [1.371]
 [1.376]
 [1.376]
 [1.376]] [[0.433]
 [0.433]
 [0.374]
 [0.432]
 [0.433]
 [0.433]
 [0.433]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6066666666666668  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7405121
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
probs:  [0.08840415954775034, 0.2604006814345858, 0.2097108508854157, 0.44148430813224826]
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[0.745]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]] [[0.416]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]]
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
probs:  [0.08840415954775034, 0.2604006814345858, 0.2097108508854157, 0.44148430813224826]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.56 ]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]] [[1.345]
 [0.985]
 [1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.345]] [[0.478]
 [0.355]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
probs:  [0.08843217931694673, 0.26029714606122867, 0.2096460866907473, 0.44162458793107734]
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.049]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]] [[-1.104]
 [-0.388]
 [-1.104]
 [-1.104]
 [-1.104]
 [-1.104]
 [-1.104]] [[0.135]
 [0.339]
 [0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]]
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
probs:  [0.08843217931694673, 0.26029714606122867, 0.2096460866907473, 0.44162458793107734]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  40 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
probs:  [0.08843178391022595, 0.260297941316291, 0.20964765342236064, 0.44162262135112235]
start point for exploration sampling:  11715
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
probs:  [0.08842171436759061, 0.2603351489291424, 0.20967092809127272, 0.44157220861199425]
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  47 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
probs:  [0.0884200065662803, 0.2603494505802974, 0.20966687447220453, 0.44156366838121786]
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
probs:  [0.08841961327196626, 0.26035024116511696, 0.20966843324974394, 0.4415617123131729]
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
probs:  [0.08841961327196626, 0.26035024116511696, 0.20966843324974394, 0.4415617123131729]
maxi score, test score, baseline:  -0.006900000000000112 0.6893333333333335 0.6893333333333335
probs:  [0.08841961327196626, 0.26035024116511696, 0.20966843324974394, 0.4415617123131729]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6600000000000004  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.187]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]] [[1.231]
 [1.704]
 [1.231]
 [1.231]
 [1.231]
 [1.231]
 [1.231]] [[0.227]
 [0.326]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08771617480771683, 0.25941366700878804, 0.2148271249969417, 0.4380430331865534]
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08771617480771683, 0.25941366700878804, 0.2148271249969417, 0.4380430331865534]
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08771617480771683, 0.25941366700878804, 0.2148271249969417, 0.4380430331865534]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.554]
 [0.475]
 [0.496]
 [0.479]
 [0.475]
 [0.455]] [[0.396]
 [0.321]
 [0.35 ]
 [0.487]
 [0.277]
 [0.296]
 [0.402]] [[ 0.035]
 [ 0.074]
 [ 0.01 ]
 [ 0.099]
 [-0.022]
 [-0.017]
 [ 0.016]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.5933333333333337  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  31 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08672575479502029, 0.2677849970596112, 0.21239901400525552, 0.43309023414011305]
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08672575479502029, 0.2677849970596112, 0.21239901400525552, 0.43309023414011305]
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08672575479502029, 0.2677849970596112, 0.21239901400525552, 0.43309023414011305]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08672575479502029, 0.2677849970596112, 0.21239901400525554, 0.43309023414011305]
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08672575479502029, 0.2677849970596112, 0.21239901400525554, 0.43309023414011305]
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08672575479502029, 0.2677849970596112, 0.21239901400525554, 0.43309023414011305]
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08672575479502029, 0.2677849970596112, 0.21239901400525554, 0.43309023414011305]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08672575479502029, 0.2677849970596112, 0.21239901400525554, 0.43309023414011305]
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.009340000000000116 0.6893333333333335 0.6893333333333335
probs:  [0.08672575479502029, 0.2677849970596112, 0.21239901400525554, 0.43309023414011305]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.007]
 [ 0.152]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[0.878]
 [1.158]
 [0.936]
 [0.878]
 [0.878]
 [0.878]
 [0.878]] [[-0.209]
 [ 0.03 ]
 [-0.191]
 [-0.209]
 [-0.209]
 [-0.209]
 [-0.209]]
siam score:  -0.7283661
maxi score, test score, baseline:  -0.009300000000000114 0.6893333333333335 0.6893333333333335
probs:  [0.08672575479502029, 0.2677849970596112, 0.21239901400525554, 0.43309023414011305]
maxi score, test score, baseline:  -0.009300000000000114 0.6893333333333335 0.6893333333333335
probs:  [0.08672575479502029, 0.2677849970596112, 0.21239901400525554, 0.43309023414011305]
actor:  1 policy actor:  1  step number:  39 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.009300000000000114 0.6893333333333335 0.6893333333333335
probs:  [0.08662865866269054, 0.2686056829908027, 0.21216097339305284, 0.43260468495345383]
actor:  1 policy actor:  1  step number:  40 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.333
from probs:  [0.08662865866269054, 0.2686056829908027, 0.21216097339305284, 0.43260468495345383]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  27 total reward:  0.8  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.007726666666666778 0.6893333333333335 0.6893333333333335
probs:  [0.08656929704704885, 0.26910742535315035, 0.21201544261865804, 0.4323078349811428]
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.167]
 [0.086]
 [0.162]
 [0.162]
 [0.162]
 [0.092]] [[1.293]
 [1.251]
 [0.632]
 [1.293]
 [1.293]
 [1.293]
 [0.611]] [[-0.433]
 [-0.442]
 [-0.729]
 [-0.433]
 [-0.433]
 [-0.433]
 [-0.73 ]]
actor:  0 policy actor:  1  step number:  57 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.005273333333333445 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  38 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.005273333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08656929704704885, 0.26910742535315035, 0.21201544261865804, 0.4323078349811428]
maxi score, test score, baseline:  -0.005273333333333445 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.005273333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08656929704704885, 0.26910742535315035, 0.212015442618658, 0.4323078349811428]
maxi score, test score, baseline:  -0.005273333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08656929704704885, 0.26910742535315035, 0.212015442618658, 0.4323078349811428]
maxi score, test score, baseline:  -0.005273333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08656929704704885, 0.26910742535315035, 0.212015442618658, 0.4323078349811428]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [ 0.014]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.835]
 [1.012]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]] [[0.18]
 [0.27]
 [0.18]
 [0.18]
 [0.18]
 [0.18]
 [0.18]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.005273333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08656929704704885, 0.26910742535315035, 0.212015442618658, 0.4323078349811428]
maxi score, test score, baseline:  -0.005273333333333445 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.005273333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08656929704704885, 0.26910742535315035, 0.212015442618658, 0.4323078349811428]
maxi score, test score, baseline:  -0.005273333333333441 0.6893333333333335 0.6893333333333335
probs:  [0.08656929704704885, 0.26910742535315035, 0.212015442618658, 0.4323078349811428]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5866666666666671  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08656929704704885, 0.26910742535315035, 0.212015442618658, 0.4323078349811428]
actor:  1 policy actor:  1  step number:  29 total reward:  0.8133333333333335  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.005273333333333441 0.6893333333333335 0.6893333333333335
probs:  [0.08792883751274705, 0.2640157145133667, 0.20894147312182718, 0.43911397485205905]
maxi score, test score, baseline:  -0.005273333333333441 0.6893333333333335 0.6893333333333335
probs:  [0.08792883751274705, 0.2640157145133667, 0.20894147312182718, 0.43911397485205905]
maxi score, test score, baseline:  -0.005273333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08792883751274705, 0.2640157145133667, 0.20894147312182718, 0.43911397485205905]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]] [[-4.738]
 [-4.738]
 [-4.738]
 [-4.738]
 [-4.738]
 [-4.738]
 [-4.738]] [[0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]]
maxi score, test score, baseline:  -0.005273333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08792883751274705, 0.2640157145133667, 0.20894147312182718, 0.43911397485205905]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.005273333333333445 0.6893333333333335 0.6893333333333335
probs:  [0.08792883751274705, 0.2640157145133667, 0.20894147312182718, 0.43911397485205905]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6063658212672927
actor:  1 policy actor:  1  step number:  34 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.007766666666666778 0.6893333333333335 0.6893333333333335
probs:  [0.0875170867902284, 0.2674635857605067, 0.20796442188444605, 0.4370549055648188]
maxi score, test score, baseline:  -0.007766666666666778 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.007766666666666778 0.6893333333333335 0.6893333333333335
probs:  [0.0875170867902284, 0.2674635857605067, 0.20796442188444605, 0.4370549055648188]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.667
UNIT TEST: sample policy line 217 mcts : [0.02  0.51  0.02  0.143 0.184 0.02  0.102]
maxi score, test score, baseline:  -0.007766666666666778 0.6893333333333335 0.6893333333333335
probs:  [0.08842306853691127, 0.2640245116198168, 0.20596203694734067, 0.4415903828959313]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7266666666666669  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.0848856521389276
maxi score, test score, baseline:  -0.010086666666666775 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.010086666666666775 0.6893333333333335 0.6893333333333335
probs:  [0.08828590698885347, 0.2636144951497378, 0.20719513874651088, 0.44090445911489795]
actor:  1 policy actor:  1  step number:  36 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.86  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.010086666666666775 0.6893333333333335 0.6893333333333335
probs:  [0.090819868655813, 0.2540598364038825, 0.2015305026754978, 0.4535897922648067]
maxi score, test score, baseline:  -0.010086666666666775 0.6893333333333335 0.6893333333333335
probs:  [0.090819868655813, 0.2540598364038825, 0.2015305026754978, 0.4535897922648067]
maxi score, test score, baseline:  -0.010086666666666775 0.6893333333333335 0.6893333333333335
probs:  [0.090819868655813, 0.2540598364038825, 0.2015305026754978, 0.4535897922648067]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  0.8066666666666668  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]] [[2.525]
 [2.525]
 [2.525]
 [2.525]
 [2.525]
 [2.525]
 [2.525]] [[0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]]
maxi score, test score, baseline:  -0.010086666666666775 0.6893333333333335 0.6893333333333335
probs:  [0.09186327037435318, 0.25012493870900687, 0.19919858704936794, 0.458813203867272]
siam score:  -0.7382559
actor:  1 policy actor:  1  step number:  30 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.010086666666666775 0.6893333333333335 0.6893333333333335
probs:  [0.09240302473754804, 0.2480897288323766, 0.1979919640047117, 0.4615152824253637]
line 256 mcts: sample exp_bonus 2.1174770413181383
siam score:  -0.73727083
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.012460000000000105 0.6893333333333335 0.6893333333333335
probs:  [0.09240265897951228, 0.2480905160390374, 0.19799336340494467, 0.46151346157650563]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]] [[1.702]
 [1.702]
 [1.702]
 [1.702]
 [1.702]
 [1.702]
 [1.702]] [[0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]]
maxi score, test score, baseline:  -0.012460000000000105 0.6893333333333335 0.6893333333333335
probs:  [0.09240265897951228, 0.2480905160390374, 0.19799336340494467, 0.46151346157650563]
from probs:  [0.09240265897951228, 0.2480905160390374, 0.19799336340494467, 0.46151346157650563]
maxi score, test score, baseline:  -0.015180000000000103 0.6893333333333335 0.6893333333333335
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
first move QE:  -0.08403563983228884
actor:  0 policy actor:  0  step number:  53 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09213592933434049, 0.24909624796816518, 0.19858964335167195, 0.46017817934582245]
siam score:  -0.7221971
maxi score, test score, baseline:  -0.012886666666666772 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.012886666666666772 0.6893333333333335 0.6893333333333335
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.7284639
actor:  0 policy actor:  0  step number:  57 total reward:  0.22666666666666646  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.010433333333333438 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.010433333333333438 0.6893333333333335 0.6893333333333335
probs:  [0.09213592933434049, 0.24909624796816518, 0.19858964335167195, 0.46017817934582245]
actor:  1 policy actor:  1  step number:  44 total reward:  0.6466666666666668  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.010433333333333438 0.6893333333333335 0.6893333333333335
probs:  [0.09213556573235378, 0.2490970252434473, 0.19859103970278777, 0.46017636932141115]
maxi score, test score, baseline:  -0.010433333333333438 0.6893333333333335 0.6893333333333335
probs:  [0.09213556573235378, 0.2490970252434473, 0.19859103970278777, 0.46017636932141115]
Printing some Q and Qe and total Qs values:  [[ 0.443]
 [ 0.349]
 [ 0.349]
 [ 0.454]
 [ 0.445]
 [-0.012]
 [ 0.148]] [[1.299]
 [1.115]
 [1.115]
 [1.831]
 [2.051]
 [1.528]
 [2.022]] [[ 0.062]
 [-0.063]
 [-0.063]
 [ 0.162]
 [ 0.189]
 [-0.354]
 [-0.113]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.08503451661168561
maxi score, test score, baseline:  -0.012860000000000107 0.6893333333333335 0.6893333333333335
probs:  [0.09215221204820488, 0.24906144026227628, 0.1985271123796041, 0.46025923530991464]
maxi score, test score, baseline:  -0.012860000000000107 0.6893333333333335 0.6893333333333335
probs:  [0.09215221204820488, 0.24906144026227628, 0.1985271123796041, 0.46025923530991464]
actor:  1 policy actor:  1  step number:  30 total reward:  0.8066666666666668  reward:  1.0 rdn_beta:  0.333
using another actor
from probs:  [0.09215221204820488, 0.24906144026227628, 0.1985271123796041, 0.46025923530991464]
maxi score, test score, baseline:  -0.012860000000000107 0.6893333333333335 0.6893333333333335
probs:  [0.0909045975149915, 0.25923858529557936, 0.19583698098816157, 0.4540198362012677]
maxi score, test score, baseline:  -0.012860000000000107 0.6893333333333335 0.6893333333333335
probs:  [0.0909045975149915, 0.25923858529557936, 0.19583698098816157, 0.4540198362012677]
maxi score, test score, baseline:  -0.012860000000000107 0.6893333333333335 0.6893333333333335
probs:  [0.0909045975149915, 0.25923858529557936, 0.19583698098816157, 0.4540198362012677]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.01567333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.0909045975149915, 0.2592385852955793, 0.1958369809881616, 0.4540198362012677]
actor:  1 policy actor:  1  step number:  42 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.01567333333333344 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  35 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7366578
maxi score, test score, baseline:  -0.01567333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09116232296608338, 0.25822528003814277, 0.19530239893515436, 0.4553099980606195]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.01567333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09116232296608338, 0.25822528003814277, 0.19530239893515436, 0.4553099980606195]
maxi score, test score, baseline:  -0.01567333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09116232296608338, 0.25822528003814277, 0.19530239893515433, 0.4553099980606195]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]] [[1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]] [[0.33]
 [0.33]
 [0.33]
 [0.33]
 [0.33]
 [0.33]
 [0.33]]
maxi score, test score, baseline:  -0.01567333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09116232296608338, 0.25822528003814277, 0.19530239893515433, 0.4553099980606195]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  45 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.01567333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09116232296608338, 0.25822528003814277, 0.19530239893515433, 0.4553099980606195]
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.038]
 [-0.038]
 [-0.037]
 [-0.037]
 [-0.039]
 [-0.036]] [[-1.112]
 [-0.245]
 [-1.111]
 [-1.192]
 [-1.214]
 [-1.368]
 [-0.462]] [[-0.774]
 [-0.485]
 [-0.774]
 [-0.799]
 [-0.807]
 [-0.861]
 [-0.556]]
maxi score, test score, baseline:  -0.01567333333333344 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.01567333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09116232296608338, 0.25822528003814277, 0.19530239893515433, 0.4553099980606195]
maxi score, test score, baseline:  -0.01567333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09116232296608338, 0.25822528003814277, 0.19530239893515433, 0.4553099980606195]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.01567333333333344 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  35 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.01567333333333344 0.6893333333333335 0.6893333333333335
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  36 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.017553333333333435 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.017553333333333435 0.6893333333333335 0.6893333333333335
probs:  [0.09087080255710896, 0.26059980629554585, 0.19467730910893521, 0.45385208203841]
maxi score, test score, baseline:  -0.017553333333333435 0.6893333333333335 0.6893333333333335
probs:  [0.09087080255710896, 0.26059980629554585, 0.19467730910893521, 0.45385208203841]
Printing some Q and Qe and total Qs values:  [[0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]] [[0.97]
 [0.97]
 [0.97]
 [0.97]
 [0.97]
 [0.97]
 [0.97]] [[0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.5
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5495745022147894
maxi score, test score, baseline:  -0.019660000000000104 0.6893333333333335 0.6893333333333335
probs:  [0.09087080255710896, 0.26059980629554585, 0.19467730910893521, 0.45385208203841]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.019660000000000104 0.6893333333333335 0.6893333333333335
probs:  [0.09087080255710896, 0.26059980629554585, 0.19467730910893521, 0.45385208203841]
maxi score, test score, baseline:  -0.019660000000000104 0.6893333333333335 0.6893333333333335
probs:  [0.09087080255710896, 0.26059980629554585, 0.19467730910893521, 0.45385208203841]
maxi score, test score, baseline:  -0.019660000000000104 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.019660000000000104 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.019660000000000104 0.6893333333333335 0.6893333333333335
probs:  [0.09087080255710896, 0.26059980629554585, 0.19467730910893521, 0.45385208203841]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  -0.0196600000000001 0.6893333333333335 0.6893333333333335
probs:  [0.09120001846881665, 0.259293584448145, 0.19400628889884466, 0.4555001081841937]
Printing some Q and Qe and total Qs values:  [[0.824]
 [0.843]
 [0.783]
 [0.794]
 [0.296]
 [0.563]
 [0.782]] [[ 0.732]
 [ 0.736]
 [ 0.089]
 [ 0.026]
 [-0.09 ]
 [-0.391]
 [ 0.397]] [[0.824]
 [0.843]
 [0.783]
 [0.794]
 [0.296]
 [0.563]
 [0.782]]
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.867]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]] [[1.368]
 [1.311]
 [1.368]
 [1.368]
 [1.368]
 [1.368]
 [1.368]] [[0.761]
 [0.867]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]]
maxi score, test score, baseline:  -0.0196600000000001 0.6893333333333335 0.6893333333333335
probs:  [0.09120001846881665, 0.259293584448145, 0.19400628889884466, 0.4555001081841937]
actor:  0 policy actor:  1  step number:  37 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09120001846881665, 0.259293584448145, 0.19400628889884466, 0.4555001081841937]
maxi score, test score, baseline:  -0.0218600000000001 0.6893333333333335 0.6893333333333335
probs:  [0.09120001846881665, 0.259293584448145, 0.19400628889884466, 0.4555001081841937]
Printing some Q and Qe and total Qs values:  [[ 0.488]
 [ 0.346]
 [ 0.346]
 [ 0.419]
 [ 0.434]
 [ 0.346]
 [-0.044]] [[1.961]
 [2.422]
 [2.422]
 [2.465]
 [2.948]
 [2.422]
 [2.254]] [[0.519]
 [0.527]
 [0.527]
 [0.582]
 [0.694]
 [0.527]
 [0.241]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0218600000000001 0.6893333333333335 0.6893333333333335
probs:  [0.09067754574984317, 0.26354139842538693, 0.1928938830916604, 0.4528871727331095]
maxi score, test score, baseline:  -0.0218600000000001 0.6893333333333335 0.6893333333333335
probs:  [0.09067754574984317, 0.26354139842538693, 0.1928938830916604, 0.4528871727331095]
maxi score, test score, baseline:  -0.0218600000000001 0.6893333333333335 0.6893333333333335
actor:  0 policy actor:  1  step number:  42 total reward:  0.7266666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.021340000000000105 0.6893333333333335 0.6893333333333335
probs:  [0.09067754574984317, 0.26354139842538693, 0.1928938830916604, 0.4528871727331095]
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.039]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]] [[-1.548]
 [-0.591]
 [-1.548]
 [-1.548]
 [-1.548]
 [-1.548]
 [-1.548]] [[0.077]
 [0.231]
 [0.077]
 [0.077]
 [0.077]
 [0.077]
 [0.077]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.78  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.02472666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.09121100543631258, 0.2613910382742996, 0.19184036996898352, 0.4555575863204042]
maxi score, test score, baseline:  -0.02472666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.09121100543631258, 0.2613910382742996, 0.19184036996898352, 0.4555575863204042]
using explorer policy with actor:  0
siam score:  -0.73967123
actor:  0 policy actor:  1  step number:  30 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09121063610610344, 0.261391695202448, 0.19184192070145753, 0.45555574798999104]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09121063610610344, 0.261391695202448, 0.19184192070145753, 0.45555574798999104]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.333
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.2690],
        [ 0.7093],
        [ 0.0387],
        [-0.0233],
        [-0.0295],
        [-0.0000],
        [-0.0275],
        [ 0.3668],
        [ 0.0703],
        [-0.0000]], dtype=torch.float64)
-0.032346567066 0.23670262930092847
-0.058094434398 0.6511574067430334
-0.070771701198 -0.032024101008462445
-0.032346567066 -0.05561775174674398
-0.057834381198 -0.08730964668116784
0.94099335 0.94099335
-0.032346567066 -0.05982728618104801
-0.071159833866 0.2956134336848984
-0.032346567066 0.03791540955377041
0.94099335 0.94099335
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09121063610610344, 0.261391695202448, 0.19184192070145753, 0.45555574798999104]
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09121063610610344, 0.261391695202448, 0.19184192070145753, 0.45555574798999104]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09121063610610344, 0.261391695202448, 0.19184192070145753, 0.45555574798999104]
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09121063610610344, 0.261391695202448, 0.19184192070145753, 0.45555574798999104]
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09121063610610344, 0.261391695202448, 0.19184192070145753, 0.45555574798999104]
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09121063610610344, 0.261391695202448, 0.19184192070145753, 0.45555574798999104]
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09121063610610344, 0.261391695202448, 0.19184192070145753, 0.45555574798999104]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.693]
 [0.566]
 [0.565]
 [0.559]
 [0.619]
 [0.576]] [[-0.494]
 [-0.114]
 [-0.199]
 [-0.215]
 [-0.321]
 [-0.21 ]
 [-0.269]] [[-0.24 ]
 [ 0.142]
 [-0.042]
 [-0.054]
 [-0.131]
 [ 0.003]
 [-0.08 ]]
line 256 mcts: sample exp_bonus 0.31841464140182363
actor:  1 policy actor:  1  step number:  47 total reward:  0.6000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09085967216294079, 0.2642366872782318, 0.1911031062108823, 0.45380053434794515]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09089666925880611, 0.2640860057366366, 0.19103159093500122, 0.453985734069556]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09057322529505502, 0.2654033262975546, 0.19165680715153696, 0.4523666412558536]
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09057322529505502, 0.2654033262975546, 0.19165680715153696, 0.4523666412558536]
maxi score, test score, baseline:  -0.02455333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.09057322529505502, 0.2654033262975546, 0.19165680715153696, 0.4523666412558536]
using another actor
actor:  1 policy actor:  1  step number:  53 total reward:  0.4666666666666669  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.632]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.626]] [[0.856]
 [1.116]
 [0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.912]] [[0.376]
 [0.47 ]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.397]]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.031300000000000106 0.6893333333333335 0.6893333333333335
probs:  [0.09024359421618659, 0.26674584570238435, 0.19229398304312745, 0.45071657703830165]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.031300000000000106 0.6893333333333335 0.6893333333333335
probs:  [0.09024359421618659, 0.26674584570238435, 0.19229398304312745, 0.45071657703830165]
maxi score, test score, baseline:  -0.031300000000000106 0.6893333333333335 0.6893333333333335
actor:  0 policy actor:  0  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.031620000000000106 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.031620000000000106 0.6893333333333335 0.6893333333333335
probs:  [0.09024359421618659, 0.26674584570238435, 0.19229398304312745, 0.45071657703830165]
actor:  0 policy actor:  0  step number:  57 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.03604666666666676 0.6893333333333335 0.6893333333333335
probs:  [0.09024359421618659, 0.26674584570238435, 0.19229398304312745, 0.45071657703830165]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  0 policy actor:  1  step number:  48 total reward:  0.4733333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.03648666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.09024359421618659, 0.26674584570238435, 0.19229398304312745, 0.45071657703830165]
maxi score, test score, baseline:  -0.03648666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.09024359421618659, 0.26674584570238435, 0.19229398304312745, 0.45071657703830165]
Printing some Q and Qe and total Qs values:  [[-0.035]
 [ 0.059]
 [-0.037]
 [-0.039]
 [-0.038]
 [-0.036]
 [-0.039]] [[0.463]
 [1.138]
 [0.644]
 [0.512]
 [0.566]
 [0.666]
 [0.674]] [[-0.424]
 [-0.063]
 [-0.345]
 [-0.405]
 [-0.38 ]
 [-0.335]
 [-0.333]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.03648666666666677 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.03648666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.09051442573602766, 0.26564280477531416, 0.19177046652363558, 0.45207230296502265]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03648666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.09051442573602766, 0.26564280477531416, 0.19177046652363558, 0.45207230296502265]
actor:  1 policy actor:  1  step number:  27 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.333
using another actor
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.45 ]
 [0.421]
 [0.406]
 [0.406]
 [0.408]
 [0.422]] [[1.257]
 [0.57 ]
 [0.853]
 [0.524]
 [1.165]
 [0.664]
 [0.446]] [[-0.133]
 [-0.281]
 [-0.217]
 [-0.341]
 [-0.127]
 [-0.292]
 [-0.351]]
maxi score, test score, baseline:  -0.03648666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08970799983986466, 0.272192300235416, 0.19006041719942962, 0.44803928272528964]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.03648666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08928868597282448, 0.2755978137259976, 0.1891712500752413, 0.4459422502259365]
maxi score, test score, baseline:  -0.03648666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08928868597282448, 0.2755978137259976, 0.1891712500752413, 0.4459422502259365]
start point for exploration sampling:  11715
actor:  0 policy actor:  1  step number:  36 total reward:  0.5933333333333333  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.043406666666666774 0.6893333333333335 0.6893333333333335
from probs:  [0.08928868597282448, 0.2755978137259976, 0.1891712500752413, 0.4459422502259365]
maxi score, test score, baseline:  -0.043406666666666774 0.6893333333333335 0.6893333333333335
Printing some Q and Qe and total Qs values:  [[0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]] [[1.884]
 [1.884]
 [1.884]
 [1.884]
 [1.884]
 [1.884]
 [1.884]] [[0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]]
maxi score, test score, baseline:  -0.043406666666666774 0.6893333333333335 0.6893333333333335
actor:  0 policy actor:  1  step number:  41 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  44 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.044033333333333424 0.6893333333333335 0.6893333333333335
probs:  [0.08928868597282448, 0.2755978137259976, 0.1891712500752413, 0.4459422502259365]
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.19 ]
 [0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.164]] [[1.371]
 [1.608]
 [1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.639]] [[-0.363]
 [-0.255]
 [-0.363]
 [-0.363]
 [-0.363]
 [-0.363]
 [-0.271]]
maxi score, test score, baseline:  -0.044033333333333424 0.6893333333333335 0.6893333333333335
probs:  [0.08928868597282448, 0.2755978137259976, 0.1891712500752413, 0.4459422502259365]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]] [[8.63 ]
 [3.278]
 [3.278]
 [3.278]
 [3.278]
 [3.278]
 [3.278]] [[0.807]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]]
Printing some Q and Qe and total Qs values:  [[ 0.241]
 [ 0.379]
 [ 0.273]
 [ 0.266]
 [ 0.196]
 [-0.123]
 [ 0.256]] [[3.053]
 [6.738]
 [3.716]
 [3.092]
 [2.911]
 [0.407]
 [3.206]] [[ 0.351]
 [ 0.869]
 [ 0.446]
 [ 0.363]
 [ 0.32 ]
 [-0.091]
 [ 0.375]]
siam score:  -0.7618996
maxi score, test score, baseline:  -0.044033333333333424 0.6893333333333335 0.6893333333333335
probs:  [0.08928868597282448, 0.2755978137259976, 0.1891712500752413, 0.4459422502259365]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.044033333333333424 0.6893333333333335 0.6893333333333335
probs:  [0.08928868597282448, 0.2755978137259976, 0.1891712500752413, 0.4459422502259365]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.044033333333333424 0.6893333333333335 0.6893333333333335
probs:  [0.08928868597282448, 0.2755978137259976, 0.1891712500752413, 0.4459422502259365]
actor:  1 policy actor:  1  step number:  24 total reward:  0.8066666666666666  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 9.75
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.044033333333333424 0.6893333333333335 0.6893333333333335
probs:  [0.08815277364567217, 0.2848232771754679, 0.18676251529049995, 0.44026143388836]
maxi score, test score, baseline:  -0.044033333333333424 0.6893333333333335 0.6893333333333335
probs:  [0.08815277364567217, 0.2848232771754679, 0.18676251529049995, 0.44026143388836]
maxi score, test score, baseline:  -0.044033333333333424 0.6893333333333335 0.6893333333333335
probs:  [0.08815277364567217, 0.2848232771754679, 0.18676251529049995, 0.44026143388836]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.044033333333333424 0.6893333333333335 0.6893333333333335
probs:  [0.08794415839187761, 0.28651757377995013, 0.18632014064024652, 0.43921812718792574]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9687911960190162
maxi score, test score, baseline:  -0.044033333333333424 0.6893333333333335 0.6893333333333335
probs:  [0.08842117938981364, 0.2844409365366647, 0.18553204446569577, 0.44160583960782585]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.69 ]
 [0.508]
 [0.508]
 [0.64 ]
 [0.508]] [[1.487]
 [1.487]
 [3.313]
 [1.487]
 [1.487]
 [3.715]
 [1.487]] [[0.245]
 [0.245]
 [0.747]
 [0.245]
 [0.245]
 [0.8  ]
 [0.245]]
maxi score, test score, baseline:  -0.044033333333333424 0.6893333333333335 0.6893333333333335
probs:  [0.08842117938981364, 0.2844409365366647, 0.18553204446569577, 0.44160583960782585]
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.634]
 [0.67 ]] [[1.647]
 [1.647]
 [1.647]
 [1.647]
 [1.647]
 [1.749]
 [2.493]] [[0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.436]
 [0.673]]
maxi score, test score, baseline:  -0.044033333333333424 0.6893333333333335 0.6893333333333335
probs:  [0.08842117938981364, 0.2844409365366647, 0.18553204446569577, 0.44160583960782585]
first move QE:  -0.08626387152681696
actor:  1 policy actor:  1  step number:  34 total reward:  0.6866666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.044033333333333424 0.6893333333333335 0.6893333333333335
probs:  [0.08768485185392733, 0.2876464247455572, 0.1867485462900582, 0.4379201771104572]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.005]
 [0.004]
 [0.004]
 [0.003]
 [0.003]
 [0.003]] [[0.004]
 [0.   ]
 [0.004]
 [0.003]
 [0.003]
 [0.005]
 [0.005]] [[-0.49 ]
 [-0.49 ]
 [-0.49 ]
 [-0.49 ]
 [-0.491]
 [-0.491]
 [-0.491]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6000000000000002  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7601777
maxi score, test score, baseline:  -0.044033333333333424 0.6893333333333335 0.6893333333333335
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
UNIT TEST: sample policy line 217 mcts : [0.041 0.408 0.02  0.224 0.    0.224 0.082]
siam score:  -0.76072496
line 256 mcts: sample exp_bonus 0.32128737882936836
actor:  0 policy actor:  1  step number:  43 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.0443800000000001 0.6893333333333335 0.6893333333333335
probs:  [0.08768453449297299, 0.28764673770061966, 0.18675012926737256, 0.43791859853903475]
maxi score, test score, baseline:  -0.0443800000000001 0.6893333333333335 0.6893333333333335
probs:  [0.08768453449297299, 0.28764673770061966, 0.18675012926737256, 0.43791859853903475]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]] [[0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]]
maxi score, test score, baseline:  -0.0443800000000001 0.6893333333333335 0.6893333333333335
probs:  [0.08768453449297299, 0.28764673770061966, 0.18675012926737256, 0.43791859853903475]
maxi score, test score, baseline:  -0.0443800000000001 0.6893333333333335 0.6893333333333335
probs:  [0.08768453449297299, 0.28764673770061966, 0.18675012926737256, 0.43791859853903475]
maxi score, test score, baseline:  -0.0443800000000001 0.6893333333333335 0.6893333333333335
probs:  [0.08768453449297299, 0.2876467377006196, 0.18675012926737256, 0.43791859853903475]
actor:  1 policy actor:  1  step number:  30 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0443800000000001 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.0443800000000001 0.6893333333333335 0.6893333333333335
probs:  [0.08681261706403991, 0.29473565288623527, 0.18489366212322503, 0.43355806792649976]
using another actor
maxi score, test score, baseline:  -0.0443800000000001 0.6893333333333335 0.6893333333333335
probs:  [0.08681261706403991, 0.29473565288623527, 0.18489366212322503, 0.43355806792649976]
siam score:  -0.7624801
maxi score, test score, baseline:  -0.04776666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08681261706403991, 0.29473565288623527, 0.18489366212322503, 0.43355806792649976]
maxi score, test score, baseline:  -0.04776666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08681231676978446, 0.29473587216246483, 0.18489523653123446, 0.4335565745365161]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.04776666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08681231676978446, 0.29473587216246483, 0.18489523653123446, 0.4335565745365161]
maxi score, test score, baseline:  -0.04776666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08681231676978446, 0.29473587216246494, 0.18489523653123446, 0.4335565745365161]
actor:  1 policy actor:  1  step number:  37 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.04776666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08678835369576476, 0.2949307608139773, 0.18484415249028827, 0.4334367329999697]
maxi score, test score, baseline:  -0.04776666666666677 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  42 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.08678835369576476, 0.29493076081397734, 0.18484415249028827, 0.4334367329999697]
maxi score, test score, baseline:  -0.04776666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08678835369576476, 0.29493076081397734, 0.18484415249028827, 0.4334367329999697]
actor:  1 policy actor:  1  step number:  52 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.04776666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08678835369576476, 0.29493076081397734, 0.18484415249028827, 0.4334367329999697]
maxi score, test score, baseline:  -0.04776666666666677 0.6893333333333335 0.6893333333333335
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.466]
 [0.409]
 [0.472]
 [0.471]
 [0.466]] [[0.445]
 [0.548]
 [0.63 ]
 [0.619]
 [0.582]
 [0.559]
 [0.537]] [[0.39 ]
 [0.432]
 [0.464]
 [0.425]
 [0.448]
 [0.438]
 [0.426]]
maxi score, test score, baseline:  -0.04776666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08678835369576476, 0.29493076081397734, 0.18484415249028827, 0.4334367329999697]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.265]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]] [[1.537]
 [1.504]
 [1.537]
 [1.537]
 [1.537]
 [1.537]
 [1.537]] [[0.588]
 [0.672]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]]
actor:  0 policy actor:  1  step number:  62 total reward:  0.28666666666666596  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08678835369576476, 0.2949307608139773, 0.18484415249028827, 0.4334367329999697]
from probs:  [0.08678835369576476, 0.2949307608139773, 0.18484415249028827, 0.4334367329999697]
maxi score, test score, baseline:  -0.0485400000000001 0.6893333333333335 0.6893333333333335
probs:  [0.08678835369576476, 0.2949307608139773, 0.18484415249028827, 0.4334367329999697]
siam score:  -0.76334196
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.05188666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08678835369576476, 0.2949307608139773, 0.18484415249028827, 0.4334367329999697]
actor:  0 policy actor:  0  step number:  47 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.05251333333333343 0.6893333333333335 0.6893333333333335
first move QE:  -0.08520405298778153
maxi score, test score, baseline:  -0.05251333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.087136631356481, 0.2933837969081118, 0.18429958266118845, 0.43517998907421873]
actor:  1 policy actor:  1  step number:  40 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.05251333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.087136631356481, 0.2933837969081117, 0.18429958266118845, 0.43517998907421873]
maxi score, test score, baseline:  -0.05251333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08707152815803082, 0.2936729693401068, 0.18440137856041716, 0.4348541239414453]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.05251333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08707152815803082, 0.2936729693401068, 0.18440137856041716, 0.4348541239414453]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  -0.05251333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08729811216450248, 0.29266653878427606, 0.18404708993499325, 0.4359882591162282]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7078425400279462
maxi score, test score, baseline:  -0.05251333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08729811216450248, 0.29266653878427606, 0.18404708993499325, 0.4359882591162282]
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.756]] [[-0.182]
 [-0.182]
 [-0.182]
 [-0.182]
 [-0.182]
 [-0.182]
 [-0.093]] [[0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.251]]
actor:  0 policy actor:  1  step number:  53 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.4666666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.05311333333333345 0.6893333333333335 0.6893333333333335
probs:  [0.08729811216450248, 0.29266653878427606, 0.18404708993499325, 0.4359882591162282]
maxi score, test score, baseline:  -0.05311333333333345 0.6893333333333335 0.6893333333333335
probs:  [0.08729811216450248, 0.29266653878427606, 0.18404708993499325, 0.4359882591162282]
maxi score, test score, baseline:  -0.05311333333333345 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.05311333333333345 0.6893333333333335 0.6893333333333335
probs:  [0.08729811216450248, 0.29266653878427606, 0.18404708993499325, 0.4359882591162282]
maxi score, test score, baseline:  -0.05311333333333345 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.05311333333333345 0.6893333333333335 0.6893333333333335
probs:  [0.08729811216450248, 0.29266653878427606, 0.18404708993499325, 0.4359882591162282]
actor:  1 policy actor:  1  step number:  32 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.037]
 [-0.027]
 [-0.026]
 [-0.026]
 [-0.03 ]
 [-0.025]] [[-2.471]
 [ 0.825]
 [-1.104]
 [-1.128]
 [-1.008]
 [-0.859]
 [-1.203]] [[-0.011]
 [ 0.691]
 [ 0.283]
 [ 0.278]
 [ 0.304]
 [ 0.334]
 [ 0.263]]
maxi score, test score, baseline:  -0.05311333333333345 0.6893333333333335 0.6893333333333335
probs:  [0.08729781155661445, 0.29266677905075134, 0.18404864541719573, 0.43598676397543845]
actor:  0 policy actor:  1  step number:  45 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.667
first move QE:  -0.08567132022013646
using explorer policy with actor:  1
from probs:  [0.08729781155661445, 0.29266677905075134, 0.18404864541719573, 0.43598676397543845]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7382830620931407
maxi score, test score, baseline:  -0.05344666666666676 0.6893333333333335 0.6893333333333335
probs:  [0.08729781155661445, 0.29266677905075134, 0.18404864541719573, 0.43598676397543845]
maxi score, test score, baseline:  -0.05344666666666676 0.6893333333333335 0.6893333333333335
probs:  [0.08729781155661445, 0.29266677905075134, 0.18404864541719573, 0.43598676397543845]
siam score:  -0.759104
actor:  1 policy actor:  1  step number:  35 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.179]
 [0.111]
 [0.079]
 [0.079]
 [0.046]
 [0.079]] [[0.423]
 [0.863]
 [0.617]
 [0.423]
 [0.423]
 [0.359]
 [0.423]] [[-0.411]
 [-0.14 ]
 [-0.3  ]
 [-0.411]
 [-0.411]
 [-0.463]
 [-0.411]]
Printing some Q and Qe and total Qs values:  [[0.984]
 [0.982]
 [0.981]
 [0.979]
 [0.923]
 [0.921]
 [0.983]] [[-0.02 ]
 [-0.075]
 [ 0.186]
 [-0.032]
 [-0.054]
 [-0.136]
 [-0.061]] [[0.984]
 [0.982]
 [0.981]
 [0.979]
 [0.923]
 [0.921]
 [0.983]]
Printing some Q and Qe and total Qs values:  [[0.983]
 [0.982]
 [0.981]
 [0.982]
 [0.923]
 [0.937]
 [0.983]] [[ 0.008]
 [-0.073]
 [ 0.196]
 [-0.019]
 [-0.054]
 [-0.105]
 [-0.058]] [[0.983]
 [0.982]
 [0.981]
 [0.982]
 [0.923]
 [0.937]
 [0.983]]
actor:  0 policy actor:  0  step number:  47 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.05346000000000011 0.6893333333333335 0.6893333333333335
probs:  [0.08752170615236728, 0.2916723006933506, 0.18369855553706058, 0.43710743761722154]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
siam score:  -0.7520409
maxi score, test score, baseline:  -0.05346000000000011 0.6893333333333335 0.6893333333333335
probs:  [0.08752170615236728, 0.2916723006933506, 0.18369855553706058, 0.43710743761722154]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4933333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.05346000000000011 0.6893333333333335 0.6893333333333335
probs:  [0.08752170615236728, 0.2916723006933506, 0.18369855553706058, 0.43710743761722154]
first move QE:  -0.08564084865987187
maxi score, test score, baseline:  -0.05346000000000011 0.6893333333333335 0.6893333333333335
probs:  [0.08752170615236728, 0.2916723006933506, 0.18369855553706058, 0.43710743761722154]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.333
from probs:  [0.08752170615236728, 0.2916723006933506, 0.18369855553706058, 0.43710743761722154]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.003]
 [-0.015]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[-0.014]
 [-0.706]
 [ 0.002]
 [-0.706]
 [-0.706]
 [-0.706]
 [-0.706]] [[0.105]
 [0.005]
 [0.111]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  -0.05346000000000011 0.6893333333333335 0.6893333333333335
probs:  [0.0870519870078933, 0.2954735924727505, 0.18271609227764024, 0.43475832824171595]
maxi score, test score, baseline:  -0.05346000000000011 0.6893333333333335 0.6893333333333335
probs:  [0.0870519870078933, 0.2954735924727505, 0.18271609227764024, 0.43475832824171595]
siam score:  -0.7502802
actor:  0 policy actor:  0  step number:  44 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  0.8266666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.05067333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08929880314924782, 0.2939163876992265, 0.17085196425140897, 0.44593284490011664]
maxi score, test score, baseline:  -0.05067333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08929880314924782, 0.2939163876992265, 0.17085196425140897, 0.44593284490011664]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.05067333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08929880314924782, 0.2939163876992265, 0.17085196425140897, 0.44593284490011664]
maxi score, test score, baseline:  -0.05067333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08929880314924782, 0.2939163876992265, 0.17085196425140897, 0.44593284490011664]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.05067333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08901258076279926, 0.2929732179147139, 0.17351259186478157, 0.4445016094577052]
siam score:  -0.75315267
maxi score, test score, baseline:  -0.05067333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08901258076279926, 0.2929732179147139, 0.17351259186478157, 0.4445016094577052]
maxi score, test score, baseline:  -0.0531000000000001 0.6893333333333335 0.6893333333333335
probs:  [0.08901258076279926, 0.2929732179147139, 0.17351259186478157, 0.4445016094577052]
actor:  0 policy actor:  0  step number:  55 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.05083333333333342 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.05083333333333342 0.6893333333333335 0.6893333333333335
probs:  [0.08901258076279926, 0.2929732179147139, 0.17351259186478157, 0.4445016094577052]
first move QE:  -0.08723283236677588
maxi score, test score, baseline:  -0.05083333333333342 0.6893333333333335 0.6893333333333335
probs:  [0.08901258076279926, 0.2929732179147139, 0.17351259186478157, 0.4445016094577052]
maxi score, test score, baseline:  -0.05083333333333342 0.6893333333333335 0.6893333333333335
probs:  [0.08901258076279926, 0.2929732179147139, 0.17351259186478157, 0.4445016094577052]
actor:  1 policy actor:  1  step number:  40 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  37 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.047446666666666755 0.6893333333333335 0.6893333333333335
probs:  [0.08900270656535286, 0.2929406801220091, 0.1736043791030608, 0.44445223420957736]
using another actor
maxi score, test score, baseline:  -0.05043333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08900270656535286, 0.2929406801220091, 0.1736043791030608, 0.44445223420957736]
maxi score, test score, baseline:  -0.05043333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08900270656535286, 0.2929406801220091, 0.1736043791030608, 0.44445223420957736]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.05043333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08900270656535286, 0.2929406801220091, 0.1736043791030608, 0.44445223420957736]
maxi score, test score, baseline:  -0.05043333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08900270656535286, 0.2929406801220091, 0.1736043791030608, 0.44445223420957736]
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.127]
 [0.249]
 [0.127]
 [0.127]
 [0.127]
 [0.127]] [[1.444]
 [1.444]
 [0.974]
 [1.444]
 [1.444]
 [1.444]
 [1.444]] [[0.03 ]
 [0.03 ]
 [0.074]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [ 0.004]
 [ 0.004]
 [ 0.004]
 [ 0.004]
 [ 0.004]
 [-0.003]] [[-0.163]
 [-0.313]
 [-0.313]
 [-0.313]
 [-0.313]
 [-0.313]
 [-0.12 ]] [[0.763]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.806]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.147]
 [0.264]
 [0.218]
 [0.152]
 [0.264]
 [0.236]] [[-0.164]
 [ 0.22 ]
 [-0.101]
 [-1.181]
 [-0.727]
 [-0.101]
 [-0.197]] [[0.157]
 [0.147]
 [0.264]
 [0.218]
 [0.152]
 [0.264]
 [0.236]]
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.929]
 [0.929]
 [0.929]
 [0.944]
 [0.929]
 [0.89 ]] [[1.257]
 [1.305]
 [1.305]
 [1.305]
 [1.07 ]
 [1.305]
 [0.975]] [[0.923]
 [0.929]
 [0.929]
 [0.929]
 [0.944]
 [0.929]
 [0.89 ]]
from probs:  [0.08899640044179695, 0.29294533141221335, 0.17363739548002813, 0.44442087266596164]
maxi score, test score, baseline:  -0.05043333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08899640044179695, 0.29294533141221335, 0.17363739548002813, 0.44442087266596164]
line 256 mcts: sample exp_bonus 1.227404279306337
maxi score, test score, baseline:  -0.05043333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08899640044179695, 0.29294533141221335, 0.17363739548002813, 0.44442087266596164]
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.681]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]] [[-0.491]
 [ 0.09 ]
 [-0.491]
 [-0.491]
 [-0.491]
 [-0.491]
 [-0.491]] [[0.64 ]
 [0.681]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]]
Printing some Q and Qe and total Qs values:  [[0.891]
 [0.87 ]
 [0.856]
 [0.852]
 [0.856]
 [0.807]
 [0.821]] [[1.145]
 [1.322]
 [1.239]
 [0.945]
 [1.239]
 [1.19 ]
 [1.083]] [[0.891]
 [0.87 ]
 [0.856]
 [0.852]
 [0.856]
 [0.807]
 [0.821]]
maxi score, test score, baseline:  -0.05043333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08899640044179695, 0.29294533141221335, 0.17363739548002813, 0.44442087266596164]
actor:  0 policy actor:  1  step number:  43 total reward:  0.6266666666666668  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7549794
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.025]
 [ 0.119]
 [ 0.025]
 [ 0.025]
 [ 0.025]
 [ 0.069]] [[0.565]
 [0.834]
 [0.938]
 [0.834]
 [0.834]
 [0.834]
 [1.159]] [[0.151]
 [0.306]
 [0.445]
 [0.306]
 [0.306]
 [0.306]
 [0.502]]
maxi score, test score, baseline:  -0.04923333333333344 0.6893333333333335 0.6893333333333335
probs:  [0.08899640044179695, 0.29294533141221335, 0.17363739548002813, 0.44442087266596164]
maxi score, test score, baseline:  -0.04923333333333344 0.6893333333333335 0.6893333333333335
actor:  0 policy actor:  1  step number:  50 total reward:  0.566666666666667  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.04900666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08899640044179695, 0.29294533141221335, 0.17363739548002813, 0.44442087266596164]
maxi score, test score, baseline:  -0.04900666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08899640044179695, 0.29294533141221335, 0.17363739548002813, 0.44442087266596164]
siam score:  -0.7514436
actor:  1 policy actor:  1  step number:  37 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.04900666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08871216070795422, 0.29520591816981534, 0.17308237032971216, 0.44299955079251824]
siam score:  -0.7543234
maxi score, test score, baseline:  -0.04900666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08871216070795422, 0.29520591816981534, 0.17308237032971216, 0.44299955079251824]
actor:  1 policy actor:  1  step number:  48 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.04900666666666677 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.04900666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08870597568245686, 0.2952099967196593, 0.17311523428014844, 0.4429687933177355]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.04900666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08870597568245686, 0.2952099967196593, 0.17311523428014844, 0.4429687933177355]
maxi score, test score, baseline:  -0.04900666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08870597568245686, 0.2952099967196593, 0.17311523428014844, 0.4429687933177355]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.04900666666666677 0.6893333333333335 0.6893333333333335
probs:  [0.08827775257501906, 0.2937831995116293, 0.17711155657055783, 0.44082749134279386]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.05178000000000011 0.6893333333333335 0.6893333333333335
probs:  [0.08827775257501906, 0.2937831995116293, 0.1771115565705578, 0.44082749134279386]
start point for exploration sampling:  11715
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]] [[1.232]
 [2.388]
 [2.388]
 [2.388]
 [2.388]
 [2.388]
 [2.388]] [[0.36 ]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.347]
 [0.347]
 [0.451]
 [0.347]
 [0.347]
 [0.484]] [[1.385]
 [1.964]
 [1.964]
 [3.488]
 [1.964]
 [1.964]
 [1.96 ]] [[0.153]
 [0.285]
 [0.285]
 [0.824]
 [0.285]
 [0.285]
 [0.35 ]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.05178000000000011 0.6893333333333335 0.6893333333333335
probs:  [0.08856696984955117, 0.29245554649699135, 0.17670184960490354, 0.442275634048554]
maxi score, test score, baseline:  -0.05475333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08856696984955117, 0.29245554649699135, 0.17670184960490348, 0.442275634048554]
maxi score, test score, baseline:  -0.05475333333333343 0.6893333333333335 0.6893333333333335
siam score:  -0.7575953
maxi score, test score, baseline:  -0.05475333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08856696984955117, 0.29245554649699135, 0.17670184960490348, 0.442275634048554]
maxi score, test score, baseline:  -0.05475333333333343 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.05475333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08856092819000129, 0.29246018945361185, 0.17673329239954783, 0.44224558995683905]
siam score:  -0.7528515
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.05475333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08856092819000129, 0.29246018945361185, 0.17673329239954783, 0.44224558995683905]
maxi score, test score, baseline:  -0.05475333333333343 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.05475333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08856092819000129, 0.29246018945361185, 0.17673329239954783, 0.44224558995683905]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5866666666666668  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7482776
maxi score, test score, baseline:  -0.05475333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08878974435631022, 0.291409952810254, 0.17640900646299346, 0.44339129637044244]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.05475333333333343 0.6893333333333335 0.6893333333333335
probs:  [0.08878974435631022, 0.291409952810254, 0.17640900646299346, 0.44339129637044244]
Printing some Q and Qe and total Qs values:  [[-0.025]
 [ 0.074]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]] [[0.4  ]
 [0.796]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]] [[0.028]
 [0.391]
 [0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]]
maxi score, test score, baseline:  -0.054753333333333425 0.6893333333333335 0.6893333333333335
probs:  [0.08878974435631022, 0.291409952810254, 0.17640900646299346, 0.44339129637044244]
Printing some Q and Qe and total Qs values:  [[ 0.056]
 [ 0.104]
 [-0.021]
 [-0.025]
 [-0.025]
 [-0.021]
 [-0.007]] [[ 0.429]
 [ 0.561]
 [-0.425]
 [-0.123]
 [ 0.484]
 [-0.425]
 [ 0.149]] [[0.649]
 [0.683]
 [0.495]
 [0.54 ]
 [0.632]
 [0.495]
 [0.587]]
maxi score, test score, baseline:  -0.054753333333333425 0.6893333333333335 0.6893333333333335
probs:  [0.08878372040222011, 0.29141478230222284, 0.17644015876630656, 0.4433613385292505]
maxi score, test score, baseline:  -0.054753333333333425 0.6893333333333335 0.6893333333333335
probs:  [0.08878372040222011, 0.29141478230222284, 0.17644015876630656, 0.4433613385292505]
maxi score, test score, baseline:  -0.054753333333333425 0.6893333333333335 0.6893333333333335
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.741]
 [0.703]
 [0.646]
 [0.659]
 [0.662]
 [0.716]] [[0.506]
 [0.878]
 [0.949]
 [0.953]
 [1.176]
 [1.259]
 [1.877]] [[0.259]
 [0.421]
 [0.42 ]
 [0.386]
 [0.462]
 [0.49 ]
 [0.713]]
maxi score, test score, baseline:  -0.054753333333333425 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  41 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  45 total reward:  0.52  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.054753333333333425 0.6893333333333335 0.6893333333333335
probs:  [0.08878372040222011, 0.29141478230222284, 0.17644015876630653, 0.4433613385292505]
maxi score, test score, baseline:  -0.054753333333333425 0.6893333333333335 0.6893333333333335
probs:  [0.08878372040222011, 0.29141478230222284, 0.17644015876630653, 0.4433613385292505]
maxi score, test score, baseline:  -0.054753333333333425 0.6893333333333335 0.6893333333333335
probs:  [0.08878372040222011, 0.29141478230222284, 0.17644015876630653, 0.4433613385292505]
maxi score, test score, baseline:  -0.054753333333333425 0.6893333333333335 0.6893333333333335
probs:  [0.08878372040222011, 0.29141478230222284, 0.17644015876630653, 0.4433613385292505]
maxi score, test score, baseline:  -0.054753333333333425 0.6893333333333335 0.6893333333333335
probs:  [0.08878372040222011, 0.29141478230222284, 0.17644015876630653, 0.4433613385292505]
maxi score, test score, baseline:  -0.054753333333333425 0.6893333333333335 0.6893333333333335
probs:  [0.08878372040222011, 0.29141478230222284, 0.17644015876630653, 0.4433613385292505]
maxi score, test score, baseline:  -0.054753333333333425 0.6893333333333335 0.6893333333333335
probs:  [0.08878372040222011, 0.29141478230222284, 0.17644015876630653, 0.4433613385292505]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.649]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.488]] [[0.88 ]
 [0.866]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.693]] [[ 0.091]
 [ 0.189]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.03 ]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  55 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.011]
 [-0.004]
 [-0.02 ]
 [-0.003]
 [-0.003]
 [-0.008]] [[-0.706]
 [ 0.868]
 [-0.085]
 [ 0.989]
 [-0.114]
 [-0.028]
 [ 0.825]] [[-0.628]
 [ 0.152]
 [-0.318]
 [ 0.205]
 [-0.332]
 [-0.289]
 [ 0.132]]
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]] [[1.87]
 [1.87]
 [1.87]
 [1.87]
 [1.87]
 [1.87]
 [1.87]] [[0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]]
maxi score, test score, baseline:  -0.05400666666666676 0.6893333333333335 0.6893333333333335
probs:  [0.08856712921911614, 0.29314522695354345, 0.17600936476807638, 0.4422782790592641]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.05400666666666676 0.6893333333333335 0.6893333333333335
probs:  [0.08856712921911614, 0.29314522695354345, 0.17600936476807638, 0.4422782790592641]
maxi score, test score, baseline:  -0.05400666666666676 0.6893333333333335 0.6893333333333335
probs:  [0.08856120129924301, 0.2931496258034969, 0.17604037274347464, 0.44224880015378537]
siam score:  -0.7558255
maxi score, test score, baseline:  -0.05400666666666676 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  -0.05400666666666676 0.6893333333333335 0.6893333333333335
probs:  [0.08855530496900099, 0.2931540012121674, 0.1760712154787003, 0.44221947834013137]
maxi score, test score, baseline:  -0.05400666666666675 0.6893333333333335 0.6893333333333335
probs:  [0.08854943997655136, 0.29315835336643337, 0.17610189429107986, 0.44219031236593537]
maxi score, test score, baseline:  -0.05400666666666675 0.6893333333333335 0.6893333333333335
probs:  [0.08854943997655136, 0.29315835336643337, 0.17610189429107986, 0.44219031236593537]
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.003]
 [-0.012]
 [-0.015]
 [-0.009]
 [-0.012]
 [-0.01 ]] [[ 0.425]
 [ 0.345]
 [ 0.   ]
 [-0.109]
 [ 0.316]
 [ 0.   ]
 [ 0.151]] [[0.259]
 [0.231]
 [0.074]
 [0.025]
 [0.214]
 [0.074]
 [0.141]]
maxi score, test score, baseline:  -0.05400666666666675 0.6893333333333335 0.6893333333333335
probs:  [0.08854943997655136, 0.2931583533664334, 0.1761018942910799, 0.44219031236593537]
maxi score, test score, baseline:  -0.05400666666666675 0.6893333333333335 0.6893333333333335
probs:  [0.08854943997655136, 0.2931583533664334, 0.1761018942910799, 0.44219031236593537]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]] [[1.568]
 [1.568]
 [1.568]
 [1.568]
 [1.568]
 [1.568]
 [1.568]] [[0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]]
maxi score, test score, baseline:  -0.05400666666666675 0.6893333333333335 0.6893333333333335
probs:  [0.08854943997655136, 0.2931583533664334, 0.1761018942910799, 0.44219031236593537]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]] [[1.195]
 [1.195]
 [1.195]
 [1.195]
 [1.195]
 [1.195]
 [1.195]] [[0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]]
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.883]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]] [[ 0.159]
 [-0.449]
 [ 0.159]
 [ 0.159]
 [ 0.159]
 [ 0.159]
 [ 0.159]] [[0.751]
 [0.883]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]]
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.765]
 [0.647]
 [0.796]
 [0.351]
 [0.67 ]
 [0.765]] [[ 0.804]
 [-0.105]
 [ 1.329]
 [ 0.263]
 [ 0.429]
 [ 1.272]
 [ 0.256]] [[0.193]
 [0.765]
 [0.647]
 [0.796]
 [0.351]
 [0.67 ]
 [0.765]]
start point for exploration sampling:  11715
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.833]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]] [[0.408]
 [0.308]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]] [[0.793]
 [0.833]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]]
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.838]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]] [[ 0.194]
 [-0.311]
 [ 0.194]
 [ 0.194]
 [ 0.194]
 [ 0.194]
 [ 0.194]] [[0.732]
 [0.838]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]]
Printing some Q and Qe and total Qs values:  [[ 0.193]
 [ 0.249]
 [ 0.193]
 [ 0.193]
 [ 0.193]
 [-0.115]
 [ 0.245]] [[ 1.417]
 [ 1.688]
 [ 1.417]
 [ 1.417]
 [ 1.417]
 [-0.426]
 [ 1.956]] [[ 0.36 ]
 [ 0.46 ]
 [ 0.36 ]
 [ 0.36 ]
 [ 0.36 ]
 [-0.274]
 [ 0.519]]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.010313333333333428 0.7026666666666668 0.7026666666666668
probs:  [0.09004620261260429, 0.29085739831990776, 0.16947254467714154, 0.4496238543903464]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.333
from probs:  [0.09004620261260429, 0.29085739831990776, 0.16947254467714154, 0.4496238543903464]
maxi score, test score, baseline:  -0.010313333333333428 0.7026666666666668 0.7026666666666668
probs:  [0.09004620261260429, 0.29085739831990776, 0.16947254467714154, 0.4496238543903464]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5333333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.010313333333333428 0.7026666666666668 0.7026666666666668
probs:  [0.09004620261260429, 0.29085739831990776, 0.16947254467714154, 0.4496238543903464]
maxi score, test score, baseline:  -0.010313333333333428 0.7026666666666668 0.7026666666666668
probs:  [0.09003932889005797, 0.2908627957195928, 0.16950820086280005, 0.44958967452754917]
actor:  0 policy actor:  1  step number:  38 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.009380000000000091 0.7026666666666668 0.7026666666666668
probs:  [0.09003932889005797, 0.2908627957195928, 0.16950820086280005, 0.44958967452754917]
maxi score, test score, baseline:  -0.009380000000000091 0.7026666666666668 0.7026666666666668
probs:  [0.09003932889005797, 0.2908627957195928, 0.16950820086280005, 0.44958967452754917]
maxi score, test score, baseline:  -0.009380000000000091 0.7026666666666668 0.7026666666666668
probs:  [0.09003932889005797, 0.2908627957195927, 0.16950820086280002, 0.44958967452754917]
siam score:  -0.75525373
maxi score, test score, baseline:  -0.009380000000000091 0.7026666666666668 0.7026666666666668
probs:  [0.09003932889005797, 0.2908627957195927, 0.16950820086280002, 0.44958967452754917]
maxi score, test score, baseline:  -0.009380000000000091 0.7026666666666668 0.7026666666666668
probs:  [0.09003932889005797, 0.2908627957195927, 0.16950820086280002, 0.44958967452754917]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7333333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  62 total reward:  0.17999999999999927  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.009780000000000098 0.7026666666666668 0.7026666666666668
probs:  [0.09012092819699948, 0.2904492110316609, 0.16943122668726004, 0.44999863408407964]
maxi score, test score, baseline:  -0.009780000000000098 0.7026666666666668 0.7026666666666668
probs:  [0.09012092819699948, 0.2904492110316609, 0.16943122668726004, 0.44999863408407964]
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.241]
 [0.249]
 [0.247]
 [0.259]
 [0.25 ]
 [0.252]] [[-0.596]
 [-0.416]
 [-0.549]
 [-0.886]
 [-0.862]
 [-0.801]
 [-0.789]] [[0.25 ]
 [0.241]
 [0.249]
 [0.247]
 [0.259]
 [0.25 ]
 [0.252]]
maxi score, test score, baseline:  -0.009780000000000098 0.7026666666666668 0.7026666666666668
probs:  [0.09012092819699948, 0.2904492110316609, 0.16943122668726004, 0.44999863408407964]
siam score:  -0.74913055
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
probs:  [0.09012092819699948, 0.2904492110316609, 0.16943122668726004, 0.44999863408407964]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
probs:  [0.09011412296773583, 0.29045464067364496, 0.16946644234024935, 0.4499647940183698]
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
probs:  [0.09011412296773583, 0.29045464067364496, 0.16946644234024935, 0.4499647940183698]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 9.083998203277587e-05
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
probs:  [0.09011412296773583, 0.29045464067364496, 0.16946644234024935, 0.4499647940183698]
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
probs:  [0.09011412296773583, 0.29045464067364496, 0.16946644234024935, 0.4499647940183698]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[0.375]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]] [[0.396]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]]
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
probs:  [0.09011412296773583, 0.29045464067364496, 0.16946644234024935, 0.4499647940183698]
actor:  1 policy actor:  1  step number:  40 total reward:  0.54  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.262]
 [0.239]
 [0.309]
 [0.282]
 [0.262]
 [0.29 ]] [[1.583]
 [1.333]
 [1.827]
 [2.242]
 [1.403]
 [1.333]
 [1.503]] [[-0.298]
 [-0.346]
 [-0.287]
 [-0.147]
 [-0.314]
 [-0.346]
 [-0.289]]
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
probs:  [0.09010735385969174, 0.2904600414958373, 0.1695014710734033, 0.44993113357106757]
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
probs:  [0.09010735385969174, 0.2904600414958373, 0.1695014710734033, 0.44993113357106757]
siam score:  -0.74651164
Printing some Q and Qe and total Qs values:  [[ 0.434]
 [ 0.434]
 [-0.024]
 [ 0.434]
 [ 0.434]
 [ 0.434]
 [ 0.203]] [[1.914]
 [1.914]
 [2.491]
 [1.914]
 [1.914]
 [1.914]
 [2.458]] [[0.278]
 [0.278]
 [0.25 ]
 [0.278]
 [0.278]
 [0.278]
 [0.327]]
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
probs:  [0.09010062058603874, 0.29046541372708784, 0.16953631437099986, 0.4498976513158735]
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
probs:  [0.0900939228629772, 0.29047075759382973, 0.16957097370164345, 0.44986434584154955]
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
probs:  [0.0900939228629772, 0.29047075759382973, 0.16957097370164345, 0.44986434584154955]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
probs:  [0.08918229313674442, 0.29478668476782965, 0.17073279126825708, 0.44529823082716885]
actor:  1 policy actor:  1  step number:  29 total reward:  0.7733333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.012713333333333425 0.7026666666666668 0.7026666666666668
probs:  [0.08857411796295869, 0.29960088496091514, 0.1695675492536643, 0.4422574478224619]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.744]
 [0.688]
 [0.676]
 [0.694]
 [0.694]
 [0.682]] [[0.867]
 [0.852]
 [0.641]
 [0.093]
 [1.398]
 [1.398]
 [0.822]] [[0.702]
 [0.744]
 [0.688]
 [0.676]
 [0.694]
 [0.694]
 [0.682]]
using explorer policy with actor:  1
siam score:  -0.7519078
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2254110682048835
actor:  1 policy actor:  1  step number:  31 total reward:  0.7733333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.012713333333333429 0.7026666666666668 0.7026666666666668
probs:  [0.0878333378524432, 0.3050312285737661, 0.16858298477567962, 0.4385524487981111]
maxi score, test score, baseline:  -0.012713333333333429 0.7026666666666668 0.7026666666666668
siam score:  -0.7502103
actor:  1 policy actor:  1  step number:  40 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.012713333333333429 0.7026666666666668 0.7026666666666668
probs:  [0.08697414463966982, 0.3091591232361227, 0.16961726324280116, 0.4342494688814063]
maxi score, test score, baseline:  -0.015166666666666759 0.7026666666666668 0.7026666666666668
probs:  [0.08697414463966982, 0.3091591232361227, 0.16961726324280116, 0.4342494688814063]
maxi score, test score, baseline:  -0.015166666666666759 0.7026666666666668 0.7026666666666668
probs:  [0.08697414463966982, 0.3091591232361227, 0.16961726324280116, 0.4342494688814063]
maxi score, test score, baseline:  -0.015166666666666759 0.7026666666666668 0.7026666666666668
probs:  [0.08697414463966982, 0.3091591232361227, 0.16961726324280116, 0.4342494688814063]
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.32 ]
 [0.271]
 [0.216]
 [0.32 ]
 [0.262]
 [0.261]] [[1.571]
 [2.714]
 [2.564]
 [1.439]
 [2.714]
 [3.063]
 [2.399]] [[ 0.005]
 [ 0.363]
 [ 0.294]
 [-0.081]
 [ 0.363]
 [ 0.445]
 [ 0.238]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.047]
 [-0.035]
 [-0.047]
 [-0.047]
 [-0.047]
 [-0.047]
 [-0.047]] [[0.114]
 [1.079]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[-0.3  ]
 [-0.127]
 [-0.3  ]
 [-0.3  ]
 [-0.3  ]
 [-0.3  ]
 [-0.3  ]]
Printing some Q and Qe and total Qs values:  [[ 0.186]
 [ 0.249]
 [-0.031]
 [ 0.186]
 [ 0.186]
 [-0.007]
 [ 0.286]] [[2.006]
 [0.988]
 [1.5  ]
 [2.006]
 [2.006]
 [1.459]
 [0.131]] [[ 0.091]
 [-0.016]
 [-0.211]
 [ 0.091]
 [ 0.091]
 [-0.193]
 [-0.122]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.015166666666666759 0.7026666666666668 0.7026666666666668
probs:  [0.08759850616492632, 0.30613893498900085, 0.1688860112563056, 0.4373765475897673]
maxi score, test score, baseline:  -0.015166666666666759 0.7026666666666668 0.7026666666666668
probs:  [0.08759850616492632, 0.30613893498900085, 0.1688860112563056, 0.4373765475897673]
maxi score, test score, baseline:  -0.01807333333333343 0.7026666666666668 0.7026666666666668
probs:  [0.08759850616492632, 0.30613893498900085, 0.1688860112563056, 0.4373765475897673]
maxi score, test score, baseline:  -0.01807333333333343 0.7026666666666668 0.7026666666666668
probs:  [0.08759234184237266, 0.3061404978506855, 0.16892124640559977, 0.43734591390134214]
maxi score, test score, baseline:  -0.01807333333333343 0.7026666666666668 0.7026666666666668
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.01807333333333343 0.7026666666666668 0.7026666666666668
probs:  [0.08759234184237266, 0.3061404978506855, 0.16892124640559977, 0.43734591390134214]
maxi score, test score, baseline:  -0.01807333333333343 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.01807333333333343 0.7026666666666668 0.7026666666666668
probs:  [0.08759234184237266, 0.3061404978506855, 0.16892124640559977, 0.43734591390134214]
maxi score, test score, baseline:  -0.01807333333333343 0.7026666666666668 0.7026666666666668
probs:  [0.08759234184237266, 0.3061404978506855, 0.16892124640559977, 0.43734591390134214]
maxi score, test score, baseline:  -0.01807333333333343 0.7026666666666668 0.7026666666666668
actor:  0 policy actor:  0  step number:  46 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.017673333333333423 0.7026666666666668 0.7026666666666668
actor:  1 policy actor:  1  step number:  36 total reward:  0.6600000000000003  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.017673333333333423 0.7026666666666668 0.7026666666666668
probs:  [0.08758620987602198, 0.3061420525089918, 0.16895629660745934, 0.43731544100752684]
maxi score, test score, baseline:  -0.017673333333333423 0.7026666666666668 0.7026666666666668
probs:  [0.08758620987602198, 0.3061420525089918, 0.16895629660745934, 0.43731544100752684]
maxi score, test score, baseline:  -0.017673333333333423 0.7026666666666668 0.7026666666666668
probs:  [0.08758620987602198, 0.3061420525089918, 0.16895629660745934, 0.43731544100752684]
maxi score, test score, baseline:  -0.017673333333333423 0.7026666666666668 0.7026666666666668
probs:  [0.08758011001178713, 0.3061435990283395, 0.1689911633142417, 0.4372851276456318]
maxi score, test score, baseline:  -0.017673333333333423 0.7026666666666668 0.7026666666666668
probs:  [0.08758011001178713, 0.3061435990283395, 0.1689911633142417, 0.4372851276456318]
actor:  0 policy actor:  0  step number:  55 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.015006666666666758 0.7026666666666668 0.7026666666666668
probs:  [0.08758011001178713, 0.3061435990283395, 0.1689911633142417, 0.4372851276456318]
maxi score, test score, baseline:  -0.015006666666666758 0.7026666666666668 0.7026666666666668
probs:  [0.08758011001178713, 0.3061435990283395, 0.1689911633142417, 0.4372851276456318]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.015006666666666758 0.7026666666666668 0.7026666666666668
probs:  [0.08758011001178713, 0.3061435990283395, 0.1689911633142417, 0.4372851276456318]
maxi score, test score, baseline:  -0.015006666666666758 0.7026666666666668 0.7026666666666668
probs:  [0.08758011001178713, 0.3061435990283395, 0.1689911633142417, 0.4372851276456318]
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.318]
 [0.318]
 [0.417]
 [0.318]
 [0.356]
 [0.513]] [[1.451]
 [1.451]
 [1.451]
 [2.825]
 [1.451]
 [3.127]
 [3.336]] [[0.185]
 [0.185]
 [0.185]
 [0.609]
 [0.185]
 [0.658]
 [0.799]]
maxi score, test score, baseline:  -0.015006666666666758 0.7026666666666668 0.7026666666666668
probs:  [0.08758011001178713, 0.3061435990283395, 0.1689911633142417, 0.4372851276456318]
actor:  1 policy actor:  1  step number:  30 total reward:  0.8066666666666668  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]] [[0.955]
 [0.955]
 [0.955]
 [0.955]
 [0.955]
 [0.955]
 [0.955]] [[0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.038]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]] [[0.025]
 [0.935]
 [0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]] [[-0.08 ]
 [ 0.334]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]]
maxi score, test score, baseline:  -0.017846666666666757 0.7026666666666668 0.7026666666666668
probs:  [0.08630274754788435, 0.3016730058846023, 0.18112570066218356, 0.43089854590532983]
maxi score, test score, baseline:  -0.017846666666666757 0.7026666666666668 0.7026666666666668
probs:  [0.08630274754788435, 0.3016730058846023, 0.18112570066218356, 0.43089854590532983]
maxi score, test score, baseline:  -0.017846666666666757 0.7026666666666668 0.7026666666666668
probs:  [0.08630274754788435, 0.3016730058846023, 0.18112570066218356, 0.43089854590532983]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.017846666666666757 0.7026666666666668 0.7026666666666668
probs:  [0.08630274754788435, 0.3016730058846023, 0.18112570066218356, 0.43089854590532983]
actor:  1 policy actor:  1  step number:  48 total reward:  0.6466666666666671  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.017846666666666757 0.7026666666666668 0.7026666666666668
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.835]
 [0.835]
 [0.601]
 [0.706]
 [0.626]
 [0.895]] [[1.621]
 [1.175]
 [1.175]
 [0.516]
 [1.034]
 [0.71 ]
 [1.521]] [[0.859]
 [0.835]
 [0.835]
 [0.601]
 [0.706]
 [0.626]
 [0.895]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.017846666666666757 0.7026666666666668 0.7026666666666668
probs:  [0.08630274754788435, 0.3016730058846023, 0.18112570066218356, 0.43089854590532983]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.017846666666666757 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.017846666666666757 0.7026666666666668 0.7026666666666668
probs:  [0.08630274754788435, 0.3016730058846023, 0.18112570066218356, 0.43089854590532983]
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.389]
 [0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.432]] [[2.066]
 [2.124]
 [2.066]
 [2.066]
 [2.066]
 [2.066]
 [2.065]] [[0.423]
 [0.568]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.569]]
maxi score, test score, baseline:  -0.017846666666666757 0.7026666666666668 0.7026666666666668
actor:  0 policy actor:  1  step number:  49 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.667
siam score:  -0.73371387
from probs:  [0.08630274754788435, 0.3016730058846023, 0.18112570066218356, 0.43089854590532983]
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.145]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.086]] [[ 0.055]
 [-0.759]
 [-0.52 ]
 [-0.52 ]
 [-0.52 ]
 [-0.52 ]
 [-0.212]] [[0.038]
 [0.145]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.086]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.039]
 [-0.039]
 [-0.036]
 [-0.051]
 [-0.039]
 [-0.035]] [[1.406]
 [0.   ]
 [0.   ]
 [0.301]
 [0.882]
 [0.   ]
 [0.88 ]] [[ 0.171]
 [-0.338]
 [-0.338]
 [-0.232]
 [-0.037]
 [-0.338]
 [-0.028]]
maxi score, test score, baseline:  -0.01494000000000009 0.7026666666666668 0.7026666666666668
probs:  [0.08630442128651766, 0.30166537341618155, 0.1811232770753924, 0.43090692822190835]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[2.609]
 [2.609]
 [2.609]
 [2.609]
 [2.609]
 [2.609]
 [2.609]] [[0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]]
maxi score, test score, baseline:  -0.01494000000000009 0.7026666666666668 0.7026666666666668
probs:  [0.08630442128651766, 0.30166537341618155, 0.1811232770753924, 0.43090692822190835]
maxi score, test score, baseline:  -0.01494000000000009 0.7026666666666668 0.7026666666666668
actor:  0 policy actor:  0  step number:  57 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.012353333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08630442128651766, 0.30166537341618155, 0.1811232770753924, 0.43090692822190835]
siam score:  -0.73838806
maxi score, test score, baseline:  -0.012353333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08630442128651766, 0.30166537341618155, 0.1811232770753924, 0.43090692822190835]
maxi score, test score, baseline:  -0.012353333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08630442128651766, 0.30166537341618155, 0.1811232770753924, 0.43090692822190835]
actor:  1 policy actor:  1  step number:  36 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.012353333333333424 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.012353333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08630442128651766, 0.30166537341618155, 0.1811232770753924, 0.43090692822190835]
maxi score, test score, baseline:  -0.012353333333333424 0.7026666666666668 0.7026666666666668
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.012353333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08630442128651766, 0.30166537341618155, 0.1811232770753924, 0.43090692822190835]
siam score:  -0.7476107
actor:  1 policy actor:  1  step number:  30 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.012353333333333424 0.7026666666666668 0.7026666666666668
line 256 mcts: sample exp_bonus 0.6041730301803073
maxi score, test score, baseline:  -0.012353333333333424 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.012353333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08705125618123424, 0.2982597080177712, 0.18004185423553043, 0.434647181565464]
maxi score, test score, baseline:  -0.012353333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08705125618123424, 0.2982597080177712, 0.18004185423553043, 0.434647181565464]
siam score:  -0.7463765
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.012353333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08705125618123424, 0.2982597080177712, 0.18004185423553043, 0.434647181565464]
actor:  1 policy actor:  1  step number:  41 total reward:  0.706666666666667  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  -0.012353333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08703653206931292, 0.2983138157287806, 0.18007608323560761, 0.4345735689662989]
maxi score, test score, baseline:  -0.012353333333333424 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.012353333333333423 0.7026666666666668 0.7026666666666668
probs:  [0.08703081548758716, 0.2983171192582615, 0.18010690253878156, 0.4345451627153698]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.012353333333333423 0.7026666666666668 0.7026666666666668
probs:  [0.08703081548758716, 0.2983171192582615, 0.18010690253878156, 0.4345451627153698]
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.116]
 [0.308]
 [0.116]
 [0.116]
 [0.116]
 [0.116]] [[0.922]
 [0.922]
 [0.973]
 [0.922]
 [0.922]
 [0.922]
 [0.922]] [[-0.359]
 [-0.359]
 [-0.159]
 [-0.359]
 [-0.359]
 [-0.359]
 [-0.359]]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.503]] [[1.118]
 [1.233]
 [1.233]
 [1.233]
 [1.233]
 [1.233]
 [0.998]] [[-0.082]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.13 ]]
maxi score, test score, baseline:  -0.012353333333333423 0.7026666666666668 0.7026666666666668
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.103]] [[-0.206]
 [-0.206]
 [-0.206]
 [-0.206]
 [-0.206]
 [-0.206]
 [-0.206]]
maxi score, test score, baseline:  -0.012353333333333423 0.7026666666666668 0.7026666666666668
probs:  [0.08703081548758716, 0.2983171192582615, 0.18010690253878156, 0.4345451627153698]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.012353333333333423 0.7026666666666668 0.7026666666666668
probs:  [0.08622631520662306, 0.29555614891176335, 0.18769478869093673, 0.43052274719067685]
actor:  1 policy actor:  1  step number:  41 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.012353333333333423 0.7026666666666668 0.7026666666666668
first move QE:  -0.09341577430434622
maxi score, test score, baseline:  -0.012353333333333423 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.012353333333333423 0.7026666666666668 0.7026666666666668
probs:  [0.08635398080729022, 0.2954650356896419, 0.18702398667060907, 0.4311569968324587]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.012353333333333423 0.7026666666666668 0.7026666666666668
probs:  [0.08635398080729022, 0.2954650356896419, 0.18702398667060907, 0.4311569968324587]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 1.373549729518773
siam score:  -0.7502228
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using another actor
from probs:  [0.08635398080729022, 0.2954650356896419, 0.18702398667060907, 0.4311569968324587]
maxi score, test score, baseline:  -0.012353333333333423 0.7026666666666668 0.7026666666666668
probs:  [0.08634819702260893, 0.2954691634989783, 0.1870543768025559, 0.4311282626758568]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.012353333333333423 0.7026666666666668 0.7026666666666668
probs:  [0.08634819702260893, 0.2954691634989783, 0.1870543768025559, 0.4311282626758568]
actor:  0 policy actor:  0  step number:  59 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.012246666666666756 0.7026666666666668 0.7026666666666668
probs:  [0.08634819702260893, 0.2954691634989783, 0.1870543768025559, 0.4311282626758568]
maxi score, test score, baseline:  -0.012246666666666756 0.7026666666666668 0.7026666666666668
probs:  [0.08634819702260893, 0.2954691634989783, 0.1870543768025559, 0.4311282626758568]
maxi score, test score, baseline:  -0.012246666666666756 0.7026666666666668 0.7026666666666668
probs:  [0.08634819702260893, 0.2954691634989783, 0.1870543768025559, 0.4311282626758568]
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.993]
 [0.996]
 [0.953]
 [0.953]
 [0.982]
 [0.992]] [[0.601]
 [0.374]
 [0.614]
 [0.308]
 [0.308]
 [0.248]
 [0.471]] [[0.622]
 [0.993]
 [0.996]
 [0.953]
 [0.953]
 [0.982]
 [0.992]]
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.951]
 [0.837]
 [0.868]
 [0.871]
 [0.924]
 [0.902]] [[ 0.051]
 [ 0.169]
 [-0.241]
 [-0.104]
 [-0.342]
 [-0.222]
 [-0.21 ]] [[0.929]
 [0.951]
 [0.837]
 [0.868]
 [0.871]
 [0.924]
 [0.902]]
actor:  0 policy actor:  1  step number:  60 total reward:  0.2866666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.009673333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08634819702260893, 0.2954691634989783, 0.1870543768025559, 0.4311282626758568]
maxi score, test score, baseline:  -0.009673333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08634819702260893, 0.2954691634989783, 0.1870543768025559, 0.4311282626758568]
maxi score, test score, baseline:  -0.009673333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08634819702260893, 0.2954691634989783, 0.1870543768025559, 0.4311282626758568]
actor:  1 policy actor:  1  step number:  34 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.5
siam score:  -0.745264
maxi score, test score, baseline:  -0.009673333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08634819702260893, 0.2954691634989783, 0.1870543768025559, 0.4311282626758568]
maxi score, test score, baseline:  -0.009673333333333426 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.009673333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08634819702260893, 0.2954691634989783, 0.1870543768025559, 0.4311282626758568]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.009673333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08634819702260893, 0.2954691634989783, 0.1870543768025559, 0.4311282626758568]
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.198]] [[1.726]
 [1.726]
 [1.726]
 [1.726]
 [1.726]
 [1.726]
 [1.612]] [[0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.359]]
maxi score, test score, baseline:  -0.009673333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08634819702260893, 0.2954691634989783, 0.1870543768025559, 0.4311282626758568]
actor:  1 policy actor:  1  step number:  41 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.009673333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08637882911424682, 0.29533422490369116, 0.18700527518004398, 0.431281670802018]
maxi score, test score, baseline:  -0.009673333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08637882911424682, 0.29533422490369116, 0.18700527518004398, 0.431281670802018]
from probs:  [0.08637882911424682, 0.29533422490369116, 0.18700527518004398, 0.431281670802018]
maxi score, test score, baseline:  -0.009673333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08637882911424682, 0.29533422490369116, 0.18700527518004398, 0.431281670802018]
maxi score, test score, baseline:  -0.009673333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08637882911424682, 0.29533422490369116, 0.18700527518004398, 0.431281670802018]
line 256 mcts: sample exp_bonus 1.0729698451364518
actor:  1 policy actor:  1  step number:  72 total reward:  0.4733333333333338  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.009673333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08524461868107128, 0.30033057854555245, 0.18882335450669466, 0.42560144826668156]
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.089]
 [0.081]
 [0.081]
 [0.066]
 [0.065]
 [0.064]] [[1.008]
 [1.071]
 [1.008]
 [1.008]
 [0.763]
 [0.827]
 [0.801]] [[-0.303]
 [-0.264]
 [-0.303]
 [-0.303]
 [-0.441]
 [-0.41 ]
 [-0.424]]
maxi score, test score, baseline:  -0.009673333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08524461868107128, 0.30033057854555245, 0.18882335450669466, 0.42560144826668156]
maxi score, test score, baseline:  -0.009673333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08524461868107128, 0.30033057854555245, 0.18882335450669466, 0.42560144826668156]
maxi score, test score, baseline:  -0.009673333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08524461868107128, 0.30033057854555245, 0.18882335450669466, 0.42560144826668156]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.606]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.59 ]] [[1.417]
 [1.578]
 [1.088]
 [1.088]
 [1.088]
 [1.088]
 [1.085]] [[0.603]
 [0.606]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.59 ]]
maxi score, test score, baseline:  -0.009673333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08524461868107128, 0.30033057854555245, 0.18882335450669466, 0.42560144826668156]
maxi score, test score, baseline:  -0.009673333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08524461868107128, 0.30033057854555245, 0.18882335450669466, 0.42560144826668156]
Printing some Q and Qe and total Qs values:  [[-0.042]
 [ 0.178]
 [ 0.157]
 [ 0.147]
 [ 0.19 ]
 [ 0.167]
 [ 0.133]] [[2.311]
 [1.883]
 [1.817]
 [1.475]
 [1.732]
 [1.77 ]
 [1.829]] [[0.225]
 [0.231]
 [0.182]
 [0.016]
 [0.173]
 [0.169]
 [0.166]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  54 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.00926000000000009 0.7026666666666668 0.7026666666666668
probs:  [0.08524461868107128, 0.30033057854555245, 0.18882335450669466, 0.42560144826668156]
from probs:  [0.08524461868107128, 0.30033057854555245, 0.18882335450669466, 0.42560144826668156]
maxi score, test score, baseline:  -0.00926000000000009 0.7026666666666668 0.7026666666666668
actor:  0 policy actor:  0  step number:  48 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.216]
 [0.103]] [[1.859]
 [2.097]
 [2.097]
 [2.097]
 [2.097]
 [2.799]
 [2.291]] [[0.183]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.557]
 [0.269]]
maxi score, test score, baseline:  -0.006526666666666757 0.7026666666666668 0.7026666666666668
probs:  [0.08523897421662192, 0.3003335971883558, 0.18885401138466776, 0.42557341721035447]
maxi score, test score, baseline:  -0.006526666666666757 0.7026666666666668 0.7026666666666668
probs:  [0.08523897421662192, 0.3003335971883558, 0.18885401138466776, 0.42557341721035447]
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.79 ]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[0.29 ]
 [0.399]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]] [[0.683]
 [0.79 ]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]]
line 256 mcts: sample exp_bonus 0.2148527413397615
actor:  0 policy actor:  0  step number:  48 total reward:  0.3399999999999993  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  -0.0063400000000000895 0.7026666666666668 0.7026666666666668
probs:  [0.08523897421662192, 0.3003335971883558, 0.18885401138466776, 0.42557341721035447]
maxi score, test score, baseline:  -0.0063400000000000895 0.7026666666666668 0.7026666666666668
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.234]
 [0.253]
 [0.234]
 [0.234]
 [0.234]
 [0.264]] [[1.672]
 [1.672]
 [1.765]
 [1.672]
 [1.672]
 [1.672]
 [1.468]] [[-0.194]
 [-0.194]
 [-0.144]
 [-0.194]
 [-0.194]
 [-0.194]
 [-0.232]]
actor:  0 policy actor:  1  step number:  34 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08523897421662192, 0.3003335971883558, 0.18885401138466776, 0.42557341721035447]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.582]] [[0.767]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [0.834]] [[-0.099]
 [ 0.116]
 [ 0.116]
 [ 0.116]
 [ 0.116]
 [ 0.116]
 [ 0.131]]
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08523897421662192, 0.3003335971883558, 0.18885401138466776, 0.42557341721035447]
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08523897421662192, 0.3003335971883558, 0.18885401138466776, 0.42557341721035447]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.166]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]] [[-0.036]
 [ 0.841]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]] [[-0.418]
 [-0.045]
 [-0.418]
 [-0.418]
 [-0.418]
 [-0.418]
 [-0.418]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.235]
 [0.211]
 [0.211]
 [0.212]
 [0.204]
 [0.217]] [[2.621]
 [1.901]
 [1.824]
 [1.824]
 [2.688]
 [2.396]
 [2.708]] [[ 0.171]
 [-0.051]
 [-0.101]
 [-0.101]
 [ 0.188]
 [ 0.083]
 [ 0.199]]
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08433503696825786, 0.2997410640164767, 0.19486993631099442, 0.42105396270427103]
siam score:  -0.7470216
actor:  1 policy actor:  1  step number:  39 total reward:  0.6800000000000004  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7464988
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08433503696825786, 0.2997410640164767, 0.19486993631099442, 0.42105396270427103]
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08433503696825786, 0.2997410640164767, 0.19486993631099442, 0.42105396270427103]
actor:  1 policy actor:  1  step number:  30 total reward:  0.8066666666666668  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1202255059548174
first move QE:  -0.0963920647672416
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08313061637391414, 0.30975310118381394, 0.19208411394127953, 0.4150321685009923]
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [ 0.146]] [[1.69 ]
 [1.69 ]
 [1.69 ]
 [1.69 ]
 [1.69 ]
 [1.69 ]
 [0.729]] [[0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.031]]
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08312564729393325, 0.30975376415494976, 0.19211308001563326, 0.4150075085354837]
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08312564729393325, 0.30975376415494976, 0.19211308001563326, 0.4150075085354837]
actor:  1 policy actor:  1  step number:  27 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08258127831309973, 0.31427922946892833, 0.19085369023898816, 0.4122858019789838]
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08258127831309973, 0.31427922946892833, 0.19085369023898816, 0.4122858019789838]
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[0.304]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]] [[0.23 ]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]]
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08257949391288594, 0.3142871831463548, 0.19085645644074722, 0.4122768665000121]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08257949391288594, 0.3142871831463548, 0.19085645644074722, 0.4122768665000121]
line 256 mcts: sample exp_bonus 1.005720368019113
line 256 mcts: sample exp_bonus 1.4790868261419
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08257949391288594, 0.3142871831463548, 0.19085645644074722, 0.4122768665000121]
maxi score, test score, baseline:  -0.0028600000000000934 0.7026666666666668 0.7026666666666668
probs:  [0.08257949391288594, 0.3142871831463548, 0.19085645644074722, 0.4122768665000121]
actor:  0 policy actor:  0  step number:  56 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.003540000000000093 0.7026666666666668 0.7026666666666668
probs:  [0.08257949391288594, 0.3142871831463548, 0.19085645644074722, 0.4122768665000121]
siam score:  -0.7347439
maxi score, test score, baseline:  -0.003540000000000093 0.7026666666666668 0.7026666666666668
probs:  [0.08257949391288594, 0.3142871831463548, 0.19085645644074722, 0.4122768665000121]
maxi score, test score, baseline:  -0.003540000000000093 0.7026666666666668 0.7026666666666668
probs:  [0.08257949391288594, 0.31428718314635473, 0.19085645644074722, 0.4122768665000121]
maxi score, test score, baseline:  -0.003540000000000093 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.003540000000000093 0.7026666666666668 0.7026666666666668
probs:  [0.08257471246181033, 0.31428676039644077, 0.19088538464778557, 0.4122531424939632]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.003540000000000093 0.7026666666666668 0.7026666666666668
probs:  [0.08257471246181033, 0.31428676039644077, 0.19088538464778557, 0.4122531424939632]
maxi score, test score, baseline:  -0.006140000000000087 0.7026666666666668 0.7026666666666668
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.31 ]
 [0.237]
 [0.161]
 [0.098]
 [0.123]
 [0.218]] [[2.383]
 [2.025]
 [2.087]
 [1.995]
 [2.058]
 [1.825]
 [1.973]] [[0.484]
 [0.47 ]
 [0.456]
 [0.397]
 [0.388]
 [0.332]
 [0.415]]
maxi score, test score, baseline:  -0.006140000000000087 0.7026666666666668 0.7026666666666668
probs:  [0.08257471246181033, 0.31428676039644077, 0.19088538464778557, 0.4122531424939632]
maxi score, test score, baseline:  -0.006140000000000087 0.7026666666666668 0.7026666666666668
probs:  [0.08257471246181033, 0.31428676039644077, 0.19088538464778557, 0.4122531424939632]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.006140000000000087 0.7026666666666668 0.7026666666666668
actor:  1 policy actor:  1  step number:  46 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  0.7266666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.006140000000000087 0.7026666666666668 0.7026666666666668
probs:  [0.08263884850814414, 0.31400091709756195, 0.19078592756864712, 0.41257430682564683]
maxi score, test score, baseline:  -0.006140000000000087 0.7026666666666668 0.7026666666666668
probs:  [0.08263884850814414, 0.31400091709756195, 0.19078592756864712, 0.41257430682564683]
maxi score, test score, baseline:  -0.006140000000000087 0.7026666666666668 0.7026666666666668
siam score:  -0.7250742
maxi score, test score, baseline:  -0.006140000000000087 0.7026666666666668 0.7026666666666668
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.189]
 [0.254]
 [0.472]
 [0.418]
 [0.038]
 [0.294]] [[2.548]
 [2.334]
 [2.563]
 [2.712]
 [2.447]
 [2.475]
 [2.5  ]] [[0.636]
 [0.252]
 [0.426]
 [0.678]
 [0.495]
 [0.207]
 [0.424]]
siam score:  -0.72497195
maxi score, test score, baseline:  -0.006140000000000087 0.7026666666666668 0.7026666666666668
probs:  [0.08263884850814414, 0.31400091709756195, 0.19078592756864712, 0.41257430682564683]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.006140000000000087 0.7026666666666668 0.7026666666666668
probs:  [0.08263884850814414, 0.31400091709756195, 0.19078592756864712, 0.41257430682564683]
actor:  1 policy actor:  1  step number:  55 total reward:  0.5200000000000004  reward:  1.0 rdn_beta:  0.167
from probs:  [0.08263884850814414, 0.31400091709756195, 0.19078592756864712, 0.41257430682564683]
maxi score, test score, baseline:  -0.006140000000000087 0.7026666666666668 0.7026666666666668
probs:  [0.08263408358357288, 0.31400056468586274, 0.1908146876069683, 0.41255066412359603]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.339]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]] [[1.411]
 [1.999]
 [1.411]
 [1.411]
 [1.411]
 [1.411]
 [1.411]] [[0.462]
 [0.634]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.006140000000000087 0.7026666666666668 0.7026666666666668
actor:  1 policy actor:  1  step number:  45 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.008873333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08263408358357288, 0.31400056468586274, 0.19081468760696835, 0.41255066412359603]
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.12 ]] [[2.354]
 [2.354]
 [2.354]
 [2.354]
 [2.354]
 [2.354]
 [4.438]] [[0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.553]]
using another actor
maxi score, test score, baseline:  -0.008873333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.0826293430054981, 0.31400021407481954, 0.1908433006951833, 0.41252714222449893]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  33 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.011193333333333423 0.7026666666666668 0.7026666666666668
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  32 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08372897614968254, 0.309100455499799, 0.1891369716252592, 0.4180335967252594]
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08372897614968254, 0.309100455499799, 0.1891369716252592, 0.4180335967252594]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6466666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08372897614968254, 0.309100455499799, 0.1891369716252592, 0.4180335967252594]
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08372897614968254, 0.309100455499799, 0.1891369716252592, 0.4180335967252594]
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08372897614968254, 0.309100455499799, 0.1891369716252592, 0.4180335967252594]
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08372897614968254, 0.309100455499799, 0.1891369716252592, 0.4180335967252594]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08372897614968254, 0.309100455499799, 0.1891369716252592, 0.4180335967252594]
actor:  1 policy actor:  1  step number:  37 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.295]] [[2.032]
 [2.032]
 [2.032]
 [2.032]
 [2.032]
 [2.032]
 [2.25 ]] [[0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.48 ]]
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
actor:  1 policy actor:  1  step number:  30 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08428299935068836, 0.30887227287215535, 0.18605945069717358, 0.4207852770799827]
actor:  1 policy actor:  1  step number:  50 total reward:  0.6733333333333337  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.044]
 [ 0.234]
 [ 0.262]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]] [[1.778]
 [1.509]
 [1.704]
 [1.778]
 [1.778]
 [1.778]
 [1.778]] [[0.196]
 [0.324]
 [0.387]
 [0.196]
 [0.196]
 [0.196]
 [0.196]]
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
siam score:  -0.73241836
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08428299935068836, 0.30887227287215535, 0.18605945069717358, 0.4207852770799827]
actor:  1 policy actor:  1  step number:  62 total reward:  0.24666666666666592  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08428299935068836, 0.30887227287215535, 0.18605945069717358, 0.4207852770799827]
siam score:  -0.73392886
actor:  1 policy actor:  1  step number:  34 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08412998874675265, 0.31012839051058116, 0.18572134197765536, 0.4200202787650108]
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08412998874675265, 0.31012839051058116, 0.18572134197765536, 0.4200202787650108]
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.641]
 [0.451]
 [0.557]
 [0.557]
 [0.553]
 [0.621]] [[1.458]
 [1.072]
 [1.676]
 [1.382]
 [1.382]
 [1.94 ]
 [1.649]] [[0.733]
 [0.648]
 [0.657]
 [0.662]
 [0.662]
 [0.795]
 [0.774]]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.684]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.764]] [[0.926]
 [1.617]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [1.14 ]] [[0.232]
 [0.526]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.447]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
siam score:  -0.7295398
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
maxi score, test score, baseline:  -0.014300000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
actor:  0 policy actor:  1  step number:  51 total reward:  0.5466666666666671  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.014593333333333422 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.014593333333333422 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.014593333333333422 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.014593333333333422 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
actor:  1 policy actor:  1  step number:  46 total reward:  0.526666666666667  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.01798000000000009 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.02136666666666676 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
first move QE:  -0.09456693488890518
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.306]
 [0.299]
 [0.205]
 [0.205]
 [0.237]
 [0.205]] [[2.105]
 [2.461]
 [2.325]
 [2.105]
 [2.105]
 [3.028]
 [2.105]] [[0.251]
 [0.437]
 [0.383]
 [0.251]
 [0.251]
 [0.606]
 [0.251]]
line 256 mcts: sample exp_bonus 1.662046736799707
maxi score, test score, baseline:  -0.02136666666666676 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
from probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
first move QE:  -0.09452152223371785
maxi score, test score, baseline:  -0.02136666666666676 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
actor:  1 policy actor:  1  step number:  54 total reward:  0.486666666666667  reward:  1.0 rdn_beta:  0.333
siam score:  -0.74429315
maxi score, test score, baseline:  -0.02136666666666676 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
actor:  0 policy actor:  1  step number:  36 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.021273333333333422 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
line 256 mcts: sample exp_bonus 1.6945663053297915
maxi score, test score, baseline:  -0.021273333333333422 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
UNIT TEST: sample policy line 217 mcts : [0.531 0.02  0.02  0.    0.143 0.    0.286]
UNIT TEST: sample policy line 217 mcts : [0.102 0.306 0.061 0.122 0.143 0.082 0.184]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.566]
 [0.593]
 [0.566]
 [0.566]
 [0.592]
 [0.566]] [[-0.1  ]
 [ 0.124]
 [-0.   ]
 [ 0.124]
 [ 0.124]
 [-0.011]
 [ 0.124]] [[0.047]
 [0.133]
 [0.097]
 [0.133]
 [0.133]
 [0.091]
 [0.133]]
actor:  1 policy actor:  1  step number:  67 total reward:  0.4666666666666668  reward:  1.0 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  -0.021273333333333422 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.021273333333333422 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
from probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.502]
 [0.567]
 [0.514]
 [0.119]
 [0.378]
 [0.538]] [[ 0.968]
 [ 0.836]
 [ 1.473]
 [ 0.554]
 [-0.113]
 [ 0.801]
 [ 1.007]] [[0.243]
 [0.502]
 [0.567]
 [0.514]
 [0.119]
 [0.378]
 [0.538]]
maxi score, test score, baseline:  -0.024660000000000088 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
maxi score, test score, baseline:  -0.024660000000000088 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
actor:  0 policy actor:  1  step number:  43 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.02474000000000009 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
maxi score, test score, baseline:  -0.02474000000000009 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
maxi score, test score, baseline:  -0.02474000000000009 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
maxi score, test score, baseline:  -0.02474000000000009 0.7026666666666668 0.7026666666666668
actor:  0 policy actor:  0  step number:  69 total reward:  0.19999999999999873  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.02572666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
maxi score, test score, baseline:  -0.02572666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
maxi score, test score, baseline:  -0.02572666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
maxi score, test score, baseline:  -0.02572666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.08377132041749341, 0.31307282444613077, 0.18492878978572663, 0.41822706535064913]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  33 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.02572666666666675 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.02572666666666675 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.02572666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.08376958209315545, 0.31308709495945497, 0.1849249485969404, 0.4182183743504492]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.02572666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.08376958209315545, 0.31308709495945497, 0.1849249485969404, 0.4182183743504492]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[ 0.003]
 [ 0.115]
 [ 0.076]
 [ 0.051]
 [-0.01 ]
 [ 0.006]
 [ 0.046]] [[1.283]
 [1.351]
 [1.456]
 [1.476]
 [1.265]
 [1.061]
 [1.185]] [[-0.136]
 [-0.005]
 [ 0.02 ]
 [ 0.011]
 [-0.157]
 [-0.257]
 [-0.155]]
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08376958209315545, 0.31308709495945497, 0.1849249485969404, 0.4182183743504492]
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.467]
 [0.34 ]
 [0.352]
 [0.39 ]
 [0.338]
 [0.407]] [[0.948]
 [0.935]
 [0.932]
 [0.633]
 [0.868]
 [0.895]
 [0.904]] [[-0.007]
 [ 0.095]
 [-0.034]
 [-0.221]
 [-0.026]
 [-0.061]
 [ 0.014]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08376958209315545, 0.31308709495945497, 0.1849249485969404, 0.4182183743504492]
siam score:  -0.71832854
actor:  1 policy actor:  1  step number:  40 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08376958209315545, 0.31308709495945497, 0.1849249485969404, 0.4182183743504492]
actor:  1 policy actor:  1  step number:  47 total reward:  0.42666666666666675  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 1.4163269270463934
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08376958209315545, 0.31308709495945497, 0.1849249485969404, 0.4182183743504492]
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08376958209315545, 0.31308709495945497, 0.1849249485969404, 0.4182183743504492]
actor:  1 policy actor:  1  step number:  41 total reward:  0.706666666666667  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7105345
siam score:  -0.71004725
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08377129726436305, 0.3130792792669164, 0.184922459562358, 0.41822696390636244]
line 256 mcts: sample exp_bonus 1.4419397771358489
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08377129726436305, 0.3130792792669164, 0.184922459562358, 0.41822696390636244]
Printing some Q and Qe and total Qs values:  [[-0.184]
 [-0.045]
 [ 0.066]
 [-0.045]
 [-0.144]
 [ 0.066]
 [-0.048]] [[1.135]
 [1.426]
 [2.379]
 [1.426]
 [0.175]
 [2.946]
 [1.181]] [[-0.136]
 [ 0.02 ]
 [ 0.304]
 [ 0.02 ]
 [-0.322]
 [ 0.43 ]
 [-0.035]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  0.7133333333333335  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08377970383550504, 0.31307950419921426, 0.18487210287815084, 0.41826868908712983]
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08377970383550504, 0.31307950419921426, 0.18487210287815084, 0.41826868908712983]
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08377970383550504, 0.31307950419921426, 0.18487210287815084, 0.41826868908712983]
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08377970383550504, 0.31307950419921426, 0.18487210287815084, 0.41826868908712983]
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08377970383550504, 0.31307950419921426, 0.18487210287815084, 0.41826868908712983]
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08377970383550504, 0.31307950419921426, 0.18487210287815084, 0.41826868908712983]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08377970383550504, 0.31307950419921426, 0.18487210287815084, 0.41826868908712983]
actor:  1 policy actor:  1  step number:  32 total reward:  0.6600000000000003  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08377356705120743, 0.313079339998975, 0.18490886318193023, 0.4182382297678874]
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08377356705120743, 0.313079339998975, 0.18490886318193023, 0.4182382297678874]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.029113333333333415 0.7026666666666668 0.7026666666666668
probs:  [0.08377356705120743, 0.313079339998975, 0.18490886318193023, 0.4182382297678874]
first move QE:  -0.0920262137562463
actor:  0 policy actor:  1  step number:  36 total reward:  0.6200000000000003  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.03 ]
 [-0.002]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]] [[1.481]
 [2.314]
 [1.481]
 [1.481]
 [1.481]
 [1.481]
 [1.481]] [[0.408]
 [0.63 ]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.029246666666666758 0.7026666666666668 0.7026666666666668
probs:  [0.08463636689377751, 0.3091475159512676, 0.18365699447191616, 0.4225591226830387]
maxi score, test score, baseline:  -0.029246666666666758 0.7026666666666668 0.7026666666666668
probs:  [0.08463636689377751, 0.3091475159512676, 0.18365699447191616, 0.4225591226830387]
maxi score, test score, baseline:  -0.029246666666666758 0.7026666666666668 0.7026666666666668
probs:  [0.08463636689377751, 0.3091475159512676, 0.18365699447191616, 0.4225591226830387]
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.196]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]] [[1.886]
 [1.779]
 [2.045]
 [2.045]
 [2.045]
 [2.045]
 [2.045]] [[0.129]
 [0.083]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]] [[-0.05]
 [-0.05]
 [-0.05]
 [-0.05]
 [-0.05]
 [-0.05]
 [-0.05]]
maxi score, test score, baseline:  -0.029246666666666758 0.7026666666666668 0.7026666666666668
probs:  [0.08463636689377751, 0.3091475159512676, 0.18365699447191616, 0.4225591226830387]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5800000000000002  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  63 total reward:  0.5333333333333338  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.029246666666666758 0.7026666666666668 0.7026666666666668
probs:  [0.08463636689377751, 0.3091475159512676, 0.18365699447191616, 0.4225591226830387]
maxi score, test score, baseline:  -0.029246666666666758 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.029246666666666758 0.7026666666666668 0.7026666666666668
probs:  [0.08463636689377751, 0.3091475159512676, 0.18365699447191616, 0.4225591226830387]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.03251333333333343 0.7026666666666668 0.7026666666666668
probs:  [0.08463014734160763, 0.30914845931882184, 0.18369315158135655, 0.422528241758214]
actor:  0 policy actor:  0  step number:  56 total reward:  0.11333333333333273  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.03360666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.0846239597850668, 0.30914939783335166, 0.18372912268553002, 0.4224975196960515]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.143]
 [0.043]
 [0.031]
 [0.11 ]
 [0.018]
 [0.026]] [[0.743]
 [0.788]
 [0.671]
 [0.639]
 [0.817]
 [0.627]
 [0.711]] [[-0.681]
 [-0.548]
 [-0.706]
 [-0.734]
 [-0.566]
 [-0.753]
 [-0.704]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.25 ]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]] [[1.811]
 [1.985]
 [1.811]
 [1.811]
 [1.811]
 [1.811]
 [1.811]] [[-0.117]
 [ 0.008]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]]
maxi score, test score, baseline:  -0.03360666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.0846239597850668, 0.30914939783335166, 0.18372912268553002, 0.4224975196960515]
actor:  1 policy actor:  1  step number:  56 total reward:  0.5266666666666672  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.03360666666666675 0.7026666666666668 0.7026666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03360666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.0846239597850668, 0.30914939783335166, 0.18372912268553002, 0.4224975196960515]
maxi score, test score, baseline:  -0.03360666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.0846239597850668, 0.30914939783335166, 0.18372912268553002, 0.4224975196960515]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03360666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.0846239597850668, 0.30914939783335166, 0.18372912268553002, 0.4224975196960515]
maxi score, test score, baseline:  -0.03360666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.0846239597850668, 0.30914939783335166, 0.18372912268553002, 0.4224975196960515]
maxi score, test score, baseline:  -0.03360666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.08461780397789284, 0.30915033153220967, 0.1837649092160719, 0.42246695527382566]
actor:  1 policy actor:  1  step number:  34 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.03360666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.0846116796763442, 0.30915126045236585, 0.18380051258996305, 0.4224365472813269]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.591]
 [0.604]
 [0.576]
 [0.575]
 [0.028]
 [0.513]] [[0.775]
 [0.721]
 [1.226]
 [1.141]
 [0.775]
 [0.637]
 [0.908]] [[0.557]
 [0.555]
 [0.696]
 [0.653]
 [0.557]
 [0.098]
 [0.543]]
from probs:  [0.0846116796763442, 0.30915126045236585, 0.18380051258996305, 0.42243654728132685]
siam score:  -0.7148166
maxi score, test score, baseline:  -0.036846666666666764 0.7026666666666668 0.7026666666666668
probs:  [0.08398794183706033, 0.3119919884121479, 0.1847071789586628, 0.4193128907921289]
siam score:  -0.71613127
line 256 mcts: sample exp_bonus 0.9101376840856965
maxi score, test score, baseline:  -0.036846666666666764 0.7026666666666668 0.7026666666666668
probs:  [0.08398794183706033, 0.311991988412148, 0.1847071789586628, 0.4193128907921289]
maxi score, test score, baseline:  -0.036846666666666764 0.7026666666666668 0.7026666666666668
probs:  [0.08398794183706033, 0.311991988412148, 0.1847071789586628, 0.4193128907921289]
maxi score, test score, baseline:  -0.036846666666666764 0.7026666666666668 0.7026666666666668
probs:  [0.08398794183706033, 0.311991988412148, 0.1847071789586628, 0.4193128907921289]
actor:  0 policy actor:  1  step number:  41 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.036833333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08398794183706033, 0.311991988412148, 0.1847071789586628, 0.4193128907921289]
maxi score, test score, baseline:  -0.036833333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08398794183706033, 0.311991988412148, 0.1847071789586628, 0.4193128907921289]
maxi score, test score, baseline:  -0.036833333333333426 0.7026666666666668 0.7026666666666668
actor:  1 policy actor:  1  step number:  31 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.036833333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08398794183706033, 0.311991988412148, 0.1847071789586628, 0.4193128907921289]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.036833333333333426 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.036833333333333426 0.7026666666666668 0.7026666666666668
actor:  1 policy actor:  1  step number:  39 total reward:  0.6800000000000003  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.5333333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.036833333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.08398794183706033, 0.311991988412148, 0.1847071789586628, 0.4193128907921289]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.04007333333333342 0.7026666666666668 0.7026666666666668
probs:  [0.08398794183706033, 0.3119919884121479, 0.1847071789586628, 0.4193128907921289]
maxi score, test score, baseline:  -0.04007333333333342 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.04007333333333342 0.7026666666666668 0.7026666666666668
probs:  [0.08398794183706033, 0.3119919884121479, 0.1847071789586628, 0.4193128907921289]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.04007333333333342 0.7026666666666668 0.7026666666666668
probs:  [0.0834511685192529, 0.3163940829276996, 0.18352555034157217, 0.41662919821147537]
actor:  1 policy actor:  1  step number:  51 total reward:  0.6133333333333337  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.04007333333333342 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.04007333333333342 0.7026666666666668 0.7026666666666668
probs:  [0.0834511685192529, 0.3163940829276996, 0.18352555034157217, 0.41662919821147537]
maxi score, test score, baseline:  -0.04007333333333342 0.7026666666666668 0.7026666666666668
probs:  [0.0834511685192529, 0.3163940829276996, 0.18352555034157217, 0.41662919821147537]
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.463]
 [0.662]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[2.429]
 [2.429]
 [1.424]
 [2.429]
 [2.429]
 [2.429]
 [2.429]] [[0.359]
 [0.359]
 [0.113]
 [0.359]
 [0.359]
 [0.359]
 [0.359]]
maxi score, test score, baseline:  -0.04007333333333342 0.7026666666666668 0.7026666666666668
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.591]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.548]] [[0.972]
 [1.086]
 [0.972]
 [0.972]
 [0.972]
 [0.972]
 [1.248]] [[0.416]
 [0.536]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.574]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.5
siam score:  -0.72143424
maxi score, test score, baseline:  -0.04332666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.0834511685192529, 0.3163940829276996, 0.18352555034157217, 0.41662919821147537]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.043473333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.0834511685192529, 0.3163940829276996, 0.18352555034157217, 0.41662919821147537]
maxi score, test score, baseline:  -0.043473333333333426 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.043473333333333426 0.7026666666666668 0.7026666666666668
probs:  [0.0834511685192529, 0.3163940829276996, 0.18352555034157217, 0.41662919821147537]
from probs:  [0.0834511685192529, 0.3163940829276996, 0.18352555034157217, 0.41662919821147537]
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.222]
 [0.238]
 [0.229]
 [0.208]
 [0.205]
 [0.238]] [[-0.122]
 [ 0.586]
 [ 0.237]
 [ 0.117]
 [ 0.07 ]
 [-0.052]
 [ 0.102]] [[0.185]
 [0.222]
 [0.238]
 [0.229]
 [0.208]
 [0.205]
 [0.238]]
maxi score, test score, baseline:  -0.04668666666666676 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.04668666666666676 0.7026666666666668 0.7026666666666668
probs:  [0.08345116851925288, 0.3163940829276996, 0.1835255503415722, 0.4166291982114753]
maxi score, test score, baseline:  -0.04668666666666676 0.7026666666666668 0.7026666666666668
probs:  [0.08345116851925288, 0.3163940829276996, 0.1835255503415722, 0.4166291982114753]
actor:  0 policy actor:  0  step number:  52 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.047126666666666754 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.047126666666666754 0.7026666666666668 0.7026666666666668
probs:  [0.08345116851925288, 0.3163940829276996, 0.1835255503415722, 0.4166291982114753]
actor:  1 policy actor:  1  step number:  47 total reward:  0.6400000000000003  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.047126666666666754 0.7026666666666668 0.7026666666666668
actor:  1 policy actor:  1  step number:  33 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.047126666666666754 0.7026666666666668 0.7026666666666668
probs:  [0.08396496556694388, 0.314029881127938, 0.18280293306742995, 0.41920222023768816]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.047126666666666754 0.7026666666666668 0.7026666666666668
probs:  [0.08396496556694388, 0.314029881127938, 0.18280293306742995, 0.41920222023768816]
actor:  1 policy actor:  1  step number:  45 total reward:  0.6000000000000003  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08396496556694388, 0.314029881127938, 0.18280293306742995, 0.41920222023768816]
maxi score, test score, baseline:  -0.047126666666666754 0.7026666666666668 0.7026666666666668
probs:  [0.08395313092182949, 0.31408433743399244, 0.1828195776142047, 0.4191429540299733]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.047126666666666754 0.7026666666666668 0.7026666666666668
probs:  [0.08392125180421299, 0.31434509172070024, 0.18275008926093128, 0.4189835672141555]
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.807]
 [0.872]
 [0.616]
 [0.807]
 [0.243]
 [0.264]] [[2.025]
 [2.025]
 [1.715]
 [1.474]
 [2.025]
 [0.14 ]
 [0.174]] [[0.807]
 [0.807]
 [0.872]
 [0.616]
 [0.807]
 [0.243]
 [0.264]]
maxi score, test score, baseline:  -0.047126666666666754 0.7026666666666668 0.7026666666666668
probs:  [0.08392125180421299, 0.31434509172070024, 0.18275008926093128, 0.4189835672141555]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.047126666666666754 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.047126666666666754 0.7026666666666668 0.7026666666666668
probs:  [0.08392125180421299, 0.31434509172070024, 0.18275008926093128, 0.4189835672141555]
maxi score, test score, baseline:  -0.047126666666666754 0.7026666666666668 0.7026666666666668
probs:  [0.08392125180421299, 0.31434509172070024, 0.18275008926093128, 0.4189835672141555]
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.764]
 [0.767]
 [0.766]
 [0.766]
 [0.767]
 [0.766]] [[0.203]
 [0.423]
 [1.01 ]
 [0.759]
 [0.808]
 [1.329]
 [0.811]] [[0.765]
 [0.764]
 [0.767]
 [0.766]
 [0.766]
 [0.767]
 [0.766]]
actor:  0 policy actor:  1  step number:  41 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.046180000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08392125180421299, 0.3143450917207003, 0.18275008926093125, 0.4189835672141555]
maxi score, test score, baseline:  -0.046180000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08392125180421299, 0.3143450917207003, 0.18275008926093125, 0.4189835672141555]
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]] [[1.833]
 [1.833]
 [1.833]
 [1.833]
 [1.833]
 [1.833]
 [1.833]] [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]]
siam score:  -0.72310287
maxi score, test score, baseline:  -0.046180000000000096 0.7026666666666668 0.7026666666666668
probs:  [0.08392125180421299, 0.3143450917207003, 0.18275008926093125, 0.4189835672141555]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  45 total reward:  0.6800000000000003  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.045540000000000073 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.045540000000000073 0.7026666666666668 0.7026666666666668
probs:  [0.08357404557910564, 0.31718505462862934, 0.18199326816195213, 0.41724763163031287]
maxi score, test score, baseline:  -0.045540000000000073 0.7026666666666668 0.7026666666666668
probs:  [0.08356828315453506, 0.3171837778292791, 0.18202890852775275, 0.4172190304884331]
maxi score, test score, baseline:  -0.045540000000000073 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.045540000000000073 0.7026666666666668 0.7026666666666668
probs:  [0.08356828315453506, 0.3171837778292791, 0.18202890852775275, 0.4172190304884331]
start point for exploration sampling:  11715
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.045540000000000073 0.7026666666666668 0.7026666666666668
probs:  [0.08356828315453506, 0.3171837778292791, 0.18202890852775275, 0.4172190304884331]
maxi score, test score, baseline:  -0.045540000000000073 0.7026666666666668 0.7026666666666668
actor:  1 policy actor:  1  step number:  54 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.045540000000000073 0.7026666666666668 0.7026666666666668
probs:  [0.08356828315453506, 0.3171837778292791, 0.18202890852775275, 0.4172190304884331]
maxi score, test score, baseline:  -0.045540000000000073 0.7026666666666668 0.7026666666666668
probs:  [0.08356828315453506, 0.3171837778292791, 0.18202890852775275, 0.4172190304884331]
maxi score, test score, baseline:  -0.045540000000000073 0.7026666666666668 0.7026666666666668
actor:  1 policy actor:  1  step number:  60 total reward:  0.3400000000000001  reward:  1.0 rdn_beta:  0.5
from probs:  [0.08356828315453506, 0.3171837778292791, 0.18202890852775275, 0.4172190304884331]
siam score:  -0.72996277
maxi score, test score, baseline:  -0.045540000000000073 0.7026666666666668 0.7026666666666668
probs:  [0.08356828315453506, 0.3171837778292791, 0.18202890852775275, 0.4172190304884331]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.542]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]] [[0.739]
 [1.031]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[-0.116]
 [ 0.134]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]]
maxi score, test score, baseline:  -0.045540000000000073 0.7026666666666668 0.7026666666666668
probs:  [0.08356828315453506, 0.3171837778292791, 0.18202890852775275, 0.4172190304884331]
maxi score, test score, baseline:  -0.045540000000000073 0.7026666666666668 0.7026666666666668
probs:  [0.08356828315453506, 0.3171837778292791, 0.18202890852775275, 0.4172190304884331]
actor:  0 policy actor:  0  step number:  45 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.627]
 [0.835]
 [0.627]
 [0.627]
 [0.627]
 [0.579]] [[0.869]
 [0.875]
 [0.183]
 [0.875]
 [0.875]
 [0.875]
 [0.746]] [[0.75 ]
 [0.627]
 [0.835]
 [0.627]
 [0.627]
 [0.627]
 [0.579]]
maxi score, test score, baseline:  -0.0453800000000001 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.0453800000000001 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.0453800000000001 0.7026666666666668 0.7026666666666668
probs:  [0.08356828315453506, 0.3171837778292791, 0.18202890852775275, 0.4172190304884331]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.6800000000000003  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.0453800000000001 0.7026666666666668 0.7026666666666668
probs:  [0.08356828315453506, 0.3171837778292791, 0.18202890852775275, 0.4172190304884331]
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]] [[0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]]
maxi score, test score, baseline:  -0.0453800000000001 0.7026666666666668 0.7026666666666668
probs:  [0.08356828315453506, 0.3171837778292791, 0.18202890852775272, 0.4172190304884331]
actor:  0 policy actor:  0  step number:  63 total reward:  0.14666666666666583  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08356828315453506, 0.3171837778292791, 0.18202890852775272, 0.4172190304884331]
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
actor:  1 policy actor:  1  step number:  40 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08358017139131427, 0.3171286940496938, 0.1820125704406619, 0.41727856411833003]
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]] [[1.503]
 [1.503]
 [1.503]
 [1.503]
 [1.503]
 [1.503]
 [1.503]] [[-0.365]
 [-0.365]
 [-0.365]
 [-0.365]
 [-0.365]
 [-0.365]
 [-0.365]]
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.233]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]] [[2.191]
 [1.72 ]
 [2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]] [[0.474]
 [0.349]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]]
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08358017139131425, 0.3171286940496938, 0.18201257044066188, 0.41727856411833]
actor:  1 policy actor:  1  step number:  36 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08384659954617393, 0.31589420738609925, 0.1816464163609284, 0.4186127767067983]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08384659954617393, 0.31589420738609925, 0.1816464163609284, 0.4186127767067983]
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08384659954617393, 0.31589420738609925, 0.1816464163609284, 0.4186127767067983]
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08384659954617393, 0.31589420738609925, 0.1816464163609284, 0.4186127767067983]
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08384659954617393, 0.31589420738609925, 0.1816464163609284, 0.4186127767067983]
actor:  1 policy actor:  1  step number:  33 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.026]
 [-0.025]
 [-0.023]
 [-0.015]
 [-0.031]
 [-0.023]] [[0.735]
 [1.065]
 [1.143]
 [0.731]
 [0.735]
 [0.836]
 [0.929]] [[0.067]
 [0.197]
 [0.23 ]
 [0.06 ]
 [0.067]
 [0.099]
 [0.142]]
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08384659954617395, 0.31589420738609925, 0.18164641636092843, 0.41861277670679836]
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08384659954617395, 0.31589420738609925, 0.18164641636092843, 0.41861277670679836]
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08384659954617395, 0.31589420738609925, 0.18164641636092843, 0.41861277670679836]
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08384659954617395, 0.31589420738609925, 0.18164641636092843, 0.41861277670679836]
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.084]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.094]] [[1.595]
 [1.711]
 [1.595]
 [1.595]
 [1.595]
 [1.595]
 [1.961]] [[0.261]
 [0.317]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.438]]
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08384659954617395, 0.31589420738609925, 0.18164641636092843, 0.41861277670679836]
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08384659954617395, 0.31589420738609925, 0.18164641636092843, 0.41861277670679836]
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.08384659954617395, 0.31589420738609925, 0.18164641636092843, 0.41861277670679836]
actor:  1 policy actor:  1  step number:  42 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.267]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.278]] [[2.235]
 [2.644]
 [2.235]
 [2.235]
 [2.235]
 [2.235]
 [3.718]] [[0.053]
 [0.242]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.616]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  26 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08384659954617395, 0.31589420738609925, 0.18164641636092843, 0.41861277670679836]
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
probs:  [0.0844525317210847, 0.31307275296375475, 0.18082755259635572, 0.42164716271880476]
maxi score, test score, baseline:  -0.043086666666666766 0.7026666666666668 0.7026666666666668
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]] [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]]
actor:  0 policy actor:  0  step number:  50 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.04056666666666676 0.7026666666666668 0.7026666666666668
probs:  [0.0844525317210847, 0.31307275296375475, 0.18082755259635572, 0.42164716271880476]
maxi score, test score, baseline:  -0.04056666666666676 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.04056666666666676 0.7026666666666668 0.7026666666666668
probs:  [0.0844525317210847, 0.31307275296375475, 0.18082755259635572, 0.42164716271880476]
maxi score, test score, baseline:  -0.04056666666666676 0.7026666666666668 0.7026666666666668
probs:  [0.0844525317210847, 0.31307275296375475, 0.18082755259635572, 0.42164716271880476]
maxi score, test score, baseline:  -0.04056666666666676 0.7026666666666668 0.7026666666666668
probs:  [0.08444668600525274, 0.31307260313480884, 0.1808625731321038, 0.4216181377278347]
actor:  0 policy actor:  0  step number:  41 total reward:  0.626666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0398600000000001 0.7026666666666668 0.7026666666666668
probs:  [0.08444668600525274, 0.31307260313480884, 0.1808625731321038, 0.4216181377278347]
maxi score, test score, baseline:  -0.0398600000000001 0.7026666666666668 0.7026666666666668
probs:  [0.08444668600525274, 0.31307260313480884, 0.1808625731321038, 0.4216181377278347]
maxi score, test score, baseline:  -0.0398600000000001 0.7026666666666668 0.7026666666666668
probs:  [0.08444668600525274, 0.31307260313480884, 0.1808625731321038, 0.4216181377278347]
maxi score, test score, baseline:  -0.0398600000000001 0.7026666666666668 0.7026666666666668
probs:  [0.08444668600525274, 0.31307260313480884, 0.1808625731321038, 0.4216181377278347]
maxi score, test score, baseline:  -0.0398600000000001 0.7026666666666668 0.7026666666666668
probs:  [0.08444668600525274, 0.31307260313480884, 0.1808625731321038, 0.4216181377278347]
actor:  1 policy actor:  1  step number:  44 total reward:  0.7000000000000003  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  52 total reward:  0.5800000000000005  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.0398600000000001 0.7026666666666668 0.7026666666666668
probs:  [0.08444086988798094, 0.3130724540644904, 0.18089741634868933, 0.4215892596988393]
maxi score, test score, baseline:  -0.0398600000000001 0.7026666666666668 0.7026666666666668
probs:  [0.08444086988798094, 0.3130724540644904, 0.18089741634868933, 0.4215892596988393]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.289]
 [0.318]
 [0.289]
 [0.248]
 [0.337]
 [0.321]] [[0.967]
 [0.989]
 [1.305]
 [0.989]
 [1.111]
 [1.247]
 [1.076]] [[0.002]
 [0.014]
 [0.175]
 [0.014]
 [0.041]
 [0.162]
 [0.075]]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[0.075]
 [1.501]
 [1.501]
 [1.501]
 [1.501]
 [1.501]
 [1.501]] [[-0.096]
 [ 0.119]
 [ 0.119]
 [ 0.119]
 [ 0.119]
 [ 0.119]
 [ 0.119]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.03986000000000009 0.7026666666666668 0.7026666666666668
probs:  [0.08443508314503785, 0.31307230574705236, 0.1809320835894393, 0.4215605275184705]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  44 total reward:  0.6600000000000003  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.039433333333333424 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.039433333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08442932555445119, 0.3130721581768054, 0.18096657618414586, 0.4215319400845975]
maxi score, test score, baseline:  -0.039433333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08442932555445119, 0.3130721581768054, 0.18096657618414586, 0.4215319400845975]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08442932555445119, 0.3130721581768054, 0.18096657618414586, 0.4215319400845975]
maxi score, test score, baseline:  -0.039433333333333424 0.7026666666666668 0.7026666666666668
probs:  [0.08512323003133518, 0.30985905793675866, 0.1800108707028086, 0.4250068413290976]
actor:  0 policy actor:  0  step number:  33 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.03903333333333342 0.7026666666666668 0.7026666666666668
probs:  [0.08453665277552307, 0.31257518325982814, 0.18081875581853055, 0.42206940814611826]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  0 policy actor:  0  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.03883333333333342 0.7026666666666668 0.7026666666666668
probs:  [0.08453665277552307, 0.31257518325982814, 0.18081875581853055, 0.42206940814611826]
maxi score, test score, baseline:  -0.03883333333333342 0.7026666666666668 0.7026666666666668
probs:  [0.08453665277552307, 0.31257518325982814, 0.18081875581853055, 0.42206940814611826]
line 256 mcts: sample exp_bonus 1.884467026744353
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.188]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[2.177]
 [1.559]
 [2.177]
 [2.177]
 [2.177]
 [2.177]
 [2.177]] [[0.432]
 [0.291]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]]
first move QE:  -0.09023722049208309
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]] [[1.553]
 [1.553]
 [1.553]
 [1.553]
 [1.553]
 [1.553]
 [1.553]] [[0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]]
first move QE:  -0.09018200603429612
using explorer policy with actor:  1
first move QE:  -0.09018200603429612
actor:  1 policy actor:  1  step number:  59 total reward:  0.6000000000000004  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  36 total reward:  0.7266666666666669  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.08447695566735022, 0.31306116511826815, 0.1806909465527474, 0.4217709326616342]
siam score:  -0.7179097
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.08447695566735022, 0.31306116511826815, 0.1806909465527474, 0.4217709326616342]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5800000000000003  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
from probs:  [0.08447695566735022, 0.31306116511826815, 0.1806909465527474, 0.4217709326616342]
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.08447695566735022, 0.31306116511826815, 0.1806909465527474, 0.4217709326616342]
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.08447695566735022, 0.31306116511826815, 0.1806909465527474, 0.4217709326616342]
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.08447695566735022, 0.31306116511826815, 0.1806909465527474, 0.4217709326616342]
line 256 mcts: sample exp_bonus 1.8569361568737168
actor:  1 policy actor:  1  step number:  41 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.08450685791910771, 0.31292254930434105, 0.1806499174148225, 0.4219206753617286]
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.08450685791910771, 0.31292254930434105, 0.1806499174148225, 0.4219206753617286]
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.021]
 [-0.016]
 [-0.016]
 [-0.024]
 [-0.016]
 [-0.035]] [[0.188]
 [0.354]
 [0.041]
 [0.041]
 [0.436]
 [0.041]
 [0.445]] [[0.597]
 [0.64 ]
 [0.559]
 [0.559]
 [0.661]
 [0.559]
 [0.659]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.08450685791910771, 0.31292254930434105, 0.1806499174148225, 0.4219206753617286]
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.08450685791910771, 0.31292254930434105, 0.1806499174148225, 0.4219206753617286]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  36 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.08244288503209171, 0.32249036660376246, 0.18348191244119402, 0.4115848359229517]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.437296931689489
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.639]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]] [[2.65 ]
 [1.253]
 [2.65 ]
 [2.65 ]
 [2.65 ]
 [2.65 ]
 [2.65 ]] [[0.359]
 [0.125]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]]
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.08244288503209171, 0.32249036660376246, 0.18348191244119402, 0.4115848359229517]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.08244288503209171, 0.32249036660376246, 0.18348191244119402, 0.4115848359229517]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7333333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.0825470043016594, 0.3225424026894525, 0.18280922525900728, 0.4121013677498808]
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.0825470043016594, 0.3225424026894525, 0.18280922525900728, 0.4121013677498808]
maxi score, test score, baseline:  -0.04122000000000008 0.7026666666666668 0.7026666666666668
probs:  [0.0825470043016594, 0.3225424026894525, 0.18280922525900728, 0.4121013677498808]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.04072666666666675 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.04072666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.0825470043016594, 0.3225424026894525, 0.18280922525900728, 0.4121013677498808]
maxi score, test score, baseline:  -0.04072666666666675 0.7026666666666668 0.7026666666666668
actor:  1 policy actor:  1  step number:  47 total reward:  0.5333333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.04072666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.0825470043016594, 0.3225424026894525, 0.18280922525900728, 0.4121013677498808]
maxi score, test score, baseline:  -0.04072666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.0825470043016594, 0.3225424026894525, 0.18280922525900728, 0.4121013677498808]
actor:  1 policy actor:  1  step number:  39 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.04072666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.08180007163074181, 0.3286795037493693, 0.18115341189058984, 0.40836701272929915]
maxi score, test score, baseline:  -0.04072666666666675 0.7026666666666668 0.7026666666666668
probs:  [0.08180007163074181, 0.3286795037493693, 0.18115341189058984, 0.40836701272929915]
start point for exploration sampling:  11715
actor:  0 policy actor:  1  step number:  37 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08180007163074181, 0.3286795037493693, 0.18115341189058984, 0.40836701272929915]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08180007163074181, 0.3286795037493693, 0.18115341189058984, 0.40836701272929915]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
actor:  1 policy actor:  1  step number:  59 total reward:  0.5866666666666671  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08180007163074181, 0.3286795037493693, 0.18115341189058984, 0.40836701272929915]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08180007163074181, 0.3286795037493693, 0.18115341189058984, 0.40836701272929915]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08180007163074181, 0.3286795037493693, 0.18115341189058984, 0.40836701272929915]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7333333333333335  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.286115241640414
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08189332463497705, 0.32817991976532335, 0.18109223357639437, 0.4088345220233052]
first move QE:  -0.08802687489925912
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08189332463497705, 0.32817991976532335, 0.18109223357639437, 0.4088345220233052]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08189332463497705, 0.32817991976532335, 0.18109223357639437, 0.4088345220233052]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08189332463497705, 0.32817991976532335, 0.18109223357639437, 0.4088345220233052]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08155058772981767, 0.3309948669654022, 0.18033357170382286, 0.4071209736009573]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08155058772981767, 0.3309948669654022, 0.18033357170382286, 0.4071209736009573]
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[7.228]
 [2.086]
 [2.086]
 [2.086]
 [2.086]
 [2.086]
 [2.086]] [[0.837]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148661235194693, 0.33129785937889344, 0.18041492005742502, 0.40680060821173464]
siam score:  -0.71666354
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148661235194693, 0.33129785937889344, 0.18041492005742502, 0.40680060821173464]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148661235194693, 0.33129785937889344, 0.18041492005742502, 0.40680060821173464]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148661235194693, 0.33129785937889344, 0.18041492005742502, 0.40680060821173464]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148661235194693, 0.33129785937889344, 0.18041492005742502, 0.40680060821173464]
siam score:  -0.71127677
actor:  1 policy actor:  1  step number:  84 total reward:  0.206666666666665  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148130981962368, 0.3312926199420127, 0.18045175648362036, 0.40677431375474327]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148130981962368, 0.3312926199420127, 0.18045175648362036, 0.40677431375474327]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148130981962368, 0.3312926199420127, 0.18045175648362036, 0.40677431375474327]
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.182]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]] [[2.087]
 [2.487]
 [2.087]
 [2.087]
 [2.087]
 [2.087]
 [2.087]] [[-0.161]
 [ 0.208]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148130981962368, 0.3312926199420127, 0.18045175648362036, 0.40677431375474327]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6066666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148130981962368, 0.3312926199420127, 0.18045175648362036, 0.40677431375474327]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148130981962368, 0.3312926199420127, 0.18045175648362036, 0.40677431375474327]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148130981962368, 0.3312926199420127, 0.18045175648362036, 0.40677431375474327]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148130981962368, 0.3312926199420127, 0.18045175648362036, 0.40677431375474327]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148130981962368, 0.3312926199420127, 0.18045175648362036, 0.40677431375474327]
maxi score, test score, baseline:  -0.037366666666666735 0.7026666666666668 0.7026666666666668
probs:  [0.08148130981962368, 0.3312926199420127, 0.18045175648362036, 0.40677431375474327]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.333
using another actor
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]] [[2.522]
 [2.522]
 [2.522]
 [2.522]
 [2.522]
 [2.522]
 [2.522]] [[0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.042]]
line 256 mcts: sample exp_bonus 1.9798534142428736
maxi score, test score, baseline:  -0.040260000000000074 0.7026666666666668 0.7026666666666668
probs:  [0.08148130981962368, 0.3312926199420127, 0.1804517564836203, 0.40677431375474327]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.7983952301618928
maxi score, test score, baseline:  -0.040260000000000074 0.7026666666666668 0.7026666666666668
probs:  [0.08148130981962368, 0.3312926199420127, 0.1804517564836203, 0.40677431375474327]
maxi score, test score, baseline:  -0.040260000000000074 0.7026666666666668 0.7026666666666668
actor:  1 policy actor:  1  step number:  48 total reward:  0.6200000000000003  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.040260000000000074 0.7026666666666668 0.7026666666666668
probs:  [0.08148130981962368, 0.3312926199420127, 0.18045175648362036, 0.40677431375474327]
Starting evaluation
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.040260000000000074 0.7026666666666668 0.7026666666666668
probs:  [0.08148130981962368, 0.3312926199420127, 0.18045175648362036, 0.40677431375474327]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.865]
 [0.754]
 [0.783]
 [0.799]
 [0.771]
 [0.839]] [[2.453]
 [0.809]
 [2.574]
 [1.995]
 [1.114]
 [2.026]
 [1.935]] [[0.739]
 [0.865]
 [0.754]
 [0.783]
 [0.799]
 [0.771]
 [0.839]]
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.891]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.884]] [[1.582]
 [0.382]
 [1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.622]] [[0.871]
 [0.891]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.884]]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.742]
 [0.7  ]
 [0.734]
 [0.718]
 [0.708]
 [0.737]] [[2.915]
 [1.082]
 [2.263]
 [1.805]
 [0.842]
 [1.904]
 [1.209]] [[0.657]
 [0.742]
 [0.7  ]
 [0.734]
 [0.718]
 [0.708]
 [0.737]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.4933333333333336  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.847]
 [0.839]
 [0.817]
 [0.817]
 [0.817]
 [0.834]] [[ 0.265]
 [-0.033]
 [ 0.041]
 [ 0.054]
 [ 0.054]
 [ 0.054]
 [ 0.1  ]] [[0.82 ]
 [0.847]
 [0.839]
 [0.817]
 [0.817]
 [0.817]
 [0.834]]
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7133333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.163]
 [0.333]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.161]] [[1.69 ]
 [1.612]
 [1.753]
 [1.753]
 [1.753]
 [1.753]
 [1.609]] [[-0.206]
 [-0.062]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.235]]
maxi score, test score, baseline:  -0.003260000000000078 0.6920000000000001 0.6920000000000001
probs:  [0.08083348165457363, 0.3327910202141801, 0.18280365371517257, 0.40357184441607363]
line 256 mcts: sample exp_bonus 0.6248358625133588
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.708]] [[1.391]
 [1.391]
 [1.391]
 [1.391]
 [1.391]
 [1.391]
 [1.039]] [[0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.292]]
siam score:  -0.71412766
maxi score, test score, baseline:  -0.003260000000000078 0.6920000000000001 0.6920000000000001
probs:  [0.08083348165457363, 0.3327910202141801, 0.18280365371517257, 0.40357184441607363]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  50 total reward:  0.486666666666667  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.003260000000000078 0.6920000000000001 0.6920000000000001
probs:  [0.08083348165457363, 0.3327910202141801, 0.18280365371517257, 0.40357184441607363]
maxi score, test score, baseline:  -0.003260000000000078 0.6920000000000001 0.6920000000000001
probs:  [0.08083348165457363, 0.3327910202141801, 0.18280365371517257, 0.40357184441607363]
line 256 mcts: sample exp_bonus 0.90217794795691
maxi score, test score, baseline:  -0.003260000000000078 0.6920000000000001 0.6920000000000001
probs:  [0.08083348165457363, 0.3327910202141801, 0.18280365371517257, 0.40357184441607363]
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.788]
 [0.71 ]
 [0.769]
 [0.495]
 [0.678]
 [0.789]] [[1.077]
 [0.675]
 [1.916]
 [0.879]
 [1.653]
 [2.422]
 [1.028]] [[0.692]
 [0.788]
 [0.71 ]
 [0.769]
 [0.495]
 [0.678]
 [0.789]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.003260000000000078 0.6920000000000001 0.6920000000000001
probs:  [0.08083348165457363, 0.3327910202141801, 0.18280365371517257, 0.40357184441607363]
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]] [[1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.178]] [[0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]]
maxi score, test score, baseline:  -0.003260000000000078 0.6920000000000001 0.6920000000000001
probs:  [0.08083348165457363, 0.3327910202141801, 0.18280365371517257, 0.40357184441607363]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.003260000000000078 0.6920000000000001 0.6920000000000001
probs:  [0.08083348165457363, 0.3327910202141801, 0.18280365371517257, 0.40357184441607363]
actor:  0 policy actor:  0  step number:  46 total reward:  0.4600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  56 total reward:  0.39333333333333353  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.053]
 [-0.069]
 [-0.044]
 [-0.053]
 [-0.053]
 [-0.071]
 [-0.091]] [[1.394]
 [1.421]
 [1.301]
 [1.394]
 [1.394]
 [1.501]
 [1.643]] [[0.309]
 [0.305]
 [0.296]
 [0.309]
 [0.309]
 [0.32 ]
 [0.336]]
maxi score, test score, baseline:  -0.002980000000000073 0.6920000000000001 0.6920000000000001
probs:  [0.08083348165457363, 0.3327910202141801, 0.18280365371517257, 0.40357184441607363]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08083348165457363, 0.3327910202141801, 0.18280365371517257, 0.40357184441607363]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7333333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.002980000000000073 0.6920000000000001 0.6920000000000001
probs:  [0.08089782344300903, 0.33295898803007007, 0.18224832877782954, 0.4038948597490914]
actor:  0 policy actor:  0  step number:  44 total reward:  0.54  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.181]
 [0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.143]] [[1.457]
 [1.275]
 [1.272]
 [1.272]
 [1.272]
 [1.272]
 [1.642]] [[ 0.066]
 [ 0.059]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [ 0.204]]
maxi score, test score, baseline:  -0.0025133333333334047 0.6920000000000001 0.6920000000000001
probs:  [0.08089782344300903, 0.33295898803007007, 0.18224832877782954, 0.4038948597490914]
maxi score, test score, baseline:  -0.0025133333333334047 0.6920000000000001 0.6920000000000001
probs:  [0.08089782344300903, 0.33295898803007007, 0.18224832877782954, 0.4038948597490914]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.0025133333333334047 0.6920000000000001 0.6920000000000001
probs:  [0.08089782344300903, 0.33295898803007007, 0.18224832877782954, 0.4038948597490914]
actor:  1 policy actor:  1  step number:  48 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  36 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0025133333333334047 0.6920000000000001 0.6920000000000001
probs:  [0.08012736166054334, 0.33658827628631877, 0.18324694907709238, 0.40003741297604545]
maxi score, test score, baseline:  -0.0025133333333334047 0.6920000000000001 0.6920000000000001
probs:  [0.08012736166054334, 0.33658827628631877, 0.18324694907709238, 0.40003741297604545]
maxi score, test score, baseline:  -0.0025133333333334047 0.6920000000000001 0.6920000000000001
probs:  [0.08012736166054334, 0.33658827628631877, 0.18324694907709238, 0.40003741297604545]
Printing some Q and Qe and total Qs values:  [[ 0.   ]
 [ 0.152]
 [ 0.001]
 [ 0.001]
 [ 0.001]
 [ 0.001]
 [-0.01 ]] [[1.273]
 [2.576]
 [1.313]
 [1.313]
 [1.313]
 [1.313]
 [1.228]] [[ 0.006]
 [ 0.559]
 [ 0.021]
 [ 0.021]
 [ 0.021]
 [ 0.021]
 [-0.016]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0025133333333334047 0.6920000000000001 0.6920000000000001
probs:  [0.08012736166054334, 0.33658827628631877, 0.18324694907709238, 0.40003741297604545]
maxi score, test score, baseline:  -0.0025133333333334047 0.6920000000000001 0.6920000000000001
actor:  1 policy actor:  1  step number:  37 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.8133333333333335  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  11715
first move QE:  -0.08427771411385489
start point for exploration sampling:  11715
actor:  1 policy actor:  1  step number:  35 total reward:  0.68  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.0025133333333334047 0.6920000000000001 0.6920000000000001
probs:  [0.07889010648351502, 0.3323098419709409, 0.19494961062762597, 0.39385044091791815]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.0025133333333334047 0.6920000000000001 0.6920000000000001
actor:  0 policy actor:  1  step number:  38 total reward:  0.6866666666666669  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.0014066666666667316 0.6920000000000001 0.6920000000000001
probs:  [0.07889010648351502, 0.3323098419709409, 0.19494961062762597, 0.39385044091791815]
maxi score, test score, baseline:  -0.0014066666666667316 0.6920000000000001 0.6920000000000001
probs:  [0.07889010648351502, 0.3323098419709409, 0.19494961062762597, 0.39385044091791815]
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.03 ]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]] [[-1.441]
 [ 0.161]
 [-1.441]
 [-1.441]
 [-1.441]
 [-1.441]
 [-1.441]] [[-0.003]
 [ 0.498]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]]
maxi score, test score, baseline:  -0.0014066666666667316 0.6920000000000001 0.6920000000000001
probs:  [0.07889010648351502, 0.3323098419709409, 0.19494961062762597, 0.39385044091791815]
maxi score, test score, baseline:  -0.0014066666666667316 0.6920000000000001 0.6920000000000001
probs:  [0.07889010648351502, 0.3323098419709409, 0.19494961062762597, 0.39385044091791815]
actor:  1 policy actor:  1  step number:  38 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  35 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.0014066666666667316 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.0014066666666667316 0.6920000000000001 0.6920000000000001
probs:  [0.07921800910817518, 0.3327052940003535, 0.19260172401843859, 0.3954749728730327]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.0014066666666667316 0.6920000000000001 0.6920000000000001
probs:  [0.07921784920228152, 0.3327060187424553, 0.19260195981158823, 0.39547417224367487]
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.023]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]] [[0.891]
 [1.398]
 [0.891]
 [0.891]
 [0.891]
 [0.891]
 [0.891]] [[-0.334]
 [ 0.015]
 [-0.334]
 [-0.334]
 [-0.334]
 [-0.334]
 [-0.334]]
maxi score, test score, baseline:  -0.0014066666666667316 0.6920000000000001 0.6920000000000001
probs:  [0.07921784920228152, 0.3327060187424553, 0.19260195981158823, 0.39547417224367487]
actor:  0 policy actor:  0  step number:  49 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.047]
 [-0.025]
 [-0.03 ]
 [-0.055]
 [-0.025]
 [-0.05 ]] [[ 0.28 ]
 [ 0.313]
 [-0.13 ]
 [-0.394]
 [ 0.268]
 [-0.13 ]
 [ 0.259]] [[0.535]
 [0.543]
 [0.456]
 [0.398]
 [0.531]
 [0.456]
 [0.53 ]]
Printing some Q and Qe and total Qs values:  [[-0.099]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.036]] [[1.035]
 [1.357]
 [1.357]
 [1.357]
 [1.357]
 [1.357]
 [1.561]] [[0.451]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.623]]
maxi score, test score, baseline:  0.0010999999999999344 0.6920000000000001 0.6920000000000001
probs:  [0.07826506647932249, 0.3370243195718468, 0.1940069088976314, 0.39070370505119945]
maxi score, test score, baseline:  0.0010999999999999344 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  0.0010999999999999344 0.6920000000000001 0.6920000000000001
probs:  [0.07826506647932249, 0.3370243195718468, 0.1940069088976314, 0.39070370505119945]
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.0010999999999999344 0.6920000000000001 0.6920000000000001
probs:  [0.07826506647932249, 0.3370243195718468, 0.1940069088976314, 0.39070370505119945]
actor:  1 policy actor:  1  step number:  30 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.4837397669027864
actor:  1 policy actor:  1  step number:  35 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0010999999999999344 0.6920000000000001 0.6920000000000001
probs:  [0.07611230520160235, 0.34997958013707897, 0.19397686934879038, 0.37993124531252814]
maxi score, test score, baseline:  0.0010999999999999344 0.6920000000000001 0.6920000000000001
probs:  [0.07611230520160235, 0.34997958013707897, 0.19397686934879038, 0.37993124531252814]
actor:  1 policy actor:  1  step number:  30 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0010999999999999344 0.6920000000000001 0.6920000000000001
probs:  [0.07532986912199145, 0.3463116642285891, 0.20233908612856327, 0.37601938052085626]
maxi score, test score, baseline:  0.0010999999999999344 0.6920000000000001 0.6920000000000001
probs:  [0.07532986912199145, 0.3463116642285891, 0.20233908612856327, 0.37601938052085626]
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.287]
 [0.16 ]
 [0.158]
 [0.235]
 [0.16 ]
 [0.22 ]] [[1.438]
 [1.257]
 [1.064]
 [1.282]
 [1.854]
 [1.175]
 [1.672]] [[0.505]
 [0.418]
 [0.288]
 [0.361]
 [0.593]
 [0.326]
 [0.524]]
maxi score, test score, baseline:  0.0010999999999999344 0.6920000000000001 0.6920000000000001
probs:  [0.07532986912199145, 0.3463116642285891, 0.20233908612856327, 0.37601938052085626]
using explorer policy with actor:  1
first move QE:  -0.08708367197496443
maxi score, test score, baseline:  0.0010999999999999344 0.6920000000000001 0.6920000000000001
probs:  [0.07532986912199145, 0.3463116642285891, 0.20233908612856327, 0.37601938052085626]
siam score:  -0.73398453
maxi score, test score, baseline:  0.0010999999999999344 0.6920000000000001 0.6920000000000001
probs:  [0.07532986912199145, 0.3463116642285891, 0.20233908612856327, 0.37601938052085626]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0016999999999999305 0.6920000000000001 0.6920000000000001
probs:  [0.07532986912199145, 0.3463116642285891, 0.20233908612856327, 0.37601938052085626]
maxi score, test score, baseline:  0.0016999999999999305 0.6920000000000001 0.6920000000000001
probs:  [0.07532986912199145, 0.3463116642285891, 0.20233908612856327, 0.37601938052085626]
maxi score, test score, baseline:  0.0016999999999999305 0.6920000000000001 0.6920000000000001
probs:  [0.07532986912199145, 0.3463116642285891, 0.20233908612856327, 0.37601938052085626]
maxi score, test score, baseline:  0.0016999999999999305 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  0.0016999999999999305 0.6920000000000001 0.6920000000000001
probs:  [0.07532986912199145, 0.3463116642285891, 0.20233908612856324, 0.37601938052085626]
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.031]
 [0.204]
 [0.031]
 [0.031]
 [0.031]
 [0.031]] [[1.069]
 [1.069]
 [0.456]
 [1.069]
 [1.069]
 [1.069]
 [1.069]] [[-0.335]
 [-0.335]
 [-0.265]
 [-0.335]
 [-0.335]
 [-0.335]
 [-0.335]]
maxi score, test score, baseline:  0.0016999999999999305 0.6920000000000001 0.6920000000000001
probs:  [0.07532986912199145, 0.3463116642285891, 0.20233908612856324, 0.37601938052085626]
maxi score, test score, baseline:  0.0016999999999999305 0.6920000000000001 0.6920000000000001
probs:  [0.07532986912199145, 0.3463116642285891, 0.20233908612856324, 0.37601938052085626]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  38 total reward:  0.6866666666666669  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0016999999999999305 0.6920000000000001 0.6920000000000001
probs:  [0.07532135136607333, 0.34628927196896214, 0.20241206822243973, 0.37597730844252475]
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]] [[2.294]
 [2.294]
 [2.294]
 [2.294]
 [2.294]
 [2.294]
 [2.294]] [[0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]]
actor:  0 policy actor:  1  step number:  36 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  0.667
siam score:  -0.74199533
actor:  1 policy actor:  1  step number:  26 total reward:  0.8200000000000001  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.272]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]] [[1.788]
 [2.347]
 [1.788]
 [1.788]
 [1.788]
 [1.788]
 [1.788]] [[0.162]
 [0.44 ]
 [0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]]
maxi score, test score, baseline:  0.004833333333333263 0.6920000000000001 0.6920000000000001
probs:  [0.07409919443541838, 0.3569101599303557, 0.19912363721592904, 0.369867008418297]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.004833333333333263 0.6920000000000001 0.6920000000000001
probs:  [0.07409919443541838, 0.3569101599303557, 0.19912363721592904, 0.369867008418297]
maxi score, test score, baseline:  0.004833333333333263 0.6920000000000001 0.6920000000000001
probs:  [0.07409919443541838, 0.3569101599303557, 0.19912363721592904, 0.369867008418297]
siam score:  -0.74068034
maxi score, test score, baseline:  0.004833333333333263 0.6920000000000001 0.6920000000000001
probs:  [0.07409919443541838, 0.3569101599303557, 0.19912363721592904, 0.369867008418297]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7379936
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.311]] [[1.869]
 [1.869]
 [1.869]
 [1.869]
 [1.869]
 [1.869]
 [2.251]] [[0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.726]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7333333333333335  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.404]
 [0.404]
 [0.404]
 [0.536]
 [0.404]
 [0.404]] [[0.72 ]
 [1.503]
 [1.503]
 [1.503]
 [0.7  ]
 [1.503]
 [1.503]] [[0.449]
 [0.624]
 [0.624]
 [0.624]
 [0.416]
 [0.624]
 [0.624]]
maxi score, test score, baseline:  0.004833333333333263 0.6920000000000001 0.6920000000000001
probs:  [0.07427127410631644, 0.35612684801383704, 0.19887335949822685, 0.3707285183816197]
maxi score, test score, baseline:  0.004833333333333263 0.6920000000000001 0.6920000000000001
probs:  [0.07427127410631644, 0.35612684801383704, 0.19887335949822685, 0.3707285183816197]
maxi score, test score, baseline:  0.004833333333333263 0.6920000000000001 0.6920000000000001
probs:  [0.07427127410631644, 0.35612684801383704, 0.19887335949822685, 0.3707285183816197]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]] [[-0.208]
 [-0.208]
 [-0.208]
 [-0.208]
 [-0.208]
 [-0.208]
 [-0.208]] [[-0.638]
 [-0.638]
 [-0.638]
 [-0.638]
 [-0.638]
 [-0.638]
 [-0.638]]
maxi score, test score, baseline:  0.004833333333333263 0.6920000000000001 0.6920000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.004833333333333263 0.6920000000000001 0.6920000000000001
probs:  [0.07427127410631644, 0.35612684801383704, 0.19887335949822685, 0.3707285183816197]
start point for exploration sampling:  11715
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  55 total reward:  0.5600000000000004  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.004833333333333263 0.6920000000000001 0.6920000000000001
probs:  [0.07426746904910846, 0.35611278230473686, 0.19891000470250444, 0.3707097439436502]
maxi score, test score, baseline:  0.004833333333333263 0.6920000000000001 0.6920000000000001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0019933333333332724 0.6920000000000001 0.6920000000000001
probs:  [0.07426746904910846, 0.35611278230473686, 0.19891000470250444, 0.3707097439436502]
maxi score, test score, baseline:  0.0019933333333332724 0.6920000000000001 0.6920000000000001
probs:  [0.07426746904910846, 0.35611278230473686, 0.19891000470250444, 0.3707097439436502]
maxi score, test score, baseline:  0.0019933333333332724 0.6920000000000001 0.6920000000000001
probs:  [0.07426746904910846, 0.35611278230473686, 0.19891000470250444, 0.3707097439436502]
maxi score, test score, baseline:  0.0019933333333332724 0.6920000000000001 0.6920000000000001
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.888]
 [0.845]
 [0.869]
 [0.885]
 [0.869]
 [0.84 ]] [[ 0.032]
 [-0.283]
 [-0.027]
 [ 0.   ]
 [-0.958]
 [ 0.   ]
 [ 0.14 ]] [[0.836]
 [0.888]
 [0.845]
 [0.869]
 [0.885]
 [0.869]
 [0.84 ]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5733333333333338  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  41 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0018333333333332615 0.6920000000000001 0.6920000000000001
probs:  [0.07426746904910846, 0.35611278230473686, 0.19891000470250444, 0.3707097439436502]
maxi score, test score, baseline:  0.0018333333333332615 0.6920000000000001 0.6920000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0018333333333332615 0.6920000000000001 0.6920000000000001
probs:  [0.07426746904910846, 0.35611278230473686, 0.19891000470250444, 0.3707097439436502]
maxi score, test score, baseline:  0.0018333333333332615 0.6920000000000001 0.6920000000000001
probs:  [0.07426746904910846, 0.35611278230473686, 0.19891000470250444, 0.3707097439436502]
maxi score, test score, baseline:  0.0018333333333332615 0.6920000000000001 0.6920000000000001
probs:  [0.07426746904910846, 0.35611278230473686, 0.19891000470250444, 0.3707097439436502]
maxi score, test score, baseline:  0.0018333333333332615 0.6920000000000001 0.6920000000000001
probs:  [0.07426746904910846, 0.35611278230473686, 0.19891000470250444, 0.3707097439436502]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0018333333333332615 0.6920000000000001 0.6920000000000001
probs:  [0.07426368322835959, 0.3560987877048012, 0.19894646464702467, 0.37069106441981453]
maxi score, test score, baseline:  0.0018333333333332615 0.6920000000000001 0.6920000000000001
probs:  [0.07426368322835959, 0.3560987877048012, 0.19894646464702467, 0.37069106441981453]
maxi score, test score, baseline:  0.0018333333333332615 0.6920000000000001 0.6920000000000001
probs:  [0.07426368322835959, 0.3560987877048012, 0.19894646464702467, 0.37069106441981453]
maxi score, test score, baseline:  0.0018333333333332615 0.6920000000000001 0.6920000000000001
probs:  [0.07426368322835959, 0.3560987877048012, 0.19894646464702467, 0.37069106441981453]
actor:  0 policy actor:  0  step number:  41 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.005006666666666599 0.6920000000000001 0.6920000000000001
probs:  [0.07426368322835959, 0.3560987877048012, 0.19894646464702467, 0.37069106441981453]
maxi score, test score, baseline:  0.005006666666666599 0.6920000000000001 0.6920000000000001
probs:  [0.07426368322835959, 0.3560987877048012, 0.19894646464702467, 0.37069106441981453]
maxi score, test score, baseline:  0.005006666666666599 0.6920000000000001 0.6920000000000001
probs:  [0.07426368322835959, 0.3560987877048012, 0.19894646464702467, 0.37069106441981453]
maxi score, test score, baseline:  0.005006666666666599 0.6920000000000001 0.6920000000000001
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  44 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.005606666666666597 0.6920000000000001 0.6920000000000001
probs:  [0.07426368322835959, 0.3560987877048012, 0.19894646464702467, 0.37069106441981453]
from probs:  [0.07426368322835959, 0.3560987877048012, 0.19894646464702467, 0.37069106441981453]
siam score:  -0.7348564
maxi score, test score, baseline:  0.0026199999999999353 0.6920000000000001 0.6920000000000001
probs:  [0.07426368322835959, 0.3560987877048012, 0.19894646464702467, 0.37069106441981453]
maxi score, test score, baseline:  0.0004999999999999311 0.6920000000000001 0.6920000000000001
probs:  [0.07426368322835959, 0.3560987877048012, 0.19894646464702467, 0.37069106441981453]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.00202000000000006 0.6920000000000001 0.6920000000000001
probs:  [0.07426368322835959, 0.3560987877048012, 0.19894646464702467, 0.37069106441981453]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[7.39 ]
 [1.223]
 [1.223]
 [1.223]
 [1.223]
 [1.223]
 [1.223]] [[0.921]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 1.5206414053228792
maxi score, test score, baseline:  -0.00202000000000006 0.6920000000000001 0.6920000000000001
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.389]
 [0.535]
 [0.431]
 [0.389]
 [0.134]
 [0.475]] [[1.145]
 [1.62 ]
 [2.144]
 [2.052]
 [1.62 ]
 [2.38 ]
 [1.531]] [[-0.074]
 [ 0.261]
 [ 0.605]
 [ 0.478]
 [ 0.261]
 [ 0.366]
 [ 0.296]]
maxi score, test score, baseline:  -0.00202000000000006 0.6920000000000001 0.6920000000000001
probs:  [0.07411047273235234, 0.3574914256070411, 0.19847292925482582, 0.3699251724057807]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.5
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.00202000000000006 0.6920000000000001 0.6920000000000001
probs:  [0.0739428683907055, 0.35668164283513615, 0.2002882756423393, 0.3690872131318192]
actor:  0 policy actor:  1  step number:  35 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7361012
actor:  1 policy actor:  1  step number:  40 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]] [[2.034]
 [1.688]
 [1.688]
 [1.688]
 [1.688]
 [1.688]
 [1.688]] [[0.411]
 [0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]]
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
probs:  [0.07393922183133478, 0.3566676007441562, 0.20032395003793171, 0.36906922738657727]
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.001]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]] [[1.969]
 [1.741]
 [1.969]
 [1.969]
 [1.969]
 [1.969]
 [1.969]] [[0.617]
 [0.575]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.539]
 [0.539]
 [0.194]
 [0.116]
 [0.145]
 [0.24 ]] [[1.947]
 [2.09 ]
 [2.09 ]
 [2.638]
 [2.759]
 [2.417]
 [2.263]] [[0.152]
 [0.403]
 [0.403]
 [0.42 ]
 [0.423]
 [0.248]
 [0.242]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
probs:  [0.07386034374151483, 0.3570251577001776, 0.2004401663523615, 0.36867433220594603]
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.015]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.03 ]] [[ 0.507]
 [ 0.171]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [ 0.266]] [[0.36 ]
 [0.298]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.308]]
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
probs:  [0.07386034374151483, 0.3570251577001776, 0.2004401663523615, 0.36867433220594603]
actor:  1 policy actor:  1  step number:  52 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.667
siam score:  -0.73884904
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
probs:  [0.07386034374151483, 0.3570251577001776, 0.2004401663523615, 0.36867433220594603]
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
actor:  1 policy actor:  1  step number:  46 total reward:  0.566666666666667  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.415]
 [0.415]
 [0.517]
 [0.415]
 [0.415]
 [0.545]] [[1.866]
 [1.866]
 [1.866]
 [2.123]
 [1.866]
 [1.866]
 [1.293]] [[0.093]
 [0.093]
 [0.093]
 [0.281]
 [0.093]
 [0.093]
 [0.033]]
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
probs:  [0.07386034374151483, 0.3570251577001776, 0.2004401663523615, 0.36867433220594603]
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
probs:  [0.07386034374151483, 0.3570251577001776, 0.2004401663523615, 0.36867433220594603]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
probs:  [0.07386034374151483, 0.3570251577001776, 0.2004401663523615, 0.36867433220594603]
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
probs:  [0.07386034374151483, 0.3570251577001776, 0.2004401663523615, 0.36867433220594603]
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
probs:  [0.07386034374151483, 0.3570251577001776, 0.2004401663523615, 0.36867433220594603]
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.309]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.381]] [[1.814]
 [2.283]
 [1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.345]] [[-0.196]
 [-0.007]
 [-0.196]
 [-0.196]
 [-0.196]
 [-0.196]
 [-0.248]]
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
probs:  [0.07386034374151483, 0.3570251577001776, 0.2004401663523615, 0.36867433220594603]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
siam score:  -0.7400107
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.564492062890935
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
probs:  [0.07386034374151483, 0.3570251577001776, 0.2004401663523615, 0.36867433220594603]
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
probs:  [0.07386034374151483, 0.3570251577001776, 0.2004401663523615, 0.36867433220594603]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7266666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.000753333333333392 0.6920000000000001 0.6920000000000001
probs:  [0.07377321251066815, 0.3577846703101264, 0.20020340832931352, 0.368238708849892]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]] [[1.542]
 [1.542]
 [1.542]
 [1.542]
 [1.542]
 [1.542]
 [1.542]] [[0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]]
maxi score, test score, baseline:  -0.0007533333333333956 0.6920000000000001 0.6920000000000001
probs:  [0.07377321251066815, 0.3577846703101264, 0.20020340832931352, 0.368238708849892]
actor:  1 policy actor:  1  step number:  36 total reward:  0.7000000000000003  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0007533333333333956 0.6920000000000001 0.6920000000000001
probs:  [0.07376857816265756, 0.3578250673723361, 0.2001908156106931, 0.36821553885431324]
actor:  1 policy actor:  1  step number:  31 total reward:  0.8000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0007533333333333956 0.6920000000000001 0.6920000000000001
probs:  [0.07588431194689016, 0.3481892212751652, 0.19711848857815747, 0.37880797819978723]
maxi score, test score, baseline:  -0.0007533333333333956 0.6920000000000001 0.6920000000000001
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.279]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.169]] [[1.764]
 [0.859]
 [1.764]
 [1.764]
 [1.764]
 [1.764]
 [1.7  ]] [[0.438]
 [0.196]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.432]]
using explorer policy with actor:  1
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.0007533333333333956 0.6920000000000001 0.6920000000000001
from probs:  [0.07588431194689016, 0.3481892212751652, 0.19711848857815745, 0.37880797819978723]
maxi score, test score, baseline:  -0.0007533333333333956 0.6920000000000001 0.6920000000000001
probs:  [0.07588431194689016, 0.3481892212751652, 0.19711848857815745, 0.37880797819978723]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.0007533333333333956 0.6920000000000001 0.6920000000000001
probs:  [0.07588431194689016, 0.3481892212751652, 0.19711848857815745, 0.37880797819978723]
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.914]
 [0.847]
 [0.847]
 [0.847]
 [0.837]
 [0.857]] [[0.083]
 [0.931]
 [0.158]
 [0.238]
 [0.238]
 [0.054]
 [0.247]] [[0.831]
 [0.914]
 [0.847]
 [0.847]
 [0.847]
 [0.837]
 [0.857]]
Printing some Q and Qe and total Qs values:  [[0.996]
 [0.986]
 [0.998]
 [0.986]
 [0.986]
 [0.986]
 [0.996]] [[0.679]
 [0.98 ]
 [0.233]
 [0.98 ]
 [0.98 ]
 [0.98 ]
 [0.229]] [[0.996]
 [0.986]
 [0.998]
 [0.986]
 [0.986]
 [0.986]
 [0.996]]
actor:  0 policy actor:  1  step number:  46 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  0.167
using another actor
start point for exploration sampling:  11715
siam score:  -0.7441031
maxi score, test score, baseline:  -0.00036666666666673606 0.6920000000000001 0.6920000000000001
probs:  [0.07588431194689016, 0.34818922127516516, 0.19711848857815745, 0.37880797819978723]
actor:  0 policy actor:  1  step number:  48 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0024733333333332706 0.6920000000000001 0.6920000000000001
probs:  [0.07588431194689016, 0.34818922127516516, 0.19711848857815745, 0.37880797819978723]
actor:  0 policy actor:  1  step number:  38 total reward:  0.6200000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0030466666666666155 0.6920000000000001 0.6920000000000001
line 256 mcts: sample exp_bonus 2.9262469200339183
actor:  1 policy actor:  1  step number:  37 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0030466666666666155 0.6920000000000001 0.6920000000000001
probs:  [0.07588418503770182, 0.3481903127699141, 0.1971181585151768, 0.37880734367720725]
maxi score, test score, baseline:  0.0006866666666666163 0.6920000000000001 0.6920000000000001
probs:  [0.07588418503770182, 0.3481903127699141, 0.1971181585151768, 0.37880734367720725]
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.629]
 [0.615]
 [0.564]
 [0.61 ]
 [0.601]
 [0.63 ]] [[-0.257]
 [ 0.128]
 [-0.416]
 [ 1.502]
 [-0.461]
 [-0.532]
 [-0.227]] [[0.627]
 [0.629]
 [0.615]
 [0.564]
 [0.61 ]
 [0.601]
 [0.63 ]]
maxi score, test score, baseline:  0.0006866666666666163 0.6920000000000001 0.6920000000000001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.0006866666666666163 0.6920000000000001 0.6920000000000001
first move QE:  -0.08949702888963366
maxi score, test score, baseline:  0.0006866666666666163 0.6920000000000001 0.6920000000000001
probs:  [0.07588418503770182, 0.3481903127699141, 0.1971181585151768, 0.37880734367720725]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  43 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.368]
 [0.666]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[1.004]
 [1.004]
 [0.884]
 [1.004]
 [1.004]
 [1.004]
 [1.004]] [[-0.11 ]
 [-0.11 ]
 [ 0.147]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]]
maxi score, test score, baseline:  0.001059999999999944 0.6920000000000001 0.6920000000000001
probs:  [0.07588418503770182, 0.3481903127699141, 0.1971181585151768, 0.37880734367720725]
maxi score, test score, baseline:  0.001059999999999944 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  0.001059999999999944 0.6920000000000001 0.6920000000000001
probs:  [0.07588418503770182, 0.3481903127699141, 0.1971181585151768, 0.37880734367720725]
maxi score, test score, baseline:  0.001059999999999944 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  0.001059999999999944 0.6920000000000001 0.6920000000000001
start point for exploration sampling:  11715
actor:  1 policy actor:  1  step number:  41 total reward:  0.6000000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.001059999999999944 0.6920000000000001 0.6920000000000001
probs:  [0.07588418503770182, 0.3481903127699141, 0.1971181585151768, 0.37880734367720725]
Printing some Q and Qe and total Qs values:  [[-0.15 ]
 [ 0.051]
 [ 0.005]
 [ 0.117]
 [-0.087]
 [-0.035]
 [ 0.117]] [[0.597]
 [1.375]
 [0.422]
 [1.396]
 [0.273]
 [0.285]
 [1.396]] [[-0.366]
 [ 0.22 ]
 [-0.299]
 [ 0.297]
 [-0.464]
 [-0.407]
 [ 0.297]]
maxi score, test score, baseline:  -0.00128666666666672 0.6920000000000001 0.6920000000000001
probs:  [0.07588418503770182, 0.3481903127699141, 0.1971181585151768, 0.37880734367720725]
maxi score, test score, baseline:  -0.00128666666666672 0.6920000000000001 0.6920000000000001
probs:  [0.07588418503770182, 0.3481903127699141, 0.1971181585151768, 0.37880734367720725]
siam score:  -0.73795426
actor:  1 policy actor:  1  step number:  28 total reward:  0.8200000000000001  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.00128666666666672 0.6920000000000001 0.6920000000000001
probs:  [0.07473237968469663, 0.3428971374012134, 0.20932195397560074, 0.3730485289384891]
maxi score, test score, baseline:  -0.00128666666666672 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.00128666666666672 0.6920000000000001 0.6920000000000001
probs:  [0.07473237968469663, 0.3428971374012134, 0.20932195397560074, 0.3730485289384891]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.00128666666666672 0.6920000000000001 0.6920000000000001
probs:  [0.07473237968469663, 0.3428971374012134, 0.20932195397560074, 0.3730485289384891]
maxi score, test score, baseline:  -0.00128666666666672 0.6920000000000001 0.6920000000000001
probs:  [0.07473237968469663, 0.3428971374012134, 0.20932195397560074, 0.3730485289384891]
maxi score, test score, baseline:  -0.00128666666666672 0.6920000000000001 0.6920000000000001
actor:  0 policy actor:  0  step number:  44 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0012066666666667146 0.6920000000000001 0.6920000000000001
probs:  [0.07473237968469663, 0.3428971374012134, 0.20932195397560074, 0.3730485289384891]
maxi score, test score, baseline:  -0.0012066666666667146 0.6920000000000001 0.6920000000000001
probs:  [0.07473237968469663, 0.3428971374012134, 0.20932195397560074, 0.3730485289384891]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0012066666666667146 0.6920000000000001 0.6920000000000001
probs:  [0.07473237968469663, 0.3428971374012134, 0.20932195397560074, 0.3730485289384891]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.0012066666666667146 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.0012066666666667146 0.6920000000000001 0.6920000000000001
probs:  [0.07473237968469663, 0.3428971374012134, 0.20932195397560074, 0.3730485289384891]
maxi score, test score, baseline:  -0.0012066666666667146 0.6920000000000001 0.6920000000000001
probs:  [0.07473237968469663, 0.3428971374012134, 0.20932195397560074, 0.3730485289384891]
maxi score, test score, baseline:  -0.0012066666666667146 0.6920000000000001 0.6920000000000001
probs:  [0.07473237968469665, 0.3428971374012134, 0.20932195397560077, 0.3730485289384892]
actor:  0 policy actor:  1  step number:  51 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0017266666666666119 0.6920000000000001 0.6920000000000001
probs:  [0.07473237968469665, 0.3428971374012134, 0.20932195397560077, 0.3730485289384892]
siam score:  -0.73447907
maxi score, test score, baseline:  0.0017266666666666119 0.6920000000000001 0.6920000000000001
probs:  [0.07375505301700018, 0.34712971036209944, 0.21095943090506647, 0.36815580571583373]
maxi score, test score, baseline:  0.0017266666666666119 0.6920000000000001 0.6920000000000001
actor:  0 policy actor:  0  step number:  40 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.004699999999999941 0.6920000000000001 0.6920000000000001
probs:  [0.07375505301700018, 0.34712971036209944, 0.21095943090506647, 0.36815580571583373]
maxi score, test score, baseline:  0.004699999999999941 0.6920000000000001 0.6920000000000001
probs:  [0.07375505301700018, 0.34712971036209944, 0.21095943090506647, 0.36815580571583373]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5866666666666668  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.067]
 [0.028]
 [0.067]
 [0.067]
 [0.067]
 [0.067]] [[ 0.284]
 [ 0.284]
 [-0.643]
 [ 0.284]
 [ 0.284]
 [ 0.284]
 [ 0.284]] [[-0.349]
 [-0.349]
 [-0.698]
 [-0.349]
 [-0.349]
 [-0.349]
 [-0.349]]
maxi score, test score, baseline:  0.004699999999999941 0.6920000000000001 0.6920000000000001
probs:  [0.07375505301700018, 0.34712971036209944, 0.21095943090506647, 0.36815580571583373]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.379]
 [0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.292]] [[0.852]
 [1.375]
 [1.294]
 [1.294]
 [1.294]
 [1.294]
 [1.434]] [[0.431]
 [0.552]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.52 ]]
maxi score, test score, baseline:  0.004699999999999941 0.6920000000000001 0.6920000000000001
probs:  [0.07375505301700018, 0.34712971036209944, 0.21095943090506647, 0.36815580571583373]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6600000000000003  reward:  1.0 rdn_beta:  0.5
using another actor
actor:  1 policy actor:  1  step number:  55 total reward:  0.5866666666666671  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.004699999999999941 0.6920000000000001 0.6920000000000001
probs:  [0.07375505301700021, 0.34712971036209955, 0.2109594309050665, 0.36815580571583384]
maxi score, test score, baseline:  0.004699999999999941 0.6920000000000001 0.6920000000000001
probs:  [0.07375505301700021, 0.34712971036209955, 0.2109594309050665, 0.36815580571583384]
maxi score, test score, baseline:  0.002179999999999941 0.6920000000000001 0.6920000000000001
probs:  [0.07375505301700021, 0.34712971036209955, 0.2109594309050665, 0.36815580571583384]
maxi score, test score, baseline:  0.002179999999999941 0.6920000000000001 0.6920000000000001
probs:  [0.07375505301700021, 0.34712971036209955, 0.2109594309050665, 0.36815580571583384]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7733333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.002179999999999941 0.6920000000000001 0.6920000000000001
probs:  [0.07329716591325704, 0.35118832567585534, 0.2096480149216416, 0.3658664934892459]
maxi score, test score, baseline:  0.002179999999999941 0.6920000000000001 0.6920000000000001
probs:  [0.07329716591325704, 0.35118832567585534, 0.2096480149216416, 0.3658664934892459]
Printing some Q and Qe and total Qs values:  [[-0.007]
 [ 0.195]
 [-0.022]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[1.278]
 [1.826]
 [1.334]
 [1.278]
 [1.278]
 [1.278]
 [1.278]] [[-0.119]
 [ 0.24 ]
 [-0.116]
 [-0.119]
 [-0.119]
 [-0.119]
 [-0.119]]
maxi score, test score, baseline:  0.002179999999999941 0.6920000000000001 0.6920000000000001
probs:  [0.07329716591325704, 0.35118832567585534, 0.2096480149216416, 0.3658664934892459]
maxi score, test score, baseline:  0.002179999999999941 0.6920000000000001 0.6920000000000001
from probs:  [0.07329716591325704, 0.35118832567585534, 0.2096480149216416, 0.3658664934892459]
maxi score, test score, baseline:  0.002179999999999941 0.6920000000000001 0.6920000000000001
probs:  [0.07329716591325704, 0.35118832567585534, 0.2096480149216416, 0.3658664934892459]
actor:  0 policy actor:  1  step number:  44 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.005286666666666614 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  0.005286666666666614 0.6920000000000001 0.6920000000000001
probs:  [0.07329716591325704, 0.35118832567585534, 0.2096480149216416, 0.3658664934892459]
actor:  0 policy actor:  1  step number:  48 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.07329716591325704, 0.35118832567585534, 0.2096480149216416, 0.3658664934892459]
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.07329716591325704, 0.35118832567585534, 0.2096480149216416, 0.3658664934892459]
siam score:  -0.73382586
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.344]
 [0.658]
 [0.517]
 [0.517]
 [0.62 ]
 [0.517]] [[0.158]
 [0.301]
 [0.715]
 [0.158]
 [0.158]
 [0.147]
 [0.158]] [[-0.142]
 [-0.292]
 [ 0.091]
 [-0.142]
 [-0.142]
 [-0.042]
 [-0.142]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.6466666666666668  reward:  1.0 rdn_beta:  0.167
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.0000],
        [ 0.9059],
        [ 0.4549],
        [-0.0000],
        [ 0.2575],
        [-0.0000],
        [ 0.1976],
        [-0.0626],
        [-0.0000]], dtype=torch.float64)
-0.7710780000000002 -0.7710780000000002
-0.8649271488 -0.8649271488
-0.045154513866 0.8607944772602482
-0.09703970119800001 0.3578320215799202
-0.793138606986 -0.793138606986
-0.032346567066 0.22516511901047062
-0.7570299000000001 -0.7570299000000001
-0.084359833866 0.11324039625678615
-0.032346567066 -0.09496677302826825
-0.6270000000000002 -0.6270000000000002
actor:  1 policy actor:  1  step number:  40 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.333
siam score:  -0.74014145
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.07329390441312222, 0.3511766722847273, 0.20967900551730123, 0.3658504177848492]
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.07329390441312222, 0.3511766722847273, 0.20967900551730123, 0.3658504177848492]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
siam score:  -0.7399025
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.07329390441312222, 0.3511766722847273, 0.20967900551730123, 0.3658504177848492]
actor:  1 policy actor:  1  step number:  46 total reward:  0.6600000000000003  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  45 total reward:  0.706666666666667  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.073314817868523, 0.35108527251431226, 0.20964479554502222, 0.3659551140721426]
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.073314817868523, 0.35108527251431226, 0.20964479554502222, 0.3659551140721426]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.254]
 [0.213]
 [0.19 ]
 [0.025]
 [0.213]
 [0.228]] [[1.278]
 [1.602]
 [1.278]
 [1.145]
 [0.662]
 [1.278]
 [1.397]] [[-0.126]
 [ 0.073]
 [-0.126]
 [-0.214]
 [-0.594]
 [-0.126]
 [-0.053]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.073314817868523, 0.35108527251431226, 0.20964479554502222, 0.3659551140721426]
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.073314817868523, 0.35108527251431226, 0.20964479554502222, 0.3659551140721426]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.073314817868523, 0.35108527251431226, 0.20964479554502222, 0.3659551140721426]
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.073314817868523, 0.35108527251431226, 0.20964479554502222, 0.3659551140721426]
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.073314817868523, 0.35108527251431226, 0.20964479554502222, 0.3659551140721426]
using explorer policy with actor:  1
first move QE:  -0.09033780119060511
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.073314817868523, 0.35108527251431226, 0.20964479554502222, 0.3659551140721426]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  0.31999999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.073314817868523, 0.35108527251431215, 0.20964479554502222, 0.3659551140721426]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.073314817868523, 0.35108527251431226, 0.20964479554502222, 0.3659551140721426]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.073314817868523, 0.35108527251431226, 0.20964479554502222, 0.3659551140721426]
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
probs:  [0.073314817868523, 0.35108527251431226, 0.20964479554502222, 0.3659551140721426]
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  0.005086666666666608 0.6920000000000001 0.6920000000000001
actor:  0 policy actor:  1  step number:  52 total reward:  0.40666666666666684  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.836]
 [0.899]
 [0.836]
 [0.836]
 [0.864]
 [0.955]] [[1.951]
 [2.015]
 [3.386]
 [2.015]
 [2.015]
 [2.71 ]
 [3.095]] [[0.579]
 [0.836]
 [0.899]
 [0.836]
 [0.836]
 [0.864]
 [0.955]]
maxi score, test score, baseline:  0.005393333333333284 0.6920000000000001 0.6920000000000001
probs:  [0.073314817868523, 0.35108527251431215, 0.20964479554502222, 0.3659551140721426]
actor:  1 policy actor:  1  step number:  29 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  45 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.5
siam score:  -0.72809446
first move QE:  -0.09179846318720014
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.313]
 [0.34 ]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[1.73]
 [1.73]
 [1.94]
 [1.73]
 [1.73]
 [1.73]
 [1.73]] [[0.151]
 [0.151]
 [0.317]
 [0.151]
 [0.151]
 [0.151]
 [0.151]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  0.6600000000000004  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.07283138219746296
actor:  1 policy actor:  1  step number:  35 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.003273333333333282 0.6920000000000001 0.6920000000000001
probs:  [0.07324618759571408, 0.35149170945663494, 0.20965012210285192, 0.3656119808447992]
maxi score, test score, baseline:  0.003273333333333282 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  0.003273333333333282 0.6920000000000001 0.6920000000000001
probs:  [0.07324618759571408, 0.35149170945663494, 0.20965012210285192, 0.3656119808447992]
maxi score, test score, baseline:  0.003273333333333282 0.6920000000000001 0.6920000000000001
probs:  [0.07324618759571408, 0.35149170945663494, 0.20965012210285192, 0.3656119808447992]
maxi score, test score, baseline:  0.003273333333333282 0.6920000000000001 0.6920000000000001
actor:  1 policy actor:  1  step number:  52 total reward:  0.6600000000000004  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.003273333333333282 0.6920000000000001 0.6920000000000001
probs:  [0.07307955821566861, 0.35222028598514493, 0.20992234833242887, 0.3647778074667576]
actor:  1 policy actor:  1  step number:  38 total reward:  0.7266666666666669  reward:  1.0 rdn_beta:  0.333
siam score:  -0.73348916
Printing some Q and Qe and total Qs values:  [[-0.108]
 [ 0.23 ]
 [ 0.23 ]
 [ 0.23 ]
 [ 0.23 ]
 [ 0.278]
 [ 0.23 ]] [[2.302]
 [1.989]
 [1.989]
 [1.989]
 [1.989]
 [3.071]
 [1.989]] [[-0.49 ]
 [-0.257]
 [-0.257]
 [-0.257]
 [-0.257]
 [ 0.152]
 [-0.257]]
maxi score, test score, baseline:  0.003273333333333282 0.6920000000000001 0.6920000000000001
probs:  [0.07299710053675579, 0.35295218760958225, 0.20968516978027163, 0.36436554207339034]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.085]
 [-0.013]
 [-0.014]
 [-0.013]
 [-0.013]
 [-0.026]] [[1.28 ]
 [0.551]
 [0.726]
 [0.483]
 [0.726]
 [0.726]
 [0.966]] [[0.434]
 [0.189]
 [0.179]
 [0.069]
 [0.179]
 [0.179]
 [0.276]]
using explorer policy with actor:  1
siam score:  -0.73540556
actor:  1 policy actor:  1  step number:  68 total reward:  0.4733333333333338  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]] [[-1.893]
 [-1.893]
 [-1.893]
 [-1.893]
 [-1.893]
 [-1.893]
 [-1.893]] [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  0.003273333333333282 0.6920000000000001 0.6920000000000001
probs:  [0.0729434083030653, 0.35269215366444995, 0.2102673421503411, 0.3640970958821438]
actor:  0 policy actor:  1  step number:  35 total reward:  0.6533333333333333  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0031933333333332803 0.6920000000000001 0.6920000000000001
probs:  [0.0729434083030653, 0.35269215366444995, 0.2102673421503411, 0.3640970958821438]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  53 total reward:  0.5466666666666671  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
siam score:  -0.7241887
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
probs:  [0.0729434083030653, 0.35269215366444995, 0.2102673421503411, 0.3640970958821438]
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]] [[1.431]
 [1.431]
 [1.431]
 [1.431]
 [1.431]
 [1.431]
 [1.431]] [[-0.105]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]]
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
probs:  [0.0729434083030653, 0.35269215366444995, 0.2102673421503411, 0.3640970958821438]
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
probs:  [0.0729434083030653, 0.35269215366444995, 0.2102673421503411, 0.3640970958821438]
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
probs:  [0.0729434083030653, 0.35269215366444995, 0.2102673421503411, 0.3640970958821438]
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
probs:  [0.0729434083030653, 0.35269215366444995, 0.2102673421503411, 0.3640970958821438]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6466666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
probs:  [0.0729434083030653, 0.35269215366444995, 0.2102673421503411, 0.3640970958821438]
first move QE:  -0.09205937645128301
actor:  1 policy actor:  1  step number:  38 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
probs:  [0.07286125803831862, 0.3534221579474609, 0.21003021654036497, 0.3636863674738554]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
probs:  [0.07286125803831862, 0.3534221579474609, 0.21003021654036497, 0.3636863674738554]
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
probs:  [0.07286125803831862, 0.3534221579474609, 0.21003021654036497, 0.3636863674738554]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
probs:  [0.07284017646797072, 0.3535144357414681, 0.2100645574681274, 0.36358083032243366]
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
probs:  [0.07284017646797072, 0.3535144357414681, 0.2100645574681274, 0.36358083032243366]
actor:  1 policy actor:  1  step number:  47 total reward:  0.6400000000000002  reward:  1.0 rdn_beta:  0.667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3068639528104817
siam score:  -0.72762
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
probs:  [0.07284017646797072, 0.3535144357414681, 0.2100645574681274, 0.36358083032243366]
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
probs:  [0.07284017646797072, 0.3535144357414681, 0.2100645574681274, 0.36358083032243366]
actor:  1 policy actor:  1  step number:  28 total reward:  0.8066666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
probs:  [0.07235013418305943, 0.36113248665571845, 0.20762971026607654, 0.35888766889514556]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.647]
 [0.507]
 [0.494]
 [0.494]
 [0.494]
 [0.546]] [[1.889]
 [2.651]
 [2.133]
 [1.889]
 [1.889]
 [1.889]
 [1.121]] [[0.414]
 [0.721]
 [0.485]
 [0.414]
 [0.414]
 [0.414]
 [0.26 ]]
first move QE:  -0.09340234901806466
maxi score, test score, baseline:  0.002899999999999951 0.6920000000000001 0.6920000000000001
actor:  1 policy actor:  1  step number:  31 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0004866666666667228 0.6920000000000001 0.6920000000000001
probs:  [0.07251619512803067, 0.36196379452298444, 0.20736939462746595, 0.35815061572151896]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.042]] [[0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [1.575]] [[0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.912]]
maxi score, test score, baseline:  -0.0004866666666667228 0.6920000000000001 0.6920000000000001
probs:  [0.07251619512803067, 0.36196379452298444, 0.20736939462746598, 0.35815061572151896]
maxi score, test score, baseline:  -0.0004866666666667228 0.6920000000000001 0.6920000000000001
probs:  [0.07251619512803067, 0.36196379452298444, 0.20736939462746598, 0.35815061572151896]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7000000000000003  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0004866666666667228 0.6920000000000001 0.6920000000000001
probs:  [0.07251192194896572, 0.3619424296634377, 0.20735715846027253, 0.35818848992732394]
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.077]
 [0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.091]] [[1.93 ]
 [2.133]
 [1.768]
 [1.768]
 [1.768]
 [1.768]
 [3.122]] [[0.438]
 [0.493]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.718]]
Printing some Q and Qe and total Qs values:  [[-0.007]
 [ 0.056]
 [-0.   ]
 [-0.004]
 [-0.007]
 [-0.005]
 [-0.002]] [[1.32 ]
 [1.734]
 [1.543]
 [1.385]
 [1.355]
 [1.353]
 [1.502]] [[-0.391]
 [-0.143]
 [-0.277]
 [-0.357]
 [-0.374]
 [-0.373]
 [-0.298]]
siam score:  -0.71851504
maxi score, test score, baseline:  -0.0004866666666667228 0.6920000000000001 0.6920000000000001
probs:  [0.07251192194896572, 0.3619424296634377, 0.20735715846027253, 0.35818848992732394]
maxi score, test score, baseline:  -0.0004866666666667228 0.6920000000000001 0.6920000000000001
probs:  [0.07251192194896572, 0.3619424296634377, 0.20735715846027253, 0.35818848992732394]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.0004866666666667228 0.6920000000000001 0.6920000000000001
probs:  [0.07251192194896572, 0.3619424296634377, 0.20735715846027253, 0.35818848992732394]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  40 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.064]
 [-0.051]
 [-0.051]
 [-0.051]
 [-0.051]
 [-0.051]
 [-0.051]] [[1.204]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]] [[ 0.123]
 [-0.391]
 [-0.391]
 [-0.391]
 [-0.391]
 [-0.391]
 [-0.391]]
first move QE:  -0.09124304391054566
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.773]
 [0.689]
 [0.766]
 [0.766]
 [0.766]
 [0.766]] [[0.137]
 [0.2  ]
 [0.06 ]
 [0.043]
 [0.043]
 [0.043]
 [0.043]] [[0.669]
 [0.773]
 [0.689]
 [0.766]
 [0.766]
 [0.766]
 [0.766]]
actor:  0 policy actor:  0  step number:  35 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
probs:  [0.07251192194896572, 0.3619424296634377, 0.20735715846027253, 0.35818848992732394]
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.035]
 [ 0.141]
 [-0.035]
 [-0.035]
 [-0.019]
 [-0.035]] [[1.902]
 [1.902]
 [1.736]
 [1.902]
 [1.902]
 [1.512]
 [1.902]] [[0.482]
 [0.482]
 [0.515]
 [0.482]
 [0.482]
 [0.362]
 [0.482]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.171]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]] [[1.646]
 [1.539]
 [1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]] [[0.129]
 [0.069]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]]
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
probs:  [0.07251192194896572, 0.3619424296634377, 0.20735715846027253, 0.35818848992732394]
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
probs:  [0.07251192194896572, 0.3619424296634377, 0.20735715846027253, 0.35818848992732394]
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
probs:  [0.07251192194896572, 0.3619424296634377, 0.20735715846027256, 0.35818848992732405]
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
probs:  [0.07251192194896572, 0.3619424296634377, 0.20735715846027256, 0.35818848992732405]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
probs:  [0.07295617899869485, 0.3641663983568686, 0.2066609052673305, 0.35621651737710613]
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
probs:  [0.07295617899869485, 0.3641663983568686, 0.2066609052673305, 0.35621651737710613]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.406]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[1.194]
 [1.407]
 [1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.194]] [[-0.   ]
 [ 0.179]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
probs:  [0.07347565780512637, 0.3667636863872241, 0.20813437845670468, 0.3516262773509449]
from probs:  [0.07347565780512637, 0.3667636863872241, 0.20813437845670468, 0.3516262773509449]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.7203821
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
probs:  [0.07347565780512637, 0.3667636863872241, 0.2081343784567047, 0.35162627735094487]
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
probs:  [0.07347565780512637, 0.3667636863872241, 0.2081343784567047, 0.35162627735094487]
maxi score, test score, baseline:  -0.0008600000000000647 0.6920000000000001 0.6920000000000001
probs:  [0.07347565780512637, 0.3667636863872241, 0.2081343784567047, 0.35162627735094487]
actor:  0 policy actor:  0  step number:  54 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0013533333333333865 0.6920000000000001 0.6920000000000001
probs:  [0.07347243793195686, 0.36674781076216456, 0.20816492014700774, 0.35161483115887077]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.0013533333333333865 0.6920000000000001 0.6920000000000001
probs:  [0.07347243793195686, 0.36674781076216456, 0.20816492014700774, 0.35161483115887077]
actor:  0 policy actor:  1  step number:  54 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.0018200000000000586 0.6920000000000001 0.6920000000000001
probs:  [0.07347243793195686, 0.36674781076216456, 0.20816492014700774, 0.35161483115887077]
siam score:  -0.719281
maxi score, test score, baseline:  -0.0018200000000000586 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.0018200000000000586 0.6920000000000001 0.6920000000000001
probs:  [0.07347243793195686, 0.36674781076216456, 0.20816492014700774, 0.35161483115887077]
maxi score, test score, baseline:  -0.0018200000000000586 0.6920000000000001 0.6920000000000001
probs:  [0.07347243793195686, 0.36674781076216456, 0.20816492014700774, 0.35161483115887077]
maxi score, test score, baseline:  -0.0018200000000000586 0.6920000000000001 0.6920000000000001
probs:  [0.07347243793195686, 0.36674781076216456, 0.20816492014700774, 0.35161483115887077]
Printing some Q and Qe and total Qs values:  [[0.741]
 [0.738]
 [0.747]
 [0.75 ]
 [0.618]
 [0.624]
 [0.749]] [[0.126]
 [0.129]
 [0.336]
 [0.392]
 [0.207]
 [0.57 ]
 [0.281]] [[0.36 ]
 [0.359]
 [0.502]
 [0.542]
 [0.293]
 [0.535]
 [0.469]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.5000000000000004  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0018200000000000586 0.6920000000000001 0.6920000000000001
probs:  [0.07347243793195686, 0.36674781076216456, 0.20816492014700774, 0.35161483115887077]
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.025]
 [-0.058]
 [-0.034]
 [-0.056]
 [-0.056]
 [-0.056]] [[1.785]
 [1.364]
 [1.249]
 [1.108]
 [1.785]
 [1.785]
 [1.785]] [[0.242]
 [0.133]
 [0.062]
 [0.039]
 [0.242]
 [0.242]
 [0.242]]
maxi score, test score, baseline:  -0.0018200000000000586 0.6920000000000001 0.6920000000000001
probs:  [0.07347243793195686, 0.36674781076216456, 0.20816492014700774, 0.35161483115887077]
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.896]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.88 ]] [[1.03 ]
 [0.655]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.854]] [[0.908]
 [0.896]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.88 ]]
maxi score, test score, baseline:  -0.0018200000000000586 0.6920000000000001 0.6920000000000001
probs:  [0.07346923396025874, 0.36673201353950907, 0.20819531100596184, 0.3516034414942703]
actor:  1 policy actor:  1  step number:  42 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.0018200000000000586 0.6920000000000001 0.6920000000000001
probs:  [0.07346491327195305, 0.3667104109534074, 0.20824194534373414, 0.3515827304309055]
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.837]
 [0.705]
 [0.705]
 [0.81 ]
 [0.704]
 [0.807]] [[1.167]
 [1.348]
 [1.259]
 [1.099]
 [0.89 ]
 [1.058]
 [0.853]] [[0.733]
 [0.837]
 [0.705]
 [0.705]
 [0.81 ]
 [0.704]
 [0.807]]
maxi score, test score, baseline:  -0.0018200000000000586 0.6920000000000001 0.6920000000000001
probs:  [0.07346491327195305, 0.3667104109534074, 0.2082419453437341, 0.3515827304309055]
maxi score, test score, baseline:  -0.0018200000000000586 0.6920000000000001 0.6920000000000001
probs:  [0.07346491327195305, 0.3667104109534074, 0.2082419453437341, 0.3515827304309055]
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[2.056]
 [2.056]
 [2.056]
 [2.056]
 [2.056]
 [2.056]
 [2.056]] [[0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.41969946959546756
actor:  0 policy actor:  0  step number:  51 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.0025666666666667283 0.6920000000000001 0.6920000000000001
probs:  [0.07346491327195305, 0.3667104109534074, 0.2082419453437341, 0.3515827304309055]
actor:  1 policy actor:  1  step number:  40 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
siam score:  -0.70935243
first move QE:  -0.0928854347003895
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  39 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.002860000000000065 0.6920000000000001 0.6920000000000001
probs:  [0.07343102654311165, 0.36654120482054897, 0.20860352599468293, 0.3514242426416565]
maxi score, test score, baseline:  -0.002860000000000065 0.6920000000000001 0.6920000000000001
probs:  [0.07343102654311165, 0.36654120482054897, 0.20860352599468293, 0.3514242426416565]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.432]
 [-0.432]
 [-0.432]
 [-0.432]
 [-0.432]
 [-0.432]
 [-0.432]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6266666666666668  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.082]
 [-0.084]
 [-0.081]
 [-0.086]
 [-0.083]
 [-0.086]
 [-0.087]] [[1.419]
 [1.388]
 [1.346]
 [2.649]
 [1.271]
 [1.081]
 [1.019]] [[0.287]
 [0.273]
 [0.26 ]
 [0.754]
 [0.229]
 [0.154]
 [0.13 ]]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.022]
 [ 0.116]
 [-0.037]
 [-0.006]
 [-0.102]
 [-0.037]] [[4.488]
 [1.834]
 [5.224]
 [1.894]
 [4.488]
 [2.375]
 [1.815]] [[ 0.524]
 [-0.086]
 [ 0.733]
 [-0.077]
 [ 0.524]
 [ 0.01 ]
 [-0.096]]
siam score:  -0.70820177
maxi score, test score, baseline:  -0.002860000000000065 0.6920000000000001 0.6920000000000001
probs:  [0.07343102654311165, 0.36654120482054897, 0.20860352599468293, 0.3514242426416565]
maxi score, test score, baseline:  -0.002860000000000065 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.002860000000000065 0.6920000000000001 0.6920000000000001
probs:  [0.07343102654311165, 0.36654120482054897, 0.20860352599468293, 0.3514242426416565]
actor:  1 policy actor:  1  step number:  31 total reward:  0.8000000000000002  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.002860000000000065 0.6920000000000001 0.6920000000000001
probs:  [0.07265290019155068, 0.3626507247746744, 0.2063900873980471, 0.35830628763572775]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.684]
 [0.496]
 [0.548]
 [0.047]
 [0.48 ]
 [0.566]] [[3.045]
 [1.618]
 [1.867]
 [2.092]
 [1.245]
 [1.994]
 [1.939]] [[ 0.849]
 [ 0.306]
 [ 0.293]
 [ 0.416]
 [-0.233]
 [ 0.334]
 [ 0.365]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.002860000000000065 0.6920000000000001 0.6920000000000001
probs:  [0.07304931628395717, 0.36463272794713747, 0.20751772261757134, 0.354800233151334]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.588]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[2.122]
 [2.613]
 [2.122]
 [2.122]
 [2.122]
 [2.122]
 [2.122]] [[0.383]
 [0.543]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]]
maxi score, test score, baseline:  -0.002860000000000065 0.6920000000000001 0.6920000000000001
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.401]
 [0.415]
 [0.401]
 [0.366]
 [0.423]
 [0.401]] [[2.62 ]
 [1.939]
 [2.218]
 [1.939]
 [1.991]
 [1.725]
 [1.939]] [[0.339]
 [0.401]
 [0.415]
 [0.401]
 [0.366]
 [0.423]
 [0.401]]
maxi score, test score, baseline:  -0.002860000000000065 0.6920000000000001 0.6920000000000001
probs:  [0.07304931628395717, 0.36463272794713747, 0.20751772261757134, 0.354800233151334]
from probs:  [0.07304931628395717, 0.36463272794713747, 0.20751772261757134, 0.354800233151334]
maxi score, test score, baseline:  -0.006246666666666724 0.6920000000000001 0.6920000000000001
probs:  [0.07304931628395717, 0.36463272794713747, 0.20751772261757134, 0.354800233151334]
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.767]
 [0.692]
 [0.701]
 [0.789]
 [0.722]
 [0.744]] [[1.465]
 [1.951]
 [1.167]
 [1.123]
 [2.171]
 [1.283]
 [1.175]] [[0.693]
 [0.767]
 [0.692]
 [0.701]
 [0.789]
 [0.722]
 [0.744]]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.472]
 [0.454]
 [0.45 ]
 [0.447]
 [0.448]
 [0.452]] [[0.532]
 [0.412]
 [0.225]
 [0.251]
 [0.248]
 [0.225]
 [0.682]] [[0.253]
 [0.224]
 [0.153]
 [0.16 ]
 [0.158]
 [0.151]
 [0.304]]
maxi score, test score, baseline:  -0.006246666666666724 0.6920000000000001 0.6920000000000001
probs:  [0.07304931628395717, 0.36463272794713747, 0.20751772261757134, 0.354800233151334]
maxi score, test score, baseline:  -0.006246666666666724 0.6920000000000001 0.6920000000000001
probs:  [0.07304931628395717, 0.36463272794713747, 0.20751772261757134, 0.354800233151334]
first move QE:  -0.08980465775832065
actor:  0 policy actor:  0  step number:  50 total reward:  0.3133333333333327  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]] [[2.332]
 [1.981]
 [1.981]
 [1.981]
 [1.981]
 [1.981]
 [1.981]] [[ 0.069]
 [-0.205]
 [-0.205]
 [-0.205]
 [-0.205]
 [-0.205]
 [-0.205]]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304931628395717, 0.36463272794713747, 0.20751772261757134, 0.354800233151334]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304931628395717, 0.36463272794713747, 0.20751772261757134, 0.354800233151334]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304931628395717, 0.36463272794713747, 0.20751772261757134, 0.354800233151334]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304919765809956, 0.36463213484097795, 0.2075190114557805, 0.3547996560451419]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304919765809956, 0.36463213484097795, 0.2075190114557805, 0.3547996560451419]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304616277410475, 0.36461717959499096, 0.20754913566219751, 0.3547875219687067]
Printing some Q and Qe and total Qs values:  [[ 0.04 ]
 [ 0.132]
 [ 0.088]
 [ 0.119]
 [-0.05 ]
 [ 0.102]
 [ 0.128]] [[1.882]
 [1.63 ]
 [1.582]
 [1.439]
 [0.693]
 [1.58 ]
 [1.664]] [[ 0.14 ]
 [ 0.082]
 [ 0.025]
 [-0.025]
 [-0.532]
 [ 0.034]
 [ 0.096]]
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.198]
 [0.238]
 [0.198]
 [0.184]
 [0.263]
 [0.267]] [[1.287]
 [1.287]
 [1.849]
 [1.287]
 [1.209]
 [1.693]
 [1.751]] [[-0.229]
 [-0.229]
 [ 0.126]
 [-0.229]
 [-0.285]
 [ 0.058]
 [ 0.095]]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304616277410475, 0.36461717959499096, 0.20754913566219751, 0.3547875219687067]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304616277410475, 0.36461717959499096, 0.20754913566219751, 0.3547875219687067]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.746]
 [0.696]
 [0.688]
 [0.696]
 [0.701]
 [0.702]] [[-0.096]
 [ 0.046]
 [ 0.133]
 [-0.156]
 [ 0.093]
 [ 0.088]
 [ 0.023]] [[0.117]
 [0.282]
 [0.29 ]
 [0.089]
 [0.263]
 [0.265]
 [0.223]]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304616277410475, 0.36461717959499096, 0.20754913566219751, 0.3547875219687067]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110151, 0.3547754473367196]
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[6.254]
 [1.235]
 [1.235]
 [1.235]
 [1.235]
 [1.235]
 [1.235]] [[0.926]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
actor:  1 policy actor:  1  step number:  62 total reward:  0.5933333333333338  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.147]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]] [[1.604]
 [1.761]
 [1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]] [[0.124]
 [0.22 ]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110151, 0.3547754473367196]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110151, 0.3547754473367196]
Printing some Q and Qe and total Qs values:  [[ 0.543]
 [ 0.586]
 [ 0.34 ]
 [ 0.543]
 [ 0.543]
 [-0.006]
 [ 0.568]] [[1.925]
 [2.21 ]
 [2.116]
 [1.925]
 [1.925]
 [1.275]
 [2.67 ]] [[ 0.387]
 [ 0.519]
 [ 0.255]
 [ 0.387]
 [ 0.387]
 [-0.343]
 [ 0.648]]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110146, 0.3547754473367196]
using another actor
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110146, 0.3547754473367196]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110146, 0.3547754473367196]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110146, 0.3547754473367196]
maxi score, test score, baseline:  -0.007006666666666722 0.6920000000000001 0.6920000000000001
siam score:  -0.71682084
actor:  0 policy actor:  0  step number:  39 total reward:  0.5600000000000002  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110146, 0.3547754473367196]
maxi score, test score, baseline:  -0.007273333333333396 0.6920000000000001 0.6920000000000001
probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110146, 0.3547754473367196]
actor:  0 policy actor:  1  step number:  59 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.007513333333333402 0.6920000000000001 0.6920000000000001
probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110146, 0.3547754473367196]
maxi score, test score, baseline:  -0.007513333333333402 0.6920000000000001 0.6920000000000001
probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110146, 0.3547754473367196]
siam score:  -0.71248317
actor:  1 policy actor:  1  step number:  55 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  86 total reward:  0.019999999999999463  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.008860000000000066 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.008860000000000066 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.008860000000000066 0.6920000000000001 0.6920000000000001
probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110146, 0.3547754473367196]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]] [[1.459]
 [1.459]
 [1.459]
 [1.459]
 [1.459]
 [1.459]
 [1.459]] [[-0.294]
 [-0.294]
 [-0.294]
 [-0.294]
 [-0.294]
 [-0.294]
 [-0.294]]
maxi score, test score, baseline:  -0.008860000000000066 0.6920000000000001 0.6920000000000001
probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110146, 0.3547754473367196]
maxi score, test score, baseline:  -0.008860000000000066 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.008860000000000066 0.6920000000000001 0.6920000000000001
probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110146, 0.3547754473367196]
maxi score, test score, baseline:  -0.008860000000000066 0.6920000000000001 0.6920000000000001
probs:  [0.07304314275790848, 0.3646022976142704, 0.20757911229110146, 0.3547754473367196]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2529772282611529
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]]
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.025]
 [-0.058]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.034]] [[1.319]
 [1.216]
 [1.453]
 [1.216]
 [1.216]
 [1.216]
 [1.967]] [[-0.128]
 [-0.158]
 [-0.058]
 [-0.158]
 [-0.158]
 [-0.158]
 [ 0.235]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.6200000000000003  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07393059766626182, 0.36903940443754885, 0.21010450534656971, 0.3469254925496196]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.008860000000000066 0.6920000000000001 0.6920000000000001
probs:  [0.07393059766626182, 0.36903940443754885, 0.21010450534656971, 0.3469254925496196]
maxi score, test score, baseline:  -0.008860000000000066 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.008860000000000066 0.6920000000000001 0.6920000000000001
line 256 mcts: sample exp_bonus 1.681331493583203
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  57 total reward:  0.5200000000000005  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.008860000000000066 0.6920000000000001 0.6920000000000001
probs:  [0.07446346375552391, 0.3717036341784668, 0.2116208599338849, 0.34221204213212447]
actor:  0 policy actor:  0  step number:  50 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.009180000000000061 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.009180000000000061 0.6920000000000001 0.6920000000000001
probs:  [0.07446346375552391, 0.3717036341784668, 0.2116208599338849, 0.34221204213212447]
maxi score, test score, baseline:  -0.009180000000000061 0.6920000000000001 0.6920000000000001
probs:  [0.07446346375552391, 0.3717036341784668, 0.2116208599338849, 0.34221204213212447]
maxi score, test score, baseline:  -0.012526666666666728 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.012526666666666728 0.6920000000000001 0.6920000000000001
probs:  [0.07446346375552391, 0.3717036341784668, 0.2116208599338849, 0.34221204213212447]
maxi score, test score, baseline:  -0.012526666666666728 0.6920000000000001 0.6920000000000001
probs:  [0.07446346375552391, 0.3717036341784668, 0.2116208599338849, 0.34221204213212447]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.012526666666666728 0.6920000000000001 0.6920000000000001
probs:  [0.07446346375552391, 0.3717036341784668, 0.2116208599338849, 0.34221204213212447]
actor:  1 policy actor:  1  step number:  47 total reward:  0.52  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[1.249]
 [1.249]
 [1.249]
 [1.249]
 [1.249]
 [1.249]
 [1.249]] [[0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]]
maxi score, test score, baseline:  -0.012526666666666728 0.6920000000000001 0.6920000000000001
probs:  [0.07430183302150607, 0.37089447770873046, 0.21189757379714258, 0.34290611547262084]
maxi score, test score, baseline:  -0.012526666666666728 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.01590000000000006 0.6920000000000001 0.6920000000000001
probs:  [0.07430183302150607, 0.37089447770873046, 0.21189757379714258, 0.34290611547262084]
Printing some Q and Qe and total Qs values:  [[-0.075]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.075]] [[1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]] [[0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]]
maxi score, test score, baseline:  -0.01590000000000006 0.6920000000000001 0.6920000000000001
probs:  [0.07430183302150607, 0.37089447770873046, 0.21189757379714258, 0.34290611547262084]
line 256 mcts: sample exp_bonus 1.036140134036825
maxi score, test score, baseline:  -0.019246666666666735 0.6920000000000001 0.6920000000000001
probs:  [0.07430183302150607, 0.37089447770873046, 0.21189757379714258, 0.34290611547262084]
maxi score, test score, baseline:  -0.019246666666666735 0.6920000000000001 0.6920000000000001
probs:  [0.07430183302150607, 0.37089447770873046, 0.21189757379714258, 0.34290611547262084]
maxi score, test score, baseline:  -0.019246666666666735 0.6920000000000001 0.6920000000000001
probs:  [0.07430183302150607, 0.37089447770873046, 0.21189757379714258, 0.34290611547262084]
maxi score, test score, baseline:  -0.019246666666666735 0.6920000000000001 0.6920000000000001
probs:  [0.07430183302150607, 0.37089447770873046, 0.21189757379714258, 0.34290611547262084]
maxi score, test score, baseline:  -0.019246666666666735 0.6920000000000001 0.6920000000000001
probs:  [0.07430183302150607, 0.37089447770873046, 0.21189757379714258, 0.34290611547262084]
maxi score, test score, baseline:  -0.019246666666666735 0.6920000000000001 0.6920000000000001
probs:  [0.07430183302150607, 0.37089447770873046, 0.21189757379714258, 0.34290611547262084]
maxi score, test score, baseline:  -0.019246666666666735 0.6920000000000001 0.6920000000000001
probs:  [0.07429839347207633, 0.37087750230020433, 0.21192682657591247, 0.3428972776518069]
maxi score, test score, baseline:  -0.019246666666666735 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.019246666666666735 0.6920000000000001 0.6920000000000001
probs:  [0.07429839347207633, 0.37087750230020433, 0.21192682657591247, 0.3428972776518069]
maxi score, test score, baseline:  -0.019246666666666735 0.6920000000000001 0.6920000000000001
probs:  [0.07429839347207633, 0.37087750230020433, 0.21192682657591247, 0.3428972776518069]
maxi score, test score, baseline:  -0.019246666666666735 0.6920000000000001 0.6920000000000001
probs:  [0.07429839347207633, 0.37087750230020433, 0.21192682657591247, 0.3428972776518069]
maxi score, test score, baseline:  -0.019246666666666735 0.6920000000000001 0.6920000000000001
probs:  [0.07429839347207633, 0.37087750230020433, 0.21192682657591247, 0.3428972776518069]
maxi score, test score, baseline:  -0.019246666666666735 0.6920000000000001 0.6920000000000001
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  60 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.4396618640577614
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.02002000000000007 0.6920000000000001 0.6920000000000001
probs:  [0.07429839347207633, 0.37087750230020433, 0.21192682657591247, 0.3428972776518069]
maxi score, test score, baseline:  -0.02002000000000007 0.6920000000000001 0.6920000000000001
probs:  [0.07429839347207633, 0.37087750230020433, 0.21192682657591247, 0.3428972776518069]
Printing some Q and Qe and total Qs values:  [[-0.054]
 [-0.035]
 [-0.036]
 [-0.034]
 [-0.036]
 [-0.03 ]
 [-0.036]] [[ 1.35 ]
 [ 0.535]
 [ 0.25 ]
 [-0.009]
 [ 0.015]
 [ 0.145]
 [ 0.694]] [[-0.47 ]
 [-0.587]
 [-0.635]
 [-0.677]
 [-0.675]
 [-0.647]
 [-0.561]]
maxi score, test score, baseline:  -0.02002000000000007 0.6920000000000001 0.6920000000000001
probs:  [0.07429839347207633, 0.37087750230020433, 0.21192682657591247, 0.3428972776518069]
maxi score, test score, baseline:  -0.02002000000000007 0.6920000000000001 0.6920000000000001
probs:  [0.07429839347207633, 0.37087750230020433, 0.21192682657591247, 0.3428972776518069]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
siam score:  -0.7153321
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  0 policy actor:  0  step number:  40 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  11715
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.307]
 [0.261]
 [0.307]
 [0.307]
 [0.307]
 [0.17 ]] [[2.509]
 [2.509]
 [3.807]
 [2.509]
 [2.509]
 [2.509]
 [3.857]] [[0.277]
 [0.277]
 [0.614]
 [0.277]
 [0.277]
 [0.277]
 [0.577]]
maxi score, test score, baseline:  -0.020140000000000064 0.6920000000000001 0.6920000000000001
probs:  [0.07407209411121968, 0.3697450884137657, 0.21236094229318786, 0.3438218751818267]
start point for exploration sampling:  11715
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.020140000000000064 0.6920000000000001 0.6920000000000001
probs:  [0.07407209411121968, 0.3697450884137657, 0.21236094229318786, 0.3438218751818267]
actor:  1 policy actor:  1  step number:  45 total reward:  0.6800000000000003  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 1.6394202709834103
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.020140000000000064 0.6920000000000001 0.6920000000000001
actor:  0 policy actor:  1  step number:  49 total reward:  0.5333333333333337  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.019446666666666723 0.6920000000000001 0.6920000000000001
probs:  [0.07407209411121968, 0.3697450884137657, 0.21236094229318786, 0.3438218751818267]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.019446666666666723 0.6920000000000001 0.6920000000000001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.21888781907378344
maxi score, test score, baseline:  -0.019446666666666723 0.6920000000000001 0.6920000000000001
actor:  1 policy actor:  1  step number:  58 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.019446666666666723 0.6920000000000001 0.6920000000000001
probs:  [0.07407209411121968, 0.3697450884137657, 0.21236094229318786, 0.3438218751818267]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.019446666666666723 0.6920000000000001 0.6920000000000001
probs:  [0.07407209411121968, 0.3697450884137657, 0.21236094229318786, 0.3438218751818267]
maxi score, test score, baseline:  -0.019446666666666723 0.6920000000000001 0.6920000000000001
probs:  [0.07407209411121968, 0.3697450884137657, 0.21236094229318786, 0.3438218751818267]
maxi score, test score, baseline:  -0.019446666666666723 0.6920000000000001 0.6920000000000001
probs:  [0.07407209411121968, 0.3697450884137657, 0.21236094229318786, 0.3438218751818267]
maxi score, test score, baseline:  -0.019446666666666723 0.6920000000000001 0.6920000000000001
probs:  [0.07407209411121968, 0.3697450884137657, 0.21236094229318786, 0.3438218751818267]
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]] [[0.947]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]] [[0.37 ]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]]
maxi score, test score, baseline:  -0.019446666666666723 0.6920000000000001 0.6920000000000001
probs:  [0.07407209411121968, 0.3697450884137657, 0.21236094229318786, 0.3438218751818267]
maxi score, test score, baseline:  -0.019446666666666723 0.6920000000000001 0.6920000000000001
probs:  [0.07407209411121968, 0.3697450884137657, 0.21236094229318786, 0.3438218751818267]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5200000000000004  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.043]
 [ 0.198]
 [ 0.099]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[1.616]
 [1.146]
 [1.354]
 [1.616]
 [1.616]
 [1.616]
 [1.616]] [[-0.235]
 [-0.229]
 [-0.223]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]]
maxi score, test score, baseline:  -0.019446666666666723 0.6920000000000001 0.6920000000000001
probs:  [0.07407209411121968, 0.3697450884137657, 0.21236094229318786, 0.3438218751818267]
maxi score, test score, baseline:  -0.019446666666666723 0.6920000000000001 0.6920000000000001
actor:  1 policy actor:  1  step number:  43 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.019446666666666723 0.6920000000000001 0.6920000000000001
probs:  [0.07407209411121968, 0.3697450884137657, 0.21236094229318786, 0.3438218751818267]
maxi score, test score, baseline:  -0.019446666666666723 0.6920000000000001 0.6920000000000001
actor:  1 policy actor:  1  step number:  48 total reward:  0.5133333333333335  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.3806062035294604
actor:  0 policy actor:  0  step number:  41 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.018900000000000066 0.6920000000000001 0.6920000000000001
probs:  [0.0740687531846259, 0.36972860422144377, 0.2123897855798486, 0.34381285701408165]
maxi score, test score, baseline:  -0.021340000000000064 0.6920000000000001 0.6920000000000001
probs:  [0.0740687531846259, 0.36972860422144377, 0.2123897855798486, 0.34381285701408165]
actor:  1 policy actor:  1  step number:  46 total reward:  0.6600000000000004  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.038]
 [ 0.092]
 [-0.038]
 [-0.038]
 [-0.088]
 [-0.038]] [[1.564]
 [1.564]
 [0.976]
 [1.564]
 [1.564]
 [0.821]
 [1.564]] [[0.422]
 [0.422]
 [0.343]
 [0.422]
 [0.422]
 [0.218]
 [0.422]]
first move QE:  -0.08451070449466808
siam score:  -0.7230728
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3991932411678134
actor:  1 policy actor:  1  step number:  37 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.039]
 [ 0.064]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.051]] [[1.359]
 [1.486]
 [1.359]
 [1.359]
 [1.359]
 [1.359]
 [1.669]] [[0.211]
 [0.316]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.32 ]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.021340000000000064 0.6920000000000001 0.6920000000000001
probs:  [0.07401857191834518, 0.36947786662479704, 0.21252172655468157, 0.3439818349021762]
maxi score, test score, baseline:  -0.021340000000000064 0.6920000000000001 0.6920000000000001
probs:  [0.07401857191834518, 0.36947786662479704, 0.21252172655468157, 0.3439818349021762]
actor:  1 policy actor:  1  step number:  44 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.021340000000000064 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.021340000000000064 0.6920000000000001 0.6920000000000001
probs:  [0.07401857191834518, 0.36947786662479704, 0.21252172655468157, 0.3439818349021762]
actor:  1 policy actor:  1  step number:  50 total reward:  0.6066666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.021340000000000064 0.6920000000000001 0.6920000000000001
probs:  [0.07401528886790244, 0.36946166888790205, 0.21255015584320305, 0.34397288640099255]
Printing some Q and Qe and total Qs values:  [[0.909]
 [0.972]
 [0.817]
 [0.818]
 [0.81 ]
 [0.913]
 [0.821]] [[-0.13 ]
 [-0.078]
 [ 0.405]
 [ 0.316]
 [ 0.408]
 [-0.02 ]
 [ 0.581]] [[0.909]
 [0.972]
 [0.817]
 [0.818]
 [0.81 ]
 [0.913]
 [0.821]]
actor:  0 policy actor:  0  step number:  48 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07401528886790244, 0.36946166888790205, 0.21255015584320305, 0.34397288640099255]
maxi score, test score, baseline:  -0.020766666666666725 0.6920000000000001 0.6920000000000001
probs:  [0.07401528886790244, 0.36946166888790205, 0.21255015584320305, 0.34397288640099255]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
siam score:  -0.72647965
maxi score, test score, baseline:  -0.024206666666666717 0.6920000000000001 0.6920000000000001
probs:  [0.07401528886790244, 0.36946166888790205, 0.21255015584320305, 0.34397288640099255]
maxi score, test score, baseline:  -0.024206666666666724 0.6920000000000001 0.6920000000000001
probs:  [0.07401528886790244, 0.36946166888790205, 0.21255015584320305, 0.34397288640099255]
maxi score, test score, baseline:  -0.024206666666666724 0.6920000000000001 0.6920000000000001
probs:  [0.07401528886790244, 0.36946166888790205, 0.21255015584320305, 0.34397288640099255]
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]] [[1.449]
 [1.449]
 [1.449]
 [1.449]
 [1.449]
 [1.449]
 [1.449]] [[0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.024206666666666724 0.6920000000000001 0.6920000000000001
probs:  [0.0744038323247562, 0.3714043065584613, 0.21366741438923204, 0.34052444672755044]
maxi score, test score, baseline:  -0.024206666666666724 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.024206666666666724 0.6920000000000001 0.6920000000000001
probs:  [0.0744038323247562, 0.3714043065584613, 0.21366741438923204, 0.34052444672755044]
maxi score, test score, baseline:  -0.024206666666666724 0.6920000000000001 0.6920000000000001
probs:  [0.0744038323247562, 0.3714043065584613, 0.21366741438923204, 0.34052444672755044]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0405],
        [ 0.9255],
        [-0.0000],
        [-0.0000],
        [-0.2730],
        [ 0.7340],
        [ 0.1763],
        [ 0.2306],
        [-0.0000],
        [ 0.0911]], dtype=torch.float64)
-0.032346567066 -0.07286266854219715
-0.032346567066 0.8931177186141194
-0.932617147638 -0.932617147638
-0.9636 -0.9636
-0.032346567066 -0.30538614259361835
-0.070771701198 0.6632436626053501
-0.045154513866 0.13118796287657802
-0.032346567066 0.19825740092778468
-0.7752715212 -0.7752715212
-0.032346567066 0.058748006192967876
actor:  1 policy actor:  1  step number:  47 total reward:  0.6533333333333338  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.024206666666666724 0.6920000000000001 0.6920000000000001
probs:  [0.0744038323247562, 0.3714043065584613, 0.21366741438923204, 0.34052444672755044]
maxi score, test score, baseline:  -0.024206666666666724 0.6920000000000001 0.6920000000000001
probs:  [0.0744038323247562, 0.3714043065584613, 0.21366741438923204, 0.34052444672755044]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.5
from probs:  [0.0744038323247562, 0.3714043065584613, 0.21366741438923204, 0.34052444672755044]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.024206666666666724 0.6920000000000001 0.6920000000000001
probs:  [0.07451836381129132, 0.37197767638534185, 0.21346693672574588, 0.34003702307762096]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5933333333333334  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  48 total reward:  0.4733333333333335  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07451836381129132, 0.37197767638534185, 0.21346693672574588, 0.34003702307762096]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]] [[0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]]
maxi score, test score, baseline:  -0.023233333333333387 0.6920000000000001 0.6920000000000001
probs:  [0.07451836381129132, 0.37197767638534185, 0.21346693672574588, 0.34003702307762096]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.023233333333333387 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.023233333333333387 0.6920000000000001 0.6920000000000001
probs:  [0.07451836381129132, 0.37197767638534185, 0.21346693672574588, 0.34003702307762096]
maxi score, test score, baseline:  -0.023233333333333387 0.6920000000000001 0.6920000000000001
probs:  [0.07451836381129132, 0.37197767638534185, 0.21346693672574588, 0.34003702307762096]
actor:  1 policy actor:  1  step number:  29 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.023233333333333387 0.6920000000000001 0.6920000000000001
probs:  [0.07513963391666693, 0.3750878912576168, 0.21237945604346792, 0.33739301878224837]
actor:  0 policy actor:  1  step number:  40 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9558990559383467
maxi score, test score, baseline:  -0.022793333333333388 0.6920000000000001 0.6920000000000001
probs:  [0.07513963391666693, 0.3750878912576168, 0.21237945604346792, 0.3373930187822484]
maxi score, test score, baseline:  -0.022793333333333388 0.6920000000000001 0.6920000000000001
probs:  [0.07513963391666693, 0.3750878912576168, 0.21237945604346792, 0.3373930187822484]
first move QE:  -0.08392288538101168
actor:  1 policy actor:  1  step number:  33 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[0.002]
 [0.002]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[-0.231]
 [-0.231]
 [-0.231]
 [-0.231]
 [-0.231]
 [-0.231]
 [-0.231]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.8266666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.022793333333333388 0.6920000000000001 0.6920000000000001
probs:  [0.07389600855695065, 0.36886994088085034, 0.2254322279299068, 0.3318018226322923]
maxi score, test score, baseline:  -0.022793333333333388 0.6920000000000001 0.6920000000000001
probs:  [0.07389600855695065, 0.36886994088085034, 0.2254322279299068, 0.3318018226322923]
maxi score, test score, baseline:  -0.022793333333333388 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.022793333333333388 0.6920000000000001 0.6920000000000001
probs:  [0.07389600855695065, 0.36886994088085034, 0.2254322279299068, 0.3318018226322923]
siam score:  -0.7042121
maxi score, test score, baseline:  -0.022793333333333388 0.6920000000000001 0.6920000000000001
probs:  [0.07389600855695065, 0.36886994088085034, 0.2254322279299068, 0.3318018226322923]
maxi score, test score, baseline:  -0.022793333333333388 0.6920000000000001 0.6920000000000001
probs:  [0.07389600855695065, 0.36886994088085034, 0.2254322279299068, 0.3318018226322923]
maxi score, test score, baseline:  -0.022793333333333388 0.6920000000000001 0.6920000000000001
probs:  [0.07389600855695065, 0.36886994088085034, 0.2254322279299068, 0.3318018226322923]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.035]
 [-0.03 ]
 [-0.038]
 [-0.031]
 [-0.031]
 [-0.034]] [[ 0.063]
 [ 0.384]
 [-0.148]
 [ 0.183]
 [-0.128]
 [-0.205]
 [ 0.617]] [[-0.58 ]
 [-0.471]
 [-0.643]
 [-0.541]
 [-0.636]
 [-0.662]
 [-0.392]]
maxi score, test score, baseline:  -0.022793333333333388 0.6920000000000001 0.6920000000000001
probs:  [0.07369504183904987, 0.3678651358006663, 0.2248183073436839, 0.3336215150165999]
from probs:  [0.07369504183904987, 0.3678651358006663, 0.2248183073436839, 0.3336215150165999]
maxi score, test score, baseline:  -0.022793333333333388 0.6920000000000001 0.6920000000000001
probs:  [0.07369504183904987, 0.3678651358006663, 0.22481830734368388, 0.3336215150165999]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.022793333333333388 0.6920000000000001 0.6920000000000001
actor:  0 policy actor:  0  step number:  38 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.035]
 [-0.032]
 [-0.037]
 [-0.039]
 [-0.019]
 [-0.021]] [[1.011]
 [1.594]
 [0.982]
 [0.862]
 [0.957]
 [1.446]
 [1.619]] [[0.098]
 [0.259]
 [0.087]
 [0.05 ]
 [0.076]
 [0.226]
 [0.274]]
Starting evaluation
line 256 mcts: sample exp_bonus 0.3588937786826043
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[1.671]
 [1.979]
 [1.671]
 [1.671]
 [1.671]
 [1.671]
 [1.671]] [[0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.735]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]] [[1.055]
 [1.421]
 [1.055]
 [1.055]
 [1.055]
 [1.055]
 [1.055]] [[0.703]
 [0.735]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]]
maxi score, test score, baseline:  -0.02462000000000005 0.6920000000000001 0.6920000000000001
probs:  [0.07369504183904987, 0.3678651358006663, 0.2248183073436839, 0.3336215150165999]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
siam score:  -0.7078121
line 256 mcts: sample exp_bonus 0.6027096791716803
maxi score, test score, baseline:  -0.026713333333333384 0.6920000000000001 0.6920000000000001
probs:  [0.07369504183904987, 0.3678651358006663, 0.2248183073436839, 0.3336215150165999]
actor:  0 policy actor:  0  step number:  23 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.8200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0066733333333333705 0.6920000000000001 0.6920000000000001
probs:  [0.07369504183904987, 0.3678651358006663, 0.2248183073436839, 0.3336215150165999]
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.004019999999999973 0.7253333333333334 0.7253333333333334
actor:  1 policy actor:  1  step number:  35 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.004019999999999973 0.7253333333333334 0.7253333333333334
probs:  [0.07283388646475043, 0.36337277111435073, 0.23010709092322273, 0.3336862514976761]
maxi score, test score, baseline:  0.004019999999999973 0.7253333333333334 0.7253333333333334
probs:  [0.07283388646475043, 0.36337277111435073, 0.23010709092322273, 0.3336862514976761]
actor:  0 policy actor:  1  step number:  36 total reward:  0.5933333333333334  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07283388646475043, 0.36337277111435073, 0.23010709092322273, 0.3336862514976761]
maxi score, test score, baseline:  0.004206666666666632 0.7253333333333334 0.7253333333333334
probs:  [0.07283388646475043, 0.36337277111435073, 0.23010709092322273, 0.3336862514976761]
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]] [[1.896]
 [1.896]
 [1.896]
 [1.896]
 [1.896]
 [1.896]
 [1.896]] [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]]
maxi score, test score, baseline:  0.004206666666666634 0.7253333333333334 0.7253333333333334
probs:  [0.07283388646475043, 0.36337277111435073, 0.23010709092322273, 0.3336862514976761]
maxi score, test score, baseline:  0.004206666666666634 0.7253333333333334 0.7253333333333334
probs:  [0.07283388646475043, 0.36337277111435073, 0.23010709092322273, 0.3336862514976761]
maxi score, test score, baseline:  0.004206666666666634 0.7253333333333334 0.7253333333333334
probs:  [0.07283388646475043, 0.36337277111435073, 0.23010709092322273, 0.3336862514976761]
maxi score, test score, baseline:  0.004206666666666634 0.7253333333333334 0.7253333333333334
probs:  [0.07283388646475043, 0.36337277111435073, 0.23010709092322273, 0.3336862514976761]
maxi score, test score, baseline:  0.004206666666666634 0.7253333333333334 0.7253333333333334
probs:  [0.07283388646475043, 0.36337277111435073, 0.23010709092322273, 0.3336862514976761]
maxi score, test score, baseline:  0.004206666666666634 0.7253333333333334 0.7253333333333334
probs:  [0.07283388646475043, 0.36337277111435073, 0.23010709092322273, 0.3336862514976761]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.613]
 [0.423]
 [0.421]
 [0.415]
 [0.414]
 [0.485]] [[ 0.367]
 [-0.078]
 [ 0.559]
 [ 0.494]
 [ 0.53 ]
 [ 0.473]
 [ 0.277]] [[-0.172]
 [-0.162]
 [-0.14 ]
 [-0.163]
 [-0.158]
 [-0.178]
 [-0.172]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.004206666666666634 0.7253333333333334 0.7253333333333334
probs:  [0.07283087107604917, 0.36335796965965456, 0.23013107032338762, 0.3336800889409087]
maxi score, test score, baseline:  0.004206666666666634 0.7253333333333334 0.7253333333333334
probs:  [0.07283087107604917, 0.36335796965965456, 0.23013107032338762, 0.3336800889409087]
maxi score, test score, baseline:  0.004206666666666634 0.7253333333333334 0.7253333333333334
probs:  [0.07283087107604917, 0.36335796965965456, 0.23013107032338762, 0.3336800889409087]
maxi score, test score, baseline:  0.004206666666666634 0.7253333333333334 0.7253333333333334
probs:  [0.07283087107604917, 0.36335796965965456, 0.23013107032338762, 0.3336800889409087]
maxi score, test score, baseline:  0.004206666666666634 0.7253333333333334 0.7253333333333334
probs:  [0.07283087107604917, 0.36335796965965456, 0.23013107032338762, 0.3336800889409087]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  45 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.265]
 [0.413]
 [0.265]
 [0.265]
 [0.265]
 [0.265]] [[1.619]
 [1.619]
 [1.89 ]
 [1.619]
 [1.619]
 [1.619]
 [1.619]] [[0.   ]
 [0.   ]
 [0.238]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.004659999999999969 0.7253333333333334 0.7253333333333334
probs:  [0.07283087107604917, 0.36335796965965456, 0.23013107032338762, 0.3336800889409087]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.582]
 [ 0.708]
 [ 0.642]
 [ 0.638]
 [-0.044]
 [ 0.58 ]
 [ 0.658]] [[ 0.494]
 [ 0.218]
 [ 0.323]
 [ 0.479]
 [-0.252]
 [ 0.818]
 [ 0.32 ]] [[ 0.505]
 [ 0.538]
 [ 0.507]
 [ 0.554]
 [-0.355]
 [ 0.609]
 [ 0.522]]
Printing some Q and Qe and total Qs values:  [[0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]] [[0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]] [[0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]]
siam score:  -0.7173419
actor:  1 policy actor:  1  step number:  53 total reward:  0.4933333333333336  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  55 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.004659999999999972 0.7253333333333334 0.7253333333333334
probs:  [0.07283087107604917, 0.36335796965965456, 0.23013107032338762, 0.3336800889409087]
maxi score, test score, baseline:  0.004659999999999972 0.7253333333333334 0.7253333333333334
probs:  [0.07283087107604917, 0.36335796965965456, 0.23013107032338762, 0.3336800889409087]
maxi score, test score, baseline:  0.004659999999999972 0.7253333333333334 0.7253333333333334
probs:  [0.07283087107604917, 0.36335796965965456, 0.23013107032338762, 0.3336800889409087]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.004659999999999972 0.7253333333333334 0.7253333333333334
probs:  [0.07262079149373617, 0.362308140428152, 0.23235502638814784, 0.3327160416899641]
maxi score, test score, baseline:  0.004659999999999972 0.7253333333333334 0.7253333333333334
probs:  [0.07262079149373617, 0.362308140428152, 0.23235502638814784, 0.3327160416899641]
actor:  1 policy actor:  1  step number:  51 total reward:  0.506666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.004659999999999972 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.004659999999999972 0.7253333333333334 0.7253333333333334
probs:  [0.0726583052153697, 0.36249560748763365, 0.2324752247067413, 0.3323708625902554]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
siam score:  -0.721196
maxi score, test score, baseline:  0.004659999999999972 0.7253333333333334 0.7253333333333334
probs:  [0.0726583052153697, 0.36249560748763365, 0.2324752247067413, 0.3323708625902554]
maxi score, test score, baseline:  0.004659999999999972 0.7253333333333334 0.7253333333333334
probs:  [0.0726583052153697, 0.36249560748763365, 0.2324752247067413, 0.3323708625902554]
actor:  0 policy actor:  1  step number:  37 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.004953333333333297 0.7253333333333334 0.7253333333333334
probs:  [0.0726583052153697, 0.36249560748763365, 0.2324752247067413, 0.3323708625902554]
maxi score, test score, baseline:  0.004953333333333297 0.7253333333333334 0.7253333333333334
probs:  [0.0726583052153697, 0.36249560748763365, 0.2324752247067413, 0.3323708625902554]
maxi score, test score, baseline:  0.004953333333333297 0.7253333333333334 0.7253333333333334
probs:  [0.0726583052153697, 0.36249560748763365, 0.2324752247067413, 0.3323708625902554]
first move QE:  -0.0823571303989423
actor:  1 policy actor:  1  step number:  48 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]] [[0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]] [[0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]]
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.556]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]] [[1.224]
 [1.001]
 [1.224]
 [1.224]
 [1.224]
 [1.224]
 [1.224]] [[0.014]
 [0.007]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]]
siam score:  -0.7204512
first move QE:  -0.08231699595580722
actor:  1 policy actor:  1  step number:  49 total reward:  0.6400000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.004953333333333297 0.7253333333333334 0.7253333333333334
probs:  [0.0726583052153697, 0.36249560748763365, 0.2324752247067413, 0.3323708625902554]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.004953333333333297 0.7253333333333334 0.7253333333333334
probs:  [0.0726583052153697, 0.36249560748763365, 0.2324752247067413, 0.3323708625902554]
maxi score, test score, baseline:  0.004953333333333297 0.7253333333333334 0.7253333333333334
probs:  [0.0726583052153697, 0.36249560748763365, 0.2324752247067413, 0.3323708625902554]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.004953333333333297 0.7253333333333334 0.7253333333333334
probs:  [0.07270289029708532, 0.3627143925995081, 0.23212492262445453, 0.332457794478952]
actor:  0 policy actor:  0  step number:  69 total reward:  0.05333333333333268  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.003913333333333304 0.7253333333333334 0.7253333333333334
probs:  [0.07270289029708532, 0.3627143925995081, 0.23212492262445453, 0.332457794478952]
actor:  1 policy actor:  1  step number:  28 total reward:  0.8200000000000001  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 1.86337179327376
maxi score, test score, baseline:  0.003913333333333304 0.7253333333333334 0.7253333333333334
probs:  [0.07129667776973665, 0.3556872144516387, 0.22762881278009803, 0.34538729499852666]
maxi score, test score, baseline:  0.003913333333333304 0.7253333333333334 0.7253333333333334
probs:  [0.07129667776973665, 0.3556872144516387, 0.22762881278009803, 0.34538729499852666]
maxi score, test score, baseline:  0.003913333333333304 0.7253333333333334 0.7253333333333334
probs:  [0.07129409102079846, 0.35567455805848996, 0.227653693177834, 0.34537765774287754]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.003913333333333304 0.7253333333333334 0.7253333333333334
probs:  [0.07139888598263665, 0.35618729673886895, 0.22664573284091144, 0.34576808443758295]
from probs:  [0.07139888598263665, 0.35618729673886895, 0.22664573284091144, 0.34576808443758295]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0012199999999999692 0.7253333333333334 0.7253333333333334
probs:  [0.07139888598263665, 0.35618729673886895, 0.22664573284091144, 0.34576808443758295]
start point for exploration sampling:  11715
UNIT TEST: sample policy line 217 mcts : [0.041 0.469 0.041 0.061 0.102 0.163 0.122]
maxi score, test score, baseline:  0.0012199999999999692 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.0012199999999999692 0.7253333333333334 0.7253333333333334
probs:  [0.07139888598263665, 0.35618729673886895, 0.22664573284091144, 0.34576808443758295]
Printing some Q and Qe and total Qs values:  [[0.901]
 [0.995]
 [0.903]
 [0.867]
 [0.867]
 [0.867]
 [0.946]] [[0.405]
 [0.06 ]
 [0.076]
 [0.428]
 [0.428]
 [0.428]
 [0.257]] [[0.901]
 [0.995]
 [0.903]
 [0.867]
 [0.867]
 [0.867]
 [0.946]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.7000000000000003  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  47 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.003886666666666637 0.7253333333333334 0.7253333333333334
probs:  [0.07139888598263665, 0.35618729673886895, 0.22664573284091144, 0.34576808443758295]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.003886666666666637 0.7253333333333334 0.7253333333333334
probs:  [0.07076204073431216, 0.3530049251330911, 0.2246212435534918, 0.3516117905791049]
line 256 mcts: sample exp_bonus 3.002062031948071
actor:  1 policy actor:  1  step number:  38 total reward:  0.7133333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.003886666666666637 0.7253333333333334 0.7253333333333334
probs:  [0.07075941417246825, 0.3529920949904369, 0.22464908901595787, 0.35159940182113686]
actor:  1 policy actor:  1  step number:  29 total reward:  0.8133333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0010466666666666298 0.7253333333333334 0.7253333333333334
probs:  [0.07244634048112912, 0.34503340886462425, 0.2210766663473654, 0.36144358430688117]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0010466666666666298 0.7253333333333334 0.7253333333333334
probs:  [0.07244330206553126, 0.3450235014178633, 0.2211045185120156, 0.36142867800458967]
maxi score, test score, baseline:  0.0010466666666666298 0.7253333333333334 0.7253333333333334
probs:  [0.07244330206553126, 0.3450235014178633, 0.2211045185120156, 0.36142867800458967]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0010466666666666298 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.0010466666666666298 0.7253333333333334 0.7253333333333334
probs:  [0.07315428940228753, 0.342049650520686, 0.2198058494776684, 0.3649902105993581]
maxi score, test score, baseline:  0.0010466666666666298 0.7253333333333334 0.7253333333333334
probs:  [0.07315428940228753, 0.342049650520686, 0.2198058494776684, 0.3649902105993581]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5600000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0010466666666666298 0.7253333333333334 0.7253333333333334
probs:  [0.07315428940228753, 0.342049650520686, 0.2198058494776684, 0.3649902105993581]
maxi score, test score, baseline:  0.0010466666666666298 0.7253333333333334 0.7253333333333334
probs:  [0.07315428940228753, 0.342049650520686, 0.2198058494776684, 0.3649902105993581]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.0010466666666666298 0.7253333333333334 0.7253333333333334
probs:  [0.0731542894022875, 0.34204965052068603, 0.21980584947766843, 0.36499021059935804]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.0010466666666666298 0.7253333333333334 0.7253333333333334
probs:  [0.07402239147098617, 0.3384186349141084, 0.21822019918969673, 0.36933877442520857]
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[2.225]
 [2.225]
 [2.225]
 [2.225]
 [2.225]
 [2.225]
 [2.225]] [[0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]]
maxi score, test score, baseline:  0.0010466666666666298 0.7253333333333334 0.7253333333333334
probs:  [0.07402239147098617, 0.3384186349141084, 0.21822019918969673, 0.36933877442520857]
from probs:  [0.07402239147098617, 0.3384186349141084, 0.21822019918969673, 0.36933877442520857]
maxi score, test score, baseline:  0.001046666666666637 0.7253333333333334 0.7253333333333334
probs:  [0.07402239147098617, 0.3384186349141084, 0.21822019918969673, 0.36933877442520857]
maxi score, test score, baseline:  0.001046666666666637 0.7253333333333334 0.7253333333333334
probs:  [0.07402239147098617, 0.3384186349141084, 0.21822019918969673, 0.36933877442520857]
maxi score, test score, baseline:  0.001046666666666637 0.7253333333333334 0.7253333333333334
from probs:  [0.07402239147098617, 0.3384186349141084, 0.21822019918969673, 0.36933877442520857]
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.134]
 [0.084]
 [0.083]
 [0.092]
 [0.083]
 [0.158]] [[-0.619]
 [-0.094]
 [-0.438]
 [-0.425]
 [-0.223]
 [-0.467]
 [-0.525]] [[0.085]
 [0.134]
 [0.084]
 [0.083]
 [0.092]
 [0.083]
 [0.158]]
maxi score, test score, baseline:  0.001046666666666637 0.7253333333333334 0.7253333333333334
probs:  [0.07402239147098617, 0.3384186349141084, 0.21822019918969673, 0.36933877442520857]
maxi score, test score, baseline:  0.001046666666666637 0.7253333333333334 0.7253333333333334
probs:  [0.07402239147098617, 0.3384186349141084, 0.21822019918969673, 0.36933877442520857]
maxi score, test score, baseline:  0.001046666666666637 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  -0.0014733333333333608 0.7253333333333334 0.7253333333333334
probs:  [0.07402239147098617, 0.3384186349141084, 0.21822019918969673, 0.36933877442520857]
actor:  0 policy actor:  0  step number:  37 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0015933333333333075 0.7253333333333334 0.7253333333333334
probs:  [0.07402239147098617, 0.3384186349141084, 0.21822019918969673, 0.36933877442520857]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.0006066666666666883 0.7253333333333334 0.7253333333333334
probs:  [0.07402239147098617, 0.3384186349141084, 0.21822019918969673, 0.36933877442520857]
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]] [[0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]] [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07402239147098617, 0.3384186349141084, 0.21822019918969673, 0.36933877442520857]
maxi score, test score, baseline:  -0.0006066666666666883 0.7253333333333334 0.7253333333333334
probs:  [0.07391891369557727, 0.3388514520113042, 0.21840920872013342, 0.3688204255729852]
maxi score, test score, baseline:  -0.0006066666666666883 0.7253333333333334 0.7253333333333334
probs:  [0.07391891369557727, 0.3388514520113042, 0.21840920872013342, 0.3688204255729852]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]] [[7.442]
 [2.732]
 [2.732]
 [2.732]
 [2.732]
 [2.732]
 [2.732]] [[0.821]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]]
maxi score, test score, baseline:  -0.0006066666666666883 0.7253333333333334 0.7253333333333334
probs:  [0.07391891369557727, 0.3388514520113042, 0.21840920872013342, 0.3688204255729852]
actor:  0 policy actor:  1  step number:  36 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7217778
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]] [[7.783]
 [2.378]
 [2.378]
 [2.378]
 [2.378]
 [2.378]
 [2.378]] [[0.85 ]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]]
maxi score, test score, baseline:  0.0024999999999999753 0.7253333333333334 0.7253333333333334
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]] [[-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]]
actor:  1 policy actor:  1  step number:  84 total reward:  0.4333333333333337  reward:  1.0 rdn_beta:  0.5
siam score:  -0.718255
maxi score, test score, baseline:  0.0024999999999999753 0.7253333333333334 0.7253333333333334
probs:  [0.07391553977361356, 0.3388436879006852, 0.21843694397236407, 0.3688038283533371]
maxi score, test score, baseline:  0.0024999999999999753 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.0024999999999999753 0.7253333333333334 0.7253333333333334
probs:  [0.07391553977361356, 0.3388436879006852, 0.21843694397236407, 0.3688038283533371]
actor:  1 policy actor:  1  step number:  92 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.592]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]] [[4.025]
 [0.721]
 [1.427]
 [1.427]
 [1.427]
 [1.427]
 [1.427]] [[0.658]
 [0.092]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]]
maxi score, test score, baseline:  0.0024999999999999753 0.7253333333333334 0.7253333333333334
probs:  [0.07391553977361356, 0.3388436879006852, 0.21843694397236407, 0.3688038283533371]
actor:  1 policy actor:  1  step number:  39 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]] [[1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.111]] [[0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]]
maxi score, test score, baseline:  0.0024999999999999753 0.7253333333333334 0.7253333333333334
probs:  [0.07401899393923644, 0.33841101382664834, 0.21824793391108624, 0.3693220583230291]
Printing some Q and Qe and total Qs values:  [[-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]] [[0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]] [[0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]]
maxi score, test score, baseline:  0.0024999999999999753 0.7253333333333334 0.7253333333333334
probs:  [0.07401561263850434, 0.338403429147385, 0.21827553613547918, 0.36930542207863143]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.815]
 [0.863]
 [0.815]
 [0.815]
 [0.815]
 [0.815]] [[1.337]
 [1.337]
 [2.014]
 [1.337]
 [1.337]
 [1.337]
 [1.337]] [[0.815]
 [0.815]
 [0.863]
 [0.815]
 [0.815]
 [0.815]
 [0.815]]
maxi score, test score, baseline:  0.0024999999999999753 0.7253333333333334 0.7253333333333334
probs:  [0.07401561263850434, 0.338403429147385, 0.21827553613547918, 0.36930542207863143]
UNIT TEST: sample policy line 217 mcts : [0.041 0.449 0.143 0.082 0.204 0.041 0.041]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  51 total reward:  0.6133333333333337  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 2.913992274594489
maxi score, test score, baseline:  0.005726666666666635 0.7253333333333334 0.7253333333333334
probs:  [0.07401561263850434, 0.338403429147385, 0.21827553613547918, 0.36930542207863143]
maxi score, test score, baseline:  0.005726666666666635 0.7253333333333334 0.7253333333333334
probs:  [0.07401561263850434, 0.338403429147385, 0.21827553613547918, 0.36930542207863143]
maxi score, test score, baseline:  0.005726666666666635 0.7253333333333334 0.7253333333333334
probs:  [0.07401561263850434, 0.338403429147385, 0.21827553613547918, 0.36930542207863143]
maxi score, test score, baseline:  0.005726666666666635 0.7253333333333334 0.7253333333333334
probs:  [0.07401561263850434, 0.338403429147385, 0.21827553613547918, 0.36930542207863143]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.005726666666666635 0.7253333333333334 0.7253333333333334
probs:  [0.07363916678860054, 0.3366795691010754, 0.22225718960720334, 0.36742407450312087]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.005726666666666635 0.7253333333333334 0.7253333333333334
probs:  [0.07363916678860054, 0.3366795691010754, 0.22225718960720334, 0.36742407450312087]
actor:  1 policy actor:  1  step number:  37 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.309]
 [0.075]
 [0.225]
 [0.225]
 [0.147]
 [0.26 ]] [[2.188]
 [1.726]
 [1.347]
 [1.337]
 [1.337]
 [1.152]
 [1.959]] [[ 0.232]
 [ 0.033]
 [-0.312]
 [-0.21 ]
 [-0.21 ]
 [-0.352]
 [ 0.109]]
maxi score, test score, baseline:  0.005726666666666635 0.7253333333333334 0.7253333333333334
probs:  [0.07368310146239263, 0.3367746601178058, 0.22190211704682578, 0.36764012137297575]
actor:  1 policy actor:  1  step number:  46 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  48 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.005966666666666646 0.7253333333333334 0.7253333333333334
probs:  [0.07368310146239263, 0.3367746601178058, 0.22190211704682578, 0.36764012137297575]
first move QE:  -0.08323449356276945
maxi score, test score, baseline:  0.005966666666666646 0.7253333333333334 0.7253333333333334
probs:  [0.07368310146239263, 0.3367746601178058, 0.22190211704682578, 0.36764012137297575]
maxi score, test score, baseline:  0.005966666666666646 0.7253333333333334 0.7253333333333334
probs:  [0.07368310146239263, 0.3367746601178058, 0.22190211704682578, 0.36764012137297575]
line 256 mcts: sample exp_bonus 1.8114870371800786
maxi score, test score, baseline:  0.005966666666666646 0.7253333333333334 0.7253333333333334
probs:  [0.07368310146239263, 0.3367746601178058, 0.22190211704682578, 0.36764012137297575]
Printing some Q and Qe and total Qs values:  [[ 0.238]
 [ 0.191]
 [ 0.246]
 [ 0.234]
 [-0.11 ]
 [ 0.233]
 [ 0.225]] [[1.561]
 [1.321]
 [1.419]
 [1.57 ]
 [0.549]
 [1.461]
 [1.667]] [[ 0.383]
 [ 0.295]
 [ 0.356]
 [ 0.382]
 [-0.091]
 [ 0.357]
 [ 0.399]]
maxi score, test score, baseline:  0.005966666666666646 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.005966666666666646 0.7253333333333334 0.7253333333333334
probs:  [0.07368310146239263, 0.3367746601178058, 0.22190211704682578, 0.36764012137297575]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.005966666666666646 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.005966666666666646 0.7253333333333334 0.7253333333333334
probs:  [0.07368310146239263, 0.3367746601178058, 0.22190211704682578, 0.36764012137297575]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6600000000000003  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  50 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7228728
maxi score, test score, baseline:  0.008566666666666639 0.7253333333333334 0.7253333333333334
probs:  [0.07368310146239263, 0.3367746601178058, 0.22190211704682578, 0.36764012137297575]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.008566666666666639 0.7253333333333334 0.7253333333333334
probs:  [0.07368310146239263, 0.3367746601178058, 0.22190211704682578, 0.36764012137297575]
maxi score, test score, baseline:  0.008566666666666639 0.7253333333333334 0.7253333333333334
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.394]
 [0.243]
 [0.392]
 [0.384]
 [0.243]
 [0.243]] [[0.987]
 [0.782]
 [1.608]
 [1.372]
 [0.996]
 [1.608]
 [1.608]] [[-0.094]
 [-0.161]
 [-0.174]
 [-0.065]
 [-0.135]
 [-0.174]
 [-0.174]]
maxi score, test score, baseline:  0.008566666666666639 0.7253333333333334 0.7253333333333334
probs:  [0.07367975185629638, 0.3367674103211909, 0.22192918799353362, 0.3676236498289792]
actor:  0 policy actor:  0  step number:  47 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.011233333333333302 0.7253333333333334 0.7253333333333334
probs:  [0.07367975185629638, 0.3367674103211909, 0.22192918799353362, 0.3676236498289792]
Printing some Q and Qe and total Qs values:  [[-0.075]
 [ 0.132]
 [ 0.01 ]
 [ 0.041]
 [-0.197]
 [ 0.009]
 [ 0.048]] [[0.73 ]
 [1.218]
 [0.584]
 [1.326]
 [0.092]
 [0.992]
 [1.145]] [[ 0.101]
 [ 0.357]
 [ 0.096]
 [ 0.348]
 [-0.161]
 [ 0.226]
 [ 0.293]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.011233333333333302 0.7253333333333334 0.7253333333333334
probs:  [0.07367975185629638, 0.3367674103211909, 0.22192918799353362, 0.3676236498289792]
siam score:  -0.73596835
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5707778101854026
actor:  1 policy actor:  1  step number:  58 total reward:  0.6200000000000004  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.011233333333333302 0.7253333333333334 0.7253333333333334
probs:  [0.07367975185629638, 0.3367674103211909, 0.22192918799353362, 0.3676236498289792]
siam score:  -0.7401199
siam score:  -0.73833483
actor:  0 policy actor:  0  step number:  53 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.01065999999999997 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.01065999999999997 0.7253333333333334 0.7253333333333334
probs:  [0.07367975185629638, 0.3367674103211909, 0.22192918799353362, 0.3676236498289792]
maxi score, test score, baseline:  0.01065999999999997 0.7253333333333334 0.7253333333333334
probs:  [0.07367975185629638, 0.3367674103211909, 0.22192918799353362, 0.3676236498289792]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.01065999999999997 0.7253333333333334 0.7253333333333334
probs:  [0.07366930622728221, 0.33686156507112314, 0.22189768209057503, 0.36757144661101965]
line 256 mcts: sample exp_bonus 1.2288881006923198
maxi score, test score, baseline:  0.01065999999999997 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.01065999999999997 0.7253333333333334 0.7253333333333334
probs:  [0.07366930622728221, 0.33686156507112314, 0.22189768209057503, 0.36757144661101965]
maxi score, test score, baseline:  0.01065999999999997 0.7253333333333334 0.7253333333333334
probs:  [0.07366930622728221, 0.33686156507112314, 0.22189768209057503, 0.36757144661101965]
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.654]
 [0.595]
 [0.583]
 [0.537]
 [0.533]
 [0.373]] [[0.683]
 [0.197]
 [0.509]
 [0.456]
 [0.53 ]
 [0.598]
 [3.003]] [[ 0.086]
 [-0.017]
 [ 0.055]
 [ 0.032]
 [ 0.034]
 [ 0.054]
 [ 0.756]]
maxi score, test score, baseline:  0.01065999999999997 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.01065999999999997 0.7253333333333334 0.7253333333333334
probs:  [0.07366930622728221, 0.33686156507112314, 0.22189768209057503, 0.36757144661101965]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.01065999999999997 0.7253333333333334 0.7253333333333334
probs:  [0.07366930622728221, 0.33686156507112314, 0.22189768209057503, 0.36757144661101965]
maxi score, test score, baseline:  0.01065999999999997 0.7253333333333334 0.7253333333333334
probs:  [0.07366930622728221, 0.33686156507112314, 0.22189768209057503, 0.36757144661101965]
maxi score, test score, baseline:  0.008179999999999972 0.7253333333333334 0.7253333333333334
probs:  [0.07366930622728221, 0.33686156507112314, 0.22189768209057503, 0.36757144661101965]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.44 ]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.444]] [[2.522]
 [1.483]
 [1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.073]] [[0.683]
 [0.235]
 [0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.059]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.8000000000000002  reward:  1.0 rdn_beta:  0.333
siam score:  -0.72846603
maxi score, test score, baseline:  0.008179999999999972 0.7253333333333334 0.7253333333333334
probs:  [0.07277062628866678, 0.3449620810644426, 0.21918710114820464, 0.363080191498686]
actor:  1 policy actor:  1  step number:  60 total reward:  0.5133333333333336  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]] [[0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]]
maxi score, test score, baseline:  0.008179999999999972 0.7253333333333334 0.7253333333333334
probs:  [0.07277062628866678, 0.3449620810644426, 0.21918710114820464, 0.363080191498686]
maxi score, test score, baseline:  0.008179999999999972 0.7253333333333334 0.7253333333333334
probs:  [0.07277062628866678, 0.3449620810644426, 0.21918710114820464, 0.363080191498686]
maxi score, test score, baseline:  0.008179999999999972 0.7253333333333334 0.7253333333333334
probs:  [0.07277062628866678, 0.3449620810644426, 0.21918710114820464, 0.363080191498686]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8636806039462432
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.838]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[-0.111]
 [ 0.014]
 [ 0.046]
 [ 0.046]
 [ 0.046]
 [ 0.046]
 [ 0.046]] [[0.677]
 [0.838]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]]
Printing some Q and Qe and total Qs values:  [[0.825]
 [0.935]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]] [[-0.021]
 [-0.062]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]] [[0.825]
 [0.935]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]]
actor:  0 policy actor:  1  step number:  51 total reward:  0.5333333333333337  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  49 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.5
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.3164888878120164
maxi score, test score, baseline:  0.011406666666666633 0.7253333333333334 0.7253333333333334
probs:  [0.07277062628866678, 0.3449620810644426, 0.21918710114820464, 0.363080191498686]
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.011406666666666633 0.7253333333333334 0.7253333333333334
probs:  [0.07277062628866678, 0.3449620810644426, 0.21918710114820464, 0.363080191498686]
actor:  0 policy actor:  1  step number:  34 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.014859999999999965 0.7253333333333334 0.7253333333333334
probs:  [0.07277062628866678, 0.3449620810644426, 0.21918710114820464, 0.363080191498686]
maxi score, test score, baseline:  0.014859999999999965 0.7253333333333334 0.7253333333333334
probs:  [0.07277062628866678, 0.3449620810644426, 0.21918710114820464, 0.363080191498686]
maxi score, test score, baseline:  0.014859999999999965 0.7253333333333334 0.7253333333333334
probs:  [0.07277062628866678, 0.3449620810644426, 0.21918710114820464, 0.363080191498686]
actor:  1 policy actor:  1  step number:  60 total reward:  0.5666666666666671  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
probs:  [0.07277062628866678, 0.3449620810644426, 0.21918710114820464, 0.363080191498686]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
probs:  [0.07277062628866678, 0.3449620810644426, 0.21918710114820464, 0.363080191498686]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
probs:  [0.07277062628866678, 0.3449620810644426, 0.21918710114820464, 0.363080191498686]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
probs:  [0.07277062628866678, 0.3449620810644426, 0.21918710114820464, 0.363080191498686]
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
actor:  1 policy actor:  1  step number:  35 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[2.675]
 [0.987]
 [0.987]
 [0.987]
 [0.987]
 [0.987]
 [0.987]] [[0.86]
 [0.18]
 [0.18]
 [0.18]
 [0.18]
 [0.18]
 [0.18]]
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
probs:  [0.07269823364973456, 0.3456146134113563, 0.2189687518794317, 0.36271840105947745]
siam score:  -0.74365216
actor:  1 policy actor:  1  step number:  64 total reward:  0.43333333333333357  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
probs:  [0.07269823364973456, 0.3456146134113563, 0.2189687518794317, 0.36271840105947745]
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
probs:  [0.07269823364973456, 0.3456146134113563, 0.2189687518794317, 0.36271840105947745]
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
probs:  [0.07269823364973456, 0.3456146134113563, 0.2189687518794317, 0.36271840105947745]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
probs:  [0.07182153324143382, 0.34930935562475873, 0.22054213263488479, 0.35832697849892264]
using explorer policy with actor:  1
siam score:  -0.73983556
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.307]
 [0.392]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[1.441]
 [1.441]
 [1.536]
 [1.441]
 [1.441]
 [1.441]
 [1.441]] [[0.369]
 [0.369]
 [0.448]
 [0.369]
 [0.369]
 [0.369]
 [0.369]]
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
probs:  [0.07182153324143382, 0.34930935562475873, 0.22054213263488479, 0.35832697849892264]
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.565]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[1.884]
 [1.819]
 [1.884]
 [1.884]
 [1.884]
 [1.884]
 [1.884]] [[0.315]
 [0.43 ]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]]
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
probs:  [0.07182153324143382, 0.34930935562475873, 0.22054213263488479, 0.35832697849892264]
siam score:  -0.7382695
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
probs:  [0.07182153324143382, 0.34930935562475873, 0.22054213263488479, 0.35832697849892264]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]] [[1.321]
 [2.038]
 [2.038]
 [2.038]
 [2.038]
 [2.038]
 [2.038]] [[-0.136]
 [ 0.101]
 [ 0.101]
 [ 0.101]
 [ 0.101]
 [ 0.101]
 [ 0.101]]
maxi score, test score, baseline:  0.012219999999999962 0.7253333333333334 0.7253333333333334
probs:  [0.07182153324143382, 0.34930935562475873, 0.22054213263488479, 0.35832697849892264]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6400000000000003  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using explorer policy with actor:  1
maxi score, test score, baseline:  0.012219999999999969 0.7253333333333334 0.7253333333333334
probs:  [0.07182153324143382, 0.3493093556247588, 0.22054213263488479, 0.35832697849892264]
actor:  1 policy actor:  1  step number:  31 total reward:  0.8000000000000002  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.012219999999999969 0.7253333333333334 0.7253333333333334
probs:  [0.07153763896809838, 0.3569125981464301, 0.21797442241448858, 0.3535753404709829]
maxi score, test score, baseline:  0.012219999999999969 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.012219999999999969 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
maxi score, test score, baseline:  0.012219999999999969 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
maxi score, test score, baseline:  0.012219999999999969 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
maxi score, test score, baseline:  0.012219999999999969 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.216]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]] [[-0.939]
 [ 0.029]
 [-0.939]
 [-0.939]
 [-0.939]
 [-0.939]
 [-0.939]] [[-0.335]
 [ 0.078]
 [-0.335]
 [-0.335]
 [-0.335]
 [-0.335]
 [-0.335]]
maxi score, test score, baseline:  0.012219999999999969 0.7253333333333334 0.7253333333333334
siam score:  -0.7320311
actor:  1 policy actor:  1  step number:  67 total reward:  0.42666666666666697  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.012219999999999969 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
maxi score, test score, baseline:  0.012219999999999969 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
maxi score, test score, baseline:  0.008833333333333302 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
maxi score, test score, baseline:  0.008833333333333302 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
UNIT TEST: sample policy line 217 mcts : [0.265 0.    0.551 0.    0.184 0.    0.   ]
actor:  1 policy actor:  1  step number:  65 total reward:  0.48000000000000054  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.008833333333333302 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
actor:  1 policy actor:  1  step number:  34 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.008833333333333302 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
maxi score, test score, baseline:  0.008833333333333302 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.008833333333333302 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
maxi score, test score, baseline:  0.008833333333333302 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
actor:  1 policy actor:  1  step number:  55 total reward:  0.4666666666666668  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7349851
maxi score, test score, baseline:  0.008833333333333302 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.008833333333333302 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.008833333333333302 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
maxi score, test score, baseline:  0.008833333333333302 0.7253333333333334 0.7253333333333334
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.008833333333333302 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
actor:  0 policy actor:  1  step number:  30 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
Printing some Q and Qe and total Qs values:  [[ 0.454]
 [ 0.457]
 [-0.062]
 [ 0.395]
 [ 0.398]
 [ 0.012]
 [-0.072]] [[1.363]
 [1.222]
 [1.372]
 [1.37 ]
 [1.315]
 [1.286]
 [0.772]] [[0.523]
 [0.484]
 [0.226]
 [0.491]
 [0.477]
 [0.244]
 [0.045]]
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
line 256 mcts: sample exp_bonus 0.5732942527436911
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
siam score:  -0.7364755
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
probs:  [0.07153498781757497, 0.3568996130775289, 0.21800206777090764, 0.35356333133398843]
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [ 0.064]] [[0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [1.342]] [[-0.51 ]
 [-0.51 ]
 [-0.51 ]
 [-0.51 ]
 [-0.51 ]
 [-0.51 ]
 [-0.117]]
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
actor:  1 policy actor:  1  step number:  62 total reward:  0.5933333333333338  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.08152664608611451
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
probs:  [0.07226564351087357, 0.36055109332750396, 0.22023187439604539, 0.34695138876557696]
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
probs:  [0.07226564351087357, 0.36055109332750396, 0.22023187439604539, 0.34695138876557696]
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
probs:  [0.07226564351087357, 0.36055109332750396, 0.22023187439604539, 0.34695138876557696]
siam score:  -0.7341434
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.385]
 [0.316]
 [0.343]
 [0.316]
 [0.316]
 [0.419]] [[2.106]
 [2.311]
 [2.166]
 [1.954]
 [2.166]
 [2.166]
 [2.043]] [[0.704]
 [0.768]
 [0.721]
 [0.689]
 [0.721]
 [0.721]
 [0.727]]
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
siam score:  -0.73404574
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
probs:  [0.07226564351087357, 0.36055109332750396, 0.22023187439604539, 0.34695138876557696]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
probs:  [0.07226564351087357, 0.36055109332750396, 0.22023187439604539, 0.34695138876557696]
Printing some Q and Qe and total Qs values:  [[0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.945]
 [0.905]
 [0.905]] [[1.628]
 [1.628]
 [1.628]
 [1.628]
 [0.618]
 [1.628]
 [1.628]] [[0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.945]
 [0.905]
 [0.905]]
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
probs:  [0.07226564351087357, 0.36055109332750396, 0.22023187439604539, 0.34695138876557696]
using another actor
maxi score, test score, baseline:  0.012393333333333293 0.7253333333333334 0.7253333333333334
actor:  0 policy actor:  1  step number:  37 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
siam score:  -0.7357229
maxi score, test score, baseline:  0.015726666666666625 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.015726666666666625 0.7253333333333334 0.7253333333333334
probs:  [0.07226564351087357, 0.36055109332750396, 0.22023187439604539, 0.34695138876557696]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  49 total reward:  0.5333333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.015286666666666627 0.7253333333333334 0.7253333333333334
probs:  [0.07226564351087357, 0.36055109332750396, 0.22023187439604539, 0.34695138876557696]
maxi score, test score, baseline:  0.015286666666666627 0.7253333333333334 0.7253333333333334
probs:  [0.07226564351087357, 0.36055109332750396, 0.22023187439604539, 0.34695138876557696]
actor:  1 policy actor:  1  step number:  40 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  43 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.014726666666666633 0.7253333333333334 0.7253333333333334
probs:  [0.0723158360464783, 0.3608025098951085, 0.2201413327905968, 0.34674032126781645]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.01133999999999995 0.7253333333333334 0.7253333333333334
probs:  [0.0723158360464783, 0.3608025098951085, 0.2201413327905968, 0.34674032126781645]
maxi score, test score, baseline:  0.01133999999999995 0.7253333333333334 0.7253333333333334
probs:  [0.0723158360464783, 0.3608025098951085, 0.2201413327905968, 0.34674032126781645]
maxi score, test score, baseline:  0.01133999999999995 0.7253333333333334 0.7253333333333334
probs:  [0.0723158360464783, 0.3608025098951085, 0.2201413327905968, 0.34674032126781645]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.562]
 [0.429]
 [0.448]
 [0.448]
 [0.448]
 [0.448]] [[0.732]
 [0.987]
 [1.592]
 [0.732]
 [0.732]
 [0.732]
 [0.732]] [[0.055]
 [0.254]
 [0.323]
 [0.055]
 [0.055]
 [0.055]
 [0.055]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7133333333333335  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.041]
 [0.245]
 [0.072]
 [0.041]
 [0.041]
 [0.041]] [[1.685]
 [1.685]
 [1.702]
 [1.133]
 [1.685]
 [1.685]
 [1.685]] [[0.505]
 [0.505]
 [0.651]
 [0.337]
 [0.505]
 [0.505]
 [0.505]]
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.267]
 [0.043]
 [0.036]
 [0.046]
 [0.267]
 [0.071]] [[0.343]
 [0.709]
 [0.034]
 [0.041]
 [0.153]
 [0.709]
 [0.274]] [[ 0.202]
 [ 0.383]
 [-0.065]
 [-0.07 ]
 [-0.023]
 [ 0.383]
 [ 0.043]]
line 256 mcts: sample exp_bonus 0.3163546360802953
maxi score, test score, baseline:  0.01133999999999995 0.7253333333333334 0.7253333333333334
probs:  [0.0723158360464783, 0.3608025098951085, 0.2201413327905968, 0.34674032126781645]
using another actor
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.01133999999999995 0.7253333333333334 0.7253333333333334
probs:  [0.0723158360464783, 0.3608025098951085, 0.2201413327905968, 0.34674032126781645]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.01133999999999995 0.7253333333333334 0.7253333333333334
probs:  [0.0719500040110298, 0.35897424713986625, 0.22409240462907667, 0.3449833442200273]
maxi score, test score, baseline:  0.01133999999999995 0.7253333333333334 0.7253333333333334
probs:  [0.0719500040110298, 0.35897424713986625, 0.22409240462907667, 0.3449833442200273]
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.946]
 [0.846]
 [0.865]
 [0.859]
 [0.865]
 [0.842]] [[-0.045]
 [-0.236]
 [-0.316]
 [-0.395]
 [-0.432]
 [-0.503]
 [-0.427]] [[0.869]
 [0.946]
 [0.846]
 [0.865]
 [0.859]
 [0.865]
 [0.842]]
actor:  0 policy actor:  1  step number:  38 total reward:  0.54  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.01103333333333329 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.01103333333333329 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  0.01103333333333329 0.7253333333333334 0.7253333333333334
probs:  [0.07193505332058381, 0.3588993596733997, 0.22412030083458723, 0.3450452861714292]
maxi score, test score, baseline:  0.01103333333333329 0.7253333333333334 0.7253333333333334
probs:  [0.07193505332058381, 0.3588993596733997, 0.22412030083458723, 0.3450452861714292]
maxi score, test score, baseline:  0.01103333333333329 0.7253333333333334 0.7253333333333334
probs:  [0.07193505332058381, 0.3588993596733997, 0.22412030083458723, 0.3450452861714292]
maxi score, test score, baseline:  0.01103333333333329 0.7253333333333334 0.7253333333333334
probs:  [0.07193505332058381, 0.3588993596733997, 0.22412030083458723, 0.3450452861714292]
maxi score, test score, baseline:  0.01103333333333329 0.7253333333333334 0.7253333333333334
probs:  [0.07193228600549036, 0.3588857938671461, 0.22414614842041722, 0.34503577170694627]
actor:  1 policy actor:  1  step number:  35 total reward:  0.52  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6342609616437105
from probs:  [0.07193228600549037, 0.35888579386714614, 0.22414614842041725, 0.3450357717069463]
maxi score, test score, baseline:  0.0076466666666666315 0.7253333333333334 0.7253333333333334
probs:  [0.07193228600549037, 0.35888579386714614, 0.22414614842041725, 0.3450357717069463]
maxi score, test score, baseline:  0.0076466666666666315 0.7253333333333334 0.7253333333333334
probs:  [0.07193228600549037, 0.35888579386714614, 0.22414614842041725, 0.3450357717069463]
maxi score, test score, baseline:  0.0076466666666666315 0.7253333333333334 0.7253333333333334
probs:  [0.07193228600549037, 0.35888579386714614, 0.22414614842041725, 0.3450357717069463]
siam score:  -0.7402319
start point for exploration sampling:  11715
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  0 policy actor:  0  step number:  43 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.018]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.007]] [[1.247]
 [1.353]
 [1.995]
 [1.995]
 [1.995]
 [1.995]
 [1.47 ]] [[0.415]
 [0.456]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.498]]
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.25 ]
 [0.137]
 [0.211]
 [0.211]
 [0.211]
 [0.219]] [[5.584]
 [1.479]
 [1.007]
 [2.039]
 [2.039]
 [2.039]
 [2.252]] [[0.844]
 [0.167]
 [0.06 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.288]]
maxi score, test score, baseline:  0.007326666666666645 0.7253333333333334 0.7253333333333334
probs:  [0.07192953175724139, 0.3588722921165835, 0.2241718739578456, 0.3450263021683295]
maxi score, test score, baseline:  0.007326666666666645 0.7253333333333334 0.7253333333333334
probs:  [0.07192953175724139, 0.3588722921165835, 0.2241718739578456, 0.3450263021683295]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.562]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.561]] [[1.976]
 [1.765]
 [1.976]
 [1.976]
 [1.976]
 [1.976]
 [1.911]] [[0.557]
 [0.556]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.625]]
maxi score, test score, baseline:  0.007326666666666645 0.7253333333333334 0.7253333333333334
probs:  [0.07192953175724139, 0.3588722921165835, 0.2241718739578456, 0.3450263021683295]
from probs:  [0.07192953175724139, 0.3588722921165835, 0.2241718739578456, 0.3450263021683295]
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]] [[1.212]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[0.754]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]]
siam score:  -0.7397022
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.882]
 [0.841]
 [0.805]
 [0.821]
 [0.838]
 [0.87 ]] [[2.037]
 [1.044]
 [2.253]
 [1.121]
 [1.797]
 [1.939]
 [3.097]] [[0.832]
 [0.882]
 [0.841]
 [0.805]
 [0.821]
 [0.838]
 [0.87 ]]
maxi score, test score, baseline:  0.0005533333333333118 0.7253333333333334 0.7253333333333334
probs:  [0.07192953175724137, 0.35887229211658345, 0.22417187395784557, 0.34502630216832963]
siam score:  -0.7384552
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]] [[8.978]
 [2.284]
 [2.284]
 [2.284]
 [2.284]
 [2.284]
 [2.284]] [[0.9  ]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]]
maxi score, test score, baseline:  -0.0028333333333333656 0.7253333333333334 0.7253333333333334
probs:  [0.07192953175724137, 0.35887229211658345, 0.22417187395784557, 0.34502630216832963]
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[4.112]
 [2.725]
 [2.725]
 [2.725]
 [2.725]
 [2.725]
 [2.725]] [[0.376]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]]
line 256 mcts: sample exp_bonus 7.138446342294727
maxi score, test score, baseline:  -0.0028333333333333656 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  -0.0028333333333333656 0.7253333333333334 0.7253333333333334
probs:  [0.07192953175724137, 0.35887229211658345, 0.22417187395784557, 0.34502630216832963]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[5.795]
 [2.655]
 [2.655]
 [2.655]
 [2.655]
 [2.655]
 [2.655]] [[0.69 ]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]]
maxi score, test score, baseline:  -0.0028333333333333656 0.7253333333333334 0.7253333333333334
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[1.998]
 [1.998]
 [1.998]
 [1.998]
 [1.998]
 [1.998]
 [1.998]] [[0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.5533333333333339  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.469]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[2.098]
 [1.573]
 [1.928]
 [1.928]
 [1.928]
 [1.928]
 [1.928]] [[0.592]
 [0.302]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]]
maxi score, test score, baseline:  -0.0028333333333333656 0.7253333333333334 0.7253333333333334
probs:  [0.07192953175724139, 0.3588722921165835, 0.2241718739578456, 0.3450263021683295]
maxi score, test score, baseline:  -0.0028333333333333656 0.7253333333333334 0.7253333333333334
probs:  [0.07192953175724139, 0.3588722921165835, 0.2241718739578456, 0.3450263021683295]
first move QE:  -0.08488726650457125
maxi score, test score, baseline:  -0.0028333333333333656 0.7253333333333334 0.7253333333333334
probs:  [0.07192953175724139, 0.3588722921165835, 0.2241718739578456, 0.3450263021683295]
maxi score, test score, baseline:  -0.0028333333333333656 0.7253333333333334 0.7253333333333334
probs:  [0.07192953175724137, 0.35887229211658345, 0.22417187395784557, 0.34502630216832963]
maxi score, test score, baseline:  -0.0028333333333333656 0.7253333333333334 0.7253333333333334
probs:  [0.07192953175724137, 0.35887229211658345, 0.22417187395784557, 0.34502630216832963]
UNIT TEST: sample policy line 217 mcts : [0.245 0.265 0.082 0.184 0.082 0.082 0.061]
maxi score, test score, baseline:  -0.0028333333333333656 0.7253333333333334 0.7253333333333334
probs:  [0.07192953175724137, 0.35887229211658345, 0.22417187395784557, 0.34502630216832963]
maxi score, test score, baseline:  -0.0028333333333333656 0.7253333333333334 0.7253333333333334
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.972]] [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.929]] [[0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.768]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07192953175724137, 0.35887229211658345, 0.22417187395784557, 0.34502630216832963]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.5
siam score:  -0.73934203
first move QE:  -0.08473767240515201
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.0028333333333333656 0.7253333333333334 0.7253333333333334
probs:  [0.07189558530005834, 0.3587026429362962, 0.22453857064545427, 0.3448632011181911]
maxi score, test score, baseline:  -0.0062200000000000215 0.7253333333333334 0.7253333333333334
probs:  [0.07189558530005834, 0.3587026429362962, 0.22453857064545427, 0.3448632011181911]
actor:  0 policy actor:  1  step number:  45 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.006540000000000028 0.7253333333333334 0.7253333333333334
probs:  [0.07189558530005834, 0.3587026429362962, 0.22453857064545427, 0.3448632011181911]
first move QE:  -0.0849158464819253
maxi score, test score, baseline:  -0.006540000000000028 0.7253333333333334 0.7253333333333334
probs:  [0.07189558530005835, 0.3587026429362963, 0.2245385706454543, 0.3448632011181911]
maxi score, test score, baseline:  -0.006540000000000028 0.7253333333333334 0.7253333333333334
probs:  [0.07189558530005835, 0.3587026429362963, 0.2245385706454543, 0.3448632011181911]
maxi score, test score, baseline:  -0.006540000000000028 0.7253333333333334 0.7253333333333334
probs:  [0.07189558530005835, 0.3587026429362963, 0.2245385706454543, 0.3448632011181911]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[2.417]
 [2.058]
 [2.058]
 [2.058]
 [2.058]
 [2.058]
 [2.058]] [[0.111]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]]
maxi score, test score, baseline:  -0.006540000000000028 0.7253333333333334 0.7253333333333334
probs:  [0.07189558530005835, 0.3587026429362963, 0.2245385706454543, 0.3448632011181911]
actor:  1 policy actor:  1  step number:  60 total reward:  0.486666666666667  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [ 0.204]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[2.071]
 [2.071]
 [1.74 ]
 [2.071]
 [2.071]
 [2.071]
 [2.071]] [[-0.101]
 [-0.101]
 [-0.061]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]]
actor:  0 policy actor:  0  step number:  43 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.007046666666666697 0.7253333333333334 0.7253333333333334
probs:  [0.07178905958643929, 0.3581690614571841, 0.22473802574709048, 0.3453038532092862]
maxi score, test score, baseline:  -0.007046666666666697 0.7253333333333334 0.7253333333333334
probs:  [0.07178905958643929, 0.3581690614571841, 0.22473802574709048, 0.3453038532092862]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.007046666666666697 0.7253333333333334 0.7253333333333334
probs:  [0.07178905958643929, 0.3581690614571841, 0.22473802574709048, 0.3453038532092862]
actor:  0 policy actor:  1  step number:  35 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.5
siam score:  -0.73587465
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.496]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.302]] [[0.893]
 [0.572]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.791]] [[-0.136]
 [-0.065]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.186]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5933333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.006940000000000027 0.7253333333333334 0.7253333333333334
probs:  [0.07178635489564164, 0.3581558068873182, 0.2247635049307125, 0.34529433328632764]
maxi score, test score, baseline:  -0.006940000000000027 0.7253333333333334 0.7253333333333334
probs:  [0.07178635489564164, 0.3581558068873182, 0.2247635049307125, 0.34529433328632764]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.006940000000000027 0.7253333333333334 0.7253333333333334
probs:  [0.07178635489564164, 0.3581558068873182, 0.2247635049307125, 0.34529433328632764]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.006940000000000027 0.7253333333333334 0.7253333333333334
probs:  [0.07178635489564164, 0.3581558068873182, 0.2247635049307125, 0.34529433328632764]
maxi score, test score, baseline:  -0.006940000000000027 0.7253333333333334 0.7253333333333334
probs:  [0.07178635489564164, 0.3581558068873182, 0.2247635049307125, 0.34529433328632764]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.006940000000000027 0.7253333333333334 0.7253333333333334
probs:  [0.07178635489564165, 0.35815580688731824, 0.22476350493071254, 0.3452943332863276]
siam score:  -0.7359285
actor:  0 policy actor:  0  step number:  50 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.007593333333333356 0.7253333333333334 0.7253333333333334
probs:  [0.07178635489564165, 0.35815580688731824, 0.22476350493071254, 0.3452943332863276]
maxi score, test score, baseline:  -0.007593333333333356 0.7253333333333334 0.7253333333333334
probs:  [0.07178635489564165, 0.35815580688731824, 0.22476350493071254, 0.3452943332863276]
actor:  0 policy actor:  1  step number:  47 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.008286666666666699 0.7253333333333334 0.7253333333333334
probs:  [0.07178635489564164, 0.3581558068873182, 0.22476350493071257, 0.34529433328632764]
maxi score, test score, baseline:  -0.008286666666666699 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  -0.008286666666666699 0.7253333333333334 0.7253333333333334
probs:  [0.07178635489564164, 0.3581558068873182, 0.22476350493071257, 0.34529433328632764]
maxi score, test score, baseline:  -0.008286666666666699 0.7253333333333334 0.7253333333333334
probs:  [0.07178635489564164, 0.3581558068873182, 0.22476350493071257, 0.34529433328632764]
maxi score, test score, baseline:  -0.008286666666666699 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  -0.008286666666666699 0.7253333333333334 0.7253333333333334
siam score:  -0.734448
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]] [[0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]]
maxi score, test score, baseline:  -0.008286666666666699 0.7253333333333334 0.7253333333333334
probs:  [0.07178635489564164, 0.3581558068873182, 0.22476350493071257, 0.34529433328632764]
actor:  0 policy actor:  0  step number:  44 total reward:  0.4600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  45 total reward:  0.6133333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.008753333333333368 0.7253333333333334 0.7253333333333334
probs:  [0.07178635489564165, 0.35815580688731824, 0.22476350493071254, 0.3452943332863276]
maxi score, test score, baseline:  -0.008753333333333368 0.7253333333333334 0.7253333333333334
probs:  [0.07178635489564164, 0.3581558068873182, 0.22476350493071257, 0.34529433328632764]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  39 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.008753333333333368 0.7253333333333334 0.7253333333333334
probs:  [0.07178677174270942, 0.35815789484921057, 0.22476272428071672, 0.3452926091273633]
maxi score, test score, baseline:  -0.008753333333333368 0.7253333333333334 0.7253333333333334
probs:  [0.07178677174270942, 0.35815789484921057, 0.22476272428071672, 0.3452926091273633]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.0122066666666667 0.7253333333333334 0.7253333333333334
probs:  [0.07178677174270942, 0.35815789484921057, 0.22476272428071672, 0.3452926091273633]
maxi score, test score, baseline:  -0.0122066666666667 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  -0.0122066666666667 0.7253333333333334 0.7253333333333334
probs:  [0.07178677174270942, 0.35815789484921057, 0.22476272428071672, 0.3452926091273633]
Printing some Q and Qe and total Qs values:  [[1.004]
 [1.002]
 [0.995]
 [0.995]
 [0.995]
 [0.995]
 [1.004]] [[0.041]
 [0.147]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.604]] [[1.004]
 [1.002]
 [0.995]
 [0.995]
 [0.995]
 [0.995]
 [1.004]]
actor:  0 policy actor:  0  step number:  44 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.012740000000000033 0.7253333333333334 0.7253333333333334
probs:  [0.07178677174270942, 0.35815789484921057, 0.22476272428071672, 0.3452926091273633]
maxi score, test score, baseline:  -0.012740000000000033 0.7253333333333334 0.7253333333333334
probs:  [0.07178677174270942, 0.35815789484921057, 0.22476272428071672, 0.3452926091273633]
maxi score, test score, baseline:  -0.012740000000000033 0.7253333333333334 0.7253333333333334
probs:  [0.07178677174270941, 0.35815789484921057, 0.2247627242807167, 0.34529260912736337]
actor:  1 policy actor:  1  step number:  36 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.167
main train batch thing paused
add a thread
Adding thread: now have 3 threads
actor:  1 policy actor:  1  step number:  50 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.095]
 [0.163]
 [0.095]
 [0.095]
 [0.126]
 [0.095]] [[1.08 ]
 [1.08 ]
 [0.67 ]
 [1.08 ]
 [1.08 ]
 [0.553]
 [1.08 ]] [[0.256]
 [0.256]
 [0.255]
 [0.256]
 [0.256]
 [0.21 ]
 [0.256]]
maxi score, test score, baseline:  -0.012740000000000033 0.7253333333333334 0.7253333333333334
probs:  [0.0718022880599014, 0.3582339339712993, 0.22461656009792438, 0.34534721787087497]
actor:  0 policy actor:  1  step number:  40 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  46 total reward:  0.5800000000000003  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  37 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.012846666666666694 0.7253333333333334 0.7253333333333334
probs:  [0.07179956060330589, 0.35822056782245804, 0.2246422528225218, 0.3453376187517143]
maxi score, test score, baseline:  -0.012846666666666694 0.7253333333333334 0.7253333333333334
probs:  [0.07179956060330589, 0.3582205678224581, 0.22464225282252176, 0.3453376187517142]
maxi score, test score, baseline:  -0.012846666666666694 0.7253333333333334 0.7253333333333334
probs:  [0.07179684593206741, 0.3582072643294203, 0.22466782510866523, 0.3453280646298471]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.012846666666666694 0.7253333333333334 0.7253333333333334
probs:  [0.07179684593206741, 0.3582072643294203, 0.22466782510866523, 0.3453280646298471]
maxi score, test score, baseline:  -0.012846666666666694 0.7253333333333334 0.7253333333333334
probs:  [0.07179684593206741, 0.3582072643294203, 0.22466782510866523, 0.3453280646298471]
maxi score, test score, baseline:  -0.012846666666666694 0.7253333333333334 0.7253333333333334
probs:  [0.07179684593206741, 0.3582072643294203, 0.22466782510866523, 0.3453280646298471]
actor:  1 policy actor:  1  step number:  44 total reward:  0.6200000000000002  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.611]
 [0.552]
 [0.583]
 [0.583]
 [0.583]
 [0.528]] [[1.48 ]
 [1.906]
 [0.964]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [0.772]] [[0.583]
 [0.611]
 [0.552]
 [0.583]
 [0.583]
 [0.583]
 [0.528]]
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.836]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]] [[0.377]
 [0.511]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[0.751]
 [0.836]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.012846666666666694 0.7253333333333334 0.7253333333333334
probs:  [0.0717968459320674, 0.35820726432942024, 0.22466782510866523, 0.3453280646298471]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  53 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
first move QE:  -0.08724426951256739
line 256 mcts: sample exp_bonus 1.1272538148220448
maxi score, test score, baseline:  -0.013126666666666696 0.7253333333333334 0.7253333333333334
probs:  [0.0717968459320674, 0.35820726432942024, 0.22466782510866523, 0.3453280646298471]
maxi score, test score, baseline:  -0.013126666666666696 0.7253333333333334 0.7253333333333334
probs:  [0.0717968459320674, 0.35820726432942024, 0.22466782510866523, 0.3453280646298471]
actor:  1 policy actor:  1  step number:  38 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.191]
 [-0.034]
 [ 0.215]
 [ 0.263]
 [ 0.269]
 [ 0.344]
 [ 0.211]] [[0.018]
 [0.018]
 [0.063]
 [0.124]
 [0.047]
 [0.18 ]
 [0.049]] [[ 0.141]
 [-0.085]
 [ 0.187]
 [ 0.266]
 [ 0.233]
 [ 0.375]
 [ 0.177]]
maxi score, test score, baseline:  -0.015166666666666702 0.7253333333333334 0.7253333333333334
probs:  [0.0717968459320674, 0.35820726432942024, 0.22466782510866523, 0.3453280646298471]
maxi score, test score, baseline:  -0.015166666666666702 0.7253333333333334 0.7253333333333334
probs:  [0.0717968459320674, 0.35820726432942024, 0.22466782510866523, 0.3453280646298471]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.015166666666666702 0.7253333333333334 0.7253333333333334
probs:  [0.0717968459320674, 0.35820726432942024, 0.22466782510866523, 0.3453280646298471]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.015166666666666702 0.7253333333333334 0.7253333333333334
probs:  [0.0717968459320674, 0.35820726432942024, 0.22466782510866523, 0.3453280646298471]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.015166666666666702 0.7253333333333334 0.7253333333333334
probs:  [0.0717968459320674, 0.35820726432942024, 0.22466782510866523, 0.3453280646298471]
maxi score, test score, baseline:  -0.015166666666666702 0.7253333333333334 0.7253333333333334
probs:  [0.0717968459320674, 0.35820726432942024, 0.22466782510866523, 0.3453280646298471]
actor:  1 policy actor:  1  step number:  36 total reward:  0.6600000000000003  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.015166666666666702 0.7253333333333334 0.7253333333333334
probs:  [0.0717968459320674, 0.35820726432942024, 0.22466782510866523, 0.3453280646298471]
actor:  1 policy actor:  1  step number:  33 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  73 total reward:  0.3866666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.015166666666666702 0.7253333333333334 0.7253333333333334
probs:  [0.07225703237740727, 0.36050688465700137, 0.22618559182359993, 0.3410504911419915]
maxi score, test score, baseline:  -0.015166666666666702 0.7253333333333334 0.7253333333333334
probs:  [0.07225703237740727, 0.36050688465700137, 0.22618559182359993, 0.3410504911419915]
maxi score, test score, baseline:  -0.015166666666666702 0.7253333333333334 0.7253333333333334
probs:  [0.07225703237740727, 0.3605068846570013, 0.2261855918235999, 0.3410504911419915]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.336]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.293]] [[2.451]
 [2.1  ]
 [2.101]
 [2.101]
 [2.101]
 [2.101]
 [2.309]] [[0.493]
 [0.355]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.413]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.8  ]
 [0.737]
 [0.78 ]
 [0.75 ]
 [0.744]
 [0.799]] [[ 0.234]
 [ 0.071]
 [ 0.383]
 [ 0.144]
 [ 0.226]
 [-0.191]
 [ 0.058]] [[0.764]
 [0.8  ]
 [0.737]
 [0.78 ]
 [0.75 ]
 [0.744]
 [0.799]]
Printing some Q and Qe and total Qs values:  [[ 0.29 ]
 [ 0.411]
 [ 0.233]
 [ 0.361]
 [ 0.361]
 [-0.002]
 [ 0.228]] [[1.758]
 [1.805]
 [1.39 ]
 [2.066]
 [2.066]
 [1.173]
 [1.635]] [[-0.016]
 [ 0.127]
 [-0.252]
 [ 0.206]
 [ 0.206]
 [-0.59 ]
 [-0.137]]
using explorer policy with actor:  1
siam score:  -0.7123423
actor:  0 policy actor:  0  step number:  53 total reward:  0.31999999999999995  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.19382385023200777
maxi score, test score, baseline:  -0.015233333333333364 0.7253333333333334 0.7253333333333334
Printing some Q and Qe and total Qs values:  [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[0.001]
 [0.002]
 [0.002]
 [0.001]
 [0.001]
 [0.002]
 [0.002]] [[-0.387]
 [-0.386]
 [-0.386]
 [-0.387]
 [-0.387]
 [-0.386]
 [-0.386]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.015233333333333364 0.7253333333333334 0.7253333333333334
probs:  [0.07222261578081263, 0.3603348869371457, 0.22607770540926533, 0.34136479187277624]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7733333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.015233333333333364 0.7253333333333334 0.7253333333333334
probs:  [0.07185603840089828, 0.3585029081904235, 0.2300117542734576, 0.33962929913522066]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.015233333333333364 0.7253333333333334 0.7253333333333334
probs:  [0.07185603840089828, 0.3585029081904235, 0.2300117542734576, 0.33962929913522066]
actor:  1 policy actor:  1  step number:  39 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  34 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  -0.018153333333333358 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  -0.018153333333333358 0.7253333333333334 0.7253333333333334
probs:  [0.0719010272238503, 0.35872884118956694, 0.22995995248142, 0.3394101791051627]
maxi score, test score, baseline:  -0.018153333333333358 0.7253333333333334 0.7253333333333334
probs:  [0.0719010272238503, 0.35872884118956694, 0.22995995248142, 0.3394101791051627]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[4.352]
 [1.884]
 [1.884]
 [1.884]
 [1.884]
 [1.884]
 [1.884]] [[0.651]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]]
maxi score, test score, baseline:  -0.02139333333333336 0.7253333333333334 0.7253333333333334
actor:  1 policy actor:  1  step number:  53 total reward:  0.3733333333333334  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.02139333333333336 0.7253333333333334 0.7253333333333334
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.8004054318343736
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3861906660437902
siam score:  -0.7226417
maxi score, test score, baseline:  -0.02139333333333336 0.7253333333333334 0.7253333333333334
probs:  [0.07202523386937987, 0.35934956861445816, 0.23035777115804543, 0.3382674263581165]
actor:  1 policy actor:  1  step number:  48 total reward:  0.6200000000000003  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7216203
maxi score, test score, baseline:  -0.02139333333333336 0.7253333333333334 0.7253333333333334
probs:  [0.07202523386937987, 0.35934956861445816, 0.23035777115804543, 0.3382674263581165]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.32410685017984364
maxi score, test score, baseline:  -0.02139333333333336 0.7253333333333334 0.7253333333333334
probs:  [0.07202523386937987, 0.35934956861445816, 0.23035777115804543, 0.3382674263581165]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6466666666666668  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.442]
 [0.28 ]
 [0.265]
 [0.286]
 [0.291]
 [0.33 ]] [[1.966]
 [1.435]
 [1.638]
 [1.618]
 [1.629]
 [1.689]
 [1.565]] [[ 0.141]
 [ 0.022]
 [-0.039]
 [-0.064]
 [-0.036]
 [-0.001]
 [-0.024]]
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.298]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.283]] [[1.52 ]
 [1.212]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [1.32 ]] [[-0.215]
 [-0.295]
 [-0.438]
 [-0.438]
 [-0.438]
 [-0.438]
 [-0.274]]
maxi score, test score, baseline:  -0.02139333333333336 0.7253333333333334 0.7253333333333334
probs:  [0.07202523386937987, 0.35934956861445816, 0.23035777115804543, 0.3382674263581165]
maxi score, test score, baseline:  -0.02139333333333336 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  -0.02139333333333336 0.7253333333333334 0.7253333333333334
probs:  [0.07202523386937987, 0.35934956861445816, 0.23035777115804543, 0.3382674263581165]
maxi score, test score, baseline:  -0.02139333333333336 0.7253333333333334 0.7253333333333334
probs:  [0.07202523386937987, 0.35934956861445816, 0.23035777115804543, 0.3382674263581165]
actor:  1 policy actor:  1  step number:  60 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07202523386937987, 0.35934956861445816, 0.23035777115804543, 0.3382674263581165]
actor:  1 policy actor:  1  step number:  57 total reward:  0.5333333333333337  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  43 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.021140000000000037 0.7253333333333334 0.7253333333333334
probs:  [0.07202523386937987, 0.35934956861445816, 0.23035777115804543, 0.3382674263581165]
maxi score, test score, baseline:  -0.024580000000000036 0.7253333333333334 0.7253333333333334
probs:  [0.07202248410989655, 0.359336086528461, 0.2303814169834478, 0.33826001237819464]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.024580000000000036 0.7253333333333334 0.7253333333333334
probs:  [0.07202248410989655, 0.359336086528461, 0.2303814169834478, 0.33826001237819464]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.024580000000000036 0.7253333333333334 0.7253333333333334
probs:  [0.07202248410989655, 0.359336086528461, 0.2303814169834478, 0.33826001237819464]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.418]] [[1.573]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.199]] [[ 0.034]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.11 ]]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[1.298]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]] [[-0.02 ]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]]
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.767]
 [0.751]
 [0.716]
 [0.716]
 [0.716]
 [0.72 ]] [[0.792]
 [1.035]
 [1.346]
 [1.279]
 [1.279]
 [1.279]
 [0.575]] [[0.095]
 [0.297]
 [0.436]
 [0.368]
 [0.368]
 [0.368]
 [0.02 ]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07195035349307415, 0.35897561077253465, 0.23115334736893545, 0.33792068836545575]
Printing some Q and Qe and total Qs values:  [[0.916]
 [0.911]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.875]] [[0.888]
 [0.758]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.786]] [[0.916]
 [0.911]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.875]]
maxi score, test score, baseline:  -0.024580000000000036 0.7253333333333334 0.7253333333333334
probs:  [0.07105502037324352, 0.35449091310664727, 0.23294302847137063, 0.34151103804873856]
maxi score, test score, baseline:  -0.024580000000000036 0.7253333333333334 0.7253333333333334
probs:  [0.07105502037324352, 0.35449091310664727, 0.23294302847137063, 0.34151103804873856]
from probs:  [0.07105502037324352, 0.35449091310664727, 0.23294302847137063, 0.34151103804873856]
actor:  1 policy actor:  1  step number:  45 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  42 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0238866666666667 0.7253333333333334 0.7253333333333334
probs:  [0.0703391811811429, 0.34049035067894606, 0.23827371785516394, 0.350896750284747]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.237]
 [0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.198]] [[1.085]
 [1.553]
 [1.335]
 [1.335]
 [1.335]
 [1.335]
 [2.291]] [[-0.501]
 [-0.259]
 [-0.403]
 [-0.403]
 [-0.403]
 [-0.403]
 [-0.052]]
first move QE:  -0.09046197812295598
actor:  0 policy actor:  0  step number:  63 total reward:  0.1466666666666664  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.016]
 [-0.011]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.002]] [[0.369]
 [0.665]
 [0.336]
 [0.369]
 [0.369]
 [0.369]
 [0.235]] [[0.709]
 [0.8  ]
 [0.695]
 [0.709]
 [0.709]
 [0.709]
 [0.671]]
actor:  1 policy actor:  1  step number:  67 total reward:  0.4800000000000004  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 2.208120208458309
maxi score, test score, baseline:  -0.024540000000000034 0.7253333333333334 0.7253333333333334
probs:  [0.0703368053606542, 0.34048190080617047, 0.23829613601201804, 0.35088515782115726]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5364543097217447
maxi score, test score, baseline:  -0.024540000000000034 0.7253333333333334 0.7253333333333334
probs:  [0.0703368053606542, 0.34048190080617047, 0.23829613601201804, 0.35088515782115726]
maxi score, test score, baseline:  -0.024540000000000034 0.7253333333333334 0.7253333333333334
probs:  [0.0703368053606542, 0.34048190080617047, 0.23829613601201804, 0.35088515782115726]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6466666666666668  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.027513333333333372 0.7253333333333334 0.7253333333333334
actor:  1 policy actor:  1  step number:  62 total reward:  0.4733333333333337  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07033444062373662, 0.34047349035336216, 0.23831844958469275, 0.35087361943820833]
line 256 mcts: sample exp_bonus 1.7830060469763738
maxi score, test score, baseline:  -0.027513333333333372 0.7253333333333334 0.7253333333333334
probs:  [0.07033444062373662, 0.34047349035336216, 0.23831844958469275, 0.35087361943820833]
start point for exploration sampling:  11715
maxi score, test score, baseline:  -0.027513333333333372 0.7253333333333334 0.7253333333333334
maxi score, test score, baseline:  -0.027513333333333372 0.7253333333333334 0.7253333333333334
probs:  [0.07033444062373662, 0.34047349035336216, 0.23831844958469275, 0.35087361943820833]
siam score:  -0.72325706
actor:  1 policy actor:  1  step number:  36 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.027513333333333372 0.7253333333333334 0.7253333333333334
probs:  [0.07033444062373662, 0.34047349035336216, 0.23831844958469275, 0.35087361943820833]
maxi score, test score, baseline:  -0.027513333333333372 0.7253333333333334 0.7253333333333334
probs:  [0.07033444062373662, 0.34047349035336216, 0.23831844958469275, 0.35087361943820833]
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.748]
 [0.477]
 [0.665]
 [0.68 ]
 [0.68 ]
 [0.609]] [[1.497]
 [0.861]
 [0.352]
 [0.778]
 [1.497]
 [1.497]
 [1.21 ]] [[0.68 ]
 [0.748]
 [0.477]
 [0.665]
 [0.68 ]
 [0.68 ]
 [0.609]]
maxi score, test score, baseline:  -0.027513333333333372 0.7253333333333334 0.7253333333333334
probs:  [0.07033444062373662, 0.34047349035336216, 0.23831844958469275, 0.35087361943820833]
actor:  1 policy actor:  1  step number:  57 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[5.337]
 [1.848]
 [1.848]
 [1.848]
 [1.848]
 [1.848]
 [1.848]] [[0.648]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]]
maxi score, test score, baseline:  -0.027513333333333372 0.7253333333333334 0.7253333333333334
probs:  [0.06974008515638354, 0.34279639761951747, 0.2395667845386872, 0.3478967326854118]
actor:  0 policy actor:  0  step number:  45 total reward:  0.56  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  55 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]] [[1.476]
 [1.044]
 [1.044]
 [1.044]
 [1.044]
 [1.044]
 [1.044]] [[0.398]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]]
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.855]
 [0.769]
 [0.785]
 [0.66 ]
 [0.77 ]
 [0.776]] [[2.173]
 [2.168]
 [2.367]
 [2.431]
 [1.933]
 [2.798]
 [3.444]] [[0.807]
 [0.855]
 [0.769]
 [0.785]
 [0.66 ]
 [0.77 ]
 [0.776]]
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.912]
 [0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]] [[0.951]
 [0.565]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]] [[0.863]
 [0.912]
 [0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]]
maxi score, test score, baseline:  -0.027006666666666707 0.7253333333333334 0.7253333333333334
probs:  [0.06974008515638354, 0.34279639761951747, 0.2395667845386872, 0.3478967326854118]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.5
from probs:  [0.06974008515638354, 0.34279639761951747, 0.2395667845386872, 0.3478967326854118]
actor:  0 policy actor:  0  step number:  43 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.026793333333333367 0.7253333333333334 0.7253333333333334
probs:  [0.06974008515638354, 0.34279639761951747, 0.2395667845386872, 0.3478967326854118]
maxi score, test score, baseline:  -0.026793333333333367 0.7253333333333334 0.7253333333333334
probs:  [0.06974008515638354, 0.34279639761951747, 0.2395667845386872, 0.3478967326854118]
maxi score, test score, baseline:  -0.026793333333333367 0.7253333333333334 0.7253333333333334
probs:  [0.06974008515638354, 0.34279639761951747, 0.2395667845386872, 0.3478967326854118]
maxi score, test score, baseline:  -0.026793333333333367 0.7253333333333334 0.7253333333333334
actor:  1 policy actor:  1  step number:  34 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.026793333333333367 0.7253333333333334 0.7253333333333334
probs:  [0.06974008515638354, 0.34279639761951747, 0.2395667845386872, 0.3478967326854118]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  60 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0244866666666667 0.7253333333333334 0.7253333333333334
probs:  [0.06974008515638354, 0.34279639761951747, 0.2395667845386872, 0.3478967326854118]
using explorer policy with actor:  1
first move QE:  -0.09274530283989316
maxi score, test score, baseline:  -0.0244866666666667 0.7253333333333334 0.7253333333333334
probs:  [0.06974008515638354, 0.34279639761951747, 0.2395667845386872, 0.3478967326854118]
maxi score, test score, baseline:  -0.0244866666666667 0.7253333333333334 0.7253333333333334
probs:  [0.06974008515638354, 0.34279639761951747, 0.2395667845386872, 0.3478967326854118]
actor:  1 policy actor:  1  step number:  56 total reward:  0.5533333333333339  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  -0.0244866666666667 0.7253333333333334 0.7253333333333334
probs:  [0.0705043072680436, 0.33558408883700386, 0.24219600150956702, 0.3517156023853855]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  0 policy actor:  1  step number:  31 total reward:  0.8000000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.023953333333333358 0.7253333333333334 0.7253333333333334
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]] [[6.064]
 [1.587]
 [1.587]
 [1.587]
 [1.587]
 [1.587]
 [1.587]] [[0.655]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  76 total reward:  0.40666666666666706  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.023953333333333358 0.7253333333333334 0.7253333333333334
probs:  [0.0705043072680436, 0.33558408883700386, 0.24219600150956702, 0.3517156023853855]
maxi score, test score, baseline:  -0.023953333333333358 0.7253333333333334 0.7253333333333334
probs:  [0.0705043072680436, 0.33558408883700386, 0.24219600150956702, 0.3517156023853855]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [ 0.264]
 [ 0.264]
 [ 0.264]
 [ 0.264]
 [ 0.264]
 [ 0.264]] [[8.356]
 [2.624]
 [2.624]
 [2.624]
 [2.624]
 [2.624]
 [2.624]] [[0.847]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]]
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]] [[3.434]
 [2.998]
 [2.998]
 [2.998]
 [2.998]
 [2.998]
 [2.998]] [[0.335]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]]
maxi score, test score, baseline:  -0.023953333333333358 0.7253333333333334 0.7253333333333334
probs:  [0.0705043072680436, 0.33558408883700386, 0.24219600150956702, 0.3517156023853855]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.023953333333333358 0.7253333333333334 0.7253333333333334
probs:  [0.0705043072680436, 0.33558408883700386, 0.24219600150956702, 0.3517156023853855]
actor:  1 policy actor:  1  step number:  37 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.667
siam score:  -0.71432847
Printing some Q and Qe and total Qs values:  [[0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]] [[1.813]
 [1.813]
 [1.813]
 [1.813]
 [1.813]
 [1.813]
 [1.813]] [[0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]]
maxi score, test score, baseline:  -0.023953333333333358 0.7253333333333334 0.7253333333333334
probs:  [0.07050384691015962, 0.3355858662280281, 0.2421969905375965, 0.35171329632421583]
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.57 ]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[1.156]
 [1.917]
 [1.156]
 [1.156]
 [1.156]
 [1.156]
 [1.156]] [[0.538]
 [0.57 ]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]]
actor:  0 policy actor:  1  step number:  27 total reward:  0.8133333333333335  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  -0.020326666666666705 0.7253333333333334 0.7253333333333334
probs:  [0.07050384691015962, 0.3355858662280281, 0.2421969905375965, 0.35171329632421583]
maxi score, test score, baseline:  -0.020326666666666705 0.7253333333333334 0.7253333333333334
probs:  [0.07050384691015962, 0.3355858662280281, 0.2421969905375965, 0.35171329632421583]
actor:  0 policy actor:  0  step number:  49 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07050384691015962, 0.3355858662280281, 0.2421969905375965, 0.35171329632421583]
siam score:  -0.70422775
actor:  1 policy actor:  1  step number:  47 total reward:  0.6400000000000003  reward:  1.0 rdn_beta:  0.5
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.129]
 [0.078]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.161]] [[1.266]
 [0.692]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.965]] [[0.129]
 [0.078]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.161]]
Printing some Q and Qe and total Qs values:  [[0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]] [[2.052]
 [2.052]
 [2.052]
 [2.052]
 [2.052]
 [2.052]
 [2.052]] [[0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]]
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.709]
 [0.701]
 [0.7  ]
 [0.682]
 [0.697]
 [0.698]] [[1.762]
 [1.286]
 [2.254]
 [1.602]
 [1.448]
 [2.07 ]
 [2.097]] [[0.687]
 [0.709]
 [0.701]
 [0.7  ]
 [0.682]
 [0.697]
 [0.698]]
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.828]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]] [[1.698]
 [1.993]
 [1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]] [[0.772]
 [0.828]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.835]
 [0.768]
 [0.693]
 [0.735]
 [0.69 ]
 [0.687]] [[0.381]
 [0.294]
 [0.259]
 [0.285]
 [0.249]
 [0.367]
 [0.451]] [[0.697]
 [0.835]
 [0.768]
 [0.693]
 [0.735]
 [0.69 ]
 [0.687]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.02014000000000003 0.7253333333333334 0.7253333333333334
probs:  [0.07050384691015962, 0.3355858662280281, 0.2421969905375965, 0.35171329632421583]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.66  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.66  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.5800000000000003  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  29 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5207851077623651
first move QE:  -0.09095797230831455
actor:  0 policy actor:  0  step number:  50 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.7733333333333335  reward:  1.0 rdn_beta:  0.333
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  0.024286666666666634 0.7153333333333335 0.7153333333333335
probs:  [0.06939167733733817, 0.34622310530332084, 0.24047704467931977, 0.3439081726800211]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 1.0000731569973882
maxi score, test score, baseline:  0.024286666666666634 0.7153333333333335 0.7153333333333335
probs:  [0.06939167733733817, 0.34622310530332084, 0.24047704467931977, 0.3439081726800211]
actor:  0 policy actor:  1  step number:  46 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[2.945]
 [2.945]
 [2.945]
 [2.945]
 [2.945]
 [2.945]
 [2.945]] [[0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.146]
 [ 0.131]
 [ 0.142]
 [ 0.146]
 [ 0.113]
 [ 0.287]] [[3.509]
 [2.414]
 [3.109]
 [2.977]
 [2.414]
 [4.615]
 [2.524]] [[0.387]
 [0.279]
 [0.39 ]
 [0.373]
 [0.279]
 [0.636]
 [0.37 ]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.5800000000000005  reward:  1.0 rdn_beta:  0.667
siam score:  -0.72259384
actor:  1 policy actor:  1  step number:  50 total reward:  0.5800000000000004  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.785]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]] [[0.661]
 [0.103]
 [1.496]
 [1.496]
 [1.496]
 [1.496]
 [1.496]] [[ 0.159]
 [-0.13 ]
 [ 0.695]
 [ 0.695]
 [ 0.695]
 [ 0.695]
 [ 0.695]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.027339999999999972 0.7153333333333335 0.7153333333333335
probs:  [0.07013324125346111, 0.3418628640716783, 0.23806562768490852, 0.349938266989952]
actor:  1 policy actor:  1  step number:  51 total reward:  0.586666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.027339999999999972 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.027339999999999972 0.7153333333333335 0.7153333333333335
probs:  [0.07013324125346111, 0.34186286407167843, 0.23806562768490852, 0.349938266989952]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.027339999999999972 0.7153333333333335 0.7153333333333335
probs:  [0.07013324125346111, 0.34186286407167843, 0.23806562768490852, 0.349938266989952]
maxi score, test score, baseline:  0.027339999999999972 0.7153333333333335 0.7153333333333335
probs:  [0.07013324125346111, 0.34186286407167843, 0.23806562768490852, 0.349938266989952]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.027339999999999972 0.7153333333333335 0.7153333333333335
probs:  [0.07013324125346111, 0.34186286407167843, 0.23806562768490852, 0.349938266989952]
maxi score, test score, baseline:  0.027339999999999972 0.7153333333333335 0.7153333333333335
probs:  [0.07013324125346111, 0.34186286407167843, 0.23806562768490852, 0.349938266989952]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07013324125346111, 0.34186286407167843, 0.23806562768490852, 0.349938266989952]
Printing some Q and Qe and total Qs values:  [[0.892]
 [0.864]
 [0.873]
 [0.88 ]
 [0.873]
 [0.873]
 [0.887]] [[1.806]
 [1.508]
 [1.712]
 [1.543]
 [1.416]
 [1.416]
 [1.727]] [[0.892]
 [0.864]
 [0.873]
 [0.88 ]
 [0.873]
 [0.873]
 [0.887]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7045721
using explorer policy with actor:  0
start point for exploration sampling:  11715
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.008]
 [ 0.126]
 [ 0.108]
 [ 0.126]
 [ 0.126]
 [ 0.112]
 [ 0.126]] [[2.097]
 [1.858]
 [2.036]
 [1.858]
 [1.858]
 [1.517]
 [1.858]] [[0.214]
 [0.2  ]
 [0.257]
 [0.2  ]
 [0.2  ]
 [0.064]
 [0.2  ]]
maxi score, test score, baseline:  0.027339999999999972 0.7153333333333335 0.7153333333333335
actor:  0 policy actor:  1  step number:  37 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.030753333333333306 0.7153333333333335 0.7153333333333335
probs:  [0.07013324125346111, 0.34186286407167843, 0.23806562768490852, 0.349938266989952]
maxi score, test score, baseline:  0.028419999999999977 0.7153333333333335 0.7153333333333335
probs:  [0.07013324125346111, 0.34186286407167843, 0.23806562768490852, 0.349938266989952]
maxi score, test score, baseline:  0.028419999999999977 0.7153333333333335 0.7153333333333335
probs:  [0.07013324125346111, 0.34186286407167843, 0.23806562768490852, 0.349938266989952]
actor:  1 policy actor:  1  step number:  42 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  48 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  33 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.289]
 [0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.086]] [[1.708]
 [2.219]
 [1.708]
 [1.708]
 [1.708]
 [1.708]
 [1.708]] [[-0.004]
 [ 0.497]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]]
maxi score, test score, baseline:  0.025726666666666644 0.7153333333333335 0.7153333333333335
probs:  [0.0698964293030525, 0.3440883688404463, 0.23726056047591204, 0.34875464138058926]
actor:  1 policy actor:  1  step number:  44 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.020406666666666653 0.7153333333333335 0.7153333333333335
probs:  [0.0698964293030525, 0.3440883688404463, 0.23726056047591204, 0.34875464138058926]
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.071]
 [0.095]] [[1.727]
 [1.727]
 [1.727]
 [1.727]
 [1.727]
 [1.99 ]
 [1.958]] [[-0.327]
 [-0.327]
 [-0.327]
 [-0.327]
 [-0.327]
 [-0.3  ]
 [-0.287]]
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.065]
 [0.097]
 [0.065]
 [0.065]
 [0.079]
 [0.066]] [[1.878]
 [1.878]
 [1.83 ]
 [1.878]
 [1.878]
 [1.579]
 [2.004]] [[0.255]
 [0.255]
 [0.26 ]
 [0.255]
 [0.255]
 [0.161]
 [0.299]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  0.7266666666666669  reward:  1.0 rdn_beta:  0.5
from probs:  [0.06987894381873805, 0.3440037020687655, 0.23744984056091203, 0.3486675135515844]
actor:  1 policy actor:  1  step number:  52 total reward:  0.5533333333333338  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.017526666666666645 0.7153333333333335 0.7153333333333335
probs:  [0.06987894381873803, 0.3440037020687656, 0.237449840560912, 0.3486675135515843]
maxi score, test score, baseline:  0.017526666666666645 0.7153333333333335 0.7153333333333335
probs:  [0.06987894381873803, 0.3440037020687656, 0.237449840560912, 0.3486675135515843]
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.815]
 [0.768]
 [0.768]
 [0.768]
 [0.757]
 [0.804]] [[0.686]
 [0.549]
 [0.686]
 [0.686]
 [0.686]
 [0.699]
 [0.409]] [[0.768]
 [0.815]
 [0.768]
 [0.768]
 [0.768]
 [0.757]
 [0.804]]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.187]
 [0.206]] [[1.958]
 [1.958]
 [1.958]
 [1.958]
 [1.958]
 [1.882]
 [1.958]] [[-0.333]
 [-0.333]
 [-0.333]
 [-0.333]
 [-0.333]
 [-0.378]
 [-0.333]]
actor:  0 policy actor:  0  step number:  40 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.020446666666666644 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.020446666666666644 0.7153333333333335 0.7153333333333335
probs:  [0.06987487216357845, 0.3440198655646171, 0.23745813853782174, 0.3486471237339827]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.020446666666666644 0.7153333333333335 0.7153333333333335
probs:  [0.06987487216357845, 0.34401986556461706, 0.23745813853782174, 0.34864712373398277]
Printing some Q and Qe and total Qs values:  [[-0.024]
 [ 0.034]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.005]] [[1.656]
 [2.587]
 [1.656]
 [1.656]
 [1.656]
 [1.656]
 [2.294]] [[-0.14 ]
 [ 0.315]
 [-0.14 ]
 [-0.14 ]
 [-0.14 ]
 [-0.14 ]
 [ 0.158]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  0.667
using another actor
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.031]] [[1.242]
 [1.242]
 [1.242]
 [1.242]
 [1.242]
 [1.242]
 [1.242]] [[0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.031]]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.322]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.487]] [[1.292]
 [1.869]
 [1.292]
 [1.292]
 [1.292]
 [1.292]
 [2.151]] [[0.264]
 [0.406]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.658]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.09045431931829216
maxi score, test score, baseline:  0.01719333333333331 0.7153333333333335 0.7153333333333335
probs:  [0.06987258940771571, 0.3440101491333829, 0.2374812809964528, 0.3486359804624486]
maxi score, test score, baseline:  0.01719333333333331 0.7153333333333335 0.7153333333333335
probs:  [0.06987258940771571, 0.3440101491333829, 0.2374812809964528, 0.3486359804624486]
maxi score, test score, baseline:  0.01719333333333331 0.7153333333333335 0.7153333333333335
actor:  1 policy actor:  1  step number:  33 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.232]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]] [[1.568]
 [1.828]
 [1.93 ]
 [1.93 ]
 [1.93 ]
 [1.93 ]
 [1.93 ]] [[-0.233]
 [-0.116]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]]
Printing some Q and Qe and total Qs values:  [[0.873]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]] [[1.022]
 [1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]] [[0.873]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]]
actor:  0 policy actor:  1  step number:  46 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.082 0.245 0.184 0.02  0.02  0.102 0.347]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]] [[-0.188]
 [-0.188]
 [-0.188]
 [-0.188]
 [-0.188]
 [-0.188]
 [-0.188]] [[-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.020459999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07024302648165781, 0.34253972876560107, 0.23672621119889029, 0.35049103355385075]
maxi score, test score, baseline:  0.020459999999999985 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.020459999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07024302648165781, 0.34253972876560107, 0.23672621119889029, 0.35049103355385075]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  42 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.020459999999999985 0.7153333333333335 0.7153333333333335
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.556]
 [0.476]
 [0.432]
 [0.476]
 [0.426]
 [0.476]] [[0.   ]
 [0.226]
 [0.   ]
 [0.083]
 [0.   ]
 [0.064]
 [0.   ]] [[-0.227]
 [-0.11 ]
 [-0.227]
 [-0.258]
 [-0.227]
 [-0.267]
 [-0.227]]
maxi score, test score, baseline:  0.020459999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07024302648165781, 0.34253972876560107, 0.2367262111988904, 0.3504910335538508]
maxi score, test score, baseline:  0.020459999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07024302648165781, 0.34253972876560107, 0.2367262111988904, 0.3504910335538508]
maxi score, test score, baseline:  0.020459999999999985 0.7153333333333335 0.7153333333333335
actor:  0 policy actor:  0  step number:  44 total reward:  0.4466666666666663  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 0.030232985108752997
actor:  0 policy actor:  1  step number:  34 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07024065661507427, 0.34253057080618216, 0.23674932178857996, 0.3504794507901636]
maxi score, test score, baseline:  0.02399333333333331 0.7153333333333335 0.7153333333333335
probs:  [0.07024065661507427, 0.34253057080618216, 0.23674932178857996, 0.3504794507901636]
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.217]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[1.222]
 [1.226]
 [1.222]
 [1.222]
 [1.222]
 [1.222]
 [1.222]] [[0.239]
 [0.329]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]]
maxi score, test score, baseline:  0.02399333333333331 0.7153333333333335 0.7153333333333335
probs:  [0.07023829772144155, 0.342521455249926, 0.2367723253716701, 0.3504679216569624]
from probs:  [0.07023829772144155, 0.342521455249926, 0.2367723253716701, 0.3504679216569624]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.696]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.67 ]] [[1.479]
 [1.227]
 [1.479]
 [1.479]
 [1.479]
 [1.479]
 [1.417]] [[0.476]
 [0.696]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.67 ]]
maxi score, test score, baseline:  0.023993333333333318 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.023993333333333318 0.7153333333333335 0.7153333333333335
probs:  [0.07023829772144155, 0.34252145524992605, 0.2367723253716701, 0.3504679216569624]
Printing some Q and Qe and total Qs values:  [[0.841]
 [0.274]
 [0.786]
 [0.741]
 [0.803]
 [0.759]
 [0.676]] [[-0.078]
 [ 0.981]
 [ 1.432]
 [ 1.817]
 [-0.183]
 [ 1.649]
 [ 1.224]] [[0.841]
 [0.274]
 [0.786]
 [0.741]
 [0.803]
 [0.759]
 [0.676]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  41 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  42 total reward:  0.5133333333333333  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]] [[1.353]
 [1.068]
 [1.068]
 [1.068]
 [1.068]
 [1.068]
 [1.068]] [[0.275]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.023913333333333318 0.7153333333333335 0.7153333333333335
probs:  [0.0700661510259997, 0.34410419921666174, 0.23622189786869066, 0.34960775188864784]
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.045]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[-1.023]
 [ 0.108]
 [-1.023]
 [-1.023]
 [-1.023]
 [-1.023]
 [-1.023]] [[0.05 ]
 [0.374]
 [0.05 ]
 [0.05 ]
 [0.05 ]
 [0.05 ]
 [0.05 ]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.023913333333333318 0.7153333333333335 0.7153333333333335
probs:  [0.0700661510259997, 0.34410419921666174, 0.23622189786869066, 0.34960775188864784]
maxi score, test score, baseline:  0.023913333333333318 0.7153333333333335 0.7153333333333335
probs:  [0.0700661510259997, 0.34410419921666174, 0.23622189786869066, 0.34960775188864784]
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.018]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[1.014]
 [1.052]
 [1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]] [[-0.084]
 [-0.069]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.023913333333333318 0.7153333333333335 0.7153333333333335
probs:  [0.07007016665471916, 0.34408819650305894, 0.2362137759032291, 0.3496278609389929]
maxi score, test score, baseline:  0.023913333333333318 0.7153333333333335 0.7153333333333335
probs:  [0.07007016665471916, 0.34408819650305894, 0.2362137759032291, 0.3496278609389929]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.023913333333333318 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.023913333333333318 0.7153333333333335 0.7153333333333335
probs:  [0.07007016665471914, 0.344088196503059, 0.23621377590322906, 0.34962786093899284]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.246]
 [0.239]
 [0.246]
 [0.246]
 [0.237]
 [0.246]] [[1.966]
 [1.889]
 [2.276]
 [1.889]
 [1.889]
 [3.261]
 [1.889]] [[0.43 ]
 [0.439]
 [0.54 ]
 [0.439]
 [0.439]
 [0.805]
 [0.439]]
siam score:  -0.70869124
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.388]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.327]] [[2.271]
 [2.321]
 [2.271]
 [2.271]
 [2.271]
 [2.271]
 [5.26 ]] [[0.143]
 [0.169]
 [0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.789]]
maxi score, test score, baseline:  0.023913333333333318 0.7153333333333335 0.7153333333333335
probs:  [0.07007016665471914, 0.344088196503059, 0.23621377590322906, 0.34962786093899284]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.613]
 [0.684]
 [0.613]
 [0.613]
 [0.657]
 [0.771]] [[1.129]
 [1.129]
 [0.831]
 [1.129]
 [1.129]
 [0.858]
 [0.852]] [[0.32 ]
 [0.32 ]
 [0.242]
 [0.32 ]
 [0.32 ]
 [0.228]
 [0.339]]
actor:  0 policy actor:  1  step number:  46 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.02423333333333331 0.7153333333333335 0.7153333333333335
probs:  [0.07007016665471914, 0.344088196503059, 0.23621377590322906, 0.34962786093899284]
maxi score, test score, baseline:  0.02423333333333331 0.7153333333333335 0.7153333333333335
probs:  [0.07007016665471914, 0.344088196503059, 0.23621377590322906, 0.34962786093899284]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.706]] [[0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [1.116]] [[0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.93 ]]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  50 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]] [[0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]] [[0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.293]
 [0.439]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[1.94 ]
 [1.94 ]
 [3.158]
 [1.94 ]
 [1.94 ]
 [1.94 ]
 [1.94 ]] [[0.438]
 [0.438]
 [0.77 ]
 [0.438]
 [0.438]
 [0.438]
 [0.438]]
maxi score, test score, baseline:  0.02423333333333331 0.7153333333333335 0.7153333333333335
probs:  [0.07010589907599084, 0.34372233826317206, 0.23636504439101097, 0.3498067182698261]
actor:  0 policy actor:  1  step number:  49 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.024433333333333314 0.7153333333333335 0.7153333333333335
probs:  [0.07010361539366262, 0.3437130010578794, 0.23638782259652574, 0.34979556095193215]
Printing some Q and Qe and total Qs values:  [[0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]] [[3.136]
 [3.136]
 [3.136]
 [3.136]
 [3.136]
 [3.136]
 [3.136]] [[0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]]
maxi score, test score, baseline:  0.024433333333333314 0.7153333333333335 0.7153333333333335
actor:  1 policy actor:  1  step number:  35 total reward:  0.7333333333333335  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.431]
 [0.295]
 [0.4  ]
 [0.386]
 [0.386]
 [0.347]] [[0.456]
 [0.662]
 [0.565]
 [0.443]
 [0.462]
 [0.462]
 [0.535]] [[-0.207]
 [-0.115]
 [-0.282]
 [-0.218]
 [-0.226]
 [-0.226]
 [-0.241]]
maxi score, test score, baseline:  0.024433333333333314 0.7153333333333335 0.7153333333333335
probs:  [0.07016324925137915, 0.3434756162970877, 0.23626694544807475, 0.35009418900345834]
line 256 mcts: sample exp_bonus 0.9513092748643461
maxi score, test score, baseline:  0.024433333333333314 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.024433333333333314 0.7153333333333335 0.7153333333333335
probs:  [0.07016324925137915, 0.3434756162970877, 0.23626694544807475, 0.35009418900345834]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.024433333333333314 0.7153333333333335 0.7153333333333335
probs:  [0.0706434884483959, 0.34156392600449387, 0.2352935060810339, 0.35249907946607634]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.024433333333333314 0.7153333333333335 0.7153333333333335
actor:  0 policy actor:  0  step number:  38 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  81 total reward:  0.17333333333333167  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.0708129807944464, 0.33998188617014835, 0.23585887729153454, 0.3533462557438708]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [ 0.022]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[1.334]
 [1.949]
 [1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]] [[-0.705]
 [-0.463]
 [-0.705]
 [-0.705]
 [-0.705]
 [-0.705]
 [-0.705]]
Printing some Q and Qe and total Qs values:  [[-0.015]
 [ 0.041]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[1.251]
 [1.863]
 [1.251]
 [1.251]
 [1.251]
 [1.251]
 [1.251]] [[-0.732]
 [-0.472]
 [-0.732]
 [-0.732]
 [-0.732]
 [-0.732]
 [-0.732]]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[1.403]
 [1.403]
 [1.403]
 [1.403]
 [1.403]
 [1.403]
 [1.403]] [[0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]]
maxi score, test score, baseline:  0.02508666666666665 0.7153333333333335 0.7153333333333335
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.082]
 [0.129]
 [0.094]
 [0.082]
 [0.082]
 [0.126]] [[1.896]
 [1.59 ]
 [1.95 ]
 [1.76 ]
 [1.59 ]
 [1.59 ]
 [1.797]] [[-0.288]
 [-0.459]
 [-0.292]
 [-0.39 ]
 [-0.459]
 [-0.459]
 [-0.346]]
maxi score, test score, baseline:  0.02508666666666665 0.7153333333333335 0.7153333333333335
probs:  [0.0708129807944464, 0.33998188617014835, 0.23585887729153454, 0.3533462557438708]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.024]
 [0.11 ]
 [0.024]
 [0.024]
 [0.024]
 [0.024]] [[1.945]
 [1.945]
 [1.679]
 [1.945]
 [1.945]
 [1.945]
 [1.945]] [[-0.367]
 [-0.367]
 [-0.369]
 [-0.367]
 [-0.367]
 [-0.367]
 [-0.367]]
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.157]
 [0.122]
 [0.18 ]
 [0.157]
 [0.113]
 [0.193]] [[1.994]
 [1.994]
 [2.15 ]
 [2.101]
 [1.994]
 [1.755]
 [2.045]] [[0.468]
 [0.468]
 [0.478]
 [0.503]
 [0.468]
 [0.393]
 [0.5  ]]
maxi score, test score, baseline:  0.02508666666666665 0.7153333333333335 0.7153333333333335
probs:  [0.07121101581940209, 0.3362666312951167, 0.23718659237203618, 0.35533576051344506]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.02267333333333332 0.7153333333333335 0.7153333333333335
probs:  [0.07121101581940209, 0.3362666312951167, 0.23718659237203618, 0.35533576051344506]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[-0.03]
 [-0.03]
 [-0.03]
 [-0.03]
 [-0.03]
 [-0.03]
 [-0.03]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.113]
 [0.077]
 [0.077]
 [0.077]
 [0.077]
 [0.077]] [[0.689]
 [1.088]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]] [[-0.433]
 [-0.166]
 [-0.433]
 [-0.433]
 [-0.433]
 [-0.433]
 [-0.433]]
first move QE:  -0.0881393557305244
using explorer policy with actor:  1
maxi score, test score, baseline:  0.01972666666666665 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.01972666666666665 0.7153333333333335 0.7153333333333335
probs:  [0.07121101581940209, 0.3362666312951167, 0.23718659237203618, 0.35533576051344506]
siam score:  -0.72192824
maxi score, test score, baseline:  0.01972666666666665 0.7153333333333335 0.7153333333333335
probs:  [0.07084906784984946, 0.3376870096904671, 0.23794072204335454, 0.3535232004163289]
from probs:  [0.07084906784984946, 0.3376870096904671, 0.23794072204335454, 0.3535232004163289]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.01972666666666665 0.7153333333333335 0.7153333333333335
probs:  [0.07084906784984946, 0.3376870096904671, 0.23794072204335448, 0.3535232004163289]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.705]
 [0.506]
 [0.572]
 [0.506]
 [0.506]
 [0.65 ]] [[0.616]
 [0.683]
 [1.097]
 [0.949]
 [1.097]
 [1.097]
 [0.751]] [[0.556]
 [0.574]
 [0.529]
 [0.541]
 [0.529]
 [0.529]
 [0.55 ]]
maxi score, test score, baseline:  0.016379999999999992 0.7153333333333335 0.7153333333333335
probs:  [0.07084906784984946, 0.3376870096904671, 0.23794072204335448, 0.3535232004163289]
maxi score, test score, baseline:  0.016379999999999992 0.7153333333333335 0.7153333333333335
probs:  [0.07084906784984946, 0.3376870096904671, 0.23794072204335448, 0.3535232004163289]
actor:  1 policy actor:  1  step number:  50 total reward:  0.6333333333333337  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.016379999999999974 0.7153333333333335 0.7153333333333335
probs:  [0.07084906784984946, 0.3376870096904671, 0.23794072204335448, 0.3535232004163289]
actor:  1 policy actor:  1  step number:  62 total reward:  0.5266666666666671  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.016379999999999974 0.7153333333333335 0.7153333333333335
Printing some Q and Qe and total Qs values:  [[0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]] [[2.502]
 [2.502]
 [2.502]
 [2.502]
 [2.502]
 [2.502]
 [2.502]] [[1.939]
 [1.939]
 [1.939]
 [1.939]
 [1.939]
 [1.939]
 [1.939]]
actor:  0 policy actor:  0  step number:  42 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.5000000000000003  reward:  1.0 rdn_beta:  0.333
siam score:  -0.72488326
maxi score, test score, baseline:  0.016206666666666654 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497157, 0.23852095723587116, 0.354385469304888]
actor:  1 policy actor:  1  step number:  59 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.016206666666666654 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.016206666666666654 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497158, 0.23852095723587116, 0.354385469304888]
maxi score, test score, baseline:  0.016206666666666654 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497158, 0.23852095723587116, 0.354385469304888]
maxi score, test score, baseline:  0.016206666666666654 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497158, 0.23852095723587116, 0.354385469304888]
siam score:  -0.7170261
actor:  1 policy actor:  1  step number:  46 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  51 total reward:  0.5866666666666671  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]] [[1.849]
 [1.849]
 [1.849]
 [1.849]
 [1.849]
 [1.849]
 [1.849]] [[0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]]
maxi score, test score, baseline:  0.016206666666666654 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497158, 0.23852095723587116, 0.354385469304888]
start point for exploration sampling:  11715
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  0.016206666666666654 0.7153333333333335 0.7153333333333335
using explorer policy with actor:  0
maxi score, test score, baseline:  0.016206666666666654 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497158, 0.23852095723587116, 0.354385469304888]
maxi score, test score, baseline:  0.016206666666666654 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497158, 0.23852095723587116, 0.354385469304888]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.013539999999999983 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497158, 0.23852095723587116, 0.354385469304888]
maxi score, test score, baseline:  0.013539999999999983 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497158, 0.23852095723587116, 0.354385469304888]
actor:  0 policy actor:  1  step number:  74 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.013766666666666656 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497157, 0.23852095723587116, 0.354385469304888]
maxi score, test score, baseline:  0.013766666666666656 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497157, 0.23852095723587116, 0.354385469304888]
maxi score, test score, baseline:  0.013766666666666656 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497157, 0.23852095723587116, 0.354385469304888]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.736]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[1.55 ]
 [1.55 ]
 [1.358]
 [1.55 ]
 [1.55 ]
 [1.55 ]
 [1.55 ]] [[0.186]
 [0.186]
 [0.382]
 [0.186]
 [0.186]
 [0.186]
 [0.186]]
maxi score, test score, baseline:  0.013766666666666656 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497157, 0.23852095723587116, 0.354385469304888]
maxi score, test score, baseline:  0.013766666666666656 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497157, 0.23852095723587116, 0.354385469304888]
actor:  1 policy actor:  1  step number:  53 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  0.167
from probs:  [0.07102158140952514, 0.3360719920497157, 0.23852095723587116, 0.354385469304888]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.013766666666666656 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.013766666666666656 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497157, 0.23852095723587116, 0.354385469304888]
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]] [[8.268]
 [3.229]
 [3.229]
 [3.229]
 [3.229]
 [3.229]
 [3.229]] [[0.808]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]]
maxi score, test score, baseline:  0.013766666666666656 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.013766666666666656 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497157, 0.23852095723587116, 0.354385469304888]
maxi score, test score, baseline:  0.013766666666666656 0.7153333333333335 0.7153333333333335
probs:  [0.07102158140952514, 0.3360719920497157, 0.23852095723587116, 0.354385469304888]
actor:  0 policy actor:  0  step number:  34 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.333
siam score:  -0.71414584
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]] [[7.56 ]
 [2.124]
 [2.124]
 [2.124]
 [2.124]
 [2.124]
 [2.124]] [[0.862]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]]
actor:  1 policy actor:  1  step number:  102 total reward:  0.12666666666666582  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07102158140952514, 0.3360719920497157, 0.23852095723587116, 0.354385469304888]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  35 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.01439333333333333 0.7153333333333335 0.7153333333333335
probs:  [0.071018973382678, 0.336096407576087, 0.23851218535172147, 0.35437243368951354]
maxi score, test score, baseline:  0.01439333333333333 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.01439333333333333 0.7153333333333335 0.7153333333333335
probs:  [0.071018973382678, 0.336096407576087, 0.23851218535172147, 0.35437243368951354]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  36 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7112434
maxi score, test score, baseline:  0.014993333333333308 0.7153333333333335 0.7153333333333335
probs:  [0.071018973382678, 0.336096407576087, 0.23851218535172147, 0.35437243368951354]
maxi score, test score, baseline:  0.014993333333333308 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.014993333333333308 0.7153333333333335 0.7153333333333335
probs:  [0.071018973382678, 0.336096407576087, 0.23851218535172147, 0.35437243368951354]
using explorer policy with actor:  1
siam score:  -0.71178555
actor:  1 policy actor:  1  step number:  34 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.014993333333333308 0.7153333333333335 0.7153333333333335
probs:  [0.071018973382678, 0.336096407576087, 0.23851218535172147, 0.35437243368951354]
maxi score, test score, baseline:  0.014993333333333308 0.7153333333333335 0.7153333333333335
probs:  [0.071018973382678, 0.336096407576087, 0.23851218535172147, 0.35437243368951354]
actor:  1 policy actor:  1  step number:  41 total reward:  0.706666666666667  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  43 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.014993333333333308 0.7153333333333335 0.7153333333333335
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.014993333333333308 0.7153333333333335 0.7153333333333335
Printing some Q and Qe and total Qs values:  [[ 0.301]
 [-0.013]
 [ 0.404]
 [ 0.301]
 [ 0.34 ]
 [ 0.377]
 [ 0.301]] [[1.809]
 [1.581]
 [1.714]
 [1.809]
 [0.808]
 [1.436]
 [1.809]] [[-0.029]
 [-0.42 ]
 [ 0.042]
 [-0.029]
 [-0.324]
 [-0.077]
 [-0.029]]
maxi score, test score, baseline:  0.014993333333333308 0.7153333333333335 0.7153333333333335
probs:  [0.07101897338267801, 0.3360964075760869, 0.23851218535172145, 0.3543724336895136]
maxi score, test score, baseline:  0.014993333333333308 0.7153333333333335 0.7153333333333335
probs:  [0.07101897338267801, 0.3360964075760869, 0.23851218535172145, 0.3543724336895136]
maxi score, test score, baseline:  0.014993333333333308 0.7153333333333335 0.7153333333333335
probs:  [0.07101897338267801, 0.3360964075760869, 0.23851218535172145, 0.3543724336895136]
Printing some Q and Qe and total Qs values:  [[-0.024]
 [ 0.158]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]] [[2.05 ]
 [1.501]
 [2.05 ]
 [2.05 ]
 [2.05 ]
 [2.05 ]
 [2.05 ]] [[0.765]
 [0.588]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  38 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.071018973382678, 0.336096407576087, 0.23851218535172147, 0.35437243368951354]
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.071018973382678, 0.336096407576087, 0.23851218535172147, 0.35437243368951354]
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.071018973382678, 0.336096407576087, 0.23851218535172147, 0.35437243368951354]
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.071018973382678, 0.336096407576087, 0.23851218535172147, 0.35437243368951354]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.071018973382678, 0.336096407576087, 0.23851218535172147, 0.35437243368951354]
siam score:  -0.71749467
actor:  1 policy actor:  1  step number:  47 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.046]
 [-0.035]
 [-0.035]
 [ 0.074]
 [-0.035]
 [-0.035]
 [-0.035]] [[1.97 ]
 [1.025]
 [1.025]
 [2.037]
 [1.025]
 [1.025]
 [1.025]] [[-0.081]
 [-0.793]
 [-0.793]
 [-0.009]
 [-0.793]
 [-0.793]
 [-0.793]]
Printing some Q and Qe and total Qs values:  [[-0.014]
 [ 0.03 ]
 [ 0.03 ]
 [ 0.03 ]
 [ 0.03 ]
 [ 0.03 ]
 [ 0.03 ]] [[2.221]
 [2.039]
 [2.039]
 [2.039]
 [2.039]
 [2.039]
 [2.039]] [[0.29 ]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]] [[3.931]
 [1.724]
 [1.724]
 [1.724]
 [1.724]
 [1.724]
 [1.724]] [[0.507]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[1.341]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]] [[-0.007]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[4.581]
 [1.23 ]
 [1.23 ]
 [1.23 ]
 [1.23 ]
 [1.23 ]
 [1.23 ]] [[0.56 ]
 [0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07188793412259945, 0.3279614706660155, 0.24143486344824686, 0.3587157317631383]
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07188793412259945, 0.3279614706660155, 0.24143486344824686, 0.3587157317631383]
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07188793412259945, 0.3279614706660155, 0.24143486344824686, 0.3587157317631383]
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[1.048]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.3200000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07188793412259945, 0.3279614706660155, 0.24143486344824686, 0.3587157317631383]
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07188793412259945, 0.3279614706660155, 0.24143486344824686, 0.3587157317631383]
line 256 mcts: sample exp_bonus 0.5037758176614705
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
actor:  1 policy actor:  1  step number:  68 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07188793412259945, 0.3279614706660155, 0.24143486344824686, 0.3587157317631383]
actor:  1 policy actor:  1  step number:  81 total reward:  0.10666666666666647  reward:  1.0 rdn_beta:  0.667
siam score:  -0.72038823
actor:  1 policy actor:  1  step number:  69 total reward:  0.36  reward:  1.0 rdn_beta:  0.5
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
Printing some Q and Qe and total Qs values:  [[ 0.011]
 [ 0.082]
 [ 0.007]
 [-0.015]
 [-0.014]
 [ 0.007]
 [ 0.024]] [[0.702]
 [0.694]
 [0.711]
 [0.5  ]
 [0.838]
 [0.711]
 [0.662]] [[0.166]
 [0.236]
 [0.164]
 [0.106]
 [0.165]
 [0.164]
 [0.173]]
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  52 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  43 total reward:  0.706666666666667  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7164909
maxi score, test score, baseline:  0.018499999999999985 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
actor:  0 policy actor:  0  step number:  45 total reward:  0.45333333333333303  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.08557664217508892
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.023]] [[2.112]
 [2.112]
 [2.112]
 [1.639]
 [2.112]
 [2.112]
 [1.814]] [[0.285]
 [0.285]
 [0.285]
 [0.13 ]
 [0.285]
 [0.285]
 [0.19 ]]
line 256 mcts: sample exp_bonus 2.1650100057376322
maxi score, test score, baseline:  0.01841999999999998 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[-0.053]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.6533333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.01841999999999998 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
from probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
siam score:  -0.7220287
maxi score, test score, baseline:  0.01841999999999998 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.529]] [[1.795]
 [1.513]
 [1.513]
 [1.513]
 [1.513]
 [1.513]
 [1.968]] [[0.562]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.603]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.706666666666667  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
maxi score, test score, baseline:  0.01841999999999998 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.01841999999999998 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
actor:  0 policy actor:  1  step number:  49 total reward:  0.586666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.01508666666666665 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
maxi score, test score, baseline:  0.01508666666666665 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
maxi score, test score, baseline:  0.01508666666666665 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.01508666666666665 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
maxi score, test score, baseline:  0.01508666666666665 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[1.363]
 [1.363]
 [1.363]
 [1.363]
 [1.363]
 [1.363]
 [1.363]] [[0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.01508666666666665 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.01508666666666665 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.449]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.837]] [[0.115]
 [0.073]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.192]] [[0.779]
 [0.449]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.837]]
maxi score, test score, baseline:  0.01508666666666665 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]] [[0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]] [[-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]]
maxi score, test score, baseline:  0.01508666666666665 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.015086666666666642 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
maxi score, test score, baseline:  0.015086666666666642 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
actor:  1 policy actor:  1  step number:  65 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.015086666666666642 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.801]
 [0.713]
 [0.719]
 [0.465]
 [0.764]
 [0.731]] [[1.393]
 [0.136]
 [1.706]
 [1.165]
 [0.997]
 [1.258]
 [0.806]] [[0.492]
 [0.801]
 [0.713]
 [0.719]
 [0.465]
 [0.764]
 [0.731]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.015086666666666642 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
maxi score, test score, baseline:  0.015086666666666642 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
actor:  1 policy actor:  1  step number:  49 total reward:  0.6800000000000004  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  44 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.019]
 [0.151]
 [0.019]
 [0.019]
 [0.019]
 [0.019]] [[1.466]
 [1.466]
 [1.687]
 [1.466]
 [1.466]
 [1.466]
 [1.466]] [[0.194]
 [0.194]
 [0.318]
 [0.194]
 [0.194]
 [0.194]
 [0.194]]
maxi score, test score, baseline:  0.01589999999999999 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
maxi score, test score, baseline:  0.01589999999999999 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
maxi score, test score, baseline:  0.01589999999999999 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
maxi score, test score, baseline:  0.01589999999999999 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
maxi score, test score, baseline:  0.01589999999999999 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
actor:  1 policy actor:  1  step number:  36 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
maxi score, test score, baseline:  0.01589999999999999 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526354, 0.3599435642647215]
maxi score, test score, baseline:  0.01589999999999999 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526354, 0.3599435642647215]
actor:  0 policy actor:  1  step number:  35 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.013]
 [-0.029]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.027]] [[-1.161]
 [ 0.377]
 [-0.638]
 [-0.637]
 [-0.603]
 [-0.307]
 [-0.51 ]] [[-0.015]
 [ 0.529]
 [ 0.167]
 [ 0.168]
 [ 0.18 ]
 [ 0.283]
 [ 0.212]]
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.676]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.648]] [[1.865]
 [1.536]
 [1.337]
 [1.337]
 [1.337]
 [1.337]
 [1.904]] [[0.67 ]
 [0.676]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.648]]
maxi score, test score, baseline:  0.019313333333333325 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.019313333333333325 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
maxi score, test score, baseline:  0.019313333333333325 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.019313333333333325 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
actor:  0 policy actor:  1  step number:  38 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.019193333333333326 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.019193333333333326 0.7153333333333335 0.7153333333333335
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.019193333333333326 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.168]
 [0.181]
 [0.1  ]
 [0.168]
 [0.144]
 [0.083]] [[2.393]
 [2.393]
 [2.031]
 [1.637]
 [2.393]
 [1.997]
 [1.818]] [[ 0.401]
 [ 0.401]
 [ 0.192]
 [-0.122]
 [ 0.401]
 [ 0.137]
 [-0.027]]
maxi score, test score, baseline:  0.019193333333333326 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
actor:  1 policy actor:  1  step number:  50 total reward:  0.6066666666666669  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.019193333333333326 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.019193333333333326 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.019193333333333326 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.32566175746024434, 0.2422610925252635, 0.3599435642647215]
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.019193333333333326 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.019193333333333326 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
maxi score, test score, baseline:  0.019193333333333326 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
actor:  1 policy actor:  1  step number:  97 total reward:  0.26666666666666583  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.019193333333333333 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
maxi score, test score, baseline:  0.019193333333333333 0.7153333333333335 0.7153333333333335
probs:  [0.07213358574977062, 0.3256617574602444, 0.24226109252526346, 0.35994356426472157]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.019193333333333333 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
maxi score, test score, baseline:  0.019193333333333333 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
maxi score, test score, baseline:  0.019193333333333326 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.755]
 [0.644]
 [0.647]
 [0.583]
 [0.654]
 [0.661]] [[1.02 ]
 [0.721]
 [0.984]
 [0.818]
 [1.169]
 [0.913]
 [1.151]] [[0.426]
 [0.34 ]
 [0.405]
 [0.297]
 [0.467]
 [0.368]
 [0.533]]
maxi score, test score, baseline:  0.019193333333333326 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
actor:  1 policy actor:  1  step number:  55 total reward:  0.6000000000000005  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.8  ]
 [0.889]
 [0.766]
 [0.746]
 [0.746]
 [0.746]
 [0.763]] [[1.797]
 [1.187]
 [1.616]
 [1.225]
 [1.225]
 [1.225]
 [1.906]] [[0.8  ]
 [0.889]
 [0.766]
 [0.746]
 [0.746]
 [0.746]
 [0.763]]
actor:  0 policy actor:  1  step number:  43 total reward:  0.6800000000000004  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
maxi score, test score, baseline:  0.019139999999999983 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.019139999999999983 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
actor:  1 policy actor:  1  step number:  60 total reward:  0.5800000000000004  reward:  1.0 rdn_beta:  0.5
siam score:  -0.71253103
maxi score, test score, baseline:  0.019139999999999983 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  0.01575333333333332 0.7153333333333335 0.7153333333333335
using explorer policy with actor:  1
maxi score, test score, baseline:  0.01575333333333332 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
maxi score, test score, baseline:  0.01575333333333332 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.012366666666666654 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
siam score:  -0.7163693
siam score:  -0.71409225
maxi score, test score, baseline:  0.012366666666666654 0.7153333333333335 0.7153333333333335
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.292]
 [0.348]
 [0.326]
 [0.307]
 [0.261]
 [0.296]] [[1.908]
 [1.357]
 [1.251]
 [1.245]
 [1.25 ]
 [1.235]
 [1.265]] [[-0.274]
 [-0.253]
 [-0.232]
 [-0.256]
 [-0.274]
 [-0.324]
 [-0.279]]
actor:  1 policy actor:  1  step number:  79 total reward:  0.3733333333333334  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.012366666666666654 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
siam score:  -0.71747315
maxi score, test score, baseline:  0.008979999999999981 0.7153333333333335 0.7153333333333335
line 256 mcts: sample exp_bonus 2.1463362942734823
maxi score, test score, baseline:  0.008979999999999981 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.181]
 [0.333]] [[1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.446]
 [2.113]] [[-0.144]
 [-0.144]
 [-0.144]
 [-0.144]
 [-0.144]
 [-0.036]
 [ 0.393]]
maxi score, test score, baseline:  0.008979999999999981 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
maxi score, test score, baseline:  0.008979999999999981 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  36 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.132]
 [-0.003]
 [-0.05 ]
 [ 0.074]
 [ 0.074]
 [ 0.074]
 [ 0.051]] [[0.273]
 [0.577]
 [0.752]
 [0.883]
 [0.883]
 [0.883]
 [0.751]] [[-0.443]
 [-0.425]
 [-0.385]
 [-0.195]
 [-0.195]
 [-0.195]
 [-0.285]]
actor:  1 policy actor:  1  step number:  71 total reward:  0.5200000000000007  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0056333333333333175 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
actor:  1 policy actor:  1  step number:  43 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.5
siam score:  -0.714335
maxi score, test score, baseline:  0.002246666666666661 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
actor:  1 policy actor:  1  step number:  49 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0011400000000000162 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
siam score:  -0.7130092
maxi score, test score, baseline:  -0.0011400000000000162 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
maxi score, test score, baseline:  -0.0011400000000000162 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572803, 0.358323635920693]
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.208]
 [0.156]
 [0.208]
 [0.208]
 [0.208]
 [0.208]] [[2.232]
 [2.026]
 [2.119]
 [2.026]
 [2.026]
 [2.026]
 [2.026]] [[0.399]
 [0.27 ]
 [0.277]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.0011400000000000162 0.7153333333333335 0.7153333333333335
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  50 total reward:  0.5933333333333337  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.0011400000000000162 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  -0.0011400000000000162 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572806, 0.358323635920693]
maxi score, test score, baseline:  -0.0011400000000000162 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572806, 0.358323635920693]
maxi score, test score, baseline:  -0.0011400000000000162 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572806, 0.358323635920693]
maxi score, test score, baseline:  -0.0011400000000000162 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  -0.0011400000000000162 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572806, 0.358323635920693]
maxi score, test score, baseline:  -0.0011400000000000162 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572806, 0.358323635920693]
maxi score, test score, baseline:  -0.0011400000000000162 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572806, 0.358323635920693]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.3399999999999993  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  44 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.0014200000000000102 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572806, 0.358323635920693]
actor:  1 policy actor:  1  step number:  60 total reward:  0.5000000000000004  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.026]
 [-0.015]
 [-0.02 ]
 [-0.015]
 [-0.015]
 [-0.015]] [[ 0.945]
 [ 0.842]
 [ 0.945]
 [-0.299]
 [ 0.945]
 [ 0.945]
 [ 0.945]] [[ 0.046]
 [-0.034]
 [ 0.046]
 [-0.789]
 [ 0.046]
 [ 0.046]
 [ 0.046]]
line 256 mcts: sample exp_bonus 0.715699478191495
maxi score, test score, baseline:  -0.00480666666666668 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971373, 0.3241961788938652, 0.24567069741572806, 0.358323635920693]
maxi score, test score, baseline:  -0.00480666666666668 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971374, 0.3241961788938652, 0.24567069741572806, 0.35832363592069305]
maxi score, test score, baseline:  -0.00480666666666668 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  -0.00480666666666668 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971374, 0.3241961788938652, 0.24567069741572806, 0.35832363592069305]
line 256 mcts: sample exp_bonus -0.12622618252209467
maxi score, test score, baseline:  -0.00480666666666668 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971374, 0.3241961788938652, 0.24567069741572806, 0.35832363592069305]
maxi score, test score, baseline:  -0.00480666666666668 0.7153333333333335 0.7153333333333335
probs:  [0.07180948776971374, 0.3241961788938652, 0.24567069741572806, 0.35832363592069305]
actor:  0 policy actor:  1  step number:  46 total reward:  0.5133333333333335  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  38 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.159]
 [0.071]] [[1.602]
 [1.602]
 [1.602]
 [1.602]
 [1.602]
 [1.413]
 [1.602]] [[0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.096]
 [0.101]]
maxi score, test score, baseline:  -0.005166666666666673 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  -0.005166666666666673 0.7153333333333335 0.7153333333333335
probs:  [0.07179376468908465, 0.324125078759383, 0.24583610879100642, 0.35824504776052596]
maxi score, test score, baseline:  -0.005166666666666673 0.7153333333333335 0.7153333333333335
probs:  [0.07179376468908465, 0.324125078759383, 0.24583610879100642, 0.35824504776052596]
maxi score, test score, baseline:  -0.005166666666666673 0.7153333333333335 0.7153333333333335
probs:  [0.07179376468908465, 0.324125078759383, 0.24583610879100642, 0.35824504776052596]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5866666666666671  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.005166666666666673 0.7153333333333335 0.7153333333333335
probs:  [0.07179376468908465, 0.324125078759383, 0.24583610879100642, 0.35824504776052596]
maxi score, test score, baseline:  -0.005166666666666673 0.7153333333333335 0.7153333333333335
probs:  [0.07179376468908465, 0.324125078759383, 0.24583610879100642, 0.35824504776052596]
maxi score, test score, baseline:  -0.005166666666666673 0.7153333333333335 0.7153333333333335
probs:  [0.07179376468908465, 0.324125078759383, 0.24583610879100642, 0.35824504776052596]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  45 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.7733333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.005833333333333337 0.7153333333333335 0.7153333333333335
probs:  [0.07138441499529251, 0.3279842387188198, 0.244432337724609, 0.3561990085612786]
maxi score, test score, baseline:  -0.005833333333333337 0.7153333333333335 0.7153333333333335
probs:  [0.07138441499529251, 0.3279842387188198, 0.244432337724609, 0.3561990085612786]
actor:  1 policy actor:  1  step number:  43 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  38 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.559]
 [0.494]
 [0.436]
 [0.491]
 [0.442]
 [0.534]] [[0.456]
 [0.91 ]
 [0.977]
 [0.744]
 [0.454]
 [1.919]
 [0.552]] [[-0.201]
 [ 0.015]
 [-0.028]
 [-0.164]
 [-0.206]
 [ 0.233]
 [-0.13 ]]
maxi score, test score, baseline:  -0.005833333333333337 0.7153333333333335 0.7153333333333335
probs:  [0.07145810692404615, 0.32728950523033173, 0.24468504732468976, 0.35656734052093236]
actor:  1 policy actor:  1  step number:  45 total reward:  0.586666666666667  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  42 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.005833333333333337 0.7153333333333335 0.7153333333333335
probs:  [0.07145810692404615, 0.32728950523033173, 0.24468504732468976, 0.35656734052093236]
actor:  1 policy actor:  1  step number:  38 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.78  reward:  1.0 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  -0.009220000000000008 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  -0.009220000000000008 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  -0.009220000000000008 0.7153333333333335 0.7153333333333335
actor:  1 policy actor:  1  step number:  34 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.113]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.052]] [[2.36 ]
 [1.615]
 [2.36 ]
 [2.36 ]
 [2.36 ]
 [2.36 ]
 [1.302]] [[-0.032]
 [-0.331]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.549]]
maxi score, test score, baseline:  -0.009220000000000008 0.7153333333333335 0.7153333333333335
probs:  [0.07096277422291841, 0.32482294207824086, 0.25012252481113106, 0.3540917588877097]
maxi score, test score, baseline:  -0.009220000000000008 0.7153333333333335 0.7153333333333335
probs:  [0.07096277422291841, 0.32482294207824086, 0.25012252481113106, 0.3540917588877097]
maxi score, test score, baseline:  -0.009220000000000008 0.7153333333333335 0.7153333333333335
probs:  [0.07096277422291841, 0.32482294207824086, 0.25012252481113106, 0.3540917588877097]
maxi score, test score, baseline:  -0.009220000000000008 0.7153333333333335 0.7153333333333335
probs:  [0.07096277422291841, 0.32482294207824086, 0.25012252481113106, 0.3540917588877097]
maxi score, test score, baseline:  -0.009220000000000008 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  -0.009220000000000008 0.7153333333333335 0.7153333333333335
probs:  [0.07096277422291841, 0.32482294207824086, 0.25012252481113106, 0.3540917588877097]
Printing some Q and Qe and total Qs values:  [[ 0.099]
 [ 0.112]
 [ 0.05 ]
 [-0.006]
 [ 0.06 ]
 [ 0.06 ]
 [ 0.058]] [[ 0.279]
 [ 0.301]
 [ 0.313]
 [ 0.199]
 [-0.071]
 [ 0.218]
 [ 0.351]] [[-0.419]
 [-0.401]
 [-0.462]
 [-0.537]
 [-0.516]
 [-0.467]
 [-0.447]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.8000000000000002  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  43 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.07011658890686877, 0.32094322960734323, 0.25907788568693185, 0.3498622957988562]
maxi score, test score, baseline:  -0.01260666666666667 0.7153333333333335 0.7153333333333335
probs:  [0.07011658890686877, 0.32094322960734323, 0.25907788568693185, 0.3498622957988562]
siam score:  -0.7093409
Printing some Q and Qe and total Qs values:  [[-0.033]
 [ 0.116]
 [-0.018]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]] [[1.464]
 [1.299]
 [1.581]
 [1.464]
 [1.464]
 [1.464]
 [1.464]] [[-0.412]
 [-0.317]
 [-0.358]
 [-0.412]
 [-0.412]
 [-0.412]
 [-0.412]]
maxi score, test score, baseline:  -0.019433333333333334 0.7153333333333335 0.7153333333333335
probs:  [0.07011658890686877, 0.32094322960734323, 0.25907788568693185, 0.3498622957988562]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.019433333333333334 0.7153333333333335 0.7153333333333335
probs:  [0.07011658890686877, 0.32094322960734323, 0.25907788568693185, 0.3498622957988562]
actor:  1 policy actor:  1  step number:  44 total reward:  0.7000000000000003  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.07011658890686877, 0.32094322960734323, 0.25907788568693185, 0.3498622957988562]
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.07011658890686877, 0.32094322960734323, 0.25907788568693185, 0.3498622957988562]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.297]
 [0.373]
 [0.297]
 [0.297]
 [0.377]
 [0.349]] [[1.229]
 [1.229]
 [1.494]
 [1.229]
 [1.229]
 [1.509]
 [1.304]] [[-0.079]
 [-0.079]
 [ 0.116]
 [-0.079]
 [-0.079]
 [ 0.127]
 [ 0.005]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8199068829715543
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.07011658890686877, 0.32094322960734323, 0.25907788568693185, 0.3498622957988562]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  0.6533333333333337  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.07011658890686877, 0.32094322960734323, 0.25907788568693185, 0.3498622957988562]
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.07011658890686877, 0.32094322960734323, 0.25907788568693185, 0.3498622957988562]
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]] [[0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]] [[-0.515]
 [-0.515]
 [-0.515]
 [-0.515]
 [-0.515]
 [-0.515]
 [-0.515]]
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.07011658890686877, 0.32094322960734323, 0.25907788568693185, 0.3498622957988562]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.]
 [ 0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [ 0.]] [[-0.436]
 [-0.437]
 [-0.437]
 [-0.436]
 [-0.436]
 [-0.437]
 [-0.436]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.07011658890686877, 0.32094322960734323, 0.2590778856869318, 0.34986229579885614]
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.07011658890686877, 0.32094322960734323, 0.2590778856869318, 0.34986229579885614]
actor:  1 policy actor:  1  step number:  47 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.07 ]
 [-0.109]
 [-0.027]
 [-0.003]
 [-0.003]
 [ 0.017]] [[1.402]
 [1.258]
 [1.056]
 [1.13 ]
 [1.402]
 [1.402]
 [1.257]] [[-0.093]
 [-0.115]
 [-0.414]
 [-0.289]
 [-0.093]
 [-0.093]
 [-0.166]]
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.06815692353961524, 0.3279334591314521, 0.26386066099501565, 0.340048956333917]
siam score:  -0.6967242
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
actor:  1 policy actor:  1  step number:  38 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.5
siam score:  -0.6983395
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.06815692353961524, 0.3279334591314521, 0.26386066099501565, 0.340048956333917]
Printing some Q and Qe and total Qs values:  [[-0.034]
 [-0.027]
 [-0.036]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.035]] [[-0.171]
 [ 0.233]
 [ 0.067]
 [-0.421]
 [-0.546]
 [-0.444]
 [ 0.39 ]] [[0.311]
 [0.447]
 [0.388]
 [0.228]
 [0.187]
 [0.221]
 [0.495]]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]] [[0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]] [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]]
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.06815692353961524, 0.3279334591314521, 0.26386066099501565, 0.340048956333917]
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.06815692353961524, 0.3279334591314521, 0.26386066099501565, 0.340048956333917]
line 256 mcts: sample exp_bonus 0.016334809905290605
actor:  1 policy actor:  1  step number:  30 total reward:  0.7266666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.06814184610715743, 0.3280823593195643, 0.26380219524520665, 0.33997359932807164]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.337]
 [0.337]
 [0.337]
 [0.199]
 [0.337]
 [0.488]] [[1.714]
 [1.635]
 [1.635]
 [1.635]
 [0.968]
 [1.635]
 [1.202]] [[ 0.137]
 [ 0.101]
 [ 0.101]
 [ 0.101]
 [-0.371]
 [ 0.101]
 [ 0.036]]
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.06814184610715743, 0.3280823593195643, 0.26380219524520665, 0.33997359932807164]
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.06814184610715743, 0.3280823593195643, 0.26380219524520665, 0.33997359932807164]
actor:  1 policy actor:  1  step number:  76 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  39 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.06813929038870517, 0.32810759882631496, 0.2637922849374649, 0.3399608258475149]
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
actor:  1 policy actor:  1  step number:  54 total reward:  0.5000000000000004  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.06813929038870517, 0.32810759882631496, 0.2637922849374649, 0.3399608258475149]
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]]
using explorer policy with actor:  1
from probs:  [0.06813929038870517, 0.32810759882631496, 0.2637922849374649, 0.3399608258475149]
maxi score, test score, baseline:  -0.022793333333333346 0.7153333333333335 0.7153333333333335
probs:  [0.06813929038870518, 0.32810759882631485, 0.2637922849374649, 0.33996082584751497]
actor:  1 policy actor:  1  step number:  26 total reward:  0.8333333333333334  reward:  1.0 rdn_beta:  0.5
from probs:  [0.06813929038870518, 0.32810759882631485, 0.2637922849374649, 0.33996082584751497]
maxi score, test score, baseline:  -0.02615333333333333 0.7153333333333335 0.7153333333333335
probs:  [0.06654365658031897, 0.32041080304219455, 0.28105969213719983, 0.33198584824028654]
maxi score, test score, baseline:  -0.02615333333333333 0.7153333333333335 0.7153333333333335
probs:  [0.06654365658031897, 0.32041080304219455, 0.28105969213719983, 0.33198584824028654]
maxi score, test score, baseline:  -0.02615333333333333 0.7153333333333335 0.7153333333333335
probs:  [0.06654365658031897, 0.32041080304219455, 0.28105969213719983, 0.33198584824028654]
maxi score, test score, baseline:  -0.02615333333333333 0.7153333333333335 0.7153333333333335
probs:  [0.06676767580114963, 0.31811931774204527, 0.2820075101742649, 0.33310549628254016]
maxi score, test score, baseline:  -0.02615333333333333 0.7153333333333335 0.7153333333333335
probs:  [0.06676767580114963, 0.3181193177420454, 0.2820075101742649, 0.33310549628254016]
actor:  0 policy actor:  1  step number:  40 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.029553333333333338 0.7153333333333335 0.7153333333333335
probs:  [0.06676767580114963, 0.3181193177420454, 0.2820075101742649, 0.33310549628254016]
using explorer policy with actor:  1
first move QE:  -0.07389675548583988
actor:  1 policy actor:  1  step number:  32 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.202]
 [0.16 ]
 [0.136]
 [0.071]
 [0.141]
 [0.141]] [[2.749]
 [1.833]
 [1.642]
 [1.55 ]
 [0.97 ]
 [1.422]
 [1.855]] [[ 0.435]
 [ 0.088]
 [-0.02 ]
 [-0.076]
 [-0.356]
 [-0.123]
 [ 0.049]]
maxi score, test score, baseline:  -0.029553333333333338 0.7153333333333335 0.7153333333333335
probs:  [0.0668455824355235, 0.3182210569146571, 0.28145434313597634, 0.333479017513843]
maxi score, test score, baseline:  -0.029553333333333338 0.7153333333333335 0.7153333333333335
probs:  [0.0668455824355235, 0.3182210569146571, 0.28145434313597634, 0.333479017513843]
maxi score, test score, baseline:  -0.029553333333333338 0.7153333333333335 0.7153333333333335
probs:  [0.0668455824355235, 0.3182210569146571, 0.28145434313597634, 0.333479017513843]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.029553333333333338 0.7153333333333335 0.7153333333333335
probs:  [0.06673302507177517, 0.31768427171560787, 0.28266622063179975, 0.3329164825808172]
actor:  1 policy actor:  1  step number:  30 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.032780000000000004 0.7153333333333335 0.7153333333333335
probs:  [0.06702095169352121, 0.3167329185274288, 0.2818877980995608, 0.3343583316794892]
first move QE:  -0.07119828860322372
actor:  1 policy actor:  1  step number:  29 total reward:  0.8000000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.032780000000000004 0.7153333333333335 0.7153333333333335
probs:  [0.06840921258327544, 0.31214589418253547, 0.2781345725358474, 0.34131032069834166]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.032780000000000004 0.7153333333333335 0.7153333333333335
probs:  [0.0681102279070959, 0.3151568152884396, 0.2769169633424434, 0.33981599346202107]
Starting evaluation
siam score:  -0.70545995
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.84 ]
 [0.84 ]
 [0.915]
 [0.83 ]
 [0.84 ]
 [0.84 ]
 [0.84 ]] [[1.75 ]
 [1.75 ]
 [1.203]
 [2.216]
 [1.75 ]
 [1.75 ]
 [1.75 ]] [[0.84 ]
 [0.84 ]
 [0.915]
 [0.83 ]
 [0.84 ]
 [0.84 ]
 [0.84 ]]
Printing some Q and Qe and total Qs values:  [[-0.007]
 [ 0.113]
 [ 0.864]
 [ 0.673]
 [ 0.779]
 [ 0.046]
 [ 0.779]] [[1.685]
 [1.699]
 [1.207]
 [2.634]
 [0.784]
 [1.51 ]
 [0.784]] [[-0.007]
 [ 0.113]
 [ 0.864]
 [ 0.673]
 [ 0.779]
 [ 0.046]
 [ 0.779]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.032780000000000004 0.7153333333333335 0.7153333333333335
probs:  [0.0681102279070959, 0.3151568152884396, 0.2769169633424434, 0.33981599346202107]
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.933]
 [0.801]
 [0.794]
 [0.863]
 [0.863]
 [0.83 ]] [[0.899]
 [0.196]
 [2.012]
 [1.282]
 [0.899]
 [0.899]
 [1.328]] [[0.863]
 [0.933]
 [0.801]
 [0.794]
 [0.863]
 [0.863]
 [0.83 ]]
line 256 mcts: sample exp_bonus 0.34762805457446344
maxi score, test score, baseline:  -0.032780000000000004 0.7153333333333335 0.7153333333333335
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  34 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.032780000000000004 0.7153333333333335 0.7153333333333335
probs:  [0.06813451850352364, 0.31507569225941406, 0.27685215707971345, 0.33993763215734885]
start point for exploration sampling:  11715
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  22 total reward:  0.8466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.01451333333333333 0.7153333333333335 0.7153333333333335
probs:  [0.06813451850352364, 0.31507569225941406, 0.27685215707971345, 0.33993763215734885]
actor:  0 policy actor:  0  step number:  26 total reward:  0.78  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.018073333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.07239265274349929, 0.2751940046427591, 0.2919129812220158, 0.36050036139172575]
start point for exploration sampling:  11715
actor:  0 policy actor:  0  step number:  31 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.021433333333333335 0.7576666666666666 0.7576666666666666
probs:  [0.07239265274349929, 0.2751940046427591, 0.2919129812220158, 0.36050036139172575]
actor:  1 policy actor:  1  step number:  40 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.021433333333333335 0.7576666666666666 0.7576666666666666
probs:  [0.07239265274349929, 0.2751940046427591, 0.2919129812220158, 0.36050036139172575]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.021433333333333335 0.7576666666666666 0.7576666666666666
line 256 mcts: sample exp_bonus 1.3402152995881376
Printing some Q and Qe and total Qs values:  [[0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]] [[0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]] [[0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.8466666666666667  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.258]
 [0.165]
 [0.165]
 [0.164]
 [0.161]
 [0.162]] [[1.721]
 [1.32 ]
 [1.279]
 [1.279]
 [1.495]
 [1.603]
 [1.521]] [[-0.023]
 [-0.104]
 [-0.217]
 [-0.217]
 [-0.11 ]
 [-0.059]
 [-0.099]]
maxi score, test score, baseline:  0.021433333333333335 0.7576666666666666 0.7576666666666666
probs:  [0.06815467935373036, 0.31768688991156663, 0.2747972947780317, 0.33936113595667133]
maxi score, test score, baseline:  0.021433333333333335 0.7576666666666666 0.7576666666666666
probs:  [0.06815467935373036, 0.31768688991156663, 0.2747972947780316, 0.33936113595667133]
line 256 mcts: sample exp_bonus 1.318973207287429
actor:  1 policy actor:  1  step number:  49 total reward:  0.626666666666667  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.814]
 [0.658]
 [0.672]
 [0.143]
 [0.558]
 [0.098]] [[0.105]
 [0.08 ]
 [1.165]
 [1.055]
 [0.379]
 [0.559]
 [0.627]] [[0.762]
 [0.814]
 [0.658]
 [0.672]
 [0.143]
 [0.558]
 [0.098]]
maxi score, test score, baseline:  0.021433333333333335 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.021433333333333335 0.7576666666666666 0.7576666666666666
probs:  [0.06815467935373036, 0.31768688991156663, 0.2747972947780317, 0.33936113595667133]
using another actor
first move QE:  -0.07029762548460967
maxi score, test score, baseline:  0.021433333333333332 0.7576666666666666 0.7576666666666666
probs:  [0.06815467935373036, 0.31768688991156663, 0.2747972947780317, 0.33936113595667133]
actor:  1 policy actor:  1  step number:  30 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.021433333333333332 0.7576666666666666 0.7576666666666666
probs:  [0.06790439762410927, 0.31651824092774183, 0.27746464340686794, 0.33811271804128096]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]] [[1.92]
 [1.92]
 [1.92]
 [1.92]
 [1.92]
 [1.92]
 [1.92]] [[0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]]
actor:  0 policy actor:  1  step number:  33 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.021686666666666663 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.021686666666666663 0.7576666666666666 0.7576666666666666
actor:  0 policy actor:  1  step number:  37 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
siam score:  -0.7020872
actor:  1 policy actor:  1  step number:  73 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  41 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.022233333333333324 0.7576666666666666 0.7576666666666666
probs:  [0.06790439762410927, 0.31651824092774183, 0.27746464340686794, 0.33811271804128096]
maxi score, test score, baseline:  0.022233333333333324 0.7576666666666666 0.7576666666666666
probs:  [0.06790439762410927, 0.31651824092774183, 0.27746464340686794, 0.33811271804128096]
maxi score, test score, baseline:  0.022233333333333324 0.7576666666666666 0.7576666666666666
probs:  [0.06790439762410927, 0.31651824092774183, 0.27746464340686794, 0.33811271804128096]
maxi score, test score, baseline:  0.022233333333333324 0.7576666666666666 0.7576666666666666
probs:  [0.06790439762410927, 0.31651824092774183, 0.27746464340686794, 0.33811271804128096]
actor:  1 policy actor:  1  step number:  37 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.022233333333333324 0.7576666666666666 0.7576666666666666
probs:  [0.06790439762410927, 0.31651824092774183, 0.27746464340686794, 0.33811271804128096]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.745]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]] [[1.975]
 [0.803]
 [1.925]
 [1.925]
 [1.925]
 [1.925]
 [1.925]] [[0.689]
 [0.357]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
probs:  [0.06790439762410927, 0.31651824092774183, 0.27746464340686805, 0.3381127180412809]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.025]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]] [[-0.709]
 [ 0.963]
 [-0.709]
 [-0.709]
 [-0.709]
 [-0.709]
 [-0.709]] [[-0.   ]
 [ 0.322]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]]
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
probs:  [0.06790439762410927, 0.31651824092774183, 0.27746464340686805, 0.3381127180412809]
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
probs:  [0.06790439762410927, 0.31651824092774183, 0.27746464340686805, 0.3381127180412809]
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
actor:  1 policy actor:  1  step number:  27 total reward:  0.8266666666666668  reward:  1.0 rdn_beta:  0.667
siam score:  -0.70469743
actor:  1 policy actor:  1  step number:  29 total reward:  0.8133333333333335  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[ 0.106]
 [ 0.221]
 [-0.002]
 [-0.006]
 [-0.006]
 [-0.006]
 [ 0.029]] [[3.628]
 [1.771]
 [1.778]
 [2.343]
 [2.343]
 [2.343]
 [1.9  ]] [[0.591]
 [0.291]
 [0.16 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.203]]
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
probs:  [0.07055803130907982, 0.2935889193914982, 0.28438806224317426, 0.3514649870562477]
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
probs:  [0.07055803130907982, 0.2935889193914982, 0.28438806224317426, 0.3514649870562477]
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
probs:  [0.07055803130907982, 0.2935889193914982, 0.28438806224317426, 0.3514649870562477]
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
probs:  [0.07055803130907982, 0.2935889193914982, 0.28438806224317426, 0.3514649870562477]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
probs:  [0.07055803130907982, 0.2935889193914982, 0.28438806224317426, 0.3514649870562477]
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
probs:  [0.07055803130907982, 0.2935889193914982, 0.28438806224317426, 0.3514649870562477]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
probs:  [0.0705554290305002, 0.293615011667042, 0.2843775568236038, 0.35145200247885394]
actor:  1 policy actor:  1  step number:  59 total reward:  0.4800000000000003  reward:  1.0 rdn_beta:  0.333
siam score:  -0.70988566
line 256 mcts: sample exp_bonus 1.8074567640548846
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]] [[7.743]
 [3.59 ]
 [3.59 ]
 [3.59 ]
 [3.59 ]
 [3.59 ]
 [3.59 ]] [[0.849]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]]
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]] [[8.179]
 [3.4  ]
 [3.4  ]
 [3.4  ]
 [3.4  ]
 [3.4  ]
 [3.4  ]] [[0.833]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]]
actor:  1 policy actor:  1  step number:  76 total reward:  0.4733333333333338  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
probs:  [0.0705554290305002, 0.293615011667042, 0.2843775568236038, 0.35145200247885394]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
probs:  [0.0705554290305002, 0.293615011667042, 0.2843775568236038, 0.35145200247885394]
using explorer policy with actor:  1
siam score:  -0.7120805
maxi score, test score, baseline:  0.022233333333333327 0.7576666666666666 0.7576666666666666
probs:  [0.0705554290305002, 0.293615011667042, 0.2843775568236038, 0.35145200247885394]
maxi score, test score, baseline:  0.022233333333333324 0.7576666666666666 0.7576666666666666
probs:  [0.0705554290305002, 0.293615011667042, 0.2843775568236038, 0.35145200247885394]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.022233333333333324 0.7576666666666666 0.7576666666666666
probs:  [0.06816330993282341, 0.3010166685436731, 0.2913736289580773, 0.33944639256542625]
from probs:  [0.06816330993282341, 0.3010166685436731, 0.2913736289580773, 0.33944639256542625]
actor:  1 policy actor:  1  step number:  46 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.022233333333333324 0.7576666666666666 0.7576666666666666
start point for exploration sampling:  11715
actor:  1 policy actor:  1  step number:  50 total reward:  0.5933333333333336  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  42 total reward:  0.6466666666666671  reward:  1.0 rdn_beta:  0.333
from probs:  [0.06816330993282341, 0.3010166685436731, 0.2913736289580773, 0.33944639256542625]
maxi score, test score, baseline:  0.02007333333333332 0.7576666666666666 0.7576666666666666
probs:  [0.0671193630726733, 0.304246832319286, 0.2944267911293339, 0.3342070134787068]
Printing some Q and Qe and total Qs values:  [[-0.011]
 [ 0.015]
 [-0.016]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.012]] [[0.928]
 [0.912]
 [0.463]
 [0.573]
 [0.928]
 [0.928]
 [0.603]] [[-0.598]
 [-0.58 ]
 [-0.835]
 [-0.775]
 [-0.598]
 [-0.598]
 [-0.761]]
maxi score, test score, baseline:  0.016659999999999994 0.7576666666666666 0.7576666666666666
probs:  [0.0671193630726733, 0.304246832319286, 0.2944267911293339, 0.3342070134787068]
maxi score, test score, baseline:  0.016659999999999994 0.7576666666666666 0.7576666666666666
probs:  [0.0671193630726733, 0.304246832319286, 0.2944267911293339, 0.3342070134787068]
actor:  0 policy actor:  0  step number:  37 total reward:  0.6000000000000003  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.019859999999999996 0.7576666666666666 0.7576666666666666
from probs:  [0.0671193630726733, 0.304246832319286, 0.2944267911293339, 0.3342070134787068]
maxi score, test score, baseline:  0.019859999999999996 0.7576666666666666 0.7576666666666666
probs:  [0.0671193630726733, 0.304246832319286, 0.2944267911293339, 0.3342070134787068]
line 256 mcts: sample exp_bonus 0.7641912517872164
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5447279947842123
maxi score, test score, baseline:  0.019859999999999996 0.7576666666666666 0.7576666666666666
probs:  [0.0671193630726733, 0.304246832319286, 0.2944267911293339, 0.3342070134787068]
first move QE:  -0.06732649312566655
using explorer policy with actor:  0
maxi score, test score, baseline:  0.019859999999999996 0.7576666666666666 0.7576666666666666
probs:  [0.0671193630726733, 0.304246832319286, 0.2944267911293339, 0.3342070134787068]
siam score:  -0.70913506
maxi score, test score, baseline:  0.019859999999999996 0.7576666666666666 0.7576666666666666
probs:  [0.0671193630726733, 0.304246832319286, 0.2944267911293339, 0.3342070134787068]
actor:  0 policy actor:  1  step number:  56 total reward:  0.2866666666666656  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.01987333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.0671193630726733, 0.304246832319286, 0.2944267911293339, 0.3342070134787068]
maxi score, test score, baseline:  0.01987333333333334 0.7576666666666666 0.7576666666666666
actor:  1 policy actor:  1  step number:  113 total reward:  0.22666666666666557  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.01987333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.06665206248415782, 0.3317571977240725, 0.3207785314565373, 0.2808122083352324]
maxi score, test score, baseline:  0.01987333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.06665206248415782, 0.3317571977240725, 0.3207785314565373, 0.2808122083352324]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.233]
 [0.208]
 [0.208]
 [0.208]
 [0.208]
 [0.261]] [[1.419]
 [1.497]
 [1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.446]] [[-0.238]
 [-0.198]
 [-0.188]
 [-0.188]
 [-0.188]
 [-0.188]
 [-0.187]]
maxi score, test score, baseline:  0.01987333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.04582198687592612
from probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
maxi score, test score, baseline:  0.01987333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
maxi score, test score, baseline:  0.01987333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
siam score:  -0.70987946
maxi score, test score, baseline:  0.01987333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
maxi score, test score, baseline:  0.01987333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.715]
 [0.667]
 [0.664]
 [0.362]
 [0.635]
 [0.765]] [[ 0.104]
 [ 0.32 ]
 [ 1.055]
 [ 0.738]
 [-0.056]
 [ 0.977]
 [ 0.292]] [[0.681]
 [0.715]
 [0.667]
 [0.664]
 [0.362]
 [0.635]
 [0.765]]
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.737]
 [0.752]
 [0.729]
 [0.692]
 [0.681]
 [0.737]] [[0.093]
 [0.614]
 [0.419]
 [0.735]
 [0.603]
 [0.707]
 [0.999]] [[0.704]
 [0.737]
 [0.752]
 [0.729]
 [0.692]
 [0.681]
 [0.737]]
maxi score, test score, baseline:  0.01987333333333334 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.01987333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
actor:  0 policy actor:  0  step number:  40 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.020379999999999995 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
maxi score, test score, baseline:  0.020379999999999995 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
maxi score, test score, baseline:  0.020379999999999995 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
maxi score, test score, baseline:  0.020379999999999995 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
actor:  1 policy actor:  1  step number:  46 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.020379999999999995 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
maxi score, test score, baseline:  0.01762 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
actor:  1 policy actor:  1  step number:  67 total reward:  0.4800000000000004  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.01762 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.013298300297377863
maxi score, test score, baseline:  0.01762 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
maxi score, test score, baseline:  0.01762 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
maxi score, test score, baseline:  0.01762 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
maxi score, test score, baseline:  0.01762 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
actor:  0 policy actor:  0  step number:  40 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.020726666666666668 0.7576666666666666 0.7576666666666666
actor:  1 policy actor:  1  step number:  45 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.020726666666666668 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
maxi score, test score, baseline:  0.020726666666666668 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
using explorer policy with actor:  0
siam score:  -0.71122026
maxi score, test score, baseline:  0.020726666666666668 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.020726666666666668 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]] [[0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]]
actor:  0 policy actor:  0  step number:  34 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  56 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.021]
 [-0.024]
 [-0.023]
 [-0.029]
 [-0.03 ]
 [-0.022]] [[-0.475]
 [ 0.066]
 [-0.401]
 [-0.474]
 [-0.596]
 [-0.574]
 [-0.359]] [[-0.663]
 [-0.572]
 [-0.652]
 [-0.664]
 [-0.69 ]
 [-0.687]
 [-0.644]]
maxi score, test score, baseline:  0.021326666666666678 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
actor:  0 policy actor:  1  step number:  44 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.021326666666666667 0.7576666666666666 0.7576666666666666
actor:  0 policy actor:  0  step number:  32 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.024646666666666674 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
maxi score, test score, baseline:  0.024646666666666674 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
actor:  1 policy actor:  1  step number:  43 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.024646666666666674 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
using explorer policy with actor:  1
siam score:  -0.70672834
maxi score, test score, baseline:  0.024646666666666674 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
actor:  0 policy actor:  1  step number:  33 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
first move QE:  -0.06797836669203101
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.202]
 [0.155]
 [0.173]
 [0.249]
 [0.249]
 [0.152]] [[1.025]
 [1.831]
 [1.247]
 [1.294]
 [1.552]
 [1.552]
 [1.075]] [[-0.314]
 [-0.013]
 [-0.255]
 [-0.221]
 [-0.06 ]
 [-0.06 ]
 [-0.315]]
maxi score, test score, baseline:  0.025793333333333338 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.025793333333333338 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.632]
 [0.629]
 [0.631]
 [0.549]
 [0.562]
 [0.631]] [[0.542]
 [0.713]
 [1.573]
 [1.254]
 [0.5  ]
 [1.126]
 [1.854]] [[0.398]
 [0.458]
 [0.733]
 [0.631]
 [0.309]
 [0.524]
 [0.824]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.626666666666667  reward:  1.0 rdn_beta:  0.333
from probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
siam score:  -0.71186477
actor:  1 policy actor:  1  step number:  52 total reward:  0.5400000000000005  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.025793333333333338 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
actor:  0 policy actor:  0  step number:  61 total reward:  0.1599999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.02504666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
maxi score, test score, baseline:  0.02504666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.585]
 [0.625]
 [0.596]
 [0.578]
 [0.599]
 [0.599]] [[1.206]
 [3.185]
 [3.025]
 [3.63 ]
 [1.505]
 [2.326]
 [3.093]] [[0.608]
 [0.585]
 [0.625]
 [0.596]
 [0.578]
 [0.599]
 [0.599]]
maxi score, test score, baseline:  0.02504666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
actor:  0 policy actor:  0  step number:  47 total reward:  0.6400000000000003  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.025540000000000007 0.7576666666666666 0.7576666666666666
probs:  [0.06796576834726706, 0.33830787950116636, 0.32711233697146913, 0.2666140151800974]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.306]
 [0.458]
 [0.507]
 [0.456]
 [0.428]
 [0.521]] [[3.759]
 [3.759]
 [3.449]
 [2.754]
 [2.446]
 [3.746]
 [2.831]] [[0.302]
 [0.302]
 [0.349]
 [0.168]
 [0.014]
 [0.418]
 [0.207]]
actor:  0 policy actor:  1  step number:  29 total reward:  0.8133333333333335  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.028]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[1.629]
 [2.158]
 [1.629]
 [1.629]
 [1.629]
 [1.629]
 [1.629]] [[0.587]
 [0.803]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]]
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.133]
 [0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.141]] [[1.59 ]
 [3.512]
 [1.59 ]
 [1.59 ]
 [1.59 ]
 [1.59 ]
 [2.947]] [[0.34 ]
 [0.86 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.721]]
maxi score, test score, baseline:  0.023726666666666674 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.023726666666666674 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
actor:  1 policy actor:  1  step number:  30 total reward:  0.7133333333333335  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.15 ]] [[2.277]
 [2.277]
 [2.277]
 [2.277]
 [2.277]
 [2.277]
 [3.114]] [[0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.493]]
using explorer policy with actor:  1
siam score:  -0.70474654
maxi score, test score, baseline:  0.021260000000000008 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.021260000000000008 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
actor:  0 policy actor:  0  step number:  34 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.02134 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.02134 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
actor:  1 policy actor:  1  step number:  55 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.398]
 [ 0.399]
 [-0.076]
 [ 0.255]
 [ 0.418]
 [-0.082]
 [ 0.174]] [[3.728]
 [2.706]
 [2.101]
 [2.741]
 [2.649]
 [2.119]
 [2.493]] [[ 0.715]
 [ 0.46 ]
 [-0.05 ]
 [ 0.36 ]
 [ 0.46 ]
 [-0.05 ]
 [ 0.237]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.02134 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
from probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.207]] [[1.946]
 [1.685]
 [1.685]
 [1.685]
 [1.685]
 [1.685]
 [2.454]] [[0.357]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.539]]
actor:  0 policy actor:  1  step number:  36 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.02484666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.02484666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.02484666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
using another actor
actor:  1 policy actor:  1  step number:  73 total reward:  0.22666666666666657  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7273093
from probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.893]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.788]] [[0.741]
 [0.713]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.504]] [[0.813]
 [0.893]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.788]]
maxi score, test score, baseline:  0.024846666666666673 0.7576666666666666 0.7576666666666666
actor:  0 policy actor:  1  step number:  38 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  50 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  46 total reward:  0.7000000000000003  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
line 256 mcts: sample exp_bonus 4.058277353775954
line 256 mcts: sample exp_bonus 10.0
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[8.5  ]
 [2.427]
 [2.427]
 [2.427]
 [2.427]
 [2.427]
 [2.427]] [[0.797]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.442]
 [0.385]
 [0.381]
 [0.379]
 [0.381]
 [0.386]] [[5.194]
 [1.936]
 [2.537]
 [2.569]
 [2.549]
 [2.586]
 [2.545]] [[ 0.732]
 [-0.038]
 [ 0.084]
 [ 0.09 ]
 [ 0.084]
 [ 0.095]
 [ 0.087]]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.441]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[4.216]
 [1.988]
 [2.925]
 [2.925]
 [2.925]
 [2.925]
 [2.925]] [[0.483]
 [0.002]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]] [[1.846]
 [1.846]
 [1.846]
 [1.846]
 [1.846]
 [1.846]
 [1.846]] [[0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.6266666666666671  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
actor:  1 policy actor:  1  step number:  115 total reward:  0.14666666666666572  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.028766666666666666 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
actor:  0 policy actor:  1  step number:  38 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.029420000000000005 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
actor:  0 policy actor:  1  step number:  55 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.026]
 [ 0.045]
 [ 0.031]
 [ 0.001]
 [-0.15 ]
 [ 0.001]
 [-0.043]] [[1.382]
 [1.139]
 [1.058]
 [0.904]
 [0.032]
 [0.857]
 [1.257]] [[ 0.34 ]
 [ 0.309]
 [ 0.276]
 [ 0.21 ]
 [-0.149]
 [ 0.196]
 [ 0.292]]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.457]
 [0.266]
 [0.313]
 [0.266]
 [0.266]
 [0.437]] [[1.828]
 [1.792]
 [1.828]
 [1.327]
 [1.828]
 [1.828]
 [1.589]] [[0.112]
 [0.297]
 [0.112]
 [0.075]
 [0.112]
 [0.112]
 [0.243]]
maxi score, test score, baseline:  0.032006666666666676 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.164]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]] [[1.725]
 [1.483]
 [1.725]
 [1.725]
 [1.725]
 [1.725]
 [1.725]] [[0.357]
 [0.276]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]]
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.131]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[1.408]
 [1.538]
 [1.408]
 [1.408]
 [1.408]
 [1.408]
 [1.408]] [[-0.384]
 [-0.326]
 [-0.384]
 [-0.384]
 [-0.384]
 [-0.384]
 [-0.384]]
maxi score, test score, baseline:  0.032006666666666676 0.7576666666666666 0.7576666666666666
using explorer policy with actor:  1
maxi score, test score, baseline:  0.032006666666666676 0.7576666666666666 0.7576666666666666
actor:  1 policy actor:  1  step number:  55 total reward:  0.5733333333333339  reward:  1.0 rdn_beta:  0.333
from probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.032006666666666676 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3818916255992337
maxi score, test score, baseline:  0.032006666666666676 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.032006666666666676 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  42 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03482000000000001 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
maxi score, test score, baseline:  0.03482000000000001 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]] [[1.03 ]
 [1.167]
 [1.167]
 [1.167]
 [1.167]
 [1.167]
 [1.167]] [[-0.042]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.141]]
actor:  0 policy actor:  0  step number:  51 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.5
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.407870910797559
maxi score, test score, baseline:  0.0349 0.7576666666666666 0.7576666666666666
probs:  [0.06824521217604515, 0.33970130222104794, 0.3284596270411348, 0.26359385856177214]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]] [[0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]] [[-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.487]
 [0.314]
 [0.353]
 [0.41 ]
 [0.352]
 [0.41 ]] [[0.306]
 [0.529]
 [0.865]
 [0.696]
 [0.306]
 [0.691]
 [0.306]] [[-0.229]
 [-0.078]
 [-0.139]
 [-0.156]
 [-0.229]
 [-0.158]
 [-0.229]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.5866666666666671  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  67 total reward:  0.5466666666666672  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0349 0.7576666666666666 0.7576666666666666
probs:  [0.06824827517198225, 0.33971657558796503, 0.3284743947460704, 0.26356075449398225]
maxi score, test score, baseline:  0.0349 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.0349 0.7576666666666666 0.7576666666666666
probs:  [0.06824827517198225, 0.33971657558796503, 0.3284743947460704, 0.26356075449398225]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.8066666666666668  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  45 total reward:  0.52  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.03562 0.7576666666666666 0.7576666666666666
probs:  [0.06845155006939349, 0.33241813623097805, 0.3407635200368507, 0.25836679366277776]
siam score:  -0.72590315
maxi score, test score, baseline:  0.03562 0.7576666666666666 0.7576666666666666
probs:  [0.06845155006939349, 0.33241813623097805, 0.3407635200368507, 0.25836679366277776]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03562 0.7576666666666666 0.7576666666666666
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[-0.001]
 [ 0.55 ]
 [ 0.55 ]
 [ 0.55 ]
 [ 0.55 ]
 [ 0.55 ]
 [ 0.55 ]] [[-0.403]
 [ 0.526]
 [ 0.526]
 [ 0.526]
 [ 0.526]
 [ 0.526]
 [ 0.526]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.6066666666666669  reward:  1.0 rdn_beta:  0.333
from probs:  [0.06845155006939349, 0.33241813623097805, 0.3407635200368507, 0.25836679366277776]
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.682]
 [0.631]
 [0.674]
 [0.667]
 [0.65 ]
 [0.65 ]] [[ 0.035]
 [-0.01 ]
 [ 0.081]
 [ 0.003]
 [ 0.   ]
 [ 0.028]
 [ 0.558]] [[0.254]
 [0.263]
 [0.258]
 [0.262]
 [0.254]
 [0.251]
 [0.503]]
maxi score, test score, baseline:  0.03562 0.7576666666666666 0.7576666666666666
probs:  [0.06845155006939349, 0.33241813623097805, 0.3407635200368507, 0.25836679366277776]
actor:  1 policy actor:  1  step number:  53 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  73 total reward:  0.3333333333333325  reward:  1.0 rdn_beta:  0.5
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03562 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.03562 0.7576666666666666 0.7576666666666666
probs:  [0.06845155006939349, 0.33241813623097805, 0.3407635200368507, 0.25836679366277776]
actor:  0 policy actor:  0  step number:  35 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 1.9212387363190413
actor:  1 policy actor:  1  step number:  34 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.035753333333333345 0.7576666666666666 0.7576666666666666
probs:  [0.0681789618738523, 0.3350806249273904, 0.3394041493204077, 0.2573362638783495]
actor:  1 policy actor:  1  step number:  30 total reward:  0.8066666666666668  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.035753333333333345 0.7576666666666666 0.7576666666666666
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.617]
 [0.632]
 [0.617]
 [0.617]
 [0.631]
 [0.631]] [[ 0.013]
 [-0.273]
 [ 0.066]
 [-0.273]
 [-0.273]
 [ 0.256]
 [ 0.356]] [[0.339]
 [0.134]
 [0.374]
 [0.134]
 [0.134]
 [0.5  ]
 [0.567]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  0.667
using another actor
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.064]
 [0.051]
 [0.051]
 [0.063]
 [0.059]
 [0.055]] [[-1.281]
 [-0.428]
 [-1.705]
 [-1.823]
 [-1.006]
 [-1.012]
 [-1.66 ]] [[0.067]
 [0.064]
 [0.051]
 [0.051]
 [0.063]
 [0.059]
 [0.055]]
maxi score, test score, baseline:  0.035753333333333345 0.7576666666666666 0.7576666666666666
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]]
from probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.035753333333333345 0.7576666666666666 0.7576666666666666
probs:  [0.06845155006939352, 0.33642267154947747, 0.3407635200368508, 0.2543622583442781]
Printing some Q and Qe and total Qs values:  [[ 0.079]
 [ 0.188]
 [ 0.088]
 [-0.016]
 [ 0.079]
 [ 0.079]
 [-0.207]] [[0.943]
 [0.851]
 [1.078]
 [0.798]
 [0.943]
 [0.943]
 [0.728]] [[ 0.116]
 [ 0.156]
 [ 0.164]
 [ 0.012]
 [ 0.116]
 [ 0.116]
 [-0.129]]
maxi score, test score, baseline:  0.035753333333333345 0.7576666666666666 0.7576666666666666
probs:  [0.06845155006939352, 0.33642267154947747, 0.3407635200368508, 0.2543622583442781]
maxi score, test score, baseline:  0.035753333333333345 0.7576666666666666 0.7576666666666666
probs:  [0.06845155006939352, 0.33642267154947747, 0.3407635200368508, 0.2543622583442781]
maxi score, test score, baseline:  0.035753333333333345 0.7576666666666666 0.7576666666666666
probs:  [0.06845155006939352, 0.33642267154947747, 0.3407635200368508, 0.2543622583442781]
maxi score, test score, baseline:  0.035753333333333345 0.7576666666666666 0.7576666666666666
probs:  [0.06845155006939352, 0.33642267154947747, 0.3407635200368508, 0.2543622583442781]
actor:  0 policy actor:  1  step number:  47 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.167
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.0255],
        [ 0.3553],
        [-0.0000],
        [ 0.2433],
        [-0.0000],
        [-0.0522],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [ 0.3035]], dtype=torch.float64)
-0.032346567066 -0.006832044387758567
-0.04528388706599999 0.30998280398474753
-0.8136953616 -0.8136953616
-0.032346567066 0.21094432540526992
-0.9289668828 -0.9289668828
-0.032346567066 -0.08450458938593267
-0.8565379436459999 -0.8565379436459999
-0.3594053033999998 -0.3594053033999998
-0.9289668828 -0.9289668828
-0.032346567066 0.27119887869154624
maxi score, test score, baseline:  0.03863333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.06845155006939352, 0.33642267154947747, 0.3407635200368508, 0.2543622583442781]
maxi score, test score, baseline:  0.03863333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.06845155006939352, 0.33642267154947747, 0.3407635200368508, 0.2543622583442781]
actor:  0 policy actor:  1  step number:  40 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.03567333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.06845155006939352, 0.33642267154947747, 0.3407635200368508, 0.2543622583442781]
actor:  1 policy actor:  1  step number:  57 total reward:  0.5600000000000004  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.478]
 [0.428]
 [0.43 ]
 [0.43 ]
 [0.418]
 [0.409]] [[1.19 ]
 [1.012]
 [1.441]
 [1.418]
 [1.342]
 [1.417]
 [2.185]] [[-0.048]
 [-0.079]
 [ 0.06 ]
 [ 0.052]
 [ 0.02 ]
 [ 0.042]
 [ 0.359]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.6000000000000004  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.03567333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
using another actor
from probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.03567333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]] [[4.469]
 [4.469]
 [4.469]
 [4.469]
 [4.469]
 [4.469]
 [4.469]] [[2.505]
 [2.505]
 [2.505]
 [2.505]
 [2.505]
 [2.505]
 [2.505]]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[2.051]
 [2.036]
 [2.036]
 [2.036]
 [2.036]
 [2.036]
 [2.036]] [[0.482]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]]
siam score:  -0.73433167
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[-0.513]
 [-0.513]
 [-0.513]
 [-0.513]
 [-0.513]
 [-0.513]
 [-0.513]] [[0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]]
maxi score, test score, baseline:  0.03567333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  46 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.011]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[0.089]
 [0.558]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[0.282]
 [0.524]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
actor:  1 policy actor:  1  step number:  96 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
UNIT TEST: sample policy line 217 mcts : [0.041 0.551 0.02  0.02  0.122 0.02  0.224]
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
actor:  1 policy actor:  1  step number:  41 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
siam score:  -0.7292656
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.637]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[1.023]
 [1.512]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]] [[0.107]
 [0.564]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.532]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[1.038]
 [0.957]
 [1.513]
 [1.513]
 [1.513]
 [1.513]
 [1.513]] [[0.285]
 [0.284]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]]
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
actor:  1 policy actor:  1  step number:  37 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  46 total reward:  0.6066666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
line 256 mcts: sample exp_bonus 0.8553480052090764
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.745]
 [0.606]
 [0.583]
 [0.583]
 [0.613]
 [0.61 ]] [[1.742]
 [0.092]
 [0.169]
 [0.466]
 [0.466]
 [0.183]
 [0.341]] [[0.491]
 [0.14 ]
 [0.101]
 [0.18 ]
 [0.18 ]
 [0.108]
 [0.154]]
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
siam score:  -0.7286898
actor:  1 policy actor:  1  step number:  82 total reward:  0.12666666666666682  reward:  1.0 rdn_beta:  0.667
siam score:  -0.72915405
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.336]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.255]] [[1.802]
 [2.02 ]
 [1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.82 ]] [[0.406]
 [0.544]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.402]]
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]] [[-0.351]
 [-0.351]
 [-0.351]
 [-0.351]
 [-0.351]
 [-0.351]
 [-0.351]] [[0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]]
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
actor:  1 policy actor:  1  step number:  88 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.729]
 [0.64 ]
 [0.609]
 [0.61 ]
 [0.601]
 [0.706]] [[0.895]
 [0.646]
 [0.588]
 [0.683]
 [0.895]
 [1.005]
 [0.636]] [[0.303]
 [0.256]
 [0.128]
 [0.161]
 [0.303]
 [0.367]
 [0.227]]
maxi score, test score, baseline:  0.03864666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
actor:  0 policy actor:  1  step number:  45 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.04198 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.04198 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
actor:  1 policy actor:  1  step number:  60 total reward:  0.5800000000000004  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.04198 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.04198 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.04198 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
Printing some Q and Qe and total Qs values:  [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]] [[0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]] [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]]
maxi score, test score, baseline:  0.04198 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.04198 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.04198 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.04198 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.04198 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.3364226715494775, 0.34076352003685073, 0.25436225834427817]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  42 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  50 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.023]
 [ 0.055]
 [-0.257]
 [-0.016]
 [-0.016]
 [-0.012]
 [-0.005]] [[0.49 ]
 [0.53 ]
 [0.584]
 [0.624]
 [0.695]
 [0.   ]
 [0.507]] [[-0.04 ]
 [ 0.027]
 [-0.19 ]
 [-0.002]
 [ 0.016]
 [-0.152]
 [-0.023]]
maxi score, test score, baseline:  0.041473333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.041473333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.041473333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.041473333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.041473333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.041473333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
actor:  1 policy actor:  1  step number:  50 total reward:  0.6600000000000004  reward:  1.0 rdn_beta:  0.333
siam score:  -0.72168976
maxi score, test score, baseline:  0.041473333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
actor:  0 policy actor:  1  step number:  35 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
from probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.041580000000000006 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
actor:  1 policy actor:  1  step number:  30 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.433]
 [0.432]
 [0.433]
 [0.432]
 [0.432]
 [0.432]] [[0.003]
 [0.006]
 [0.003]
 [0.008]
 [0.   ]
 [0.007]
 [0.011]] [[0.433]
 [0.433]
 [0.432]
 [0.433]
 [0.432]
 [0.432]
 [0.432]]
actor:  0 policy actor:  1  step number:  37 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7216481
maxi score, test score, baseline:  0.04147333333333335 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03808666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[1.706]
 [1.706]
 [1.706]
 [1.706]
 [1.706]
 [1.706]
 [1.706]] [[0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6133333333333337  reward:  1.0 rdn_beta:  0.333
from probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.03470000000000001 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.03131333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
maxi score, test score, baseline:  0.03131333333333334 0.7576666666666666 0.7576666666666666
first move QE:  -0.05813252837059119
maxi score, test score, baseline:  0.03131333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.0684515500693935, 0.33642267154947764, 0.34076352003685073, 0.25436225834427817]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5800000000000002  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  42 total reward:  0.526666666666667  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03098 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.03098 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6800000000000003  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.03098 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  48 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.03056666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.03056666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.03056666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[2.511]
 [2.511]
 [2.511]
 [2.511]
 [2.511]
 [2.511]
 [2.511]] [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]]
maxi score, test score, baseline:  0.03056666666666667 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.03056666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.03056666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.266]
 [0.255]
 [0.267]
 [0.262]
 [0.252]
 [0.254]] [[2.637]
 [2.235]
 [2.818]
 [2.728]
 [2.645]
 [2.524]
 [2.772]] [[0.324]
 [0.111]
 [0.44 ]
 [0.399]
 [0.345]
 [0.266]
 [0.413]]
maxi score, test score, baseline:  0.03056666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
line 256 mcts: sample exp_bonus -0.08629309542340523
actor:  1 policy actor:  1  step number:  51 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.614]
 [0.603]
 [0.594]
 [0.571]
 [0.604]
 [0.611]] [[1.029]
 [1.563]
 [1.804]
 [1.293]
 [1.717]
 [1.101]
 [2.005]] [[0.654]
 [0.614]
 [0.603]
 [0.594]
 [0.571]
 [0.604]
 [0.611]]
maxi score, test score, baseline:  0.03056666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
actor:  0 policy actor:  0  step number:  40 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.029966666666666673 0.7576666666666666 0.7576666666666666
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.965]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]] [[1.783]
 [2.142]
 [2.132]
 [2.132]
 [2.132]
 [2.132]
 [2.132]] [[0.929]
 [0.965]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
siam score:  -0.7143895
using explorer policy with actor:  1
maxi score, test score, baseline:  0.029966666666666673 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]] [[1.182]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]] [[-0.081]
 [-0.462]
 [-0.462]
 [-0.462]
 [-0.462]
 [-0.462]
 [-0.462]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  41 total reward:  0.6000000000000003  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.029779999999999997 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.029779999999999997 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
actor:  0 policy actor:  0  step number:  26 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.029979999999999993 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.633]
 [0.594]
 [0.601]
 [0.601]
 [0.601]
 [0.616]] [[0.867]
 [0.533]
 [1.104]
 [2.116]
 [2.116]
 [2.116]
 [1.129]] [[ 0.035]
 [-0.032]
 [ 0.119]
 [ 0.463]
 [ 0.463]
 [ 0.463]
 [ 0.15 ]]
maxi score, test score, baseline:  0.029979999999999993 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.029979999999999993 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.029979999999999993 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  65 total reward:  0.5200000000000005  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.029979999999999993 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.029979999999999993 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.029979999999999993 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.029979999999999993 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.029979999999999993 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
actor:  1 policy actor:  1  step number:  52 total reward:  0.6333333333333337  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  60 total reward:  0.4466666666666669  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]] [[0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]]
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.027]] [[0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [1.99 ]] [[-1.058]
 [-1.058]
 [-1.058]
 [-1.058]
 [-1.058]
 [-1.058]
 [-0.286]]
maxi score, test score, baseline:  0.029979999999999993 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812728, 0.349459252405462, 0.2608522002630401]
maxi score, test score, baseline:  0.029979999999999993 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812728, 0.349459252405462, 0.2608522002630401]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.029979999999999993 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.029979999999999993 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.010865649914741515
Printing some Q and Qe and total Qs values:  [[-0.015]
 [ 0.062]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.037]] [[2.775]
 [2.227]
 [1.592]
 [1.592]
 [1.592]
 [1.592]
 [1.355]] [[0.643]
 [0.51 ]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.196]]
actor:  0 policy actor:  0  step number:  37 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.02315333333333334 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.02315333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812728, 0.349459252405462, 0.2608522002630401]
siam score:  -0.7207884
maxi score, test score, baseline:  0.02315333333333334 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.02315333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812728, 0.349459252405462, 0.2608522002630401]
from probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.02315333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7142691
maxi score, test score, baseline:  0.01643333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
actor:  1 policy actor:  1  step number:  64 total reward:  0.5533333333333338  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.01643333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.01643333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.01643333333333334 0.7576666666666666 0.7576666666666666
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.01643333333333334 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
actor:  0 policy actor:  0  step number:  45 total reward:  0.4533333333333329  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
maxi score, test score, baseline:  0.015966666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
from probs:  [0.07019526425022504, 0.3194932830812729, 0.349459252405462, 0.26085220026304007]
siam score:  -0.71496385
maxi score, test score, baseline:  0.015966666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07012199302935439, 0.3197530831456367, 0.3490912701323411, 0.2610336536926678]
maxi score, test score, baseline:  0.015966666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07012199302935439, 0.3197530831456367, 0.3490912701323411, 0.2610336536926678]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.015966666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07012199302935439, 0.3197530831456367, 0.3490912701323411, 0.2610336536926678]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  37 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.015193333333333333 0.7576666666666666 0.7576666666666666
probs:  [0.07012199302935439, 0.3197530831456367, 0.3490912701323411, 0.2610336536926678]
maxi score, test score, baseline:  0.015193333333333333 0.7576666666666666 0.7576666666666666
probs:  [0.07012199302935439, 0.3197530831456367, 0.3490912701323411, 0.2610336536926678]
maxi score, test score, baseline:  0.01192666666666667 0.7576666666666666 0.7576666666666666
line 256 mcts: sample exp_bonus 0.029652099868197366
actor:  1 policy actor:  1  step number:  42 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.01192666666666667 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.01192666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07026396148242978, 0.32040153558770235, 0.3497992476999343, 0.25953525522993354]
maxi score, test score, baseline:  0.01192666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07026396148242978, 0.32040153558770235, 0.3497992476999343, 0.25953525522993354]
maxi score, test score, baseline:  0.01192666666666667 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.01192666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07026396148242978, 0.32040153558770235, 0.3497992476999343, 0.25953525522993354]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07026396148242978, 0.32040153558770235, 0.3497992476999343, 0.25953525522993354]
maxi score, test score, baseline:  0.01192666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0699760632132665, 0.3231902169020978, 0.3483635377805744, 0.2584701821040613]
Printing some Q and Qe and total Qs values:  [[0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]] [[0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]]
maxi score, test score, baseline:  0.01192666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0699760632132665, 0.3231902169020978, 0.3483635377805744, 0.2584701821040613]
actor:  1 policy actor:  1  step number:  45 total reward:  0.6533333333333337  reward:  1.0 rdn_beta:  0.333
siam score:  -0.726656
maxi score, test score, baseline:  0.01192666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.0699760632132665, 0.3231902169020978, 0.3483635377805744, 0.2584701821040613]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[1.441]
 [1.266]
 [1.266]
 [1.266]
 [1.266]
 [1.266]
 [1.266]] [[0.092]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]]
maxi score, test score, baseline:  0.011926666666666667 0.7576666666666666 0.7576666666666666
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.505]
 [0.647]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[1.021]
 [1.021]
 [1.063]
 [1.021]
 [1.021]
 [1.021]
 [1.021]] [[0.054]
 [0.054]
 [0.21 ]
 [0.054]
 [0.054]
 [0.054]
 [0.054]]
maxi score, test score, baseline:  0.011926666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
actor:  1 policy actor:  1  step number:  84 total reward:  0.3933333333333333  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.011926666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
actor:  1 policy actor:  1  step number:  37 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7229776
maxi score, test score, baseline:  0.011926666666666667 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.011926666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
maxi score, test score, baseline:  0.011926666666666667 0.7576666666666666 0.7576666666666666
actor:  1 policy actor:  1  step number:  35 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.005473333333333328 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
using explorer policy with actor:  1
using explorer policy with actor:  1
first move QE:  -0.050165724330435514
maxi score, test score, baseline:  0.005473333333333328 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.005473333333333328 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
actor:  0 policy actor:  0  step number:  41 total reward:  0.586666666666667  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  34 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7288624
using explorer policy with actor:  1
maxi score, test score, baseline:  0.008646666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
maxi score, test score, baseline:  0.008646666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
maxi score, test score, baseline:  0.008646666666666667 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
maxi score, test score, baseline:  0.008646666666666667 0.7576666666666666 0.7576666666666666
actor:  1 policy actor:  1  step number:  41 total reward:  0.6400000000000003  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.593]
 [0.445]
 [0.447]
 [0.447]
 [0.447]
 [0.453]] [[0.603]
 [0.479]
 [0.568]
 [0.603]
 [0.603]
 [0.603]
 [0.758]] [[-0.011]
 [ 0.051]
 [-0.037]
 [-0.011]
 [-0.011]
 [-0.011]
 [ 0.097]]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[1.598]
 [1.521]
 [1.521]
 [1.521]
 [1.521]
 [1.521]
 [1.521]] [[0.617]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]]
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]] [[1.399]
 [1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]] [[0.413]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]]
maxi score, test score, baseline:  0.005673333333333331 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
maxi score, test score, baseline:  0.005673333333333331 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
from probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
actor:  1 policy actor:  1  step number:  69 total reward:  0.4666666666666671  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0030999999999999964 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.337]
 [0.297]] [[1.793]
 [1.971]
 [1.971]
 [1.971]
 [1.971]
 [1.618]
 [1.971]] [[0.351]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.276]
 [0.369]]
actor:  0 policy actor:  0  step number:  40 total reward:  0.6466666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.00326 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
maxi score, test score, baseline:  0.00326 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]] [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]]
using explorer policy with actor:  1
siam score:  -0.7309173
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.00326 0.7576666666666666 0.7576666666666666
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.10418812359857112
actor:  1 policy actor:  1  step number:  68 total reward:  0.42000000000000026  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.00326 0.7576666666666666 0.7576666666666666
probs:  [0.07046287185942864, 0.3184748214980754, 0.3507911869781762, 0.26027111966431976]
start point for exploration sampling:  11715
actor:  1 policy actor:  1  step number:  62 total reward:  0.5266666666666673  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.00326 0.7576666666666666 0.7576666666666666
probs:  [0.0699760632132665, 0.3231902169020978, 0.3483635377805744, 0.2584701821040613]
actor:  1 policy actor:  1  step number:  39 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.00326 0.7576666666666666 0.7576666666666666
probs:  [0.0699760632132665, 0.3231902169020978, 0.3483635377805744, 0.2584701821040613]
actor:  1 policy actor:  1  step number:  43 total reward:  0.706666666666667  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.00326 0.7576666666666666 0.7576666666666666
probs:  [0.07138407864600205, 0.3095516968128253, 0.35538512123526306, 0.2636791033059096]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.296]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]] [[0.827]
 [1.057]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]] [[-0.164]
 [ 0.024]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]]
maxi score, test score, baseline:  0.00326 0.7576666666666666 0.7576666666666666
probs:  [0.07138407864600205, 0.3095516968128253, 0.35538512123526306, 0.2636791033059096]
maxi score, test score, baseline:  0.00326 0.7576666666666666 0.7576666666666666
probs:  [0.07138407864600205, 0.3095516968128253, 0.35538512123526306, 0.2636791033059096]
siam score:  -0.7302774
maxi score, test score, baseline:  0.00326 0.7576666666666666 0.7576666666666666
actor:  1 policy actor:  1  step number:  45 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.00326 0.7576666666666666 0.7576666666666666
probs:  [0.07138407864600205, 0.3095516968128253, 0.35538512123526306, 0.2636791033059096]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.667
siam score:  -0.73396164
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.961]
 [0.949]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.98 ]] [[0.407]
 [0.472]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.624]] [[0.961]
 [0.949]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.98 ]]
actor:  0 policy actor:  0  step number:  36 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  35 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  44 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.003966666666666663 0.7576666666666666 0.7576666666666666
probs:  [0.07138407864600205, 0.3095516968128253, 0.35538512123526306, 0.2636791033059096]
siam score:  -0.7426373
maxi score, test score, baseline:  0.003966666666666663 0.7576666666666666 0.7576666666666666
maxi score, test score, baseline:  0.003966666666666663 0.7576666666666666 0.7576666666666666
probs:  [0.07138407864600205, 0.3095516968128253, 0.35538512123526306, 0.2636791033059096]
maxi score, test score, baseline:  0.003966666666666663 0.7576666666666666 0.7576666666666666
probs:  [0.07138407864600205, 0.3095516968128253, 0.35538512123526306, 0.2636791033059096]
actor:  0 policy actor:  0  step number:  44 total reward:  0.6066666666666669  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  34 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.667
Starting evaluation
maxi score, test score, baseline:  0.007180000000000006 0.7576666666666666 0.7576666666666666
probs:  [0.07138407864600205, 0.3095516968128253, 0.35538512123526306, 0.2636791033059096]
Printing some Q and Qe and total Qs values:  [[0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]] [[1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.228]] [[0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.007180000000000006 0.7576666666666666 0.7576666666666666
probs:  [0.07138407864600205, 0.3095516968128253, 0.35538512123526306, 0.2636791033059096]
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.838]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]] [[1.411]
 [1.128]
 [1.411]
 [1.411]
 [1.411]
 [1.411]
 [1.411]] [[0.746]
 [0.838]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]]
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.863]
 [0.739]
 [0.737]
 [0.328]
 [0.759]
 [0.743]] [[1.926]
 [1.159]
 [1.225]
 [1.24 ]
 [0.804]
 [1.106]
 [1.206]] [[0.738]
 [0.863]
 [0.739]
 [0.737]
 [0.328]
 [0.759]
 [0.743]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  61 total reward:  0.5733333333333338  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07138407864600205, 0.3095516968128253, 0.35538512123526306, 0.2636791033059096]
line 256 mcts: sample exp_bonus 1.453282565178709
Printing some Q and Qe and total Qs values:  [[0.934]
 [0.977]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.897]] [[0.92 ]
 [0.342]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.738]] [[0.934]
 [0.977]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.897]]
maxi score, test score, baseline:  0.007180000000000006 0.7576666666666666 0.7576666666666666
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.92 ]
 [0.767]
 [0.848]
 [0.82 ]
 [0.811]
 [0.914]] [[0.643]
 [0.077]
 [1.063]
 [0.641]
 [0.643]
 [0.718]
 [0.092]] [[0.82 ]
 [0.92 ]
 [0.767]
 [0.848]
 [0.82 ]
 [0.811]
 [0.914]]
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.916]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.787]] [[ 0.185]
 [ 0.878]
 [ 0.185]
 [ 0.185]
 [ 0.185]
 [ 0.185]
 [-0.367]] [[0.813]
 [0.916]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.787]]
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.007180000000000006 0.7576666666666666 0.7576666666666666
probs:  [0.07138407864600205, 0.3095516968128253, 0.35538512123526306, 0.2636791033059096]
actor:  0 policy actor:  0  step number:  22 total reward:  0.8466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.8400000000000001  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  24 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.78  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.78  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.03324666666666668 0.7576666666666666 0.7576666666666666
probs:  [0.07138407864600205, 0.3095516968128253, 0.35538512123526306, 0.2636791033059096]
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7133333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
siam score:  -0.73520094
maxi score, test score, baseline:  0.04855333333333335 0.7576666666666666 0.7576666666666666
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]] [[-0.502]
 [-0.502]
 [-0.502]
 [-0.502]
 [-0.502]
 [-0.502]
 [-0.502]] [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]]
maxi score, test score, baseline:  0.05868666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.06954798758542477, 0.31700318506282477, 0.34658072490378133, 0.26686810244796916]
actor:  0 policy actor:  0  step number:  50 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06147333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.06954798758542477, 0.31700318506282477, 0.34658072490378133, 0.26686810244796916]
first move QE:  -0.04933132786952957
maxi score, test score, baseline:  0.06147333333333335 0.7470000000000001 0.7470000000000001
actor:  0 policy actor:  0  step number:  63 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06176666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.06954798758542477, 0.31700318506282477, 0.34658072490378133, 0.26686810244796916]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.012]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[-0.698]
 [ 0.071]
 [-0.698]
 [-0.698]
 [-0.698]
 [-0.698]
 [-0.698]] [[-0.004]
 [ 0.336]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]]
actor:  0 policy actor:  1  step number:  65 total reward:  0.4666666666666669  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06224666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.06960983628238207, 0.3172855698619867, 0.3468894696881682, 0.26621512416746307]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06224666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.06960983628238206, 0.31728556986198664, 0.3468894696881682, 0.2662151241674632]
line 256 mcts: sample exp_bonus 0.9575839925662524
maxi score, test score, baseline:  0.06224666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.06960983628238206, 0.31728556986198664, 0.3468894696881682, 0.2662151241674632]
actor:  0 policy actor:  0  step number:  35 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06560666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.06960983628238207, 0.3172855698619867, 0.3468894696881682, 0.26621512416746307]
maxi score, test score, baseline:  0.06560666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.06960983628238206, 0.31728556986198664, 0.3468894696881682, 0.2662151241674632]
using another actor
maxi score, test score, baseline:  0.06560666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.06960983628238206, 0.31728556986198664, 0.3468894696881682, 0.2662151241674632]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.0
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  39 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  54 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.04991813087114296
using explorer policy with actor:  1
siam score:  -0.7398658
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06808666666666668 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.06808666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.06977753049430971, 0.3173977783053226, 0.34769717079676127, 0.2651275204036065]
actor:  0 policy actor:  0  step number:  38 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.115]
 [-0.122]
 [-0.115]
 [-0.115]
 [-0.117]
 [-0.119]
 [-0.119]] [[0.835]
 [0.969]
 [1.035]
 [0.61 ]
 [0.64 ]
 [0.765]
 [0.988]] [[-0.05 ]
 [ 0.032]
 [ 0.084]
 [-0.2  ]
 [-0.183]
 [-0.101]
 [ 0.048]]
maxi score, test score, baseline:  0.07156666666666668 0.7470000000000001 0.7470000000000001
actor:  0 policy actor:  0  step number:  40 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.07459333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.06977753049430971, 0.3173977783053226, 0.34769717079676127, 0.26512752040360643]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[4.443]
 [1.932]
 [1.932]
 [1.932]
 [1.932]
 [1.932]
 [1.932]] [[0.712]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.569]
 [0.632]
 [0.569]
 [0.569]
 [0.569]
 [0.569]] [[1.172]
 [1.172]
 [0.532]
 [1.172]
 [1.172]
 [1.172]
 [1.172]] [[ 0.031]
 [ 0.031]
 [-0.119]
 [ 0.031]
 [ 0.031]
 [ 0.031]
 [ 0.031]]
from probs:  [0.06977753049430971, 0.3173977783053226, 0.34769717079676127, 0.26512752040360643]
maxi score, test score, baseline:  0.07459333333333334 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.07459333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.06992318772326811, 0.31597075206178077, 0.3484242209330682, 0.265681839281883]
maxi score, test score, baseline:  0.07459333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.06992318772326811, 0.31597075206178077, 0.3484242209330682, 0.265681839281883]
actor:  1 policy actor:  1  step number:  54 total reward:  0.606666666666667  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.48 ]
 [0.429]
 [0.43 ]
 [0.412]
 [0.419]
 [0.412]] [[1.691]
 [1.48 ]
 [1.616]
 [1.6  ]
 [1.499]
 [1.593]
 [1.662]] [[0.209]
 [0.153]
 [0.17 ]
 [0.163]
 [0.094]
 [0.148]
 [0.175]]
maxi score, test score, baseline:  0.07459333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.06992318772326811, 0.31597075206178077, 0.3484242209330682, 0.265681839281883]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.419]
 [0.452]
 [0.455]
 [0.454]
 [0.444]
 [0.441]] [[2.401]
 [1.207]
 [1.183]
 [1.136]
 [1.104]
 [1.097]
 [1.145]] [[ 0.494]
 [ 0.024]
 [ 0.041]
 [ 0.024]
 [ 0.01 ]
 [-0.001]
 [ 0.017]]
maxi score, test score, baseline:  0.07459333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.06992318772326811, 0.31597075206178077, 0.3484242209330682, 0.265681839281883]
maxi score, test score, baseline:  0.07459333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.06992318772326811, 0.31597075206178077, 0.3484242209330682, 0.265681839281883]
maxi score, test score, baseline:  0.07126000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06992318772326811, 0.31597075206178077, 0.3484242209330682, 0.265681839281883]
maxi score, test score, baseline:  0.07126000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06992318772326811, 0.31597075206178077, 0.3484242209330682, 0.265681839281883]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6800000000000003  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.07126000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06992318772326811, 0.31597075206178077, 0.3484242209330682, 0.265681839281883]
maxi score, test score, baseline:  0.07126000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06992318772326811, 0.31597075206178077, 0.3484242209330682, 0.265681839281883]
actor:  1 policy actor:  1  step number:  73 total reward:  0.41333333333333355  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  44 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07126000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.0696916963207168, 0.3167727110468511, 0.34726301814469074, 0.2662725744877414]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]] [[0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]] [[0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]]
Printing some Q and Qe and total Qs values:  [[ 0.315]
 [ 0.225]
 [ 0.248]
 [ 0.256]
 [-0.011]
 [ 0.285]
 [ 0.287]] [[1.942]
 [1.891]
 [1.659]
 [1.488]
 [1.468]
 [2.036]
 [1.123]] [[ 0.166]
 [ 0.044]
 [-0.084]
 [-0.188]
 [-0.461]
 [ 0.197]
 [-0.395]]
maxi score, test score, baseline:  0.07126000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.0696916963207168, 0.3167727110468511, 0.34726301814469074, 0.2662725744877414]
actor:  1 policy actor:  1  step number:  58 total reward:  0.5666666666666671  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7440753
maxi score, test score, baseline:  0.07126000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.0696916963207168, 0.3167727110468511, 0.34726301814469074, 0.2662725744877414]
maxi score, test score, baseline:  0.07126000000000002 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.07126000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.0696916963207168, 0.3167727110468511, 0.34726301814469074, 0.2662725744877414]
actor:  1 policy actor:  1  step number:  68 total reward:  0.5000000000000004  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.07126000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
maxi score, test score, baseline:  0.06780666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
maxi score, test score, baseline:  0.06780666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
maxi score, test score, baseline:  0.06780666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.184]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]] [[1.835]
 [2.172]
 [1.835]
 [1.835]
 [1.835]
 [1.835]
 [1.835]] [[0.439]
 [0.58 ]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]]
maxi score, test score, baseline:  0.06780666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
actor:  0 policy actor:  0  step number:  38 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.167
from probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07110000000000004 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
maxi score, test score, baseline:  0.06754000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
maxi score, test score, baseline:  0.06754000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230874, 0.3168562645870591, 0.34714203614227584, 0.26633412129835626]
maxi score, test score, baseline:  0.06754000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230874, 0.3168562645870591, 0.34714203614227584, 0.26633412129835626]
Printing some Q and Qe and total Qs values:  [[ 0.]
 [ 0.]
 [-0.]
 [ 0.]
 [ 0.]
 [ 0.]
 [ 0.]] [[-0.]
 [-0.]
 [ 0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[-0.206]
 [-0.206]
 [-0.206]
 [-0.206]
 [-0.206]
 [-0.206]
 [-0.206]]
using explorer policy with actor:  1
siam score:  -0.73088974
actor:  1 policy actor:  1  step number:  84 total reward:  0.446666666666667  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.7237257241170889
siam score:  -0.7282849
maxi score, test score, baseline:  0.06754000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
maxi score, test score, baseline:  0.06754000000000002 0.7470000000000001 0.7470000000000001
from probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
maxi score, test score, baseline:  0.06754000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.06754000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
actor:  0 policy actor:  0  step number:  27 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07108666666666669 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.07108666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
maxi score, test score, baseline:  0.06802000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
maxi score, test score, baseline:  0.06802000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
maxi score, test score, baseline:  0.06802000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
maxi score, test score, baseline:  0.06567333333333336 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
maxi score, test score, baseline:  0.06567333333333336 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.23 ]
 [0.228]
 [0.24 ]
 [0.229]
 [0.252]
 [0.246]] [[2.437]
 [1.712]
 [2.438]
 [2.428]
 [1.892]
 [1.99 ]
 [2.126]] [[0.494]
 [0.038]
 [0.467]
 [0.471]
 [0.144]
 [0.222]
 [0.297]]
siam score:  -0.73685193
maxi score, test score, baseline:  0.06567333333333336 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
actor:  1 policy actor:  1  step number:  59 total reward:  0.5200000000000005  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.99 ]
 [0.999]
 [0.993]
 [0.937]
 [0.937]
 [1.001]
 [1.   ]] [[ 0.002]
 [-0.02 ]
 [-0.044]
 [ 0.   ]
 [ 0.   ]
 [-0.005]
 [ 0.078]] [[0.99 ]
 [0.999]
 [0.993]
 [0.937]
 [0.937]
 [1.001]
 [1.   ]]
actor:  0 policy actor:  0  step number:  45 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06852666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
from probs:  [0.06966757797230873, 0.3168562645870592, 0.34714203614227584, 0.2663341212983562]
siam score:  -0.74153274
start point for exploration sampling:  11715
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.0
actor:  1 policy actor:  1  step number:  42 total reward:  0.6333333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06558000000000003 0.7470000000000001 0.7470000000000001
probs:  [0.06969789437952106, 0.3167764616320256, 0.34729335817175366, 0.2662322858166996]
maxi score, test score, baseline:  0.06558000000000003 0.7470000000000001 0.7470000000000001
probs:  [0.06969789437952106, 0.3167764616320256, 0.34729335817175366, 0.2662322858166996]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.818]
 [0.831]
 [0.816]
 [0.115]
 [0.361]
 [0.784]] [[ 0.382]
 [ 0.795]
 [ 0.753]
 [ 1.114]
 [-0.009]
 [ 0.288]
 [ 0.516]] [[0.388]
 [0.818]
 [0.831]
 [0.816]
 [0.115]
 [0.361]
 [0.784]]
line 256 mcts: sample exp_bonus 1.1491610043858569
maxi score, test score, baseline:  0.06558000000000003 0.7470000000000001 0.7470000000000001
probs:  [0.06969789437952106, 0.3167764616320256, 0.34729335817175366, 0.2662322858166996]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.945]
 [0.979]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.915]] [[0.684]
 [0.61 ]
 [1.065]
 [1.065]
 [1.065]
 [1.065]
 [0.93 ]] [[0.945]
 [0.979]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.915]]
actor:  0 policy actor:  0  step number:  54 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06496666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.06969789437952106, 0.3167764616320256, 0.34729335817175366, 0.2662322858166996]
maxi score, test score, baseline:  0.06496666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.07029789885038325, 0.31088603444734136, 0.3502882346289374, 0.268527832073338]
maxi score, test score, baseline:  0.06496666666666667 0.7470000000000001 0.7470000000000001
actor:  1 policy actor:  1  step number:  45 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06496666666666667 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.06496666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.07036296020030128, 0.3102473072997273, 0.3506129833843294, 0.26877674911564203]
maxi score, test score, baseline:  0.06496666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.07036296020030128, 0.3102473072997273, 0.3506129833843294, 0.26877674911564203]
maxi score, test score, baseline:  0.06496666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.07036296020030128, 0.3102473072997273, 0.3506129833843294, 0.26877674911564203]
maxi score, test score, baseline:  0.06496666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.07176213462725622, 0.2965111845191628, 0.35759685559574744, 0.2741298252578335]
maxi score, test score, baseline:  0.06496666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.07176213462725622, 0.2965111845191628, 0.35759685559574744, 0.2741298252578335]
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.003]
 [-0.013]
 [-0.008]
 [-0.013]
 [-0.247]
 [-0.019]] [[ 0.061]
 [ 0.072]
 [ 0.   ]
 [ 0.139]
 [ 0.111]
 [-0.622]
 [ 0.036]] [[ 0.189]
 [ 0.194]
 [ 0.169]
 [ 0.21 ]
 [ 0.2  ]
 [-0.104]
 [ 0.177]]
maxi score, test score, baseline:  0.06496666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.07176213462725622, 0.2965111845191628, 0.35759685559574744, 0.2741298252578335]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.5266666666666671  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.298]
 [ 0.347]
 [-0.075]
 [ 0.253]
 [ 0.271]
 [-0.085]
 [ 0.288]] [[ 1.428]
 [ 1.123]
 [ 0.412]
 [ 1.325]
 [ 0.704]
 [-0.799]
 [ 1.165]] [[ 0.483]
 [ 0.435]
 [ 0.077]
 [ 0.439]
 [ 0.304]
 [-0.207]
 [ 0.418]]
using another actor
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.06496666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.07176213462725622, 0.2965111845191628, 0.35759685559574744, 0.2741298252578335]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.8641774626265871
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.933]
 [0.92 ]
 [0.947]
 [0.916]
 [0.922]
 [0.882]
 [0.929]] [[1.894]
 [1.119]
 [1.821]
 [1.476]
 [2.437]
 [1.677]
 [1.633]] [[0.933]
 [0.92 ]
 [0.947]
 [0.916]
 [0.922]
 [0.882]
 [0.929]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.492]
 [-0.492]
 [-0.492]
 [-0.492]
 [-0.492]
 [-0.492]
 [-0.492]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.5266666666666671  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  39 total reward:  0.706666666666667  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus -1.7038378890735588
maxi score, test score, baseline:  0.06152666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07176213462725622, 0.2965111845191628, 0.35759685559574744, 0.2741298252578335]
maxi score, test score, baseline:  0.06152666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07176213462725622, 0.2965111845191628, 0.35759685559574744, 0.2741298252578335]
maxi score, test score, baseline:  0.06152666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07176213462725622, 0.2965111845191628, 0.35759685559574744, 0.2741298252578335]
maxi score, test score, baseline:  0.06152666666666669 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.06152666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07176213462725622, 0.2965111845191628, 0.35759685559574744, 0.2741298252578335]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06152666666666669 0.7470000000000001 0.7470000000000001
actor:  1 policy actor:  1  step number:  46 total reward:  0.5933333333333337  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06152666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07176213462725622, 0.2965111845191628, 0.35759685559574744, 0.2741298252578335]
maxi score, test score, baseline:  0.06152666666666669 0.7470000000000001 0.7470000000000001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.6669401841692304
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.058820000000000025 0.7470000000000001 0.7470000000000001
probs:  [0.07176213462725622, 0.2965111845191628, 0.35759685559574744, 0.2741298252578335]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.464]
 [0.418]
 [0.323]
 [0.323]
 [0.441]
 [0.2  ]] [[2.206]
 [2.497]
 [2.906]
 [2.206]
 [2.206]
 [3.543]
 [3.219]] [[0.222]
 [0.403]
 [0.525]
 [0.222]
 [0.222]
 [0.766]
 [0.519]]
maxi score, test score, baseline:  0.058820000000000025 0.7470000000000001 0.7470000000000001
probs:  [0.07176213462725622, 0.2965111845191628, 0.35759685559574744, 0.2741298252578335]
actor:  1 policy actor:  1  step number:  29 total reward:  0.8000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.932]
 [0.853]] [[1.953]
 [1.953]
 [1.953]
 [1.953]
 [1.953]
 [2.04 ]
 [1.953]] [[0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.932]
 [0.853]]
maxi score, test score, baseline:  0.058820000000000025 0.7470000000000001 0.7470000000000001
probs:  [0.07149766577311861, 0.2834517706759771, 0.35627677952401915, 0.2887737840268851]
actor:  0 policy actor:  1  step number:  31 total reward:  0.8000000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.059406666666666684 0.7470000000000001 0.7470000000000001
probs:  [0.07149766577311861, 0.2834517706759771, 0.35627677952401915, 0.2887737840268851]
maxi score, test score, baseline:  0.059406666666666684 0.7470000000000001 0.7470000000000001
probs:  [0.07149766577311861, 0.2834517706759771, 0.35627677952401915, 0.2887737840268851]
maxi score, test score, baseline:  0.059406666666666684 0.7470000000000001 0.7470000000000001
probs:  [0.07149766577311861, 0.2834517706759771, 0.35627677952401915, 0.2887737840268851]
actor:  1 policy actor:  1  step number:  59 total reward:  0.4533333333333336  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.059406666666666684 0.7470000000000001 0.7470000000000001
actor:  1 policy actor:  1  step number:  68 total reward:  0.4733333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.059406666666666684 0.7470000000000001 0.7470000000000001
probs:  [0.07149766577311861, 0.2834517706759771, 0.35627677952401915, 0.2887737840268851]
maxi score, test score, baseline:  0.059406666666666684 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.059406666666666684 0.7470000000000001 0.7470000000000001
probs:  [0.07149766577311861, 0.2834517706759771, 0.3562767795240191, 0.2887737840268852]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.81]
 [0.81]
 [0.81]
 [0.81]
 [0.81]
 [0.81]
 [0.81]] [[0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]]
maxi score, test score, baseline:  0.059406666666666684 0.7470000000000001 0.7470000000000001
probs:  [0.07149766577311861, 0.2834517706759771, 0.3562767795240191, 0.2887737840268852]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.059406666666666684 0.7470000000000001 0.7470000000000001
probs:  [0.07149766577311861, 0.2834517706759771, 0.3562767795240191, 0.2887737840268852]
maxi score, test score, baseline:  0.059406666666666684 0.7470000000000001 0.7470000000000001
probs:  [0.07149766577311861, 0.2834517706759771, 0.3562767795240191, 0.2887737840268852]
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]] [[0.805]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[ 0.043]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]]
actor:  1 policy actor:  1  step number:  91 total reward:  0.2533333333333324  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.059406666666666684 0.7470000000000001 0.7470000000000001
probs:  [0.07149766577311861, 0.2834517706759771, 0.3562767795240191, 0.2887737840268852]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.059406666666666684 0.7470000000000001 0.7470000000000001
probs:  [0.07149766577311861, 0.2834517706759771, 0.3562767795240191, 0.2887737840268852]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 4.109903384791753
Printing some Q and Qe and total Qs values:  [[-0.062]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.086]] [[1.837]
 [1.837]
 [1.837]
 [1.837]
 [1.837]
 [1.837]
 [2.669]] [[0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.776]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.059406666666666684 0.7470000000000001 0.7470000000000001
probs:  [0.07149766577311861, 0.2834517706759771, 0.35627677952401915, 0.2887737840268851]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.7640006923995182
maxi score, test score, baseline:  0.05940666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07149766577311863, 0.28345177067597715, 0.3562767795240192, 0.288773784026885]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.706666666666667  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05940666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07149766577311863, 0.28345177067597715, 0.3562767795240192, 0.288773784026885]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.05940666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[1.13]
 [1.13]
 [1.13]
 [1.13]
 [1.13]
 [1.13]
 [1.13]] [[-0.337]
 [-0.337]
 [-0.337]
 [-0.337]
 [-0.337]
 [-0.337]
 [-0.337]]
maxi score, test score, baseline:  0.05940666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.056833333333333354 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.247]
 [0.315]
 [0.247]
 [0.247]
 [0.247]
 [0.317]] [[1.781]
 [1.781]
 [1.906]
 [1.781]
 [1.781]
 [1.781]
 [1.853]] [[0.053]
 [0.053]
 [0.183]
 [0.053]
 [0.053]
 [0.053]
 [0.154]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6800000000000003  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  49 total reward:  0.6533333333333338  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.056833333333333354 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
maxi score, test score, baseline:  0.056833333333333354 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.056833333333333354 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
actor:  1 policy actor:  1  step number:  49 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.056833333333333354 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.056833333333333354 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
actor:  0 policy actor:  0  step number:  42 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.05975333333333335 0.7470000000000001 0.7470000000000001
actor:  1 policy actor:  1  step number:  48 total reward:  0.6600000000000004  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.228]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]] [[1.779]
 [1.627]
 [1.779]
 [1.779]
 [1.779]
 [1.779]
 [1.779]] [[-0.062]
 [ 0.079]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]]
maxi score, test score, baseline:  0.05975333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.597]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]] [[1.5]
 [0. ]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0.066]
 [0.597]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]]
maxi score, test score, baseline:  0.05706000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.838]
 [0.805]
 [0.766]
 [0.742]
 [0.816]
 [0.805]] [[-0.316]
 [-0.785]
 [-0.74 ]
 [-0.537]
 [-0.522]
 [-1.042]
 [-0.614]] [[0.745]
 [0.838]
 [0.805]
 [0.766]
 [0.742]
 [0.816]
 [0.805]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  43 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  52 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.05704666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.047]
 [-0.051]
 [-0.042]
 [-0.03 ]
 [-0.02 ]
 [-0.008]] [[0.218]
 [0.043]
 [0.223]
 [0.112]
 [0.102]
 [0.238]
 [0.421]] [[-0.23 ]
 [-0.313]
 [-0.227]
 [-0.273]
 [-0.265]
 [-0.188]
 [-0.085]]
maxi score, test score, baseline:  0.05704666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
actor:  0 policy actor:  1  step number:  35 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.05744666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
actor:  0 policy actor:  0  step number:  46 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.057700000000000015 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
maxi score, test score, baseline:  0.057700000000000015 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.556]
 [0.556]
 [0.52 ]
 [0.556]
 [0.556]
 [0.642]] [[1.681]
 [1.333]
 [1.333]
 [1.519]
 [1.333]
 [1.333]
 [0.978]] [[0.664]
 [0.432]
 [0.432]
 [0.509]
 [0.432]
 [0.432]
 [0.3  ]]
maxi score, test score, baseline:  0.057700000000000015 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
maxi score, test score, baseline:  0.057700000000000015 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
Printing some Q and Qe and total Qs values:  [[-0.089]
 [-0.089]
 [-0.066]
 [-0.063]
 [-0.073]
 [-0.056]
 [-0.045]] [[0.278]
 [0.018]
 [0.194]
 [0.169]
 [0.24 ]
 [0.343]
 [0.491]] [[-0.044]
 [-0.217]
 [-0.077]
 [-0.09 ]
 [-0.053]
 [ 0.033]
 [ 0.142]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.6266666666666671  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.057700000000000015 0.7470000000000001 0.7470000000000001
probs:  [0.07018465111433005, 0.2873367344795099, 0.3496893491187348, 0.2927892652874253]
maxi score, test score, baseline:  0.057700000000000015 0.7470000000000001 0.7470000000000001
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.178]
 [ 0.017]
 [-0.004]
 [ 0.026]
 [-0.004]
 [-0.001]] [[0.431]
 [1.019]
 [0.842]
 [0.573]
 [0.736]
 [0.571]
 [1.057]] [[-0.767]
 [-0.291]
 [-0.54 ]
 [-0.696]
 [-0.584]
 [-0.697]
 [-0.451]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.057700000000000015 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
actor:  0 policy actor:  0  step number:  37 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.05811333333333335 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.05811333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.6  ]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[1.04 ]
 [0.948]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]] [[0.169]
 [0.232]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05811333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.05811333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
first move QE:  -0.04977794124133931
maxi score, test score, baseline:  0.05811333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
actor:  1 policy actor:  1  step number:  65 total reward:  0.5066666666666673  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.05811333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
from probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]] [[-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]] [[-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]]
maxi score, test score, baseline:  0.05811333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
maxi score, test score, baseline:  0.05811333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
actor:  1 policy actor:  1  step number:  63 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  40 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  47 total reward:  0.6533333333333337  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.059153333333333356 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
actor:  0 policy actor:  0  step number:  44 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7503828
using explorer policy with actor:  1
siam score:  -0.75364685
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.067]
 [0.065]
 [0.065]
 [0.024]
 [0.065]
 [0.065]] [[ 2.308]
 [-0.294]
 [ 2.308]
 [ 2.308]
 [-0.429]
 [ 2.308]
 [ 2.308]] [[ 0.69 ]
 [-0.608]
 [ 0.69 ]
 [ 0.69 ]
 [-0.719]
 [ 0.69 ]
 [ 0.69 ]]
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.489]] [[2.231]
 [2.231]
 [2.231]
 [2.231]
 [2.231]
 [2.231]
 [1.843]] [[0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.092]]
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.808389977472927
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.0703397617670834, 0.2857593472471643, 0.3504634976303948, 0.29343739335535746]
siam score:  -0.7460712
actor:  1 policy actor:  1  step number:  49 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  64 total reward:  0.5800000000000005  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.0707802004507443, 0.28755153574006287, 0.3526617022371138, 0.289006561572079]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.0707802004507443, 0.28755153574006287, 0.3526617022371138, 0.289006561572079]
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.356]
 [0.311]
 [0.319]
 [0.322]
 [0.322]
 [0.3  ]] [[1.389]
 [1.282]
 [1.244]
 [1.291]
 [1.324]
 [1.32 ]
 [1.731]] [[-0.163]
 [-0.189]
 [-0.252]
 [-0.221]
 [-0.202]
 [-0.204]
 [-0.021]]
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07078020045074429, 0.2875515357400629, 0.35266170223711374, 0.2890065615720791]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1685599685751598
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07078020045074429, 0.2875515357400629, 0.35266170223711374, 0.2890065615720791]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.377]
 [0.33 ]
 [0.336]
 [0.337]
 [0.335]
 [0.334]] [[2.199]
 [1.744]
 [2.37 ]
 [2.126]
 [1.897]
 [1.937]
 [2.288]] [[0.181]
 [0.003]
 [0.269]
 [0.153]
 [0.039]
 [0.058]
 [0.232]]
siam score:  -0.7466548
actor:  1 policy actor:  1  step number:  42 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07078020045074429, 0.2875515357400629, 0.35266170223711374, 0.2890065615720791]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07078020045074429, 0.2875515357400629, 0.35266170223711374, 0.2890065615720791]
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07078020045074429, 0.2875515357400629, 0.35266170223711374, 0.2890065615720791]
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07078020045074429, 0.2875515357400629, 0.35266170223711374, 0.2890065615720791]
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07078020045074429, 0.2875515357400629, 0.35266170223711374, 0.2890065615720791]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07078020045074429, 0.2875515357400629, 0.35266170223711374, 0.2890065615720791]
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07078020045074429, 0.2875515357400629, 0.35266170223711374, 0.2890065615720791]
actor:  1 policy actor:  1  step number:  64 total reward:  0.5400000000000005  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  80 total reward:  0.326666666666666  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]]
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.0707802004507443, 0.28755153574006287, 0.3526617022371138, 0.289006561572079]
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.0707802004507443, 0.28755153574006287, 0.3526617022371138, 0.289006561572079]
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.0707802004507443, 0.28755153574006287, 0.3526617022371138, 0.289006561572079]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07078020045074429, 0.2875515357400629, 0.35266170223711374, 0.2890065615720791]
maxi score, test score, baseline:  0.05852666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07078020045074429, 0.2875515357400629, 0.35266170223711374, 0.2890065615720791]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  52 total reward:  0.5800000000000003  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  49 total reward:  0.3466666666666659  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  37 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.061220000000000024 0.7470000000000001 0.7470000000000001
probs:  [0.07078020045074429, 0.287551535740063, 0.35266170223711374, 0.28900656157207905]
maxi score, test score, baseline:  0.061220000000000024 0.7470000000000001 0.7470000000000001
probs:  [0.0707802004507443, 0.2875515357400629, 0.3526617022371138, 0.289006561572079]
siam score:  -0.75236493
maxi score, test score, baseline:  0.061220000000000024 0.7470000000000001 0.7470000000000001
probs:  [0.0707802004507443, 0.2875515357400629, 0.3526617022371138, 0.289006561572079]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.113]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[1.41 ]
 [1.375]
 [1.41 ]
 [1.41 ]
 [1.41 ]
 [1.41 ]
 [1.41 ]] [[-0.466]
 [-0.384]
 [-0.466]
 [-0.466]
 [-0.466]
 [-0.466]
 [-0.466]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.061220000000000024 0.7470000000000001 0.7470000000000001
probs:  [0.07088563502528511, 0.2872357330168347, 0.35319070056381646, 0.28868793139406373]
maxi score, test score, baseline:  0.061220000000000024 0.7470000000000001 0.7470000000000001
probs:  [0.07088563502528511, 0.2872357330168347, 0.35319070056381646, 0.28868793139406373]
using explorer policy with actor:  1
from probs:  [0.07088563502528511, 0.2872357330168347, 0.35319070056381646, 0.28868793139406373]
maxi score, test score, baseline:  0.061220000000000024 0.7470000000000001 0.7470000000000001
probs:  [0.07088563502528511, 0.2872357330168347, 0.35319070056381646, 0.28868793139406373]
actor:  1 policy actor:  1  step number:  54 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.061220000000000024 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.061220000000000024 0.7470000000000001 0.7470000000000001
probs:  [0.07088563502528512, 0.28723573301683464, 0.3531907005638165, 0.28868793139406373]
Printing some Q and Qe and total Qs values:  [[ 0.004]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[1.256]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[0.918]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]]
maxi score, test score, baseline:  0.061220000000000024 0.7470000000000001 0.7470000000000001
probs:  [0.07088563502528512, 0.28723573301683464, 0.3531907005638165, 0.2886879313940637]
maxi score, test score, baseline:  0.061220000000000024 0.7470000000000001 0.7470000000000001
probs:  [0.07116885533023713, 0.2843836813778351, 0.3546042482497877, 0.28984321504214006]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.153]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.166]] [[1.866]
 [1.959]
 [1.866]
 [1.866]
 [1.866]
 [1.866]
 [2.793]] [[-0.211]
 [-0.062]
 [-0.211]
 [-0.211]
 [-0.211]
 [-0.211]
 [ 0.203]]
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.346]
 [0.29 ]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[3.1  ]
 [2.213]
 [2.312]
 [3.1  ]
 [3.1  ]
 [3.1  ]
 [2.458]] [[0.3  ]
 [0.06 ]
 [0.038]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.103]]
actor:  0 policy actor:  0  step number:  33 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06455333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.07116885533023713, 0.2843836813778351, 0.3546042482497877, 0.28984321504214006]
maxi score, test score, baseline:  0.06455333333333335 0.7470000000000001 0.7470000000000001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.457]
 [0.336]
 [0.338]
 [0.348]
 [0.339]
 [0.369]] [[-0.363]
 [-0.365]
 [-0.369]
 [-0.629]
 [-0.683]
 [-0.427]
 [-0.607]] [[-0.251]
 [-0.142]
 [-0.265]
 [-0.393]
 [-0.41 ]
 [-0.291]
 [-0.351]]
maxi score, test score, baseline:  0.06455333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.07145435273638001, 0.281508699143571, 0.35602916090875963, 0.2910077872112893]
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]] [[0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]] [[0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06455333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.07145435273638, 0.2815086991435711, 0.3560291609087596, 0.29100778721128934]
actor:  1 policy actor:  1  step number:  39 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06455333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.07145435273638, 0.2815086991435711, 0.3560291609087596, 0.29100778721128934]
maxi score, test score, baseline:  0.06455333333333335 0.7470000000000001 0.7470000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06455333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.07145435273638, 0.2815086991435711, 0.3560291609087596, 0.29100778721128934]
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[1.645]
 [1.645]
 [1.645]
 [1.645]
 [1.645]
 [1.645]
 [1.645]] [[0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]]
maxi score, test score, baseline:  0.06455333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.07145435273638001, 0.281508699143571, 0.35602916090875963, 0.2910077872112893]
actor:  0 policy actor:  0  step number:  32 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06456666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07145435273638001, 0.281508699143571, 0.35602916090875963, 0.2910077872112893]
maxi score, test score, baseline:  0.06456666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07145435273638001, 0.281508699143571, 0.35602916090875963, 0.2910077872112893]
first move QE:  -0.048965236823448686
actor:  0 policy actor:  1  step number:  31 total reward:  0.8000000000000002  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.06503333333333335 0.7470000000000001 0.7470000000000001
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.003]
 [-0.01 ]
 [-0.013]
 [-0.142]
 [-0.006]
 [-0.011]] [[0.694]
 [1.271]
 [1.244]
 [1.06 ]
 [0.381]
 [1.023]
 [1.242]] [[ 0.131]
 [ 0.3  ]
 [ 0.29 ]
 [ 0.237]
 [-0.007]
 [ 0.229]
 [ 0.288]]
maxi score, test score, baseline:  0.06503333333333335 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.06503333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.07145435273638001, 0.281508699143571, 0.35602916090875963, 0.2910077872112893]
actor:  1 policy actor:  1  step number:  34 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06503333333333335 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.06503333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.07147925317487848, 0.28143598593488783, 0.35615410117682006, 0.2909306597134136]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.06503333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.07147925317487848, 0.28143598593488783, 0.35615410117682006, 0.2909306597134136]
maxi score, test score, baseline:  0.06503333333333335 0.7470000000000001 0.7470000000000001
probs:  [0.0704848060397763, 0.2843399284523461, 0.3511643701068553, 0.29401089540102227]
actor:  1 policy actor:  1  step number:  79 total reward:  0.3733333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06503333333333336 0.7470000000000001 0.7470000000000001
probs:  [0.0704848060397763, 0.2843399284523461, 0.3511643701068553, 0.29401089540102227]
actor:  1 policy actor:  1  step number:  52 total reward:  0.6200000000000003  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06503333333333336 0.7470000000000001 0.7470000000000001
probs:  [0.0704848060397763, 0.2843399284523461, 0.3511643701068553, 0.29401089540102227]
maxi score, test score, baseline:  0.06503333333333336 0.7470000000000001 0.7470000000000001
probs:  [0.0704848060397763, 0.2843399284523461, 0.3511643701068553, 0.29401089540102227]
actor:  1 policy actor:  1  step number:  34 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06503333333333336 0.7470000000000001 0.7470000000000001
probs:  [0.0704848060397763, 0.2843399284523461, 0.3511643701068553, 0.29401089540102227]
maxi score, test score, baseline:  0.06503333333333336 0.7470000000000001 0.7470000000000001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.012]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[-0.011]
 [ 0.696]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[0.11 ]
 [0.354]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6600000000000003  reward:  1.0 rdn_beta:  0.333
from probs:  [0.0704848060397763, 0.2843399284523461, 0.3511643701068553, 0.29401089540102227]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.542]
 [0.392]
 [0.391]
 [0.392]
 [0.395]
 [0.441]] [[ 0.81 ]
 [ 0.912]
 [ 0.055]
 [ 0.067]
 [-0.068]
 [-0.161]
 [ 0.899]] [[0.516]
 [0.542]
 [0.392]
 [0.391]
 [0.392]
 [0.395]
 [0.441]]
maxi score, test score, baseline:  0.06503333333333336 0.7470000000000001 0.7470000000000001
probs:  [0.0704848060397763, 0.2843399284523461, 0.3511643701068553, 0.29401089540102227]
maxi score, test score, baseline:  0.06503333333333336 0.7470000000000001 0.7470000000000001
probs:  [0.0704848060397763, 0.2843399284523461, 0.3511643701068553, 0.29401089540102227]
actor:  0 policy actor:  0  step number:  43 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.333]
 [0.444]
 [0.358]
 [0.333]
 [0.378]
 [0.333]] [[3.141]
 [3.141]
 [2.557]
 [2.07 ]
 [3.141]
 [2.12 ]
 [3.141]] [[ 0.245]
 [ 0.245]
 [ 0.167]
 [-0.063]
 [ 0.245]
 [-0.029]
 [ 0.245]]
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.777]] [[1.5  ]
 [1.838]
 [1.838]
 [1.838]
 [1.838]
 [1.838]
 [1.499]] [[0.288]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.288]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.336]
 [-0.336]
 [-0.336]
 [-0.336]
 [-0.336]
 [-0.336]
 [-0.336]]
maxi score, test score, baseline:  0.06458000000000003 0.7470000000000001 0.7470000000000001
probs:  [0.0704848060397763, 0.2843399284523461, 0.3511643701068553, 0.29401089540102227]
actor:  1 policy actor:  1  step number:  29 total reward:  0.8133333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06458000000000003 0.7470000000000001 0.7470000000000001
probs:  [0.06870446093856244, 0.30244361951176746, 0.34227935128865056, 0.28657256826101946]
maxi score, test score, baseline:  0.06458000000000003 0.7470000000000001 0.7470000000000001
probs:  [0.06870446093856244, 0.30244361951176746, 0.34227935128865056, 0.28657256826101946]
actor:  1 policy actor:  1  step number:  38 total reward:  0.7266666666666669  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06458000000000003 0.7470000000000001 0.7470000000000001
using explorer policy with actor:  1
siam score:  -0.7259558
maxi score, test score, baseline:  0.06458000000000003 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
actor:  0 policy actor:  0  step number:  28 total reward:  0.8200000000000001  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06822 0.7470000000000001 0.7470000000000001
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.613]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.412]] [[1.349]
 [1.572]
 [1.418]
 [1.418]
 [1.418]
 [1.418]
 [1.24 ]] [[0.467]
 [0.533]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.167]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06822 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7333333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06822 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
actor:  1 policy actor:  1  step number:  36 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06822 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
actor:  0 policy actor:  1  step number:  39 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06815333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.015]
 [-0.008]
 [-0.008]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.564]
 [ 0.   ]
 [ 0.   ]] [[-0.694]
 [-0.694]
 [-0.694]
 [-0.694]
 [-0.795]
 [-0.694]
 [-0.694]]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.617]] [[0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.345]] [[0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.067]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.6600000000000004  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.246]
 [0.201]
 [0.277]
 [0.268]
 [0.237]
 [0.431]] [[0.618]
 [0.766]
 [1.305]
 [1.122]
 [0.677]
 [0.22 ]
 [0.456]] [[-0.349]
 [-0.409]
 [-0.363]
 [-0.318]
 [-0.401]
 [-0.509]
 [-0.275]]
maxi score, test score, baseline:  0.06815333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
actor:  0 policy actor:  1  step number:  61 total reward:  0.4666666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06758000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
maxi score, test score, baseline:  0.06758000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
actor:  1 policy actor:  1  step number:  50 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06758000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]] [[1.439]
 [1.439]
 [1.439]
 [1.439]
 [1.439]
 [1.439]
 [1.439]] [[0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]]
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.608]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[0.281]
 [0.397]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]] [[-0.126]
 [ 0.016]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]]
maxi score, test score, baseline:  0.06758000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06758000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
maxi score, test score, baseline:  0.06758000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
maxi score, test score, baseline:  0.06758000000000002 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.06758000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5733333333333336  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  41 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06726000000000001 0.7470000000000001 0.7470000000000001
actor:  1 policy actor:  1  step number:  49 total reward:  0.6133333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06726000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
maxi score, test score, baseline:  0.06726000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.06726000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
maxi score, test score, baseline:  0.06726000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
maxi score, test score, baseline:  0.06726000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  46 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  51 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06626000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
maxi score, test score, baseline:  0.06626000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
maxi score, test score, baseline:  0.06626000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
maxi score, test score, baseline:  0.06626000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
maxi score, test score, baseline:  0.06626000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
actor:  0 policy actor:  0  step number:  38 total reward:  0.6333333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06614 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3933333333333333  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06614 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
actor:  1 policy actor:  1  step number:  86 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06614 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.099]
 [0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.17 ]] [[1.886]
 [1.912]
 [1.886]
 [1.886]
 [1.886]
 [1.886]
 [2.536]] [[0.35 ]
 [0.367]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.616]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.6133333333333337  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.416]
 [0.302]
 [0.301]
 [0.276]
 [0.282]
 [0.292]] [[1.378]
 [1.348]
 [1.434]
 [1.382]
 [1.282]
 [1.591]
 [2.089]] [[ 0.017]
 [ 0.067]
 [ 0.039]
 [ 0.022]
 [-0.023]
 [ 0.081]
 [ 0.249]]
maxi score, test score, baseline:  0.06614 0.7470000000000001 0.7470000000000001
probs:  [0.06956167618516815, 0.3062235403683193, 0.34655738344778125, 0.2776573999987313]
actor:  0 policy actor:  1  step number:  48 total reward:  0.5933333333333337  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.703]
 [0.66 ]
 [0.563]
 [0.66 ]
 [0.66 ]
 [0.57 ]] [[0.202]
 [1.306]
 [1.271]
 [0.285]
 [1.271]
 [1.271]
 [0.275]] [[0.564]
 [0.703]
 [0.66 ]
 [0.563]
 [0.66 ]
 [0.66 ]
 [0.57 ]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06588666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.06954546320542335, 0.30615204866244167, 0.3464764706910166, 0.2778260174411184]
maxi score, test score, baseline:  0.06588666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.06954546320542335, 0.30615204866244167, 0.3464764706910166, 0.2778260174411184]
maxi score, test score, baseline:  0.06250000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.06954546320542335, 0.30615204866244167, 0.3464764706910166, 0.2778260174411184]
actor:  0 policy actor:  1  step number:  38 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.02 ]
 [ 0.056]
 [-0.012]
 [-0.025]
 [-0.022]
 [-0.012]
 [-0.   ]] [[0.951]
 [0.485]
 [0.653]
 [0.387]
 [0.85 ]
 [0.653]
 [0.758]] [[0.359]
 [0.156]
 [0.189]
 [0.048]
 [0.279]
 [0.189]
 [0.249]]
maxi score, test score, baseline:  0.06235333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.06954546320542335, 0.30615204866244167, 0.3464764706910166, 0.2778260174411184]
maxi score, test score, baseline:  0.06235333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.06954546320542335, 0.30615204866244167, 0.3464764706910166, 0.2778260174411184]
maxi score, test score, baseline:  0.06235333333333334 0.7470000000000001 0.7470000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06235333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.06954546320542335, 0.30615204866244167, 0.3464764706910166, 0.2778260174411184]
maxi score, test score, baseline:  0.06235333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.06954546320542335, 0.30615204866244167, 0.3464764706910166, 0.2778260174411184]
actor:  1 policy actor:  1  step number:  38 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06235333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.06957136663779302, 0.3060675051860153, 0.3466064317452662, 0.2777546964309256]
maxi score, test score, baseline:  0.06235333333333334 0.7470000000000001 0.7470000000000001
from probs:  [0.0710403383457236, 0.29139327316408525, 0.353937520049039, 0.28362886844115215]
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.83 ]
 [0.774]
 [0.734]
 [0.799]
 [0.749]
 [0.757]] [[1.013]
 [0.573]
 [0.6  ]
 [0.75 ]
 [0.834]
 [0.655]
 [1.078]] [[0.783]
 [0.83 ]
 [0.774]
 [0.734]
 [0.799]
 [0.749]
 [0.757]]
maxi score, test score, baseline:  0.06235333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.0710403383457236, 0.29139327316408525, 0.353937520049039, 0.28362886844115215]
maxi score, test score, baseline:  0.06235333333333334 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.06235333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.0710403383457236, 0.29139327316408525, 0.353937520049039, 0.28362886844115215]
maxi score, test score, baseline:  0.06235333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
line 256 mcts: sample exp_bonus 0.5106251309161535
actor:  1 policy actor:  1  step number:  44 total reward:  0.6466666666666671  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06235333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.06235333333333334 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
actor:  0 policy actor:  0  step number:  37 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06230000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.06230000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.06230000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.06230000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.06230000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
actor:  0 policy actor:  1  step number:  37 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  38 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.   ]
 [-0.   ]
 [ 0.098]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.001]] [[2.136]
 [2.136]
 [4.421]
 [2.136]
 [2.136]
 [2.136]
 [1.965]] [[ 0.01 ]
 [ 0.01 ]
 [ 0.515]
 [ 0.01 ]
 [ 0.01 ]
 [ 0.01 ]
 [-0.025]]
maxi score, test score, baseline:  0.06200666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
Printing some Q and Qe and total Qs values:  [[ 0.054]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]] [[3.275]
 [0.962]
 [0.962]
 [0.962]
 [0.962]
 [0.962]
 [0.962]] [[ 0.129]
 [-0.576]
 [-0.576]
 [-0.576]
 [-0.576]
 [-0.576]
 [-0.576]]
Printing some Q and Qe and total Qs values:  [[ 0.031]
 [-0.025]
 [ 0.031]
 [ 0.031]
 [ 0.031]
 [ 0.031]
 [ 0.031]] [[0.829]
 [1.771]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[-0.559]
 [-0.302]
 [-0.559]
 [-0.559]
 [-0.559]
 [-0.559]
 [-0.559]]
maxi score, test score, baseline:  0.06200666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
line 256 mcts: sample exp_bonus 3.387235230928395
maxi score, test score, baseline:  0.06200666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.06200666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.06200666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.06200666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.06200666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]] [[0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]] [[0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.165]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.077]] [[0.962]
 [1.183]
 [0.962]
 [0.962]
 [0.962]
 [0.962]
 [1.335]] [[-0.497]
 [-0.236]
 [-0.497]
 [-0.497]
 [-0.497]
 [-0.497]
 [-0.249]]
maxi score, test score, baseline:  0.06200666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.06200666666666668 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.06200666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
line 256 mcts: sample exp_bonus 0.8690504618186636
maxi score, test score, baseline:  0.06200666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.06200666666666668 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.05862000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.05862000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6800000000000004  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  55 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
actor:  0 policy actor:  0  step number:  39 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.058406666666666676 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.08 ]
 [0.075]
 [0.089]
 [0.083]
 [0.027]
 [0.102]] [[1.476]
 [1.207]
 [1.333]
 [1.497]
 [1.115]
 [1.388]
 [1.401]] [[-0.303]
 [-0.41 ]
 [-0.373]
 [-0.304]
 [-0.438]
 [-0.402]
 [-0.323]]
maxi score, test score, baseline:  0.058406666666666676 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.058406666666666676 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5333333333333339  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.899]
 [0.903]
 [0.903]
 [0.911]
 [0.903]
 [0.912]] [[0.873]
 [1.395]
 [1.22 ]
 [1.22 ]
 [1.242]
 [1.22 ]
 [1.726]] [[0.923]
 [0.899]
 [0.903]
 [0.903]
 [0.911]
 [0.903]
 [0.912]]
maxi score, test score, baseline:  0.05502000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
maxi score, test score, baseline:  0.05502000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.833]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]] [[-1.422]
 [-0.873]
 [-1.422]
 [-1.422]
 [-1.422]
 [-1.422]
 [-1.422]] [[0.757]
 [0.833]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]]
maxi score, test score, baseline:  0.051633333333333344 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554504, 0.2884624401422038, 0.35540173266711095, 0.28480209626514014]
siam score:  -0.7205552
UNIT TEST: sample policy line 217 mcts : [0.122 0.51  0.041 0.02  0.102 0.041 0.163]
actor:  0 policy actor:  0  step number:  60 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.36377742458592754
Printing some Q and Qe and total Qs values:  [[0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]] [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]]
maxi score, test score, baseline:  0.05058000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554503, 0.28846244014220385, 0.3554017326671109, 0.2848020962651402]
siam score:  -0.7293457
actor:  1 policy actor:  1  step number:  41 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7266589
maxi score, test score, baseline:  0.05058000000000001 0.7470000000000001 0.7470000000000001
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.316]
 [0.309]
 [0.316]
 [0.316]
 [0.316]
 [0.319]] [[3.361]
 [3.361]
 [3.466]
 [3.361]
 [3.361]
 [3.361]
 [4.313]] [[0.302]
 [0.302]
 [0.326]
 [0.302]
 [0.302]
 [0.302]
 [0.558]]
actor:  0 policy actor:  0  step number:  35 total reward:  0.7733333333333335  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07133373092554503, 0.28846244014220385, 0.3554017326671109, 0.2848020962651402]
Printing some Q and Qe and total Qs values:  [[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]]
maxi score, test score, baseline:  0.05078000000000001 0.7470000000000001 0.7470000000000001
probs:  [0.07133373092554503, 0.28846244014220385, 0.3554017326671109, 0.2848020962651402]
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.027]
 [-0.036]
 [-0.042]
 [-0.042]
 [-0.041]
 [-0.04 ]] [[0.772]
 [0.832]
 [0.645]
 [0.581]
 [0.581]
 [0.636]
 [0.62 ]] [[0.106]
 [0.137]
 [0.065]
 [0.038]
 [0.038]
 [0.057]
 [0.053]]
maxi score, test score, baseline:  0.04740666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07122520894406197, 0.28879268145614223, 0.3548571686682986, 0.28512494093149715]
maxi score, test score, baseline:  0.04740666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07122520894406197, 0.28879268145614223, 0.3548571686682986, 0.28512494093149715]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.04740666666666669 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.04740666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07122520894406197, 0.28879268145614223, 0.3548571686682986, 0.28512494093149715]
maxi score, test score, baseline:  0.04740666666666669 0.7470000000000001 0.7470000000000001
maxi score, test score, baseline:  0.04740666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07122520894406197, 0.28879268145614223, 0.3548571686682986, 0.28512494093149715]
actor:  1 policy actor:  1  step number:  35 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]] [[1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]] [[0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.917]
 [0.961]
 [0.922]
 [0.891]
 [0.894]
 [0.869]
 [0.935]] [[1.231]
 [1.039]
 [1.273]
 [1.077]
 [0.623]
 [1.134]
 [1.276]] [[0.917]
 [0.961]
 [0.922]
 [0.891]
 [0.894]
 [0.869]
 [0.935]]
maxi score, test score, baseline:  0.04740666666666669 0.7470000000000001 0.7470000000000001
probs:  [0.07129711593612126, 0.2880736068822471, 0.35521602656164347, 0.2854132506199882]
actor:  1 policy actor:  1  step number:  124 total reward:  0.1799999999999986  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07129711593612126, 0.2880736068822471, 0.35521602656164347, 0.2854132506199882]
maxi score, test score, baseline:  0.03728666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.07129711593612126, 0.2880736068822471, 0.35521602656164347, 0.2854132506199882]
actor:  1 policy actor:  1  step number:  36 total reward:  0.7266666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.03728666666666667 0.7470000000000001 0.7470000000000001
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.073]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[1.201]
 [1.432]
 [1.201]
 [1.201]
 [1.201]
 [1.201]
 [1.201]] [[0.24 ]
 [0.391]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]]
maxi score, test score, baseline:  0.03728666666666667 0.7470000000000001 0.7470000000000001
probs:  [0.07129711593612126, 0.2880736068822471, 0.35521602656164347, 0.2854132506199882]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.11639505720498419
actor:  0 policy actor:  0  step number:  38 total reward:  0.6066666666666669  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.03814000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.07129711593612126, 0.2880736068822471, 0.35521602656164347, 0.2854132506199882]
Printing some Q and Qe and total Qs values:  [[ 0.029]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[1.138]
 [1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]] [[0.462]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]]
maxi score, test score, baseline:  0.03814000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.07129711593612126, 0.2880736068822471, 0.35521602656164347, 0.2854132506199882]
Printing some Q and Qe and total Qs values:  [[-0.125]
 [ 0.138]
 [ 0.222]
 [ 0.192]
 [ 0.175]
 [ 0.214]
 [ 0.19 ]] [[1.601]
 [1.135]
 [1.239]
 [1.189]
 [1.307]
 [1.14 ]
 [1.231]] [[-0.447]
 [-0.494]
 [-0.342]
 [-0.405]
 [-0.344]
 [-0.416]
 [-0.379]]
maxi score, test score, baseline:  0.03814000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.07129711593612126, 0.2880736068822471, 0.35521602656164347, 0.2854132506199882]
maxi score, test score, baseline:  0.03814000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.07129711593612126, 0.2880736068822471, 0.35521602656164347, 0.2854132506199882]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.366]
 [0.268]
 [0.275]
 [0.276]
 [0.276]
 [0.267]] [[2.423]
 [1.801]
 [2.637]
 [2.193]
 [2.159]
 [2.118]
 [2.122]] [[0.44 ]
 [0.214]
 [0.549]
 [0.338]
 [0.322]
 [0.302]
 [0.298]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03814000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.07129711593612126, 0.2880736068822471, 0.35521602656164347, 0.2854132506199882]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  0.5600000000000005  reward:  1.0 rdn_beta:  0.667
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.093]
 [0.057]
 [0.012]
 [0.011]
 [0.007]
 [0.092]] [[ 0.072]
 [-0.12 ]
 [-0.075]
 [-0.111]
 [-0.144]
 [-0.248]
 [ 0.126]] [[0.08 ]
 [0.093]
 [0.057]
 [0.012]
 [0.011]
 [0.007]
 [0.092]]
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[1.375]
 [1.375]
 [1.375]
 [1.375]
 [1.375]
 [1.375]
 [1.375]] [[0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.606]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.53 ]] [[1.699]
 [0.722]
 [1.699]
 [1.699]
 [1.699]
 [1.699]
 [1.248]] [[0.539]
 [0.606]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.53 ]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.03814000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.07129711593612126, 0.2880736068822471, 0.35521602656164347, 0.2854132506199882]
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.03814000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.07129711593612126, 0.2880736068822471, 0.35521602656164347, 0.2854132506199882]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.86 ]
 [0.928]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]] [[0.202]
 [0.175]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]] [[0.86 ]
 [0.928]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.842]
 [0.833]
 [0.805]
 [0.833]
 [0.854]
 [0.833]
 [0.902]] [[1.384]
 [1.464]
 [1.545]
 [1.464]
 [1.214]
 [1.464]
 [1.171]] [[0.842]
 [0.833]
 [0.805]
 [0.833]
 [0.854]
 [0.833]
 [0.902]]
maxi score, test score, baseline:  0.03814000000000002 0.7470000000000001 0.7470000000000001
probs:  [0.07129711593612126, 0.2880736068822471, 0.35521602656164347, 0.2854132506199882]
actor:  0 policy actor:  0  step number:  23 total reward:  0.8266666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.8333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7400000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.07847333333333337 0.7640000000000001 0.7640000000000001
probs:  [0.0735292277967192, 0.2879297747847638, 0.36560889488474246, 0.2729321025337746]
actor:  0 policy actor:  1  step number:  38 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.08182000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.0735292277967192, 0.2879297747847638, 0.36560889488474246, 0.2729321025337746]
siam score:  -0.72681165
actor:  0 policy actor:  0  step number:  47 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.497]
 [0.435]
 [0.499]
 [0.435]
 [0.42 ]
 [0.452]] [[1.713]
 [1.504]
 [1.545]
 [1.901]
 [1.545]
 [1.648]
 [1.697]] [[ 0.103]
 [-0.005]
 [-0.046]
 [ 0.196]
 [-0.046]
 [-0.01 ]
 [ 0.047]]
maxi score, test score, baseline:  0.0847266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.0735292277967192, 0.2879297747847638, 0.36560889488474246, 0.2729321025337746]
actor:  1 policy actor:  1  step number:  48 total reward:  0.6466666666666671  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.361]
 [0.375]
 [0.361]
 [0.361]
 [0.361]
 [0.36 ]] [[3.853]
 [3.208]
 [3.05 ]
 [3.208]
 [3.208]
 [3.208]
 [4.019]] [[0.574]
 [0.379]
 [0.34 ]
 [0.379]
 [0.379]
 [0.379]
 [0.62 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.371]
 [0.39 ]
 [0.386]
 [0.371]
 [0.381]
 [0.372]] [[3.109]
 [4.035]
 [2.705]
 [2.763]
 [4.035]
 [2.979]
 [3.131]] [[0.327]
 [0.584]
 [0.215]
 [0.229]
 [0.584]
 [0.288]
 [0.326]]
maxi score, test score, baseline:  0.0847266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.0735292277967192, 0.2879297747847638, 0.36560889488474246, 0.2729321025337746]
line 256 mcts: sample exp_bonus 3.2805439970883765
actor:  1 policy actor:  1  step number:  28 total reward:  0.8200000000000001  reward:  1.0 rdn_beta:  0.333
siam score:  -0.727305
maxi score, test score, baseline:  0.0847266666666667 0.7640000000000001 0.7640000000000001
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.306]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.277]] [[2.762]
 [2.008]
 [2.762]
 [2.762]
 [2.762]
 [2.762]
 [1.887]] [[ 0.353]
 [-0.082]
 [ 0.353]
 [ 0.353]
 [ 0.353]
 [ 0.353]
 [-0.182]]
maxi score, test score, baseline:  0.0847266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07077967668940258, 0.3145944669671379, 0.35191500691561467, 0.26271084942784484]
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.669]
 [0.65 ]
 [0.651]
 [0.675]
 [0.697]
 [0.695]] [[0.873]
 [1.509]
 [0.864]
 [0.345]
 [0.365]
 [0.445]
 [0.555]] [[0.57 ]
 [0.806]
 [0.52 ]
 [0.304]
 [0.332]
 [0.384]
 [0.428]]
actor:  1 policy actor:  1  step number:  98 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7256908
maxi score, test score, baseline:  0.0847266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07077967668940258, 0.3145944669671379, 0.35191500691561467, 0.26271084942784484]
actor:  1 policy actor:  1  step number:  64 total reward:  0.4466666666666669  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.0847266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07077967668940258, 0.3145944669671379, 0.35191500691561467, 0.26271084942784484]
actor:  1 policy actor:  1  step number:  56 total reward:  0.5666666666666671  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0847266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07077967668940258, 0.3145944669671379, 0.35191500691561467, 0.26271084942784484]
maxi score, test score, baseline:  0.0847266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07077967668940258, 0.3145944669671379, 0.35191500691561467, 0.26271084942784484]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0847266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07077967668940258, 0.3145944669671379, 0.35191500691561467, 0.26271084942784484]
first move QE:  -0.03730671045343154
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.215]
 [0.174]
 [0.215]
 [0.215]
 [0.249]
 [0.317]] [[1.904]
 [1.904]
 [1.894]
 [1.904]
 [1.904]
 [1.825]
 [2.005]] [[-0.031]
 [-0.031]
 [-0.077]
 [-0.031]
 [-0.031]
 [-0.037]
 [ 0.122]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.494]
 [-0.494]
 [-0.494]
 [-0.494]
 [-0.494]
 [-0.494]
 [-0.494]]
first move QE:  -0.03730671045343154
actor:  1 policy actor:  1  step number:  41 total reward:  0.6800000000000003  reward:  1.0 rdn_beta:  0.5
siam score:  -0.73022187
actor:  0 policy actor:  0  step number:  32 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08818000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07077967668940258, 0.3145944669671379, 0.35191500691561467, 0.26271084942784484]
maxi score, test score, baseline:  0.08818000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07077967668940258, 0.3145944669671379, 0.35191500691561467, 0.26271084942784484]
from probs:  [0.07077967668940258, 0.3145944669671379, 0.35191500691561467, 0.26271084942784484]
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.155]
 [0.141]
 [0.105]
 [0.105]
 [0.105]
 [0.157]] [[1.93 ]
 [1.566]
 [1.648]
 [1.93 ]
 [1.93 ]
 [1.93 ]
 [2.079]] [[-0.059]
 [-0.191]
 [-0.164]
 [-0.059]
 [-0.059]
 [-0.059]
 [ 0.068]]
start point for exploration sampling:  11715
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.2110830868384825
maxi score, test score, baseline:  0.08818000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07077967668940258, 0.3145944669671379, 0.35191500691561467, 0.26271084942784484]
first move QE:  -0.03661988062395095
maxi score, test score, baseline:  0.08818000000000004 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.08818000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07077967668940258, 0.3145944669671379, 0.35191500691561467, 0.26271084942784484]
actor:  0 policy actor:  0  step number:  42 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  0.333
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0605],
        [ 0.6247],
        [-0.0000],
        [-0.6002],
        [-0.0000],
        [ 0.5706],
        [-0.0000],
        [ 0.1180],
        [ 0.4552],
        [ 0.0085]], dtype=torch.float64)
-0.032346567066 -0.09289042506788714
-0.032346567066 0.5923135913654214
-0.858 -0.858
-0.084359833866 -0.6845277588361006
-0.9768 -0.9768
-0.032346567066 0.5382857145070218
-0.924 -0.924
-0.04528388706599999 0.07271819083772149
-0.09703970119800001 0.3581279046210878
-0.032346567066 -0.023819857151637924
maxi score, test score, baseline:  0.08819333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07077967668940258, 0.3145944669671379, 0.35191500691561467, 0.26271084942784484]
actor:  1 policy actor:  1  step number:  48 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.08819333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07077045765648153, 0.31462667490100205, 0.35186862571048305, 0.2627342417320335]
maxi score, test score, baseline:  0.08819333333333336 0.7640000000000001 0.7640000000000001
from probs:  [0.07077045765648153, 0.31462667490100205, 0.35186862571048305, 0.2627342417320335]
maxi score, test score, baseline:  0.08819333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07077045765648153, 0.31462667490100205, 0.35186862571048305, 0.2627342417320335]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.08819333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529717, 0.3178939530670915, 0.35552283170827265, 0.2550790404893387]
maxi score, test score, baseline:  0.08819333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529717, 0.3178939530670915, 0.35552283170827265, 0.2550790404893387]
maxi score, test score, baseline:  0.08819333333333336 0.7640000000000001 0.7640000000000001
actor:  1 policy actor:  1  step number:  56 total reward:  0.566666666666667  reward:  1.0 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  0.08819333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529719, 0.3178939530670914, 0.3555228317082727, 0.2550790404893388]
from probs:  [0.07150417473529719, 0.3178939530670914, 0.3555228317082727, 0.2550790404893388]
Printing some Q and Qe and total Qs values:  [[0.909]
 [0.929]
 [0.906]
 [0.86 ]
 [0.849]
 [0.853]
 [0.898]] [[0.243]
 [0.482]
 [0.47 ]
 [0.338]
 [0.311]
 [0.529]
 [0.128]] [[0.909]
 [0.929]
 [0.906]
 [0.86 ]
 [0.849]
 [0.853]
 [0.898]]
actor:  0 policy actor:  0  step number:  41 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0878066666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529719, 0.3178939530670914, 0.3555228317082727, 0.2550790404893388]
maxi score, test score, baseline:  0.0878066666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529719, 0.3178939530670914, 0.3555228317082727, 0.2550790404893388]
actor:  0 policy actor:  1  step number:  65 total reward:  0.3599999999999993  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  27 total reward:  0.8266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  48 total reward:  0.5  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.4104286334284286
maxi score, test score, baseline:  0.09147333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529719, 0.3178939530670914, 0.3555228317082727, 0.2550790404893388]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[2.121]
 [2.121]
 [2.121]
 [2.121]
 [2.121]
 [2.121]
 [2.121]] [[0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09147333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529719, 0.3178939530670914, 0.3555228317082727, 0.2550790404893388]
actor:  1 policy actor:  1  step number:  41 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09147333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529717, 0.3178939530670915, 0.35552283170827265, 0.2550790404893387]
actor:  1 policy actor:  1  step number:  45 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [ 0.171]
 [-0.022]] [[1.673]
 [1.673]
 [1.673]
 [1.673]
 [1.673]
 [1.76 ]
 [1.673]] [[0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.377]
 [0.195]]
siam score:  -0.74552053
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.32 ]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]] [[1.672]
 [2.19 ]
 [1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.951]] [[0.057]
 [0.288]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.6600000000000004  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[1.44 ]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]] [[0.488]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]]
maxi score, test score, baseline:  0.09147333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529717, 0.3178939530670915, 0.35552283170827265, 0.2550790404893387]
maxi score, test score, baseline:  0.09147333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529717, 0.3178939530670915, 0.35552283170827265, 0.2550790404893387]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09147333333333336 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.09147333333333336 0.7640000000000001 0.7640000000000001
actor:  1 policy actor:  1  step number:  83 total reward:  0.3333333333333337  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.09147333333333336 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.09147333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529717, 0.3178939530670915, 0.35552283170827265, 0.2550790404893387]
actor:  0 policy actor:  0  step number:  42 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0945266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529717, 0.3178939530670915, 0.35552283170827265, 0.2550790404893387]
siam score:  -0.7430365
maxi score, test score, baseline:  0.0945266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529717, 0.3178939530670915, 0.35552283170827265, 0.2550790404893387]
maxi score, test score, baseline:  0.0945266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529717, 0.3178939530670915, 0.35552283170827265, 0.2550790404893387]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.0945266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529717, 0.3178939530670915, 0.35552283170827265, 0.2550790404893387]
maxi score, test score, baseline:  0.0945266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529717, 0.3178939530670915, 0.35552283170827265, 0.2550790404893387]
maxi score, test score, baseline:  0.0945266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529717, 0.3178939530670915, 0.35552283170827265, 0.2550790404893387]
maxi score, test score, baseline:  0.0945266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07150417473529717, 0.3178939530670915, 0.35552283170827265, 0.2550790404893387]
actor:  1 policy actor:  1  step number:  56 total reward:  0.6200000000000004  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7480796
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.0945266666666667 0.7640000000000001 0.7640000000000001
actor:  1 policy actor:  1  step number:  77 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0945266666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290582, 0.35384770540747995, 0.2558877434183634]
actor:  0 policy actor:  0  step number:  32 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.626]
 [0.61 ]
 [0.541]
 [0.541]
 [0.541]
 [0.621]] [[0.61 ]
 [0.863]
 [0.801]
 [0.892]
 [0.892]
 [0.892]
 [0.778]] [[0.452]
 [0.58 ]
 [0.542]
 [0.522]
 [0.522]
 [0.522]
 [0.541]]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.6066666666666671  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0947666666666667 0.7640000000000001 0.7640000000000001
actor:  1 policy actor:  1  step number:  35 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.5600000000000002  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[1.924]
 [1.924]
 [1.924]
 [1.924]
 [1.924]
 [1.924]
 [1.924]] [[0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]]
maxi score, test score, baseline:  0.0947666666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290582, 0.35384770540747995, 0.2558877434183634]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.288]
 [0.266]
 [0.263]
 [0.259]
 [0.266]
 [0.343]] [[-0.17 ]
 [ 0.185]
 [ 0.509]
 [ 0.421]
 [ 0.592]
 [ 0.509]
 [ 0.281]] [[-0.334]
 [-0.396]
 [-0.364]
 [-0.381]
 [-0.357]
 [-0.364]
 [-0.324]]
actor:  1 policy actor:  1  step number:  67 total reward:  0.5333333333333339  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0947666666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290581, 0.35384770540748, 0.25588774341836346]
actor:  1 policy actor:  1  step number:  41 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0947666666666667 0.7640000000000001 0.7640000000000001
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.392]
 [0.287]
 [0.306]
 [0.31 ]
 [0.269]
 [0.357]] [[1.012]
 [1.005]
 [0.629]
 [0.957]
 [1.012]
 [2.027]
 [1.655]] [[-0.328]
 [-0.248]
 [-0.415]
 [-0.342]
 [-0.328]
 [-0.2  ]
 [-0.173]]
maxi score, test score, baseline:  0.0947666666666667 0.7640000000000001 0.7640000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0947666666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290581, 0.35384770540748, 0.25588774341836346]
actor:  1 policy actor:  1  step number:  57 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  0.5
siam score:  -0.74302953
actor:  1 policy actor:  1  step number:  48 total reward:  0.5  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0947666666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290581, 0.35384770540748, 0.25588774341836346]
maxi score, test score, baseline:  0.0947666666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290582, 0.35384770540747995, 0.2558877434183634]
maxi score, test score, baseline:  0.0947666666666667 0.7640000000000001 0.7640000000000001
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]] [[1.143]
 [1.24 ]
 [1.24 ]
 [1.24 ]
 [1.24 ]
 [1.24 ]
 [1.24 ]] [[-0.08 ]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]]
using explorer policy with actor:  0
siam score:  -0.7417894
maxi score, test score, baseline:  0.09254000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290582, 0.35384770540747995, 0.2558877434183634]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.7067],
        [-0.5833],
        [-0.8721],
        [-0.6097],
        [-0.0000],
        [-0.0000],
        [ 0.0704],
        [-0.0000],
        [-0.0000],
        [-0.6111]], dtype=torch.float64)
-0.032346567066 0.6743549499351358
-0.09703970119800001 -0.6803702644754827
-0.032346567066 -0.9044369844624458
-0.032346567066 -0.6420436371429465
-0.6988119600000002 -0.6988119600000002
-0.5759074200000003 -0.5759074200000003
-0.032346567066 0.03805591036796494
-0.916158936 -0.916158936
0.99 0.99
-0.032346567066 -0.6434376701783355
maxi score, test score, baseline:  0.09254000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290582, 0.35384770540747995, 0.2558877434183634]
maxi score, test score, baseline:  0.09254000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290582, 0.35384770540747995, 0.2558877434183634]
actor:  1 policy actor:  1  step number:  79 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09254000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290582, 0.35384770540747995, 0.2558877434183634]
maxi score, test score, baseline:  0.08923333333333336 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.08923333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290582, 0.35384770540747995, 0.2558877434183634]
actor:  1 policy actor:  1  step number:  55 total reward:  0.6000000000000004  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  50 total reward:  0.40666666666666673  reward:  1.0 rdn_beta:  0.5
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3199668591788534
maxi score, test score, baseline:  0.0920466666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290582, 0.35384770540747995, 0.2558877434183634]
maxi score, test score, baseline:  0.0920466666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290582, 0.35384770540747995, 0.2558877434183634]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0920466666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290582, 0.35384770540747995, 0.2558877434183634]
maxi score, test score, baseline:  0.0920466666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07117126154509845, 0.3190932896290582, 0.35384770540747995, 0.2558877434183634]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0920466666666667 0.7640000000000001 0.7640000000000001
Printing some Q and Qe and total Qs values:  [[0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.038]] [[1.441]
 [1.441]
 [1.441]
 [1.441]
 [1.441]
 [1.441]
 [4.165]] [[-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [ 0.543]]
maxi score, test score, baseline:  0.0920466666666667 0.7640000000000001 0.7640000000000001
siam score:  -0.7424904
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0920466666666667 0.7640000000000001 0.7640000000000001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.4963375657596626
actor:  1 policy actor:  1  step number:  101 total reward:  0.18666666666666643  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0920466666666667 0.7640000000000001 0.7640000000000001
probs:  [0.06738645425814313, 0.3347904733631345, 0.33120490793499646, 0.2666181644437259]
maxi score, test score, baseline:  0.0920466666666667 0.7640000000000001 0.7640000000000001
probs:  [0.06738645425814313, 0.3347904733631345, 0.33120490793499646, 0.2666181644437259]
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.398]
 [0.299]
 [0.246]
 [0.283]
 [0.268]
 [0.242]] [[1.332]
 [1.212]
 [1.245]
 [1.412]
 [1.225]
 [1.403]
 [1.396]] [[-0.27 ]
 [-0.167]
 [-0.255]
 [-0.253]
 [-0.278]
 [-0.234]
 [-0.262]]
actor:  0 policy actor:  1  step number:  47 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09235333333333336 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.09235333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.06738645425814313, 0.3347904733631345, 0.33120490793499646, 0.2666181644437259]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.383]
 [-0.383]
 [-0.383]
 [-0.383]
 [-0.383]
 [-0.383]
 [-0.383]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5933333333333334  reward:  1.0 rdn_beta:  0.333
from probs:  [0.06738645425814313, 0.3347904733631345, 0.33120490793499646, 0.2666181644437259]
maxi score, test score, baseline:  0.09235333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.06725457006392102, 0.3341268388353421, 0.3316765425585844, 0.2669420485421525]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09235333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.06725457006392102, 0.3341268388353421, 0.3316765425585844, 0.2669420485421525]
maxi score, test score, baseline:  0.09235333333333336 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.09235333333333336 0.7640000000000001 0.7640000000000001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.382]
 [0.429]
 [0.354]
 [0.358]
 [0.122]
 [0.38 ]] [[2.063]
 [1.789]
 [2.374]
 [2.208]
 [1.386]
 [1.065]
 [2.064]] [[-0.024]
 [-0.114]
 [ 0.128]
 [-0.003]
 [-0.272]
 [-0.615]
 [-0.024]]
from probs:  [0.06725457006392102, 0.3341268388353421, 0.3316765425585844, 0.2669420485421525]
maxi score, test score, baseline:  0.09235333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.06725457006392102, 0.3341268388353421, 0.3316765425585844, 0.2669420485421525]
actor:  1 policy actor:  1  step number:  24 total reward:  0.8466666666666667  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]] [[0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]]
siam score:  -0.74195755
actor:  0 policy actor:  0  step number:  52 total reward:  0.4733333333333335  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]] [[0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]]
maxi score, test score, baseline:  0.09280666666666669 0.7640000000000001 0.7640000000000001
actor:  1 policy actor:  1  step number:  40 total reward:  0.7266666666666669  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  11715
actor:  1 policy actor:  1  step number:  44 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.09280666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
maxi score, test score, baseline:  0.09280666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
maxi score, test score, baseline:  0.09280666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
maxi score, test score, baseline:  0.09280666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
maxi score, test score, baseline:  0.09280666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
actor:  1 policy actor:  1  step number:  73 total reward:  0.3466666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.08955333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
maxi score, test score, baseline:  0.08955333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
maxi score, test score, baseline:  0.08955333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
maxi score, test score, baseline:  0.08955333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
actor:  1 policy actor:  1  step number:  87 total reward:  0.3600000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.08955333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
maxi score, test score, baseline:  0.08955333333333335 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.08955333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
actor:  1 policy actor:  1  step number:  43 total reward:  0.626666666666667  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.014]
 [-0.024]
 [-0.024]
 [-0.013]
 [-0.022]
 [-0.014]] [[1.001]
 [1.496]
 [1.278]
 [1.11 ]
 [1.459]
 [1.231]
 [1.427]] [[-0.39 ]
 [-0.052]
 [-0.207]
 [-0.319]
 [-0.076]
 [-0.237]
 [-0.098]]
maxi score, test score, baseline:  0.08955333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08955333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
maxi score, test score, baseline:  0.08955333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
maxi score, test score, baseline:  0.08955333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07626357021204219, 0.3794491169411416, 0.2982358680692696, 0.24605144477754667]
line 256 mcts: sample exp_bonus 1.7901026389849264
maxi score, test score, baseline:  0.08955333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.08955333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08955333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
actor:  0 policy actor:  1  step number:  48 total reward:  0.6733333333333337  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08954000000000002 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.002]
 [-0.185]
 [-0.002]
 [-0.002]
 [-0.185]
 [-0.002]] [[0.001]
 [0.003]
 [0.095]
 [0.001]
 [0.   ]
 [0.095]
 [0.001]] [[-0.   ]
 [ 0.001]
 [-0.137]
 [-0.001]
 [-0.001]
 [-0.137]
 [-0.   ]]
siam score:  -0.73110425
actor:  0 policy actor:  0  step number:  43 total reward:  0.52  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.682]
 [0.683]
 [0.611]
 [0.002]
 [0.275]
 [0.589]] [[0.808]
 [0.699]
 [1.016]
 [1.676]
 [0.492]
 [0.685]
 [1.077]] [[0.037]
 [0.682]
 [0.683]
 [0.611]
 [0.002]
 [0.275]
 [0.589]]
maxi score, test score, baseline:  0.08732666666666669 0.7640000000000001 0.7640000000000001
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.873]
 [0.761]
 [0.788]
 [0.822]
 [0.768]
 [0.789]] [[0.544]
 [0.307]
 [0.488]
 [0.547]
 [0.263]
 [0.533]
 [0.656]] [[0.78 ]
 [0.873]
 [0.761]
 [0.788]
 [0.822]
 [0.768]
 [0.789]]
maxi score, test score, baseline:  0.08732666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
line 256 mcts: sample exp_bonus -0.738462089149904
actor:  0 policy actor:  0  step number:  45 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09039333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
actor:  0 policy actor:  1  step number:  47 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
Printing some Q and Qe and total Qs values:  [[0.09 ]
 [0.165]
 [0.114]
 [0.113]
 [0.13 ]
 [0.104]
 [0.11 ]] [[3.201]
 [2.295]
 [3.097]
 [2.952]
 [3.115]
 [3.138]
 [2.704]] [[0.525]
 [0.383]
 [0.518]
 [0.487]
 [0.531]
 [0.52 ]
 [0.434]]
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.588]
 [0.427]
 [0.481]
 [0.472]
 [0.481]
 [0.481]] [[0.768]
 [2.206]
 [1.095]
 [1.96 ]
 [0.89 ]
 [1.96 ]
 [1.96 ]] [[-0.315]
 [ 0.248]
 [-0.284]
 [ 0.058]
 [-0.306]
 [ 0.058]
 [ 0.058]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
siam score:  -0.7208513
actor:  1 policy actor:  1  step number:  47 total reward:  0.6800000000000004  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
actor:  1 policy actor:  1  step number:  44 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.781]
 [0.763]
 [0.736]
 [0.555]
 [0.689]
 [0.718]] [[1.483]
 [1.922]
 [1.471]
 [1.3  ]
 [0.44 ]
 [1.594]
 [1.864]] [[0.695]
 [0.781]
 [0.763]
 [0.736]
 [0.555]
 [0.689]
 [0.718]]
actor:  0 policy actor:  0  step number:  41 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09078000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.09078000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
actor:  1 policy actor:  1  step number:  43 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]] [[1.818]
 [1.818]
 [1.818]
 [1.818]
 [1.818]
 [1.818]
 [1.818]] [[0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]]
maxi score, test score, baseline:  0.09078000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.09078000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.326]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.345]] [[2.116]
 [1.975]
 [1.985]
 [1.985]
 [1.985]
 [1.985]
 [2.382]] [[0.196]
 [0.102]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.3  ]]
maxi score, test score, baseline:  0.09078000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.09078000000000004 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.09078000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
actor:  1 policy actor:  1  step number:  71 total reward:  0.4800000000000004  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09078000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.09078000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
Printing some Q and Qe and total Qs values:  [[0.17 ]
 [0.226]
 [0.181]
 [0.181]
 [0.177]
 [0.175]
 [0.166]] [[0.289]
 [0.591]
 [0.336]
 [0.331]
 [0.313]
 [0.272]
 [0.292]] [[-0.613]
 [-0.507]
 [-0.594]
 [-0.595]
 [-0.602]
 [-0.611]
 [-0.617]]
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.246]
 [0.238]
 [0.246]
 [0.246]
 [0.235]
 [0.272]] [[0.834]
 [1.087]
 [1.038]
 [1.087]
 [1.087]
 [0.963]
 [1.069]] [[0.275]
 [0.307]
 [0.291]
 [0.307]
 [0.307]
 [0.277]
 [0.329]]
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.323]
 [0.363]
 [0.323]
 [0.323]
 [0.33 ]
 [0.323]] [[2.539]
 [2.539]
 [3.475]
 [2.539]
 [2.539]
 [2.25 ]
 [2.539]] [[0.369]
 [0.369]
 [0.686]
 [0.369]
 [0.369]
 [0.28 ]
 [0.369]]
maxi score, test score, baseline:  0.09078000000000004 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.09078000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
actor:  1 policy actor:  1  step number:  77 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
actor:  1 policy actor:  1  step number:  56 total reward:  0.606666666666667  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  63 total reward:  0.3600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09078000000000004 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.09078000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.013]
 [-0.019]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]] [[-0.858]
 [ 0.459]
 [-0.271]
 [ 0.18 ]
 [ 0.18 ]
 [ 0.18 ]
 [ 0.18 ]] [[-0.791]
 [-0.564]
 [-0.693]
 [-0.611]
 [-0.611]
 [-0.611]
 [-0.611]]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]] [[0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]]
from probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.09078000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
start point for exploration sampling:  11715
from probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
actor:  0 policy actor:  0  step number:  41 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09087333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
Printing some Q and Qe and total Qs values:  [[0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]] [[-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]] [[0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]]
actor:  0 policy actor:  0  step number:  34 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.09086000000000002 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
actor:  0 policy actor:  1  step number:  51 total reward:  0.4666666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09115333333333338 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.09115333333333338 0.7640000000000001 0.7640000000000001
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  0.09115333333333338 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.939]] [[0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.709]] [[0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.939]]
maxi score, test score, baseline:  0.09115333333333338 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
siam score:  -0.7248976
actor:  0 policy actor:  1  step number:  56 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[ 0.376]
 [ 0.381]
 [-0.133]
 [ 0.313]
 [ 0.271]
 [-0.148]
 [ 0.33 ]] [[ 1.999]
 [ 1.851]
 [-0.612]
 [ 1.397]
 [ 1.057]
 [-0.857]
 [ 1.58 ]] [[ 0.577]
 [ 0.556]
 [-0.097]
 [ 0.448]
 [ 0.372]
 [-0.144]
 [ 0.486]]
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]] [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]] [[0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.489]] [[2.168]
 [2.168]
 [2.168]
 [2.168]
 [2.168]
 [2.168]
 [1.752]] [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.347]]
using another actor
maxi score, test score, baseline:  0.0938066666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
actor:  0 policy actor:  1  step number:  44 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09414000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
maxi score, test score, baseline:  0.09414000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
actor:  1 policy actor:  1  step number:  95 total reward:  0.0799999999999983  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09414000000000003 0.7640000000000001 0.7640000000000001
from probs:  [0.07700206433918835, 0.3831292741709427, 0.30112805232268575, 0.23874060916718315]
actor:  1 policy actor:  1  step number:  57 total reward:  0.6266666666666671  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09414000000000003 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.09414000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.811]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.798]] [[ 0.908]
 [-0.395]
 [ 0.908]
 [ 0.908]
 [ 0.908]
 [ 0.908]
 [-0.349]] [[0.782]
 [0.811]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.798]]
maxi score, test score, baseline:  0.09414000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.893]
 [0.926]
 [0.883]
 [0.81 ]
 [0.83 ]
 [0.892]] [[1.108]
 [0.798]
 [1.205]
 [0.642]
 [0.581]
 [0.796]
 [0.719]] [[0.81 ]
 [0.893]
 [0.926]
 [0.883]
 [0.81 ]
 [0.83 ]
 [0.892]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.6800000000000003  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.09414000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
actor:  0 policy actor:  0  step number:  48 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
siam score:  -0.7403109
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.465]
 [0.438]
 [0.437]
 [0.441]
 [0.426]
 [0.413]] [[2.928]
 [1.55 ]
 [1.555]
 [1.568]
 [1.737]
 [2.532]
 [4.033]] [[0.443]
 [0.153]
 [0.145]
 [0.147]
 [0.186]
 [0.357]
 [0.684]]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.264]
 [0.242]
 [0.239]
 [0.239]
 [0.237]
 [0.168]] [[2.683]
 [1.575]
 [2.343]
 [2.259]
 [2.173]
 [2.314]
 [5.527]] [[ 0.081]
 [-0.131]
 [ 0.014]
 [-0.004]
 [-0.021]
 [ 0.006]
 [ 0.623]]
actor:  1 policy actor:  1  step number:  87 total reward:  0.42666666666666686  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  124 total reward:  0.1799999999999986  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
line 256 mcts: sample exp_bonus 2.2703246869533857
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]] [[2.86 ]
 [2.811]
 [2.811]
 [2.811]
 [2.811]
 [2.811]
 [2.811]] [[0.155]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]]
actor:  1 policy actor:  1  step number:  128 total reward:  0.11333333333333162  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
Printing some Q and Qe and total Qs values:  [[-0.119]
 [-0.097]
 [-0.111]
 [-0.095]
 [-0.092]
 [-0.097]
 [-0.107]] [[-0.509]
 [-0.316]
 [-0.353]
 [-0.265]
 [-0.483]
 [-0.327]
 [-0.462]] [[-0.382]
 [-0.296]
 [-0.322]
 [-0.277]
 [-0.347]
 [-0.3  ]
 [-0.355]]
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.699]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.636]] [[0.828]
 [0.507]
 [0.828]
 [0.828]
 [0.828]
 [0.828]
 [1.717]] [[0.198]
 [0.073]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.614]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.7266666666666669  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.053]
 [ 0.053]
 [ 0.053]
 [-0.154]
 [ 0.053]
 [-0.149]
 [ 0.053]] [[ 0.047]
 [ 0.047]
 [ 0.047]
 [-0.016]
 [ 0.047]
 [-0.019]
 [ 0.047]] [[ 0.023]
 [ 0.023]
 [ 0.023]
 [-0.195]
 [ 0.023]
 [-0.19 ]
 [ 0.023]]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
siam score:  -0.747538
first move QE:  -0.022464343202814735
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[1.883]
 [1.883]
 [1.883]
 [1.883]
 [1.883]
 [1.883]
 [1.883]] [[-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.317]
 [0.454]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[1.572]
 [1.572]
 [1.64 ]
 [1.572]
 [1.572]
 [1.572]
 [1.572]] [[-0.039]
 [-0.039]
 [ 0.118]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
actor:  1 policy actor:  1  step number:  36 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
actor:  1 policy actor:  1  step number:  57 total reward:  0.4533333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.28 ]
 [0.099]
 [0.123]
 [0.164]
 [0.164]
 [0.154]] [[1.688]
 [1.239]
 [1.152]
 [1.713]
 [1.708]
 [1.708]
 [1.512]] [[0.503]
 [0.46 ]
 [0.323]
 [0.508]
 [0.532]
 [0.532]
 [0.466]]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
siam score:  -0.7422962
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.497]
 [0.497]
 [0.497]
 [0.512]
 [0.497]
 [0.405]] [[1.151]
 [2.179]
 [2.179]
 [2.179]
 [0.957]
 [2.179]
 [1.516]] [[0.243]
 [0.509]
 [0.509]
 [0.509]
 [0.1  ]
 [0.509]
 [0.218]]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
start point for exploration sampling:  11715
from probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.329]
 [0.363]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[3.039]
 [3.039]
 [2.27 ]
 [3.039]
 [3.039]
 [3.039]
 [3.039]] [[0.579]
 [0.579]
 [0.161]
 [0.579]
 [0.579]
 [0.579]
 [0.579]]
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]] [[1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]] [[0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]]
maxi score, test score, baseline:  0.09343333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333337 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
actor:  1 policy actor:  1  step number:  78 total reward:  0.4333333333333338  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09343333333333337 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.09343333333333337 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.446]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[1.688]
 [1.657]
 [1.688]
 [1.688]
 [1.688]
 [1.688]
 [1.688]] [[0.288]
 [0.345]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]]
maxi score, test score, baseline:  0.09343333333333337 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.09343333333333337 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.467]
 [0.379]
 [0.373]
 [0.373]
 [0.378]
 [0.368]] [[1.348]
 [1.275]
 [1.234]
 [2.827]
 [2.827]
 [1.244]
 [1.153]] [[ 0.056]
 [ 0.078]
 [ 0.022]
 [ 0.538]
 [ 0.538]
 [ 0.025]
 [-0.01 ]]
maxi score, test score, baseline:  0.09343333333333337 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
actor:  1 policy actor:  1  step number:  53 total reward:  0.6400000000000003  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09343333333333337 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.09343333333333337 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
from probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [ 0.029]] [[1.986]
 [1.986]
 [1.986]
 [1.986]
 [1.986]
 [1.986]
 [2.676]] [[0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.505]]
maxi score, test score, baseline:  0.08992666666666671 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
siam score:  -0.7334967
maxi score, test score, baseline:  0.08992666666666671 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
line 256 mcts: sample exp_bonus 1.1391397408503439
Printing some Q and Qe and total Qs values:  [[0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]] [[0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]] [[0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]]
maxi score, test score, baseline:  0.08992666666666671 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
siam score:  -0.7327995
actor:  0 policy actor:  0  step number:  37 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.218]
 [0.153]
 [0.113]
 [0.113]
 [0.113]
 [0.158]] [[1.989]
 [1.697]
 [1.707]
 [1.989]
 [1.989]
 [1.989]
 [2.933]] [[0.372]
 [0.35 ]
 [0.317]
 [0.372]
 [0.372]
 [0.372]
 [0.65 ]]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.036]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.061]] [[1.209]
 [1.318]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.486]] [[-0.51 ]
 [-0.493]
 [-0.51 ]
 [-0.51 ]
 [-0.51 ]
 [-0.51 ]
 [-0.412]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.08958000000000003 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.08958000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.08958000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.08958000000000003 0.7640000000000001 0.7640000000000001
from probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.08958000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
actor:  1 policy actor:  1  step number:  58 total reward:  0.6200000000000004  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.894]
 [0.082]
 [0.781]
 [0.782]
 [0.782]
 [0.786]] [[1.192]
 [1.185]
 [0.25 ]
 [1.499]
 [1.269]
 [1.269]
 [1.643]] [[0.124]
 [0.894]
 [0.082]
 [0.781]
 [0.782]
 [0.782]
 [0.786]]
maxi score, test score, baseline:  0.08614000000000002 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
maxi score, test score, baseline:  0.08275333333333337 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
line 256 mcts: sample exp_bonus 2.5118911402048187
maxi score, test score, baseline:  0.08275333333333337 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.316]
 [0.261]
 [0.268]
 [0.268]
 [0.268]
 [0.268]] [[2.163]
 [1.232]
 [1.647]
 [2.163]
 [2.163]
 [2.163]
 [2.163]] [[ 0.298]
 [-0.12 ]
 [ 0.032]
 [ 0.298]
 [ 0.298]
 [ 0.298]
 [ 0.298]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.08275333333333337 0.7640000000000001 0.7640000000000001
probs:  [0.07643912928234115, 0.3802960692757336, 0.30319168666384244, 0.24007311477808294]
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.339]] [[1.467]
 [1.467]
 [1.467]
 [1.467]
 [1.467]
 [1.467]
 [6.056]] [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.708]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.5466666666666672  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [ 0.009]] [[3.294]
 [3.294]
 [3.294]
 [3.294]
 [3.294]
 [3.294]
 [3.296]] [[0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.233]]
Printing some Q and Qe and total Qs values:  [[-0.058]
 [-0.058]
 [-0.058]
 [-0.058]
 [-0.058]
 [-0.058]
 [-0.041]] [[2.796]
 [2.796]
 [2.796]
 [2.796]
 [2.796]
 [2.796]
 [2.759]] [[0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.16 ]]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.02  0.061 0.061 0.02  0.02  0.02  0.796]
maxi score, test score, baseline:  0.07936666666666671 0.7640000000000001 0.7640000000000001
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.086]
 [0.048]
 [0.081]
 [0.081]
 [0.081]
 [0.087]] [[2.551]
 [1.275]
 [1.167]
 [2.551]
 [2.551]
 [2.551]
 [3.367]] [[0.44 ]
 [0.057]
 [0.007]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.689]]
maxi score, test score, baseline:  0.07936666666666671 0.7640000000000001 0.7640000000000001
probs:  [0.07651502025467816, 0.38067423098646186, 0.30349314649418635, 0.23931760226467358]
maxi score, test score, baseline:  0.07936666666666671 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.07936666666666671 0.7640000000000001 0.7640000000000001
probs:  [0.07651502025467816, 0.38067423098646186, 0.30349314649418635, 0.23931760226467358]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07936666666666671 0.7640000000000001 0.7640000000000001
probs:  [0.07651502025467816, 0.38067423098646186, 0.30349314649418635, 0.23931760226467358]
maxi score, test score, baseline:  0.07936666666666671 0.7640000000000001 0.7640000000000001
probs:  [0.07651502025467816, 0.38067423098646186, 0.30349314649418635, 0.23931760226467358]
maxi score, test score, baseline:  0.07936666666666671 0.7640000000000001 0.7640000000000001
probs:  [0.07651502025467816, 0.38067423098646186, 0.30349314649418635, 0.23931760226467358]
maxi score, test score, baseline:  0.07936666666666671 0.7640000000000001 0.7640000000000001
probs:  [0.07651502025467816, 0.38067423098646186, 0.30349314649418635, 0.23931760226467358]
actor:  0 policy actor:  1  step number:  52 total reward:  0.6200000000000003  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.07911333333333338 0.7640000000000001 0.7640000000000001
probs:  [0.07651502025467816, 0.38067423098646186, 0.30349314649418635, 0.23931760226467358]
siam score:  -0.7309701
start point for exploration sampling:  11715
siam score:  -0.7311233
maxi score, test score, baseline:  0.07911333333333338 0.7640000000000001 0.7640000000000001
probs:  [0.07651502025467816, 0.38067423098646186, 0.30349314649418635, 0.23931760226467358]
maxi score, test score, baseline:  0.07572666666666669 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.07572666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07651502025467816, 0.38067423098646186, 0.30349314649418635, 0.23931760226467358]
maxi score, test score, baseline:  0.07572666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07554264030926909, 0.3757802483564166, 0.30706935990832085, 0.24160775142599353]
maxi score, test score, baseline:  0.07572666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07554264030926909, 0.3757802483564166, 0.30706935990832085, 0.24160775142599353]
maxi score, test score, baseline:  0.07572666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07554264030926909, 0.3757802483564166, 0.30706935990832085, 0.24160775142599353]
maxi score, test score, baseline:  0.07572666666666669 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.07572666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07554264030926909, 0.3757802483564166, 0.30706935990832085, 0.24160775142599353]
actor:  1 policy actor:  1  step number:  52 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.07572666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07554264030926909, 0.3757802483564166, 0.30706935990832085, 0.24160775142599353]
maxi score, test score, baseline:  0.07572666666666669 0.7640000000000001 0.7640000000000001
line 256 mcts: sample exp_bonus 0.2216634350486441
actor:  0 policy actor:  0  step number:  50 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.07499333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07554264030926909, 0.3757802483564166, 0.30706935990832085, 0.24160775142599353]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [ 0.184]
 [-0.022]
 [-0.033]
 [-0.036]
 [-0.036]
 [-0.036]] [[1.512]
 [0.666]
 [1.498]
 [1.239]
 [1.512]
 [1.512]
 [1.512]] [[0.332]
 [0.274]
 [0.341]
 [0.248]
 [0.332]
 [0.332]
 [0.332]]
maxi score, test score, baseline:  0.07499333333333336 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.07499333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07164282362659025, 0.3561524935929615, 0.32141208344875133, 0.25079259933169684]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.023]
 [ 1.5  ]
 [-0.019]
 [-0.019]
 [ 1.5  ]
 [ 1.5  ]] [[-0.263]
 [ 0.319]
 [ 0.   ]
 [-0.001]
 [-0.01 ]
 [ 0.   ]
 [ 0.   ]] [[-0.027]
 [ 0.263]
 [ 1.627]
 [ 0.107]
 [ 0.102]
 [ 1.627]
 [ 1.627]]
maxi score, test score, baseline:  0.07499333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07164282362659025, 0.3561524935929615, 0.32141208344875133, 0.25079259933169684]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.07499333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07164282362659025, 0.3561524935929615, 0.32141208344875133, 0.25079259933169684]
actor:  1 policy actor:  1  step number:  40 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.025]
 [0.295]
 [0.18 ]
 [0.025]
 [0.025]
 [0.025]] [[0.903]
 [0.903]
 [0.937]
 [2.341]
 [0.903]
 [0.903]
 [0.903]] [[-0.805]
 [-0.805]
 [-0.529]
 [-0.41 ]
 [-0.805]
 [-0.805]
 [-0.805]]
from probs:  [0.07164282362659027, 0.35615249359296164, 0.32141208344875144, 0.2507925993316967]
siam score:  -0.7314814
maxi score, test score, baseline:  0.07160666666666671 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.07160666666666671 0.7640000000000001 0.7640000000000001
probs:  [0.07164282362659027, 0.35615249359296164, 0.32141208344875144, 0.2507925993316967]
actor:  1 policy actor:  1  step number:  36 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]] [[1.869]
 [2.326]
 [2.326]
 [2.326]
 [2.326]
 [2.326]
 [2.326]] [[0.748]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]]
maxi score, test score, baseline:  0.07160666666666671 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
from probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
maxi score, test score, baseline:  0.06826000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
maxi score, test score, baseline:  0.06826000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
actor:  0 policy actor:  1  step number:  40 total reward:  0.6200000000000003  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7214663
actor:  1 policy actor:  1  step number:  64 total reward:  0.5533333333333338  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06807333333333336 0.7640000000000001 0.7640000000000001
actor:  0 policy actor:  0  step number:  42 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06767333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
maxi score, test score, baseline:  0.06767333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
actor:  1 policy actor:  1  step number:  83 total reward:  0.3466666666666669  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  44 total reward:  0.5533333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06767333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
Printing some Q and Qe and total Qs values:  [[0.946]
 [0.943]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.94 ]] [[1.451]
 [0.945]
 [1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.077]] [[0.946]
 [0.943]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.94 ]]
maxi score, test score, baseline:  0.06767333333333335 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
maxi score, test score, baseline:  0.06427333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
maxi score, test score, baseline:  0.06427333333333336 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.06427333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
maxi score, test score, baseline:  0.06427333333333336 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
actor:  1 policy actor:  1  step number:  65 total reward:  0.05333333333333257  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[ 0.015]
 [ 0.038]
 [-0.026]
 [-0.001]
 [-0.027]
 [-0.018]
 [-0.032]] [[1.397]
 [1.16 ]
 [0.906]
 [0.996]
 [0.875]
 [0.81 ]
 [0.824]] [[-0.006]
 [-0.103]
 [-0.271]
 [-0.209]
 [-0.287]
 [-0.312]
 [-0.315]]
maxi score, test score, baseline:  0.06092666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
maxi score, test score, baseline:  0.06092666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
actor:  1 policy actor:  1  step number:  56 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06092666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
maxi score, test score, baseline:  0.06092666666666669 0.7640000000000001 0.7640000000000001
siam score:  -0.72898626
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.002]
 [-0.015]
 [-0.018]
 [-0.018]
 [-0.008]
 [-0.018]] [[-1.065]
 [-0.01 ]
 [-0.418]
 [-0.6  ]
 [-0.599]
 [-0.421]
 [-0.548]] [[-0.033]
 [ 0.34 ]
 [ 0.191]
 [ 0.127]
 [ 0.128]
 [ 0.197]
 [ 0.144]]
maxi score, test score, baseline:  0.06092666666666669 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.06092666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
actor:  1 policy actor:  1  step number:  55 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06092666666666669 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.06092666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.06092666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.06092666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
maxi score, test score, baseline:  0.06092666666666669 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.6066666666666671  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  48 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06022000000000004 0.7640000000000001 0.7640000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06022000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
maxi score, test score, baseline:  0.06022000000000004 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.06022000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
maxi score, test score, baseline:  0.06022000000000004 0.7640000000000001 0.7640000000000001
actor:  1 policy actor:  1  step number:  93 total reward:  0.24  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.102]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  0.06022000000000004 0.7640000000000001 0.7640000000000001
using explorer policy with actor:  0
maxi score, test score, baseline:  0.06022000000000004 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
actor:  0 policy actor:  0  step number:  38 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  28 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.060593333333333374 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.060593333333333374 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
maxi score, test score, baseline:  0.060593333333333374 0.7640000000000001 0.7640000000000001
maxi score, test score, baseline:  0.060593333333333374 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
actor:  1 policy actor:  1  step number:  54 total reward:  0.6466666666666671  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7381549
maxi score, test score, baseline:  0.060593333333333374 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.060593333333333374 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472297, 0.35620049923855723, 0.32137700395486857, 0.2507701349918513]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.060593333333333374 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
start point for exploration sampling:  11715
maxi score, test score, baseline:  0.0572866666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
maxi score, test score, baseline:  0.0572866666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
maxi score, test score, baseline:  0.0572866666666667 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.05394000000000003 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
actor:  1 policy actor:  1  step number:  86 total reward:  0.3800000000000002  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11715
Printing some Q and Qe and total Qs values:  [[ 0.049]
 [ 0.049]
 [ 0.228]
 [-0.186]
 [ 0.049]
 [ 0.208]
 [-0.163]] [[0.884]
 [0.884]
 [1.347]
 [0.747]
 [0.884]
 [0.901]
 [0.57 ]] [[-0.008]
 [-0.008]
 [ 0.299]
 [-0.236]
 [-0.008]
 [ 0.117]
 [-0.285]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  36 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.05399333333333337 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.038]
 [-0.228]
 [-0.005]
 [-0.006]
 [-0.009]
 [-0.006]] [[ 0.521]
 [-0.048]
 [ 0.189]
 [ 0.733]
 [ 0.61 ]
 [ 0.113]
 [ 0.359]] [[ 0.107]
 [-0.021]
 [-0.099]
 [ 0.162]
 [ 0.129]
 [-0.004]
 [ 0.063]]
maxi score, test score, baseline:  0.05399333333333337 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
maxi score, test score, baseline:  0.05399333333333337 0.7640000000000001 0.7640000000000001
probs:  [0.07165236181472298, 0.3562004992385573, 0.3213770039548687, 0.2507701349918512]
using explorer policy with actor:  1
append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:211000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:210000
load_in_model:True
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
loaded in model from saved file
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Starting evaluation
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.95 ]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.945]] [[1.493]
 [0.972]
 [1.493]
 [1.493]
 [1.493]
 [1.493]
 [0.869]] [[0.908]
 [0.95 ]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.945]]
main train batch thing paused
add a thread
Adding thread: now have 5 threads
line 256 mcts: sample exp_bonus 0.283427719277232
main train batch thing paused
add a thread
Adding thread: now have 6 threads
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.729]
 [0.678]
 [0.661]
 [0.685]
 [0.694]
 [0.679]] [[ 1.172]
 [-0.342]
 [ 1.583]
 [ 0.834]
 [ 0.201]
 [ 1.195]
 [ 0.987]] [[0.685]
 [0.729]
 [0.678]
 [0.661]
 [0.685]
 [0.694]
 [0.679]]
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.806]
 [0.821]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.748]] [[-0.166]
 [-0.276]
 [-0.166]
 [-0.166]
 [-0.166]
 [-0.166]
 [-0.322]] [[0.806]
 [0.821]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.748]]
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.873]
 [0.862]
 [0.832]
 [0.806]
 [0.81 ]
 [0.862]] [[ 0.208]
 [-0.278]
 [ 0.374]
 [-0.186]
 [ 0.197]
 [ 0.175]
 [-0.154]] [[0.762]
 [0.873]
 [0.862]
 [0.832]
 [0.806]
 [0.81 ]
 [0.862]]
rdn beta is 0 so we're just using the maxi policy
main train batch thing paused
actor:  0 policy actor:  0  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  23 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
actor:  0 policy actor:  0  step number:  24 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
using explorer policy with actor:  1
main train batch thing paused
actor:  1 policy actor:  1  step number:  39 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  0.167
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[ 0.196]
 [ 0.279]
 [ 0.187]
 [ 0.19 ]
 [-0.19 ]
 [ 0.166]
 [ 0.211]] [[-1.068]
 [-1.035]
 [-1.012]
 [-1.174]
 [-1.091]
 [-1.143]
 [-1.132]] [[ 0.098]
 [ 0.18 ]
 [ 0.099]
 [ 0.077]
 [-0.262]
 [ 0.059]
 [ 0.102]]
main train batch thing paused
actor:  0 policy actor:  0  step number:  54 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]]
rdn beta is 0 so we're just using the maxi policy
main train batch thing paused
actor:  1 policy actor:  1  step number:  42 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]] [[-1.544]
 [-1.544]
 [-1.544]
 [-1.544]
 [-1.544]
 [-1.544]
 [-1.544]] [[0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.75 ]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.743]] [[-1.028]
 [-1.063]
 [-1.028]
 [-1.028]
 [-1.028]
 [-1.028]
 [-1.168]] [[0.701]
 [0.75 ]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.743]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  38 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.5
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.309]
 [0.234]
 [0.195]
 [0.205]
 [0.22 ]
 [0.223]] [[-1.303]
 [-1.225]
 [-1.162]
 [-1.166]
 [-1.182]
 [-1.22 ]
 [-1.094]] [[-0.534]
 [-0.478]
 [-0.542]
 [-0.583]
 [-0.575]
 [-0.566]
 [-0.542]]
actor:  0 policy actor:  0  step number:  38 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  39 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.333
main train batch thing paused
main train batch thing paused
UNIT TEST: sample policy line 217 mcts : [0.061 0.429 0.061 0.041 0.122 0.102 0.184]
Printing some Q and Qe and total Qs values:  [[0.92 ]
 [0.92 ]
 [0.956]
 [0.92 ]
 [0.92 ]
 [0.941]
 [0.92 ]] [[-0.971]
 [-0.971]
 [-0.913]
 [-0.971]
 [-0.971]
 [-1.106]
 [-0.971]] [[0.92 ]
 [0.92 ]
 [0.956]
 [0.92 ]
 [0.92 ]
 [0.941]
 [0.92 ]]
main train batch thing paused
actor:  0 policy actor:  0  step number:  39 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  36 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.167
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]] [[-0.883]
 [-0.883]
 [-0.883]
 [-0.883]
 [-0.883]
 [-0.883]
 [-0.883]] [[-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.143 0.347 0.143 0.02  0.02  0.204 0.122]
main train batch thing paused
actor:  1 policy actor:  1  step number:  40 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.167
main train batch thing paused
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.24201941422417586
main train batch thing paused
actor:  1 policy actor:  1  step number:  49 total reward:  0.49333333333333373  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[ 0.029]
 [-0.125]
 [ 0.013]
 [-0.016]
 [ 0.043]
 [ 0.078]
 [-0.008]] [[-0.705]
 [-0.441]
 [-1.374]
 [-1.101]
 [-0.484]
 [-1.504]
 [-0.643]] [[-0.094]
 [-0.203]
 [-0.221]
 [-0.205]
 [-0.042]
 [-0.178]
 [-0.12 ]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  121 total reward:  0.11999999999999877  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
main train batch thing paused
actor:  1 policy actor:  1  step number:  61 total reward:  0.6000000000000005  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[ 0.141]
 [-0.192]
 [ 0.203]
 [ 0.141]
 [ 0.141]
 [ 0.179]
 [-0.147]] [[0.511]
 [0.479]
 [0.961]
 [0.511]
 [0.511]
 [0.762]
 [0.48 ]] [[ 0.079]
 [-0.187]
 [ 0.243]
 [ 0.079]
 [ 0.079]
 [ 0.173]
 [-0.152]]
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.217]
 [0.301]
 [0.195]
 [0.026]
 [0.191]
 [0.188]] [[1.128]
 [1.336]
 [1.143]
 [1.038]
 [1.065]
 [1.011]
 [1.17 ]] [[ 0.134]
 [ 0.201]
 [ 0.171]
 [ 0.056]
 [-0.044]
 [ 0.041]
 [ 0.108]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]] [[2.127]
 [2.127]
 [2.127]
 [2.127]
 [2.127]
 [2.127]
 [2.127]] [[0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]]
main train batch thing paused
actor:  1 policy actor:  1  step number:  33 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.007]
 [ 0.03 ]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [ 0.045]] [[0.98]
 [1.22]
 [0.98]
 [0.98]
 [0.98]
 [0.98]
 [1.32]] [[0.407]
 [0.551]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.611]]
Printing some Q and Qe and total Qs values:  [[0.06]
 [0.06]
 [0.06]
 [0.06]
 [0.06]
 [0.06]
 [0.06]] [[1.144]
 [1.144]
 [1.144]
 [1.144]
 [1.144]
 [1.144]
 [1.144]] [[-0.396]
 [-0.396]
 [-0.396]
 [-0.396]
 [-0.396]
 [-0.396]
 [-0.396]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.5666666666666672  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
actor:  1 policy actor:  1  step number:  42 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.39074122555505314
main train batch thing paused
Training Flag: False
Self play flag: True
add more workers flag:  True
expV_train_flag:  False
expV_train_start_flag:  255000
actor:  1 policy actor:  1  step number:  39 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.233]
 [0.218]
 [0.228]
 [0.172]
 [0.196]
 [0.277]] [[0.907]
 [0.782]
 [0.975]
 [0.66 ]
 [0.823]
 [0.548]
 [0.938]] [[ 0.076]
 [ 0.072]
 [ 0.136]
 [ 0.024]
 [ 0.054]
 [-0.036]
 [ 0.155]]
main train batch thing paused
actor:  1 policy actor:  1  step number:  60 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  46 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  0.333
main train batch thing paused
actor:  1 policy actor:  1  step number:  42 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
using explorer policy with actor:  1
main train batch thing paused
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  0.5
main train batch thing paused
actor:  0 policy actor:  0  step number:  44 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.5
main train batch thing paused
actor:  1 policy actor:  1  step number:  45 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.165]
 [0.098]
 [0.14 ]
 [0.098]
 [0.098]
 [0.129]] [[-0.129]
 [ 0.163]
 [ 0.389]
 [ 0.   ]
 [ 0.389]
 [ 0.389]
 [ 0.036]] [[-0.341]
 [-0.215]
 [-0.207]
 [-0.294]
 [-0.207]
 [-0.207]
 [-0.293]]
main train batch thing paused
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.3551999891888696
line 256 mcts: sample exp_bonus 1.9333426997408174
main train batch thing paused
actor:  0 policy actor:  0  step number:  37 total reward:  0.7600000000000002  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  31 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
main train batch thing paused
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  43 total reward:  0.5866666666666668  reward:  1.0 rdn_beta:  0.167
main train batch thing paused
actor:  0 policy actor:  0  step number:  43 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  68 total reward:  0.31333333333333246  reward:  1.0 rdn_beta:  0.333
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.052]
 [0.068]
 [0.04 ]
 [0.042]
 [0.069]
 [0.043]] [[0.879]
 [0.878]
 [0.942]
 [0.821]
 [0.813]
 [0.826]
 [0.768]] [[-0.463]
 [-0.47 ]
 [-0.422]
 [-0.51 ]
 [-0.512]
 [-0.479]
 [-0.534]]
main train batch thing paused
actor:  1 policy actor:  1  step number:  41 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  0.167
main train batch thing paused
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.6263488975489363
main train batch thing paused
actor:  1 policy actor:  1  step number:  41 total reward:  0.706666666666667  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.06 ]
 [0.125]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]] [[0.525]
 [0.778]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[-0.454]
 [-0.246]
 [-0.454]
 [-0.454]
 [-0.454]
 [-0.454]
 [-0.454]]
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.776]
 [0.67 ]
 [0.652]
 [0.678]
 [0.668]
 [0.681]] [[1.36 ]
 [0.542]
 [0.73 ]
 [0.927]
 [1.191]
 [0.548]
 [0.663]] [[0.653]
 [0.776]
 [0.67 ]
 [0.652]
 [0.678]
 [0.668]
 [0.681]]
main train batch thing paused
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  60 total reward:  0.5133333333333338  reward:  1.0 rdn_beta:  0.333
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.253]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[1.154]
 [1.408]
 [1.154]
 [1.154]
 [1.154]
 [1.154]
 [1.154]] [[-0.178]
 [-0.002]
 [-0.178]
 [-0.178]
 [-0.178]
 [-0.178]
 [-0.178]]
main train batch thing paused
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  54 total reward:  0.6333333333333337  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  86 total reward:  0.3933333333333334  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
actor:  1 policy actor:  1  step number:  38 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.663]
 [0.633]
 [0.668]
 [0.679]
 [0.648]
 [0.675]] [[0.319]
 [0.   ]
 [0.536]
 [0.328]
 [0.174]
 [0.262]
 [0.457]] [[0.749]
 [0.663]
 [0.633]
 [0.668]
 [0.679]
 [0.648]
 [0.675]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
main train batch thing paused
using explorer policy with actor:  1
main train batch thing paused
actor:  1 policy actor:  1  step number:  31 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  39 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  32 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.265]
 [0.31 ]
 [0.265]
 [0.265]
 [0.265]
 [0.265]] [[1.57 ]
 [1.57 ]
 [1.341]
 [1.57 ]
 [1.57 ]
 [1.57 ]
 [1.57 ]] [[0.265]
 [0.265]
 [0.31 ]
 [0.265]
 [0.265]
 [0.265]
 [0.265]]
append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:211000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:210000
load_in_model:True
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
loaded in model from saved file
from probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[-0.011]
 [ 0.182]
 [-0.019]
 [ 0.07 ]
 [-0.011]
 [-0.011]
 [ 0.104]] [[0.999]
 [0.511]
 [0.382]
 [0.745]
 [0.999]
 [0.999]
 [0.62 ]] [[-0.392]
 [-0.361]
 [-0.605]
 [-0.395]
 [-0.392]
 [-0.392]
 [-0.402]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.321]
 [ 0.321]
 [ 0.321]
 [ 0.321]
 [ 0.321]
 [ 0.328]] [[1.448]
 [1.511]
 [1.511]
 [1.511]
 [1.511]
 [1.511]
 [1.024]] [[-0.481]
 [-0.134]
 [-0.134]
 [-0.134]
 [-0.134]
 [-0.134]
 [-0.289]]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Starting evaluation
main train batch thing paused
add a thread
Adding thread: now have 5 threads
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.837]
 [0.595]
 [0.768]
 [0.768]
 [0.768]
 [0.768]] [[1.119]
 [0.667]
 [1.119]
 [1.119]
 [1.119]
 [1.119]
 [1.119]] [[0.768]
 [0.837]
 [0.595]
 [0.768]
 [0.768]
 [0.768]
 [0.768]]
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.789]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.685]] [[1.217]
 [1.247]
 [1.217]
 [1.217]
 [1.217]
 [1.217]
 [0.831]] [[0.678]
 [0.789]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.685]]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.61 ]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.636]] [[0.688]
 [1.431]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [1.797]] [[0.569]
 [0.61 ]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.636]]
main train batch thing paused
add a thread
Adding thread: now have 6 threads
Printing some Q and Qe and total Qs values:  [[0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]] [[-0.553]
 [-0.553]
 [-0.553]
 [-0.553]
 [-0.553]
 [-0.553]
 [-0.553]] [[0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]]
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.715]
 [0.716]
 [0.665]
 [0.682]
 [0.672]
 [0.701]] [[ 1.229]
 [-0.199]
 [ 1.339]
 [ 0.762]
 [ 1.229]
 [ 1.745]
 [-0.114]] [[0.682]
 [0.715]
 [0.716]
 [0.665]
 [0.682]
 [0.672]
 [0.701]]
rdn beta is 0 so we're just using the maxi policy
main train batch thing paused
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.954]
 [0.878]
 [0.859]
 [0.859]
 [0.807]
 [0.859]] [[0.377]
 [0.089]
 [0.697]
 [0.377]
 [0.377]
 [0.27 ]
 [0.377]] [[0.859]
 [0.954]
 [0.878]
 [0.859]
 [0.859]
 [0.807]
 [0.859]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5946995506111719
actor:  0 policy actor:  0  step number:  21 total reward:  0.8666666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.8466666666666667  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
actor:  0 policy actor:  0  step number:  22 total reward:  0.8466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.8533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.8400000000000001  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.877]
 [0.894]
 [0.864]
 [0.846]
 [0.843]
 [0.834]
 [0.875]] [[-0.288]
 [-0.434]
 [-0.257]
 [-0.183]
 [ 0.006]
 [-0.584]
 [-0.224]] [[0.877]
 [0.894]
 [0.864]
 [0.846]
 [0.843]
 [0.834]
 [0.875]]
actor:  0 policy actor:  0  step number:  23 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7666666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7733333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]] [[2.205]
 [2.205]
 [2.205]
 [2.205]
 [2.205]
 [2.205]
 [2.205]] [[0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]]
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.386]
 [0.386]
 [0.394]
 [0.126]
 [0.373]
 [0.4  ]] [[-0.513]
 [-1.322]
 [-0.808]
 [-1.547]
 [-0.533]
 [-1.113]
 [-1.588]] [[-0.534]
 [-0.426]
 [-0.34 ]
 [-0.455]
 [-0.554]
 [-0.404]
 [-0.457]]
actor:  0 policy actor:  0  step number:  34 total reward:  0.6466666666666668  reward:  1.0 rdn_beta:  0.333
main train batch thing paused
actor:  1 policy actor:  1  step number:  44 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.167
main train batch thing paused
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  30 total reward:  0.7933333333333334  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  43 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  81 total reward:  0.26666666666666583  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5447153846153846 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5447153846153846 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5447153846153846 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5447153846153846 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5447153846153846 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 5 threads
Frames:  210494 train batches done:  14007 episodes:  4217
siam score:  -0.8144496621155157
maxi score, test score, baseline:  0.5447153846153846 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5447153846153846 0.7859999999999999 0.7859999999999999
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  35 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 4 threads
Frames:  210591 train batches done:  14026 episodes:  4220
actor:  1 policy actor:  1  step number:  71 total reward:  0.1866666666666651  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84074783
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 3 threads
Frames:  210789 train batches done:  14054 episodes:  4222
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  210789 train batches done:  14085 episodes:  4222
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8960296
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 1 threads
Frames:  210789 train batches done:  14116 episodes:  4222
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89804024
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
siam score:  -0.9099655
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9136829
siam score:  -0.91374606
siam score:  -0.9125348
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9107317
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9109001
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9108557
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9172864
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.92306983
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.23675961150294852
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9275859
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9226133
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
siam score:  -0.9274417
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
siam score:  -0.9290987
first move QE:  -0.23675961150294852
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9340715
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
siam score:  -0.9369025
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9298001
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.23675961150294852
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9321717
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
first move QE:  -0.23675961150294852
siam score:  -0.93629473
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9362057
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
siam score:  -0.93736726
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5502234567901234 0.7859999999999999 0.7859999999999999
probs:  [0.25, 0.25, 0.25, 0.25]
append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:211000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:210000
load_in_model:True
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
loaded in model from saved file
main train batch thing paused
add a thread
Adding thread: now have 3 threads
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Starting evaluation
Sims:  50 1 epoch:  210010 pick best:  True frame count:  210010
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.291]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[-0.237]
 [-0.361]
 [-0.238]
 [-0.238]
 [-0.238]
 [-0.238]
 [-0.238]] [[0.267]
 [0.291]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]]
Printing some Q and Qe and total Qs values:  [[ 0.875]
 [ 0.909]
 [ 0.861]
 [ 0.752]
 [ 0.771]
 [-0.006]
 [ 0.402]] [[-0.656]
 [-0.805]
 [ 0.   ]
 [-0.34 ]
 [-0.237]
 [-0.236]
 [-0.234]] [[ 0.875]
 [ 0.909]
 [ 0.861]
 [ 0.752]
 [ 0.771]
 [-0.006]
 [ 0.402]]
main train batch thing paused
add a thread
Adding thread: now have 5 threads
main train batch thing paused
add a thread
Adding thread: now have 6 threads
using explorer policy with actor:  1
main train batch thing paused
main train batch thing paused
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
main train batch thing paused
actor:  0 policy actor:  0  step number:  21 total reward:  0.8666666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.8466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.8466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.8466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.8466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.8466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.86  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.8466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.86  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.86  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.86  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.8400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.8533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.8400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.8400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.8400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.8400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.7866666666666667  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.8200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  62 total reward:  0.29999999999999927  reward:  1.0 rdn_beta:  0.167
main train batch thing paused
line 256 mcts: sample exp_bonus -0.470140789722991
main train batch thing paused
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.988]
 [0.993]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]] [[-0.24]
 [-0.24]
 [-0.24]
 [-0.24]
 [-0.24]
 [-0.24]
 [-0.24]] [[0.988]
 [0.993]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]]
UNIT TEST: sample policy line 217 mcts : [0.041 0.102 0.163 0.061 0.041 0.551 0.041]
actor:  0 policy actor:  0  step number:  37 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.4391200932430594
main train batch thing paused
deleting a thread, now have 5 threads
Frames:  210413 train batches done:  24570.0 episodes:  4213
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.649]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]] [[1.498]
 [4.715]
 [1.501]
 [1.501]
 [1.501]
 [1.501]
 [1.501]] [[0.131]
 [0.701]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.543]
 [0.539]
 [0.446]
 [0.6  ]
 [0.458]
 [0.588]] [[6.659]
 [1.493]
 [1.51 ]
 [2.242]
 [1.537]
 [1.518]
 [1.505]] [[0.922]
 [0.139]
 [0.14 ]
 [0.22 ]
 [0.162]
 [0.118]
 [0.154]]
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.687]
 [0.687]
 [0.617]
 [0.543]
 [0.657]
 [0.584]] [[2.222]
 [1.466]
 [1.466]
 [1.486]
 [4.766]
 [1.493]
 [1.469]] [[0.24 ]
 [0.136]
 [0.136]
 [0.112]
 [0.717]
 [0.129]
 [0.097]]
siam score:  -0.7280954881147905
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7244869021196214
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 4 threads
Frames:  210704 train batches done:  24600.0 episodes:  4218
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
deleting a thread, now have 3 threads
Frames:  210704 train batches done:  24637.0 episodes:  4218
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7575122
siam score:  -0.7565784
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  210704 train batches done:  24673.0 episodes:  4218
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7620978
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.68 ]
 [0.612]
 [0.68 ]
 [0.436]
 [0.68 ]
 [0.709]] [[ 0.74 ]
 [-0.019]
 [-0.001]
 [-0.019]
 [-0.002]
 [-0.019]
 [-0.1  ]] [[ 0.316]
 [-0.091]
 [-0.148]
 [-0.091]
 [-0.324]
 [-0.091]
 [-0.116]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.7867666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  59 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  35 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.167
siam score:  -0.77552676
maxi score, test score, baseline:  0.7798101449275364 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7798101449275364 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7798101449275364 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7804923
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.68 ]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[0.022]
 [0.004]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]] [[-0.07]
 [-0.07]
 [-0.07]
 [-0.07]
 [-0.07]
 [-0.07]
 [-0.07]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7798101449275364 0.8400000000000001 0.8400000000000001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7798101449275364 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[ 0.187]
 [-0.003]
 [ 0.403]
 [ 0.427]
 [ 0.33 ]
 [ 0.122]
 [ 0.013]] [[-0.005]
 [-0.102]
 [-0.349]
 [-0.265]
 [-0.456]
 [-0.009]
 [ 0.006]] [[-0.026]
 [-0.232]
 [ 0.133]
 [ 0.171]
 [ 0.042]
 [-0.091]
 [-0.198]]
Printing some Q and Qe and total Qs values:  [[0.936]
 [0.936]
 [0.953]
 [0.936]
 [0.936]
 [0.96 ]
 [0.936]] [[0.012]
 [0.012]
 [0.026]
 [0.012]
 [0.012]
 [0.006]
 [0.012]] [[0.936]
 [0.936]
 [0.953]
 [0.936]
 [0.936]
 [0.96 ]
 [0.936]]
maxi score, test score, baseline:  0.7798101449275364 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.77999616
Printing some Q and Qe and total Qs values:  [[0.864]
 [0.467]
 [0.932]
 [0.929]
 [0.926]
 [0.86 ]
 [0.893]] [[0.01 ]
 [0.107]
 [0.059]
 [0.03 ]
 [0.038]
 [0.078]
 [0.094]] [[0.864]
 [0.467]
 [0.932]
 [0.929]
 [0.926]
 [0.86 ]
 [0.893]]
maxi score, test score, baseline:  0.7798101449275364 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.7798101449275364 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7798101449275364 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.7798101449275364 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.7798101449275364 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.7798101449275364 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.601]
 [0.337]
 [0.401]
 [0.26 ]
 [0.138]
 [0.194]] [[0.049]
 [0.156]
 [0.012]
 [0.019]
 [0.019]
 [0.029]
 [0.04 ]] [[-0.253]
 [ 0.008]
 [-0.304]
 [-0.238]
 [-0.379]
 [-0.498]
 [-0.438]]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.798]
 [0.729]
 [0.659]
 [0.783]
 [0.783]
 [0.783]] [[0.823]
 [0.   ]
 [0.12 ]
 [0.176]
 [0.823]
 [0.823]
 [0.823]] [[0.456]
 [0.197]
 [0.168]
 [0.116]
 [0.456]
 [0.456]
 [0.456]]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.712]
 [0.692]
 [0.603]
 [0.692]
 [0.692]
 [0.692]] [[0.733]
 [0.389]
 [0.014]
 [1.474]
 [0.014]
 [0.014]
 [0.014]] [[-0.038]
 [ 0.044]
 [-0.039]
 [ 0.116]
 [-0.039]
 [-0.039]
 [-0.039]]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
actor:  1 policy actor:  1  step number:  65 total reward:  0.18666666666666554  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.875]
 [0.863]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]] [[ 0.272]
 [-0.009]
 [ 0.272]
 [ 0.272]
 [ 0.272]
 [ 0.272]
 [ 0.272]] [[0.626]
 [0.5  ]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]]
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.745]
 [0.671]
 [0.661]
 [0.658]
 [0.649]
 [0.464]] [[0.02 ]
 [0.185]
 [0.065]
 [0.119]
 [1.817]
 [0.107]
 [0.023]] [[-0.054]
 [ 0.07 ]
 [-0.025]
 [-0.026]
 [ 0.255]
 [-0.04 ]
 [-0.238]]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  75 total reward:  0.29333333333333333  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  73 total reward:  0.42666666666666686  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.637]
 [0.648]
 [0.568]
 [0.618]
 [0.602]
 [0.645]] [[-0.422]
 [-0.734]
 [-0.422]
 [-0.107]
 [-0.238]
 [-0.461]
 [-0.239]] [[0.455]
 [0.353]
 [0.468]
 [0.492]
 [0.499]
 [0.409]
 [0.525]]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
siam score:  -0.7848716
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6374333333333333 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.218]
 [0.163]
 [0.152]
 [0.234]
 [0.175]
 [0.171]] [[-0.543]
 [-1.318]
 [-0.217]
 [-0.198]
 [-1.447]
 [-0.352]
 [-0.618]] [[0.264]
 [0.218]
 [0.163]
 [0.152]
 [0.234]
 [0.175]
 [0.171]]
actor:  0 policy actor:  0  step number:  38 total reward:  0.6866666666666669  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6393307692307693 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.77838707
line 256 mcts: sample exp_bonus 0.3119951146600595
first move QE:  -0.3703039773458522
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.074]
 [-0.073]
 [-0.074]
 [-0.07 ]
 [-0.07 ]
 [-0.08 ]] [[ 0.083]
 [-0.001]
 [ 0.272]
 [ 0.022]
 [ 0.106]
 [ 0.081]
 [ 0.014]] [[-0.303]
 [-0.365]
 [-0.182]
 [-0.349]
 [-0.289]
 [-0.307]
 [-0.36 ]]
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.211]
 [0.675]
 [0.615]
 [0.524]
 [0.524]
 [0.586]] [[-0.006]
 [ 0.915]
 [ 1.26 ]
 [ 1.101]
 [-0.006]
 [-0.006]
 [ 0.505]] [[0.212]
 [0.053]
 [0.574]
 [0.487]
 [0.212]
 [0.212]
 [0.359]]
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.5786185185185185 0.8400000000000001 0.8400000000000001
siam score:  -0.7487212
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5222428571428571 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4697551724137931 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.061 0.224 0.122 0.286 0.184 0.061 0.061]
maxi score, test score, baseline:  0.4697551724137931 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4207666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  0.7533333333333335  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.79 ]
 [0.619]
 [0.615]
 [0.721]
 [0.721]
 [0.721]] [[-0.165]
 [-0.254]
 [-0.186]
 [-0.203]
 [-0.213]
 [-0.213]
 [-0.213]] [[0.684]
 [0.766]
 [0.618]
 [0.608]
 [0.711]
 [0.711]
 [0.711]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  72 total reward:  0.3133333333333327  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.545]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]] [[-0.022]
 [-0.043]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]] [[-0.277]
 [-0.152]
 [-0.277]
 [-0.277]
 [-0.277]
 [-0.277]
 [-0.277]]
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  0.3933333333333333  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.43149784946236563 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
actor:  0 policy actor:  0  step number:  40 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.43447500000000006 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.43447500000000006 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  79 total reward:  0.02666666666666606  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.43447500000000006 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  0.7866666666666668  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 0.002186299810709219
maxi score, test score, baseline:  0.44515050505050513 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.494]
 [0.452]
 [0.45 ]
 [0.461]
 [0.468]
 [0.447]] [[-0.299]
 [-0.44 ]
 [-0.323]
 [-0.43 ]
 [-0.385]
 [-0.237]
 [-0.126]] [[-0.33 ]
 [-0.289]
 [-0.312]
 [-0.331]
 [-0.313]
 [-0.282]
 [-0.284]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.44515050505050513 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.44515050505050513 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.571]
 [0.572]
 [0.627]
 [0.59 ]
 [0.587]
 [0.627]] [[-0.004]
 [-0.03 ]
 [ 0.058]
 [-0.004]
 [-0.038]
 [-0.006]
 [-0.004]] [[-0.082]
 [-0.147]
 [-0.116]
 [-0.082]
 [-0.13 ]
 [-0.123]
 [-0.082]]
maxi score, test score, baseline:  0.44515050505050513 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.44515050505050513 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.44515050505050513 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.44515050505050513 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.44515050505050513 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.528]
 [0.551]
 [0.547]
 [0.551]
 [0.551]
 [0.508]] [[ 0.083]
 [-0.085]
 [-0.18 ]
 [ 1.484]
 [-0.18 ]
 [-0.18 ]
 [ 0.159]] [[0.094]
 [0.064]
 [0.071]
 [0.345]
 [0.071]
 [0.071]
 [0.085]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.44515050505050513 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4538254901960785 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4538254901960785 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.4538254901960785 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.4538254901960785 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4538254901960785 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  58 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.44867142857142867 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.44867142857142867 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.44867142857142867 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.44867142857142867 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.44867142857142867 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.44867142857142867 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.008499755095008369
maxi score, test score, baseline:  0.44867142857142867 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.44867142857142867 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[-0.513]
 [-0.636]
 [-0.636]
 [-0.636]
 [-0.636]
 [-0.636]
 [-0.636]] [[0.436]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]]
maxi score, test score, baseline:  0.4084333333333334 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7516939
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]] [[-0.131]
 [-0.131]
 [-0.131]
 [-0.131]
 [-0.131]
 [-0.131]
 [-0.131]] [[-0.321]
 [-0.321]
 [-0.321]
 [-0.321]
 [-0.321]
 [-0.321]
 [-0.321]]
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.226]
 [0.133]
 [0.43 ]
 [0.362]
 [0.246]
 [0.507]] [[0.059]
 [0.272]
 [0.687]
 [0.334]
 [0.22 ]
 [0.199]
 [0.203]] [[0.043]
 [0.071]
 [0.167]
 [0.268]
 [0.163]
 [0.057]
 [0.278]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.37037027027027036 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.37037027027027036 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.37037027027027036 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.37037027027027036 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.37037027027027036 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7509074
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.624]
 [0.595]
 [0.576]
 [0.574]
 [0.553]
 [0.592]] [[-0.34 ]
 [-0.553]
 [-0.362]
 [-0.257]
 [-0.3  ]
 [-0.321]
 [-0.242]] [[0.533]
 [0.437]
 [0.503]
 [0.537]
 [0.514]
 [0.482]
 [0.56 ]]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74939317
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7504994
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.596]
 [0.508]
 [0.498]
 [0.485]
 [0.507]
 [0.478]] [[ 0.005]
 [-0.181]
 [ 0.007]
 [-0.081]
 [-0.094]
 [-0.119]
 [-0.063]] [[-0.168]
 [-0.202]
 [-0.196]
 [-0.249]
 [-0.269]
 [-0.26 ]
 [-0.261]]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33431052631578956 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.34283504273504284 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34283504273504284 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34283504273504284 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34283504273504284 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.606]
 [0.627]
 [0.606]
 [0.611]
 [0.606]
 [0.606]] [[ 0.   ]
 [ 0.   ]
 [-0.118]
 [ 0.   ]
 [-0.03 ]
 [ 0.   ]
 [ 0.   ]] [[0.606]
 [0.606]
 [0.627]
 [0.606]
 [0.611]
 [0.606]
 [0.606]]
maxi score, test score, baseline:  0.34283504273504284 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34283504273504284 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34283504273504284 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34283504273504284 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3092666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3092666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.756]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[-0.279]
 [-0.079]
 [-0.279]
 [-0.279]
 [-0.279]
 [-0.279]
 [-0.279]] [[0.726]
 [0.756]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]]
maxi score, test score, baseline:  0.3092666666666667 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
Printing some Q and Qe and total Qs values:  [[0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]] [[-0.595]
 [-0.595]
 [-0.595]
 [-0.595]
 [-0.595]
 [-0.595]
 [-0.595]] [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.27733577235772355 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.7616888
maxi score, test score, baseline:  0.27733577235772355 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27733577235772355 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]] [[-0.176]
 [-0.176]
 [-0.176]
 [-0.176]
 [-0.176]
 [-0.176]
 [-0.176]] [[1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]]
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.913]
 [0.934]
 [0.654]
 [0.913]
 [0.913]
 [0.913]] [[ 0.   ]
 [ 0.   ]
 [-0.941]
 [-0.008]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.913]
 [0.913]
 [0.934]
 [0.654]
 [0.913]
 [0.913]
 [0.913]]
maxi score, test score, baseline:  0.27733577235772355 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.27733577235772355 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.464]] [[-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.017]] [[-0.148]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.163]]
maxi score, test score, baseline:  0.24692539682539683 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24692539682539683 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24692539682539683 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24692539682539683 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.24692539682539683 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.24692539682539683 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24692539682539683 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.24692539682539683 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7703372
Printing some Q and Qe and total Qs values:  [[0.952]
 [0.934]
 [0.879]
 [0.894]
 [0.908]
 [0.928]
 [0.952]] [[0.356]
 [0.563]
 [0.276]
 [0.191]
 [0.217]
 [0.24 ]
 [0.804]] [[0.952]
 [0.934]
 [0.879]
 [0.894]
 [0.908]
 [0.928]
 [0.952]]
actor:  0 policy actor:  0  step number:  38 total reward:  0.7400000000000002  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.581]] [[0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [2.409]] [[0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.746]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.0012933643390424549
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7791898
using explorer policy with actor:  1
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.7792975
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.25839457364341084 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2297969696969697 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2297969696969697 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2297969696969697 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2297969696969697 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.48392234004308626
actor:  0 policy actor:  0  step number:  35 total reward:  0.7333333333333336  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.24098888888888892 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2140130434782609 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.77752095
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.367]
 [0.377]
 [0.394]
 [0.381]
 [0.378]
 [0.378]] [[-1.014]
 [-0.732]
 [-0.662]
 [-0.761]
 [-0.816]
 [-0.852]
 [-0.696]] [[-0.064]
 [ 0.03 ]
 [ 0.075]
 [ 0.043]
 [ 0.002]
 [-0.019]
 [ 0.06 ]]
maxi score, test score, baseline:  0.2140130434782609 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.214]
 [0.291]
 [0.214]
 [0.23 ]
 [0.214]
 [0.214]] [[ 0.027]
 [ 0.027]
 [ 0.097]
 [ 0.027]
 [-0.003]
 [ 0.027]
 [ 0.027]] [[-0.477]
 [-0.477]
 [-0.366]
 [-0.477]
 [-0.476]
 [-0.477]
 [-0.477]]
maxi score, test score, baseline:  0.2140130434782609 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2140130434782609 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.021276235743245067
maxi score, test score, baseline:  0.2140130434782609 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2140130434782609 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.2140130434782609 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.777775
maxi score, test score, baseline:  0.2140130434782609 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2140130434782609 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2140130434782609 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2140130434782609 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7748599
maxi score, test score, baseline:  0.2140130434782609 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.427]
 [0.488]
 [0.427]
 [0.427]
 [0.49 ]
 [0.427]] [[ 0.282]
 [ 0.282]
 [-0.015]
 [ 0.282]
 [ 0.282]
 [ 0.012]
 [ 0.282]] [[-0.092]
 [-0.092]
 [-0.229]
 [-0.092]
 [-0.092]
 [-0.208]
 [-0.092]]
maxi score, test score, baseline:  0.2140130434782609 0.8400000000000001 0.8400000000000001
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.487]
 [0.424]
 [0.431]
 [0.412]
 [0.45 ]
 [0.408]] [[0.004]
 [0.023]
 [0.521]
 [0.874]
 [0.157]
 [0.091]
 [0.253]] [[-0.283]
 [-0.205]
 [ 0.065]
 [ 0.307]
 [-0.191]
 [-0.196]
 [-0.131]]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.481]
 [0.314]
 [0.353]
 [0.44 ]
 [0.34 ]
 [0.34 ]] [[ 0.002]
 [-0.186]
 [ 0.378]
 [ 0.823]
 [ 0.625]
 [-0.085]
 [-0.085]] [[-0.286]
 [-0.201]
 [-0.18 ]
 [ 0.006]
 [ 0.028]
 [-0.308]
 [-0.308]]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.688]
 [0.656]
 [0.669]
 [0.653]
 [0.673]
 [0.669]] [[0.071]
 [0.013]
 [0.237]
 [0.246]
 [0.394]
 [0.898]
 [0.408]] [[-0.085]
 [-0.042]
 [ 0.001]
 [ 0.017]
 [ 0.05 ]
 [ 0.238]
 [ 0.071]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
actor:  1 policy actor:  1  step number:  47 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
siam score:  -0.7712265
line 256 mcts: sample exp_bonus 2.0463592350097346
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]] [[-0.102]
 [-0.102]
 [-0.102]
 [-0.102]
 [-0.102]
 [-0.102]
 [-0.102]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.524]] [[-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [ 1.422]] [[-0.211]
 [-0.211]
 [-0.211]
 [-0.211]
 [-0.211]
 [-0.211]
 [ 0.504]]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.622]
 [0.549]
 [0.502]
 [0.502]
 [0.502]
 [0.541]] [[0.695]
 [0.057]
 [0.469]
 [0.695]
 [0.695]
 [0.695]
 [0.514]] [[ 0.109]
 [-0.091]
 [ 0.042]
 [ 0.109]
 [ 0.109]
 [ 0.109]
 [ 0.057]]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[-0.532]
 [-0.532]
 [-0.532]
 [-0.532]
 [-0.532]
 [-0.532]
 [-0.532]] [[0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]]
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.648]
 [0.648]
 [0.306]
 [0.648]
 [0.668]
 [0.648]] [[-0.162]
 [-0.162]
 [-0.162]
 [-0.159]
 [-0.162]
 [-0.067]
 [-0.162]] [[-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.411]
 [-0.07 ]
 [-0.034]
 [-0.07 ]]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.194]
 [0.302]
 [0.194]
 [0.194]
 [0.194]
 [0.194]] [[0.147]
 [0.147]
 [0.105]
 [0.147]
 [0.147]
 [0.147]
 [0.147]] [[-0.321]
 [-0.321]
 [-0.234]
 [-0.321]
 [-0.321]
 [-0.321]
 [-0.321]]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.387]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]] [[-0.157]
 [-0.165]
 [-0.157]
 [-0.157]
 [-0.157]
 [-0.157]
 [-0.157]] [[-0.366]
 [-0.33 ]
 [-0.366]
 [-0.366]
 [-0.366]
 [-0.366]
 [-0.366]]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.7778784
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.534]
 [0.365]
 [0.366]
 [0.417]
 [0.389]
 [0.378]] [[0.02 ]
 [0.202]
 [0.776]
 [1.141]
 [1.067]
 [0.978]
 [1.217]] [[-0.36 ]
 [-0.059]
 [ 0.059]
 [ 0.242]
 [ 0.256]
 [ 0.183]
 [ 0.292]]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78067416
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.18818510638297875 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.16343333333333335 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.008398085862316657
maxi score, test score, baseline:  0.16343333333333335 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  0.8  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1529 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1529 0.8400000000000001 0.8400000000000001
actor:  0 policy actor:  0  step number:  29 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.5165116977355589
maxi score, test score, baseline:  0.16376013071895426 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.16376013071895426 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16376013071895426 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.16376013071895426 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.16376013071895426 0.8400000000000001 0.8400000000000001
Printing some Q and Qe and total Qs values:  [[ 0.515]
 [-0.003]
 [ 0.482]
 [ 0.489]
 [ 0.541]
 [ 0.507]
 [ 0.532]] [[-0.044]
 [ 0.017]
 [-0.101]
 [-0.319]
 [-0.484]
 [-0.418]
 [-0.602]] [[ 0.25 ]
 [-0.248]
 [ 0.198]
 [ 0.132]
 [ 0.129]
 [ 0.117]
 [ 0.081]]
maxi score, test score, baseline:  0.16376013071895426 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.16376013071895426 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.16376013071895426 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.16376013071895426 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.16376013071895426 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.16376013071895426 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.16376013071895426 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.041 0.571 0.061 0.082 0.184 0.041 0.02 ]
actor:  0 policy actor:  0  step number:  44 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17330512820512822 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17330512820512822 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78052855
maxi score, test score, baseline:  0.17330512820512822 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17330512820512822 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17330512820512822 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17330512820512822 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.17330512820512822 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17330512820512822 0.8400000000000001 0.8400000000000001
actor:  1 policy actor:  1  step number:  79 total reward:  0.013333333333331643  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17330512820512822 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17330512820512822 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.17330512820512822 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17330512820512822 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.15116918238993712 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15116918238993712 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7942456
maxi score, test score, baseline:  0.15116918238993712 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15116918238993712 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15116918238993712 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15116918238993712 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.15116918238993712 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15116918238993712 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.527340807725475
maxi score, test score, baseline:  0.15116918238993712 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15116918238993712 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  65 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.07184528691364725
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.518]
 [0.523]
 [0.473]
 [0.524]
 [0.55 ]
 [0.518]] [[-0.136]
 [-0.136]
 [-0.111]
 [-0.269]
 [-0.137]
 [-0.139]
 [-0.136]] [[0.159]
 [0.159]
 [0.176]
 [0.047]
 [0.165]
 [0.19 ]
 [0.159]]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.293]
 [0.15 ]
 [0.184]
 [0.184]
 [0.104]
 [0.184]] [[-0.065]
 [-0.072]
 [-0.034]
 [-0.065]
 [-0.065]
 [-0.045]
 [-0.065]] [[-0.391]
 [-0.284]
 [-0.414]
 [-0.391]
 [-0.391]
 [-0.464]
 [-0.391]]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[ 0.262]
 [ 0.262]
 [-0.021]
 [ 0.262]
 [-0.029]
 [-0.   ]
 [ 0.158]] [[0.11 ]
 [0.11 ]
 [0.623]
 [0.11 ]
 [1.221]
 [1.27 ]
 [0.618]] [[ 0.026]
 [ 0.026]
 [-0.086]
 [ 0.026]
 [ 0.104]
 [ 0.149]
 [ 0.091]]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.004]] [[0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.047]] [[-0.187]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.186]]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
siam score:  -0.79720914
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.339]
 [0.258]
 [0.159]
 [0.258]
 [0.258]
 [0.125]] [[-0.636]
 [-3.089]
 [ 0.   ]
 [-0.198]
 [ 0.   ]
 [ 0.   ]
 [-0.081]] [[0.217]
 [0.339]
 [0.258]
 [0.159]
 [0.258]
 [0.258]
 [0.125]]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1298530864197531 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  0.7400000000000001  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]] [[-0.04 ]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]] [[-0.319]
 [-0.411]
 [-0.411]
 [-0.411]
 [-0.411]
 [-0.411]
 [-0.411]]
maxi score, test score, baseline:  0.12057619047619052 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.12057619047619052 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80005294
maxi score, test score, baseline:  0.12057619047619052 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.16 ]
 [0.193]
 [0.095]
 [0.072]
 [0.177]
 [0.082]] [[0.082]
 [0.077]
 [0.245]
 [0.155]
 [0.038]
 [0.3  ]
 [0.048]] [[-0.565]
 [-0.48 ]
 [-0.391]
 [-0.519]
 [-0.581]
 [-0.389]
 [-0.568]]
actor:  1 policy actor:  1  step number:  81 total reward:  0.15999999999999914  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10091871345029244 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.10091871345029244 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10091871345029244 0.8400000000000001 0.8400000000000001
Printing some Q and Qe and total Qs values:  [[-0.12 ]
 [-0.113]
 [-0.13 ]
 [-0.143]
 [-0.133]
 [-0.124]
 [-0.138]] [[1.376]
 [0.952]
 [1.463]
 [1.494]
 [1.498]
 [1.467]
 [1.499]] [[-0.313]
 [-0.376]
 [-0.308]
 [-0.315]
 [-0.305]
 [-0.301]
 [-0.31 ]]
actor:  0 policy actor:  0  step number:  41 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.10906551724137935 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.10906551724137935 0.8400000000000001 0.8400000000000001
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.887]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[-0.018]
 [-0.667]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[0.548]
 [0.887]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]]
maxi score, test score, baseline:  0.07210000000000004 0.8400000000000001 0.8400000000000001
Printing some Q and Qe and total Qs values:  [[0.879]
 [0.879]
 [0.955]
 [0.879]
 [0.879]
 [0.927]
 [0.879]] [[-0.012]
 [-0.012]
 [ 0.016]
 [-0.012]
 [-0.012]
 [ 0.106]
 [-0.012]] [[0.879]
 [0.879]
 [0.955]
 [0.879]
 [0.879]
 [0.927]
 [0.879]]
maxi score, test score, baseline:  0.05452622950819676 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.05452622950819676 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.05452622950819676 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.05452622950819676 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.05452622950819676 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.5587917623050035
maxi score, test score, baseline:  0.03751935483870971 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.03751935483870971 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.03751935483870971 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.03751935483870971 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.03751935483870971 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.03751935483870971 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.03751935483870971 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8174727
maxi score, test score, baseline:  0.03751935483870971 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.417]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.195]] [[-0.005]
 [-0.002]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [ 0.69 ]] [[-0.287]
 [-0.269]
 [-0.287]
 [-0.287]
 [-0.287]
 [-0.287]
 [-0.029]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.03751935483870971 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.03751935483870971 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.03751935483870971 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.219]
 [0.224]
 [0.227]
 [0.216]
 [0.186]
 [0.208]] [[-0.965]
 [-0.803]
 [-0.675]
 [-0.827]
 [-0.928]
 [-0.892]
 [-1.003]] [[ 0.046]
 [ 0.099]
 [ 0.168]
 [ 0.095]
 [ 0.033]
 [ 0.021]
 [-0.012]]
maxi score, test score, baseline:  0.021052380952380986 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.021052380952380986 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.021052380952380986 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.002199548560596304
Printing some Q and Qe and total Qs values:  [[0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.289]
 [-0.289]
 [-0.289]
 [-0.289]
 [-0.289]
 [-0.289]
 [-0.289]]
maxi score, test score, baseline:  0.021052380952380986 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.021052380952380986 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0051000000000000255 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.0051000000000000255 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0051000000000000255 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0051000000000000255 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0051000000000000255 0.8400000000000001 0.8400000000000001
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.16 ]
 [0.131]
 [0.098]
 [0.134]
 [0.098]
 [0.098]] [[ 0.   ]
 [-0.372]
 [-0.14 ]
 [ 0.   ]
 [-0.133]
 [ 0.   ]
 [ 0.   ]] [[-0.494]
 [-0.619]
 [-0.532]
 [-0.494]
 [-0.525]
 [-0.494]
 [-0.494]]
maxi score, test score, baseline:  0.0051000000000000255 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.02  0.061 0.245 0.122 0.163 0.082 0.306]
maxi score, test score, baseline:  0.0051000000000000255 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[ 0.261]
 [-0.021]
 [-0.006]
 [ 0.143]
 [ 0.069]
 [ 0.216]
 [ 0.29 ]] [[-0.365]
 [ 0.112]
 [ 0.008]
 [-0.214]
 [ 0.001]
 [-0.35 ]
 [-0.326]] [[-0.469]
 [-0.592]
 [-0.611]
 [-0.536]
 [-0.538]
 [-0.508]
 [-0.426]]
maxi score, test score, baseline:  0.0051000000000000255 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0051000000000000255 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.0051000000000000255 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0051000000000000255 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0051000000000000255 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0051000000000000255 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.014561538461538487 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.817312
maxi score, test score, baseline:  0.014561538461538487 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.014561538461538487 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.014561538461538487 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.5595832245931244
maxi score, test score, baseline:  0.014561538461538487 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.014561538461538487 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.014561538461538487 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.014561538461538487 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.014561538461538487 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.014561538461538487 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.014561538461538487 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.8301199
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.014561538461538487 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]] [[0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]] [[0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  28 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.02535252525252528 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
siam score:  -0.8306324
maxi score, test score, baseline:  0.02535252525252528 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.02535252525252528 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.02535252525252528 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.82965815
maxi score, test score, baseline:  0.02535252525252528 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.02535252525252528 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]] [[0.22]
 [0.22]
 [0.22]
 [0.22]
 [0.22]
 [0.22]
 [0.22]] [[-0.212]
 [-0.212]
 [-0.212]
 [-0.212]
 [-0.212]
 [-0.212]
 [-0.212]]
maxi score, test score, baseline:  0.02535252525252528 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.02535252525252528 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.02535252525252528 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.02535252525252528 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.02535252525252528 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.191]
 [-0.191]
 [-0.191]
 [-0.191]] [[0.   ]
 [0.   ]
 [0.   ]
 [1.096]
 [1.096]
 [1.096]
 [1.096]] [[-0.533]
 [-0.533]
 [-0.533]
 [ 0.007]
 [ 0.007]
 [ 0.007]
 [ 0.007]]
maxi score, test score, baseline:  0.02535252525252528 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.02535252525252528 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  0.02535252525252528 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  0.6200000000000003  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]]
maxi score, test score, baseline:  0.01902156862745101 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.01902156862745101 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8313854
first move QE:  -0.5507361877879431
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.135]
 [0.106]
 [0.095]
 [0.086]
 [0.048]
 [0.015]] [[0.184]
 [0.022]
 [0.709]
 [0.159]
 [0.184]
 [0.133]
 [0.08 ]] [[-0.557]
 [-0.588]
 [-0.274]
 [-0.56 ]
 [-0.557]
 [-0.62 ]
 [-0.68 ]]
siam score:  -0.83233595
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]] [[-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]] [[0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]]
maxi score, test score, baseline:  0.004254589371980708 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.004254589371980708 0.8400000000000001 0.8400000000000001
maxi score, test score, baseline:  -0.01009047619047616 0.8400000000000001 0.8400000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
