append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.006301093550229614
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
siam score:  -0.08156082879627477
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.20006596463244614, 0.20006596463244614, 0.4996701768377692, 0.10019789389733844]
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.20006596463244614, 0.20006596463244614, 0.4996701768377692, 0.10019789389733844]
deleting a thread, now have 2 threads
Frames:  953 train batches done:  30 episodes:  33
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.20006596463244614, 0.20006596463244614, 0.4996701768377692, 0.10019789389733844]
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.20006596463244614, 0.20006596463244614, 0.4996701768377692, 0.10019789389733844]
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.20006596463244614, 0.20006596463244614, 0.4996701768377692, 0.10019789389733844]
siam score:  -0.28366596
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.20006596463244614, 0.20006596463244614, 0.4996701768377692, 0.10019789389733844]
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.20006596463244614, 0.20006596463244614, 0.4996701768377692, 0.10019789389733844]
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.20006596463244614, 0.20006596463244614, 0.4996701768377692, 0.10019789389733844]
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.20006596463244614, 0.20006596463244614, 0.4996701768377692, 0.10019789389733844]
deleting a thread, now have 1 threads
Frames:  953 train batches done:  80 episodes:  33
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.20006596463244614, 0.20006596463244614, 0.4996701768377692, 0.10019789389733844]
siam score:  -0.34324917
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.20006596463244614, 0.20006596463244614, 0.4996701768377692, 0.10019789389733844]
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
siam score:  -0.34944698
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.20006596463244614, 0.20006596463244614, 0.4996701768377692, 0.10019789389733844]
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.20006596463244614, 0.20006596463244614, 0.4996701768377692, 0.10019789389733844]
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
maxi score, test score, baseline:  -0.9628629629629629 -1.0 -0.9628629629629629
probs:  [0.20006596463244614, 0.20006596463244614, 0.4996701768377692, 0.10019789389733844]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.009]
 [-0.009]
 [-0.031]
 [-0.009]
 [-0.03 ]
 [-0.009]] [[-1.498]
 [ 0.   ]
 [ 0.   ]
 [-1.498]
 [ 0.   ]
 [-1.498]
 [ 0.   ]] [[-0.231]
 [ 0.54 ]
 [ 0.54 ]
 [-0.23 ]
 [ 0.54 ]
 [-0.23 ]
 [ 0.54 ]]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.2631292617914639, 0.2631292617914639, 0.3944218797061029, 0.07931959671096933]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.2631292617914639, 0.2631292617914639, 0.3944218797061029, 0.07931959671096933]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.27354515454768064, 0.27354515454768064, 0.3771438345574754, 0.07576585634716335]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.2098777292290705, 0.2975207036790442, 0.4102045279718674, 0.08239703912001786]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.16593245779071228, 0.24918905907836708, 0.48706506275738065, 0.09781342037354013]
from probs:  [0.16593245779071228, 0.24918905907836708, 0.48706506275738065, 0.09781342037354013]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.10495706661014952, 0.26740515200678217, 0.5226807147729189, 0.10495706661014952]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.10495706661014952, 0.26740515200678217, 0.5226807147729189, 0.10495706661014952]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.10495706584245568, 0.26740515209890536, 0.5226807162161833, 0.10495706584245568]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.10495706584245568, 0.26740515209890536, 0.5226807162161833, 0.10495706584245568]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.10495706584245568, 0.26740515209890536, 0.5226807162161833, 0.10495706584245568]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.10495706584245568, 0.26740515209890536, 0.5226807162161833, 0.10495706584245568]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.10495706584245568, 0.26740515209890536, 0.5226807162161833, 0.10495706584245568]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.12543644177484636, 0.12543644177484636, 0.6236906746754609, 0.12543644177484636]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.20549938862994951, 0.20549938862994951, 0.4903033013982729, 0.09869792134182812]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.18322393162655032, 0.2966996033722817, 0.4328704094671594, 0.08720605553400873]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.18322393162655032, 0.2966996033722817, 0.4328704094671594, 0.08720605553400873]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.18322393162655032, 0.2966996033722817, 0.4328704094671594, 0.08720605553400873]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.18322393162655032, 0.2966996033722817, 0.4328704094671594, 0.08720605553400873]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.18322393162655032, 0.2966996033722817, 0.4328704094671594, 0.08720605553400873]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.18322393162655032, 0.2966996033722817, 0.4328704094671594, 0.08720605553400873]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.18322393162655032, 0.2966996033722817, 0.4328704094671594, 0.08720605553400873]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.18322393162655032, 0.2966996033722817, 0.4328704094671594, 0.08720605553400873]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.2384946097970319, 0.3458782516914003, 0.3458782516914003, 0.06974888682016751]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.18412608456633905, 0.2974733803963255, 0.4314292754681273, 0.0869712595692081]
siam score:  -0.5251179
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.11013784312744676, 0.23321654117529342, 0.546507772569813, 0.11013784312744676]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.12695920698429045, 0.6191223790471287, 0.12695920698429045, 0.12695920698429045]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.12695920698429045, 0.6191223790471287, 0.12695920698429045, 0.12695920698429045]
using another actor
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.08617686790704687, 0.4243923664215306, 0.24471538283571131, 0.24471538283571131]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.08617686790704687, 0.4243923664215306, 0.24471538283571131, 0.24471538283571131]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.08617686543019479, 0.42439236905817984, 0.2447153827558127, 0.2447153827558127]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.08617686543019479, 0.42439236905817984, 0.2447153827558127, 0.2447153827558127]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.08617686543019479, 0.42439236905817984, 0.2447153827558127, 0.2447153827558127]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.08617686543019479, 0.42439236905817984, 0.2447153827558127, 0.2447153827558127]
siam score:  -0.5244475
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.06436878540860656, 0.3118770715304645, 0.3118770715304645, 0.3118770715304645]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.06436878540860656, 0.3118770715304645, 0.3118770715304645, 0.3118770715304645]
siam score:  -0.5275769
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.06436878540860656, 0.3118770715304645, 0.3118770715304645, 0.3118770715304645]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.06436878540860656, 0.3118770715304645, 0.3118770715304645, 0.3118770715304645]
siam score:  -0.53154594
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.06436878540860656, 0.3118770715304645, 0.3118770715304645, 0.3118770715304645]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.08550838862250315, 0.41449161137749685, 0.41449161137749685, 0.08550838862250315]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.08550838862250315, 0.41449161137749685, 0.41449161137749685, 0.08550838862250315]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.08550838397738997, 0.41449161602261003, 0.41449161602261003, 0.08550838397738997]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.12739499782195313, 0.6178150065341407, 0.12739499782195313, 0.12739499782195313]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.24503444269014168, 0.4237945058450372, 0.08613660877467948, 0.24503444269014168]
siam score:  -0.55460644
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.10239247690106464, 0.5038849397301689, 0.10239247690106464, 0.2913301064677019]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.10239247690106464, 0.5038849397301689, 0.10239247690106464, 0.2913301064677019]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.10239247690106464, 0.5038849397301689, 0.10239247690106464, 0.2913301064677019]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.10239247690106464, 0.5038849397301689, 0.10239247690106464, 0.2913301064677019]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.10239247690106464, 0.5038849397301689, 0.10239247690106464, 0.2913301064677019]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.10239247690106464, 0.5038849397301689, 0.10239247690106464, 0.2913301064677019]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.10239247690106464, 0.5038849397301689, 0.10239247690106464, 0.2913301064677019]
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
siam score:  -0.5559666
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
probs:  [0.2109220058900438, 0.4809154197406508, 0.2109220058900438, 0.09724056847926178]
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
probs:  [0.08127526751516824, 0.40333732315959786, 0.32461104289095966, 0.1907763664342743]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.08318389421968618, 0.41326913716098834, 0.28007684614958583, 0.22347012246973963]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.07624571147177167, 0.37805216797241636, 0.24008350214355023, 0.30561861841226173]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.07624571147177167, 0.37805216797241636, 0.24008350214355023, 0.30561861841226173]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.07624571147177167, 0.37805216797241636, 0.24008350214355023, 0.30561861841226173]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.07624571147177167, 0.37805216797241636, 0.24008350214355023, 0.30561861841226173]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.08158582582778287, 0.4045719132813501, 0.2569211304454335, 0.2569211304454335]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.08158582582778287, 0.4045719132813501, 0.2569211304454335, 0.2569211304454335]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.08713518613033838, 0.4321308028556187, 0.20631549008998085, 0.274418520924062]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.08601317203435552, 0.42558718235549714, 0.15983360906069075, 0.32856603654945654]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.09432444816752665, 0.4667680851757134, 0.17529045621278458, 0.26361701044397534]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.09432444816752665, 0.4667680851757134, 0.17529045621278458, 0.26361701044397534]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.1135304463599457, 0.5619304083201241, 0.1135304463599457, 0.2110086989599846]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.10777828853074747, 0.5344434229385052, 0.1788891442653737, 0.1788891442653737]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.10777828853074747, 0.5344434229385052, 0.1788891442653737, 0.1788891442653737]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.10777828853074747, 0.5344434229385052, 0.1788891442653737, 0.1788891442653737]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.10777828853074747, 0.5344434229385052, 0.1788891442653737, 0.1788891442653737]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.10777828853074747, 0.5344434229385052, 0.1788891442653737, 0.1788891442653737]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.10777828853074747, 0.5344434229385052, 0.1788891442653737, 0.1788891442653737]
using another actor
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.10777828853074747, 0.5344434229385052, 0.1788891442653737, 0.1788891442653737]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.10337589728063284, 0.5113734004997412, 0.192625351109813, 0.192625351109813]
from probs:  [0.06525047516215736, 0.31158317494594756, 0.31158317494594756, 0.31158317494594756]
siam score:  -0.55860615
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.2466555788497411, 0.08612336363731654, 0.2466555788497411, 0.42056547866320126]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.2712081238758403, 0.07730527701101533, 0.2712081238758403, 0.38027847523730396]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.2712081238758403, 0.07730527701101533, 0.2712081238758403, 0.38027847523730396]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.2712081238758403, 0.07730527701101533, 0.2712081238758403, 0.38027847523730396]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.2712081238758403, 0.07730527701101533, 0.2712081238758403, 0.38027847523730396]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.2712081238758403, 0.07730527701101533, 0.2712081238758403, 0.38027847523730396]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.2712081238758403, 0.07730527701101533, 0.2712081238758403, 0.38027847523730396]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.2712081238758403, 0.07730527701101533, 0.2712081238758403, 0.38027847523730396]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.2712081238758403, 0.07730527701101533, 0.2712081238758403, 0.38027847523730396]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.2712081238758403, 0.07730527701101533, 0.2712081238758403, 0.38027847523730396]
siam score:  -0.5583662
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.38001057763362417, 0.07729938194936546, 0.27134502020850515, 0.27134502020850515]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.21047849586183823, 0.07347061484954187, 0.35802544464430996, 0.35802544464430996]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.08511904018446124, 0.08511904018446124, 0.41488095981553874, 0.41488095981553874]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.08511904018446124, 0.08511904018446124, 0.41488095981553874, 0.41488095981553874]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.0735174063854718, 0.21064057264711947, 0.35792101048370434, 0.35792101048370434]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.08619824707979742, 0.2470217863105415, 0.4197581802991195, 0.2470217863105415]
first move QE:  -0.13159441853108156
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.08619824707979742, 0.2470217863105415, 0.4197581802991195, 0.2470217863105415]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.08619824707979742, 0.2470217863105415, 0.4197581802991195, 0.2470217863105415]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.08705938536623165, 0.41294061463376835, 0.41294061463376835, 0.08705938536623165]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.12710789533896819, 0.12710789533896819, 0.6186763139830954, 0.12710789533896819]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.1292513844401832, 0.1292513844401832, 0.6122458466794504, 0.1292513844401832]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.1292513844401832, 0.1292513844401832, 0.6122458466794504, 0.1292513844401832]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.1292513844401832, 0.1292513844401832, 0.6122458466794504, 0.1292513844401832]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.1292513844401832, 0.1292513844401832, 0.6122458466794504, 0.1292513844401832]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.1292513844401832, 0.1292513844401832, 0.6122458466794504, 0.1292513844401832]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.1292513844401832, 0.1292513844401832, 0.6122458466794504, 0.1292513844401832]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.1292513844401832, 0.1292513844401832, 0.6122458466794504, 0.1292513844401832]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.1292513844401832, 0.1292513844401832, 0.6122458466794504, 0.1292513844401832]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
siam score:  -0.63251203
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.08731541354402493, 0.4126845864559751, 0.08731541354402493, 0.4126845864559751]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.31132791055000797, 0.31132791055000797, 0.31132791055000797, 0.06601626834997608]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.31132791055000797, 0.31132791055000797, 0.31132791055000797, 0.06601626834997608]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.31132791055000797, 0.31132791055000797, 0.31132791055000797, 0.06601626834997608]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.4125568303550479, 0.4125568303550479, 0.08744316964495212, 0.08744316964495212]
UNIT TEST: sample policy line 217 mcts : [0.4 0.  0.2 0.  0.2 0.2 0. ]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.3112915881097942, 0.06612523567061751, 0.3112915881097942, 0.3112915881097942]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.3112915881097942, 0.06612523567061751, 0.3112915881097942, 0.3112915881097942]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.3112915881097942, 0.06612523567061751, 0.3112915881097942, 0.3112915881097942]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.3112915881097942, 0.06612523567061751, 0.3112915881097942, 0.3112915881097942]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.41242925832606686, 0.08757074167393318, 0.08757074167393318, 0.41242925832606686]
first move QE:  -0.10678544789660818
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.06623409572895689, 0.3112553014236811, 0.3112553014236811, 0.3112553014236811]
siam score:  -0.6302124
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.06623409572895689, 0.3112553014236811, 0.3112553014236811, 0.3112553014236811]
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.06623409572895689, 0.3112553014236811, 0.3112553014236811, 0.3112553014236811]
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.06623409572895689, 0.3112553014236811, 0.3112553014236811, 0.3112553014236811]
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.06623408741177693, 0.31125530419607433, 0.31125530419607433, 0.31125530419607433]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.08782534263000345, 0.4121746573699965, 0.4121746573699965, 0.08782534263000345]
siam score:  -0.6346343
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.08782534263000345, 0.4121746573699965, 0.4121746573699965, 0.08782534263000345]
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.436]
 [0.79 ]
 [0.906]
 [0.888]
 [0.846]
 [0.776]] [[-0.909]
 [-0.981]
 [-0.162]
 [-0.801]
 [-0.859]
 [-0.905]
 [-0.868]] [[0.527]
 [0.277]
 [0.903]
 [0.806]
 [0.769]
 [0.712]
 [0.654]]
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.621]
 [0.787]
 [0.827]
 [0.767]
 [0.847]
 [0.782]] [[-0.012]
 [-0.122]
 [-0.07 ]
 [-0.123]
 [-0.054]
 [-0.55 ]
 [-0.121]] [[0.899]
 [0.639]
 [0.822]
 [0.845]
 [0.808]
 [0.723]
 [0.8  ]]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.08782534263000345, 0.4121746573699965, 0.4121746573699965, 0.08782534263000345]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.08782534263000345, 0.4121746573699965, 0.4121746573699965, 0.08782534263000345]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.08782534263000345, 0.4121746573699965, 0.4121746573699965, 0.08782534263000345]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.08782534263000345, 0.4121746573699965, 0.4121746573699965, 0.08782534263000345]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.12995173689950704, 0.12995173689950704, 0.6101447893014789, 0.12995173689950704]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.12995173689950704, 0.12995173689950704, 0.6101447893014789, 0.12995173689950704]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.12995173689950704, 0.12995173689950704, 0.6101447893014789, 0.12995173689950704]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.12995173689950704, 0.12995173689950704, 0.6101447893014789, 0.12995173689950704]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.12995173689950704, 0.12995173689950704, 0.6101447893014789, 0.12995173689950704]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.12995173689950704, 0.12995173689950704, 0.6101447893014789, 0.12995173689950704]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.12995173689950704, 0.12995173689950704, 0.6101447893014789, 0.12995173689950704]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.12995173689950704, 0.12995173689950704, 0.6101447893014789, 0.12995173689950704]
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 2 threads
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]] [[-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]] [[-0.387]
 [-0.387]
 [-0.387]
 [-0.387]
 [-0.387]
 [-0.387]
 [-0.387]]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.31118283800821617, 0.06645148597535146, 0.31118283800821617, 0.31118283800821617]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.31118284079593733, 0.0664514776121881, 0.31118284079593733, 0.31118284079593733]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.31118284079593733, 0.0664514776121881, 0.31118284079593733, 0.31118284079593733]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.31118284079593733, 0.0664514776121881, 0.31118284079593733, 0.31118284079593733]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.31118284079593733, 0.0664514776121881, 0.31118284079593733, 0.31118284079593733]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.31118284079593733, 0.0664514776121881, 0.31118284079593733, 0.31118284079593733]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.765]
 [0.754]
 [0.766]
 [0.76 ]
 [0.738]
 [0.718]] [[0.778]
 [0.349]
 [1.011]
 [1.135]
 [1.036]
 [0.984]
 [0.96 ]] [[0.753]
 [0.765]
 [0.754]
 [0.766]
 [0.76 ]
 [0.738]
 [0.718]]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
main train batch thing paused
add a thread
Adding thread: now have 5 threads
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.066]
 [0.873]
 [1.08 ]
 [1.088]
 [1.084]
 [1.073]
 [1.077]] [[ 0.042]
 [ 0.074]
 [-0.019]
 [-0.063]
 [-0.018]
 [-0.004]
 [-0.049]] [[0.499]
 [0.327]
 [0.472]
 [0.45 ]
 [0.476]
 [0.475]
 [0.448]]
siam score:  -0.6368087
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.3111466611919915, 0.3111466611919915, 0.06656001642402541, 0.3111466611919915]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.41192073349797576, 0.41192073349797576, 0.08807926650202424, 0.08807926650202424]
Printing some Q and Qe and total Qs values:  [[1.128]
 [1.128]
 [1.131]
 [1.128]
 [1.128]
 [1.128]
 [1.128]] [[ 0.01]
 [ 0.01]
 [-0.23]
 [ 0.01]
 [ 0.01]
 [ 0.01]
 [ 0.01]] [[0.697]
 [0.697]
 [0.579]
 [0.697]
 [0.697]
 [0.697]
 [0.697]]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.41192073349797576, 0.41192073349797576, 0.08807926650202424, 0.08807926650202424]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.41192073349797576, 0.41192073349797576, 0.08807926650202424, 0.08807926650202424]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.41793779479440274, 0.24776082940274124, 0.24776082940274124, 0.08654054640011481]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.41793779479440274, 0.24776082940274124, 0.24776082940274124, 0.08654054640011481]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.3779230427615793, 0.27233576937106946, 0.27233576937106946, 0.07740541849628185]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.3779230427615793, 0.27233576937106946, 0.27233576937106946, 0.07740541849628185]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.3779230427615793, 0.27233576937106946, 0.27233576937106946, 0.07740541849628185]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.845]
 [0.73 ]
 [0.859]
 [0.852]
 [0.871]
 [0.867]
 [0.852]] [[-0.118]
 [-0.27 ]
 [-0.313]
 [ 0.   ]
 [-0.369]
 [-0.39 ]
 [ 0.   ]] [[0.845]
 [0.73 ]
 [0.859]
 [0.852]
 [0.871]
 [0.867]
 [0.852]]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.3779230427615793, 0.27233576937106946, 0.27233576937106946, 0.07740541849628185]
Printing some Q and Qe and total Qs values:  [[0.977]
 [0.909]
 [1.005]
 [0.987]
 [1.012]
 [1.002]
 [1.002]] [[-0.011]
 [ 0.231]
 [ 0.147]
 [ 0.056]
 [ 0.062]
 [ 0.012]
 [ 0.016]] [[0.339]
 [0.311]
 [0.393]
 [0.36 ]
 [0.386]
 [0.368]
 [0.369]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.3779230452428493, 0.2723357698043068, 0.2723357698043068, 0.07740541514853715]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.3779230452428493, 0.2723357698043068, 0.2723357698043068, 0.07740541514853715]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.31178363395536685, 0.31178363395536685, 0.31178363395536685, 0.06464909813389945]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.31178363395536685, 0.31178363395536685, 0.31178363395536685, 0.06464909813389945]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.31178363395536685, 0.31178363395536685, 0.31178363395536685, 0.06464909813389945]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.31178363395536685, 0.31178363395536685, 0.31178363395536685, 0.06464909813389945]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.4141616142509468, 0.4141616142509468, 0.08583838574905323, 0.08583838574905323]
main train batch thing paused
add a thread
Adding thread: now have 6 threads
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.128]
 [0.137]
 [0.139]
 [0.151]
 [0.145]
 [0.127]] [[-0.001]
 [-0.039]
 [-0.124]
 [-0.097]
 [-0.136]
 [-0.146]
 [ 0.05 ]] [[-0.378]
 [-0.4  ]
 [-0.433]
 [-0.418]
 [-0.426]
 [-0.436]
 [-0.357]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.12776131185422604, 0.6167160644373217, 0.12776131185422604, 0.12776131185422604]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.2154997695241308, 0.4722456707399026, 0.09675479021183582, 0.2154997695241308]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.24787833443215102, 0.41761157986007696, 0.08663175127562109, 0.24787833443215102]
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]] [[-0.431]
 [-0.431]
 [-0.431]
 [-0.431]
 [-0.431]
 [-0.431]
 [-0.431]] [[0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.24787833443215102, 0.41761157986007696, 0.08663175127562109, 0.24787833443215102]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.24787833443215102, 0.41761157986007696, 0.08663175127562109, 0.24787833443215102]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.24787833443215102, 0.41761157986007696, 0.08663175127562109, 0.24787833443215102]
first move QE:  -0.07714359833149825
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.24787833443215102, 0.41761157986007696, 0.08663175127562109, 0.24787833443215102]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.24787833443215102, 0.41761157986007696, 0.08663175127562109, 0.24787833443215102]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.243]
 [0.583]
 [0.593]
 [0.593]
 [0.592]
 [0.588]] [[0.3  ]
 [0.214]
 [0.081]
 [0.029]
 [0.037]
 [0.012]
 [0.016]] [[-0.158]
 [-0.245]
 [ 0.072]
 [ 0.073]
 [ 0.075]
 [ 0.07 ]
 [ 0.066]]
UNIT TEST: sample policy line 217 mcts : [0.082 0.02  0.082 0.224 0.408 0.082 0.102]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.3117467152415038, 0.3117467152415038, 0.06475985427548864, 0.3117467152415038]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.3117467152415038, 0.3117467152415038, 0.06475985427548864, 0.3117467152415038]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.3117467152415038, 0.3117467152415038, 0.06475985427548864, 0.3117467152415038]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.3117467152415038, 0.3117467152415038, 0.06475985427548864, 0.3117467152415038]
first move QE:  -0.07533771919562685
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.08596868071936453, 0.41403131928063547, 0.08596868071936453, 0.41403131928063547]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.08596868071936453, 0.41403131928063547, 0.08596868071936453, 0.41403131928063547]
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.36 ]
 [0.376]
 [0.374]
 [0.375]
 [0.377]
 [0.38 ]] [[ 0.237]
 [ 0.193]
 [ 0.287]
 [ 0.268]
 [ 0.073]
 [-0.002]
 [-0.064]] [[-0.318]
 [-0.323]
 [-0.276]
 [-0.284]
 [-0.349]
 [-0.372]
 [-0.39 ]]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.08596868071936453, 0.41403131928063547, 0.08596868071936453, 0.41403131928063547]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.08596868071936453, 0.41403131928063547, 0.08596868071936453, 0.41403131928063547]
siam score:  -0.66274506
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.10332636302590534, 0.29560440813480227, 0.10332636302590534, 0.49774286581338695]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.1279057437690791, 0.1279057437690791, 0.1279057437690791, 0.6162827686927628]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.1279057437690791, 0.1279057437690791, 0.1279057437690791, 0.6162827686927628]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.509]
 [0.525]
 [0.537]
 [0.536]
 [0.509]
 [0.529]] [[ 0.   ]
 [ 0.   ]
 [-0.193]
 [-0.168]
 [-0.199]
 [ 0.   ]
 [-0.228]] [[0.509]
 [0.509]
 [0.525]
 [0.537]
 [0.536]
 [0.509]
 [0.529]]
line 256 mcts: sample exp_bonus -0.2675946496332211
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.12790573741164635, 0.12790573741164635, 0.12790573741164635, 0.616282787765061]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.24823500128327158
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.21569858891067953, 0.09678703046770137, 0.21569858891067953, 0.4718157917109396]
Printing some Q and Qe and total Qs values:  [[ 0.121]
 [-0.021]
 [ 0.129]
 [ 0.126]
 [ 0.148]
 [ 0.158]
 [ 0.172]] [[ 0.033]
 [ 0.053]
 [ 0.055]
 [ 0.024]
 [ 0.041]
 [ 0.028]
 [-0.028]] [[-0.227]
 [-0.367]
 [-0.216]
 [-0.225]
 [-0.199]
 [-0.192]
 [-0.187]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.21569858891067953, 0.09678703046770137, 0.21569858891067953, 0.4718157917109396]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.21569858891067953, 0.09678703046770137, 0.21569858891067953, 0.4718157917109396]
siam score:  -0.65924776
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.21569858891067953, 0.09678703046770137, 0.21569858891067953, 0.4718157917109396]
line 256 mcts: sample exp_bonus -0.001967393527838569
siam score:  -0.6596925
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.17870598467598584, 0.09374296585496925, 0.26781354100046656, 0.45973750846857847]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.17870598467598584, 0.09374296585496925, 0.26781354100046656, 0.45973750846857847]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.17870598330971627, 0.09374296286047985, 0.26781354134184276, 0.45973751248796113]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
using another actor
from probs:  [0.2114765471786456, 0.0860841786059328, 0.27876025714449193, 0.4236790170709296]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.2434783233458728, 0.09909744217133891, 0.16960903530308846, 0.4878151991796999]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.2434783233458728, 0.09909744217133891, 0.16960903530308846, 0.4878151991796999]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.92 ]
 [0.84 ]
 [0.983]
 [0.979]
 [0.986]
 [0.981]
 [0.936]] [[-0.013]
 [-0.   ]
 [ 0.005]
 [-0.14 ]
 [-0.303]
 [ 0.008]
 [ 0.03 ]] [[0.398]
 [0.324]
 [0.47 ]
 [0.394]
 [0.319]
 [0.47 ]
 [0.435]]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]] [[0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.26194929069029077, 0.10660851171650634, 0.10660851171650634, 0.5248336858766965]
from probs:  [0.28024891985175254, 0.18113191124162614, 0.0906337729454238, 0.4479853959611975]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.3027735768862146, 0.20855294578831954, 0.0822572062315673, 0.40641627109389866]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.3027735768862146, 0.20855294578831954, 0.0822572062315673, 0.40641627109389866]
siam score:  -0.66750276
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.3222162932086116, 0.23232276693755044, 0.07500909596319356, 0.37045184389064434]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.3222162932086116, 0.23232276693755044, 0.07500909596319356, 0.37045184389064434]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.28954197271571736, 0.24352269281948333, 0.07862027319131185, 0.38831506127348747]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.23]
 [1.23]
 [1.23]
 [1.23]
 [1.23]
 [1.23]
 [1.23]] [[-0.189]
 [-0.189]
 [-0.189]
 [-0.189]
 [-0.189]
 [-0.189]
 [-0.189]] [[0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]]
Printing some Q and Qe and total Qs values:  [[1.044]
 [1.055]
 [1.057]
 [1.079]
 [1.061]
 [1.055]
 [1.055]] [[-0.104]
 [-0.111]
 [-0.002]
 [-0.006]
 [ 0.   ]
 [-0.111]
 [-0.111]] [[0.94 ]
 [0.95 ]
 [0.97 ]
 [0.992]
 [0.975]
 [0.95 ]
 [0.95 ]]
siam score:  -0.66688013
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.28954197313216323, 0.24352269275126595, 0.07862027138638393, 0.3883150627301869]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.28954197313216323, 0.24352269275126595, 0.07862027138638393, 0.3883150627301869]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.28954197353569866, 0.24352269268516366, 0.07862026963741284, 0.38831506414172495]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.565]
 [0.56 ]
 [0.565]
 [0.565]
 [0.565]
 [0.562]] [[0.988]
 [0.882]
 [1.22 ]
 [0.882]
 [0.882]
 [0.882]
 [1.037]] [[0.343]
 [0.277]
 [0.497]
 [0.277]
 [0.277]
 [0.277]
 [0.376]]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.28954197353569866, 0.24352269268516366, 0.07862026963741284, 0.38831506414172495]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.25527018871037177, 0.25527018871037177, 0.08240799901017568, 0.4070516235690808]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.2854373119394948, 0.23106773745698228, 0.08155140763007213, 0.4019435429734509]
using explorer policy with actor:  1
siam score:  -0.6791374
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.3010976968128319, 0.18888314694935765, 0.0860198095745062, 0.4239993466633043]
siam score:  -0.6759911
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.3010976968128319, 0.18888314694935765, 0.0860198095745062, 0.4239993466633043]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.3010976968128319, 0.18888314694935765, 0.0860198095745062, 0.4239993466633043]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
siam score:  -0.67558205
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.27914017134357627, 0.2119762253322513, 0.08604382656101718, 0.4228397767631552]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.27914017134357627, 0.2119762253322513, 0.08604382656101718, 0.4228397767631552]
line 256 mcts: sample exp_bonus -0.3921746494767023
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.27914017134357627, 0.2119762253322513, 0.08604382656101718, 0.4228397767631552]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.27914017134357627, 0.2119762253322513, 0.08604382656101718, 0.4228397767631552]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.27914017134357627, 0.2119762253322513, 0.08604382656101718, 0.4228397767631552]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.27914017134357627, 0.2119762253322513, 0.08604382656101718, 0.4228397767631552]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.27914017134357627, 0.2119762253322513, 0.08604382656101718, 0.4228397767631552]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.27914017134357627, 0.2119762253322513, 0.08604382656101718, 0.4228397767631552]
using explorer policy with actor:  1
siam score:  -0.6663641
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.27914017134357627, 0.2119762253322513, 0.08604382656101718, 0.4228397767631552]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.27914017134357627, 0.2119762253322513, 0.08604382656101718, 0.4228397767631552]
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.052]
 [0.052]
 [0.052]
 [0.052]
 [0.052]
 [0.052]] [[0.967]
 [0.967]
 [0.967]
 [0.967]
 [0.967]
 [0.967]
 [0.967]] [[-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.27914017134357627, 0.2119762253322513, 0.08604382656101718, 0.4228397767631552]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.3056991733110147, 0.22695977971472106, 0.0793234167216709, 0.3880176302525933]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.3056991733110147, 0.22695977971472106, 0.0793234167216709, 0.3880176302525933]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.3056991733110147, 0.22695977971472106, 0.0793234167216709, 0.3880176302525933]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.3056991733110147, 0.22695977971472106, 0.0793234167216709, 0.3880176302525933]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.3071770459050113, 0.24552477735227118, 0.07566648644166019, 0.37163169030105736]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.33414044600235104, 0.26336135837805613, 0.06835774961724177, 0.33414044600235104]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.92 ]
 [0.993]
 [1.033]
 [1.068]
 [1.06 ]
 [0.993]
 [0.997]] [[-0.427]
 [ 0.   ]
 [-0.357]
 [-0.102]
 [-0.022]
 [ 0.   ]
 [-0.158]] [[0.473]
 [0.831]
 [0.633]
 [0.837]
 [0.883]
 [0.831]
 [0.73 ]]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.28342321429625805, 0.28342321429625805, 0.07355651987789324, 0.35959705152959054]
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.009]
 [0.249]
 [0.297]
 [0.29 ]
 [0.276]
 [0.263]] [[0.001]
 [0.003]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.255]
 [0.009]
 [0.249]
 [0.297]
 [0.29 ]
 [0.276]
 [0.263]]
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.306]
 [0.336]
 [0.351]
 [0.346]
 [0.324]
 [0.306]] [[0.   ]
 [0.001]
 [0.001]
 [0.002]
 [0.002]
 [0.   ]
 [0.001]] [[0.231]
 [0.306]
 [0.336]
 [0.351]
 [0.346]
 [0.324]
 [0.306]]
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.345]
 [0.654]
 [0.704]
 [0.601]
 [0.626]
 [0.655]] [[0.001]
 [0.002]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[0.534]
 [0.345]
 [0.654]
 [0.704]
 [0.601]
 [0.626]
 [0.655]]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.2471178200300761, 0.34126903238091577, 0.07034411520809239, 0.34126903238091577]
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]] [[-0.002]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.205]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.2471178200300761, 0.34126903238091577, 0.07034411520809239, 0.34126903238091577]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.24711781996733165, 0.34126903436784267, 0.07034411129698301, 0.34126903436784267]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.24711781984725736, 0.3412690381701903, 0.07034410381236213, 0.3412690381701903]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.569]
 [0.583]
 [0.587]
 [0.584]
 [0.583]
 [0.573]] [[-0.003]
 [-0.004]
 [-0.003]
 [-0.002]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.593]
 [0.569]
 [0.583]
 [0.587]
 [0.584]
 [0.583]
 [0.573]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.5883316454437626
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.679]
 [0.678]
 [0.679]
 [0.679]
 [0.679]
 [0.679]] [[-0.002]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]] [[0.727]
 [0.679]
 [0.678]
 [0.679]
 [0.679]
 [0.679]
 [0.679]]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.735]
 [0.726]
 [0.733]
 [0.732]
 [0.73 ]
 [0.732]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.003]
 [-0.003]] [[0.728]
 [0.735]
 [0.726]
 [0.733]
 [0.732]
 [0.73 ]
 [0.732]]
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.704]
 [0.713]
 [0.718]
 [0.718]
 [0.714]
 [0.716]] [[-0.002]
 [-0.003]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.71 ]
 [0.704]
 [0.713]
 [0.718]
 [0.718]
 [0.714]
 [0.716]]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.3115993567716196, 0.3115993567716196, 0.06520192968514117, 0.3115993567716196]
Printing some Q and Qe and total Qs values:  [[1.084]
 [1.02 ]
 [1.075]
 [1.02 ]
 [1.02 ]
 [1.02 ]
 [1.079]] [[0.001]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.555]
 [0.492]
 [0.547]
 [0.492]
 [0.492]
 [0.492]
 [0.551]]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.31159935830393276, 0.31159935830393276, 0.06520192508820166, 0.31159935830393276]
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.3115993597956601, 0.3115993597956601, 0.06520192061301958, 0.3115993597956601]
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.3115993597956601, 0.3115993597956601, 0.06520192061301958, 0.3115993597956601]
line 256 mcts: sample exp_bonus 0.0018312886299937964
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.35643450494163614, 0.35643450494163614, 0.07456657461344122, 0.21256441550328647]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]]
siam score:  -0.65946174
siam score:  -0.6609101
using explorer policy with actor:  0
siam score:  -0.6676639
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.24828506125748945, 0.4163490580234693, 0.08708081946155181, 0.24828506125748945]
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.24828506125748945, 0.4163490580234693, 0.08708081946155181, 0.24828506125748945]
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.563]
 [0.652]
 [0.644]
 [0.653]
 [0.697]
 [0.649]] [[0.005]
 [0.003]
 [0.005]
 [0.005]
 [0.005]
 [0.004]
 [0.004]] [[0.632]
 [0.563]
 [0.652]
 [0.644]
 [0.653]
 [0.697]
 [0.649]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.24828506125748945, 0.4163490580234693, 0.08708081946155181, 0.24828506125748945]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.636]
 [0.631]
 [0.634]
 [0.636]
 [0.635]
 [0.636]] [[0.003]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.625]
 [0.636]
 [0.631]
 [0.634]
 [0.636]
 [0.635]
 [0.636]]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.58 ]
 [0.631]
 [0.632]
 [0.63 ]
 [0.627]
 [0.634]] [[0.004]
 [0.003]
 [0.005]
 [0.005]
 [0.005]
 [0.004]
 [0.004]] [[0.62 ]
 [0.58 ]
 [0.631]
 [0.632]
 [0.63 ]
 [0.627]
 [0.634]]
maxi score, test score, baseline:  -0.9879952380952381 -1.0 -0.9879952380952381
probs:  [0.24828506111470322, 0.4163490718737503, 0.08708080589684324, 0.24828506111470322]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.657]
 [0.605]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[0.003]
 [0.003]
 [0.01 ]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.657]
 [0.657]
 [0.605]
 [0.657]
 [0.657]
 [0.657]
 [0.657]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.009518339430168271
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.13187363430223684, 0.13187363430223684, 0.13187363430223684, 0.6043790970932896]
siam score:  -0.7376469
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.13187363430223684, 0.13187363430223684, 0.13187363430223684, 0.6043790970932896]
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.08713359555404371, 0.2483209649026191, 0.2483209649026191, 0.41622447464071805]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.08713359555404371, 0.2483209649026191, 0.2483209649026191, 0.41622447464071805]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.08713359555404371, 0.2483209649026191, 0.2483209649026191, 0.41622447464071805]
line 256 mcts: sample exp_bonus -0.0018380877447675447
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.08713359555404371, 0.2483209649026191, 0.2483209649026191, 0.41622447464071805]
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.462]
 [0.466]
 [0.477]
 [0.467]
 [0.474]
 [0.47 ]] [[-0.003]
 [-0.002]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.48 ]
 [0.462]
 [0.466]
 [0.477]
 [0.467]
 [0.474]
 [0.47 ]]
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.07770105358264497, 0.2728715415598258, 0.2728715415598258, 0.37655586329770346]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.002892226069816388
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.522]
 [0.557]
 [0.567]
 [0.66 ]
 [0.611]
 [0.554]] [[-0.003]
 [-0.002]
 [-0.003]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.003]] [[0.538]
 [0.522]
 [0.557]
 [0.567]
 [0.66 ]
 [0.611]
 [0.554]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.07360621784192689, 0.2835293966085596, 0.2835293966085596, 0.3593349889409539]
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.07360621784192689, 0.2835293966085596, 0.2835293966085596, 0.3593349889409539]
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.07360621784192689, 0.2835293966085596, 0.2835293966085596, 0.3593349889409539]
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.07360621784192689, 0.2835293966085596, 0.2835293966085596, 0.3593349889409539]
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
probs:  [0.06441194381408853, 0.31186268539530376, 0.31186268539530376, 0.31186268539530376]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.479]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]] [[0.496]
 [0.479]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]]
STARTED EXPV TRAINING ON FRAME NO.  11091
deleting a thread, now have 5 threads
Frames:  11096 train batches done:  1289 episodes:  250
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.07044573946163538, 0.24729315185118042, 0.34113055434359213, 0.34113055434359213]
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.08630787589137212, 0.30306062096884534, 0.4180722816221985, 0.19255922151758406]
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.07473325499513617, 0.3562640894911376, 0.3562640894911376, 0.21273856602258867]
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.07473325499513617, 0.3562640894911376, 0.3562640894911376, 0.21273856602258867]
deleting a thread, now have 4 threads
Frames:  11414 train batches done:  1320 episodes:  255
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.07473325499513617, 0.3562640894911376, 0.3562640894911376, 0.21273856602258867]
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.07473325499513617, 0.3562640894911376, 0.3562640894911376, 0.21273856602258867]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.07473325499513617, 0.3562640894911376, 0.3562640894911376, 0.21273856602258867]
from probs:  [0.07473325499513617, 0.3562640894911376, 0.3562640894911376, 0.21273856602258867]
first move QE:  -0.050929321924539236
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]]
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.10398301113983792, 0.29611062806110355, 0.4959233496592206, 0.10398301113983792]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.08996157371952877, 0.4100384262804712, 0.4100384262804712, 0.08996157371952877]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.13227813471539349, 0.13227813471539349, 0.6031655958538195, 0.13227813471539349]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.597]
 [0.633]
 [0.634]
 [0.632]
 [0.789]
 [0.637]] [[-0.006]
 [-0.005]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.005]
 [-0.006]] [[0.324]
 [0.298]
 [0.334]
 [0.335]
 [0.334]
 [0.491]
 [0.338]]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.2484203351262418, 0.08729451800291328, 0.4158648117446033, 0.2484203351262418]
first move QE:  -0.04920074811870643
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
line 256 mcts: sample exp_bonus -0.0020993688573071267
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.28362110601371154, 0.07366072968318711, 0.35909705828938976, 0.28362110601371154]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.28362110601371154, 0.07366072968318711, 0.35909705828938976, 0.28362110601371154]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.2729886470183288, 0.07781972539332846, 0.3762029805700138, 0.2729886470183288]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.3392004854820793, 0.09666844652426802, 0.4674626214693845, 0.09666844652426802]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.3392004854820793, 0.09666844652426802, 0.4674626214693845, 0.09666844652426802]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.3308033672986545, 0.08591008173791363, 0.4187137774999468, 0.16457277346348506]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.35905255474645087, 0.09323695255197628, 0.4544735401495967, 0.09323695255197628]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.38481868681080794, 0.15136050509285207, 0.38481868681080794, 0.07900212128553212]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.38481868681080794, 0.15136050509285207, 0.38481868681080794, 0.07900212128553212]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.4186092876787884, 0.1646433530199625, 0.3308185942164783, 0.08592876508477076]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.4186092876787884, 0.1646433530199625, 0.3308185942164783, 0.08592876508477076]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.4186092876787884, 0.1646433530199625, 0.3308185942164783, 0.08592876508477076]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.39387711292390576, 0.199111322254979, 0.3265506667667458, 0.0804608980543694]
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.39387711292390576, 0.199111322254979, 0.3265506667667458, 0.0804608980543694]
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.39387711292390576, 0.199111322254979, 0.3265506667667458, 0.0804608980543694]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.45142135813323914, 0.22819004402378948, 0.22819004402378948, 0.0921985538191819]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.4849866888629626, 0.17081667994013158, 0.24515154812276602, 0.09904508307413977]
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.061]
 [0.133]
 [0.137]
 [0.136]
 [0.137]
 [0.14 ]] [[-0.005]
 [-0.006]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.134]
 [0.061]
 [0.133]
 [0.137]
 [0.136]
 [0.137]
 [0.14 ]]
siam score:  -0.8633955
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.5542889730259366, 0.11379445969315222, 0.218122107587759, 0.11379445969315222]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.5542889730259366, 0.11379445969315222, 0.218122107587759, 0.11379445969315222]
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.054]
 [0.059]
 [0.054]
 [0.054]
 [0.054]
 [0.054]] [[-0.]
 [-0.]
 [ 0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[-0.292]
 [-0.292]
 [-0.287]
 [-0.292]
 [-0.292]
 [-0.292]
 [-0.292]]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.5542889730259366, 0.11379445969315222, 0.218122107587759, 0.11379445969315222]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.5329227578754778, 0.11039182689987312, 0.24629358832477602, 0.11039182689987312]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.5329227578754778, 0.11039182689987312, 0.24629358832477602, 0.11039182689987312]
siam score:  -0.87514275
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.525]
 [0.523]
 [0.525]
 [0.54 ]
 [0.546]
 [0.542]] [[-0.002]
 [-0.003]
 [-0.002]
 [-0.003]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.539]
 [0.525]
 [0.523]
 [0.525]
 [0.54 ]
 [0.546]
 [0.542]]
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.4511116628255104, 0.09220469532152231, 0.22834182092648372, 0.22834182092648372]
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.4511116628255104, 0.09220469532152231, 0.22834182092648372, 0.22834182092648372]
deleting a thread, now have 3 threads
Frames:  12897 train batches done:  1502 episodes:  292
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.41951186199024626, 0.08623430282298246, 0.2471269175933856, 0.2471269175933856]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.41951186199024626, 0.08623430282298246, 0.2471269175933856, 0.2471269175933856]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.41951186199024626, 0.08623430282298246, 0.2471269175933856, 0.2471269175933856]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.41951186199024626, 0.08623430282298246, 0.2471269175933856, 0.2471269175933856]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.3758078818811825, 0.07797697783593381, 0.2731075701414418, 0.2731075701414418]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.4172132998708512, 0.08655515935767395, 0.19303828935344342, 0.3031932514180313]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.4172132998708512, 0.08655515935767395, 0.19303828935344342, 0.3031932514180313]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.4172132998708512, 0.08655515935767395, 0.19303828935344342, 0.3031932514180313]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.4172132998708512, 0.08655515935767395, 0.19303828935344342, 0.3031932514180313]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.4688768811518158, 0.0972585793400992, 0.21693226975404242, 0.21693226975404242]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.5326401935315441, 0.11046876521860448, 0.2464222760312469, 0.11046876521860448]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.5326401935315441, 0.11046876521860448, 0.2464222760312469, 0.11046876521860448]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.45677182497923985, 0.18010232108471974, 0.2691992799660067, 0.09392657397003376]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.42068997224944416, 0.21321899086101856, 0.2800316797827151, 0.08605935710682211]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.45081841296600816, 0.22848374146792771, 0.22848374146792771, 0.09221410409813639]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.09415085102105514
using explorer policy with actor:  1
first move QE:  -0.0431498054147659
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.48437175929320647, 0.24548374052298497, 0.17107599697160472, 0.09906850321220381]
Printing some Q and Qe and total Qs values:  [[0.55]
 [0.55]
 [0.55]
 [0.55]
 [0.55]
 [0.55]
 [0.55]] [[-0.109]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]] [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.417019923893773, 0.3032158788228338, 0.19314311391815447, 0.08662108336523879]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.49444899916589785, 0.29633347043013814, 0.10460876520198194, 0.10460876520198194]
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.061]
 [0.087]
 [0.083]
 [0.072]
 [0.092]
 [0.079]] [[0.383]
 [0.598]
 [0.243]
 [0.21 ]
 [0.276]
 [0.373]
 [0.477]] [[-0.355]
 [-0.325]
 [-0.359]
 [-0.368]
 [-0.368]
 [-0.332]
 [-0.328]]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.447]
 [0.482]] [[2.79 ]
 [2.79 ]
 [2.79 ]
 [2.79 ]
 [2.79 ]
 [3.323]
 [2.79 ]] [[0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.481]
 [0.25 ]]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.49444899916589785, 0.29633347043013814, 0.10460876520198194, 0.10460876520198194]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]] [[0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]] [[-0.582]
 [-0.582]
 [-0.582]
 [-0.582]
 [-0.582]
 [-0.582]
 [-0.582]]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.49444899916589785, 0.29633347043013814, 0.10460876520198194, 0.10460876520198194]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.49444899916589785, 0.29633347043013814, 0.10460876520198194, 0.10460876520198194]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.5991741642754463, 0.1336086119081846, 0.1336086119081846, 0.1336086119081846]
first move QE:  -0.043008681627203185
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.4147847414237611, 0.24868172206860897, 0.08785181443902097, 0.24868172206860897]
line 256 mcts: sample exp_bonus 0.6248224796462906
siam score:  -0.8745367
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.31021839176824784, 0.31021839176824784, 0.06934482469525645, 0.31021839176824784]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.31021839176824784, 0.31021839176824784, 0.06934482469525645, 0.31021839176824784]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
using another actor
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.378]
 [0.33 ]
 [0.33 ]
 [0.309]
 [0.307]
 [0.359]] [[1.453]
 [0.   ]
 [1.634]
 [1.053]
 [0.929]
 [1.248]
 [1.513]] [[0.385]
 [0.378]
 [0.33 ]
 [0.33 ]
 [0.309]
 [0.307]
 [0.359]]
first move QE:  -0.04926582564976615
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.31018316072907354, 0.06945051781277949, 0.31018316072907354, 0.31018316072907354]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.31018316313364375, 0.06945051059906879, 0.31018316313364375, 0.31018316313364375]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.31018316313364375, 0.06945051059906879, 0.31018316313364375, 0.31018316313364375]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.09144026610341588, 0.09144026610341588, 0.4085597338965841, 0.4085597338965841]
using another actor
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.09144025791050675, 0.09144025791050675, 0.40855974208949325, 0.40855974208949325]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.09144025791050675, 0.09144025791050675, 0.40855974208949325, 0.40855974208949325]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.09144025791050675, 0.09144025791050675, 0.40855974208949325, 0.40855974208949325]
using another actor
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.09144024986587591, 0.09144024986587591, 0.40855975013412404, 0.40855975013412404]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.09144024986587591, 0.09144024986587591, 0.40855975013412404, 0.40855975013412404]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.09144024986587591, 0.09144024986587591, 0.40855975013412404, 0.40855975013412404]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.13387137853774375, 0.13387137853774375, 0.5983858643867688, 0.13387137853774375]
siam score:  -0.8759234
siam score:  -0.8773894
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.24872414232107923, 0.24872414232107923, 0.4145856405808111, 0.08796607477703033]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.24872414232107923, 0.24872414232107923, 0.4145856405808111, 0.08796607477703033]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.24872414232107923, 0.24872414232107923, 0.4145856405808111, 0.08796607477703033]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7118273692065428
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.10479941761722905, 0.29637296086570086, 0.494028203899841, 0.10479941761722905]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.10479941761722905, 0.29637296086570086, 0.494028203899841, 0.10479941761722905]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.10479941761722905, 0.29637296086570086, 0.494028203899841, 0.10479941761722905]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.10479941761722905, 0.29637296086570086, 0.494028203899841, 0.10479941761722905]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.10479941326615665, 0.2963729622553101, 0.4940282112123766, 0.10479941326615665]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.13400235657309698, 0.5979929302807091, 0.13400235657309698, 0.13400235657309698]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.24874436804776978, 0.4144877857421817, 0.08802347816227861, 0.24874436804776978]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
siam score:  -0.8759877
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.24874436804776978, 0.4144877857421817, 0.08802347816227861, 0.24874436804776978]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
start point for exploration sampling:  11091
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[1.905]
 [1.902]
 [1.902]
 [1.902]
 [1.902]
 [1.902]
 [1.902]] [[0.041]
 [0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.478]
 [0.475]
 [0.478]
 [0.478]
 [0.478]
 [0.485]] [[2.41 ]
 [2.41 ]
 [2.252]
 [2.41 ]
 [2.41 ]
 [2.41 ]
 [2.324]] [[0.124]
 [0.124]
 [0.095]
 [0.124]
 [0.124]
 [0.124]
 [0.117]]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.2963938840605156, 0.10492706095362496, 0.10492706095362496, 0.49375199403223446]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.2963938840605156, 0.10492706095362496, 0.10492706095362496, 0.49375199403223446]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.147]
 [0.153]
 [0.147]
 [0.147]
 [0.147]
 [0.147]] [[2.372]
 [2.878]
 [3.344]
 [2.878]
 [2.878]
 [2.878]
 [2.878]] [[-0.044]
 [ 0.224]
 [ 0.483]
 [ 0.224]
 [ 0.224]
 [ 0.224]
 [ 0.224]]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.13426354767877563, 0.13426354767877563, 0.13426354767877563, 0.597209356963673]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.08813880259090591, 0.2487829985157217, 0.2487829985157217, 0.4142952003776506]
siam score:  -0.8795781
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.08813880259090591, 0.2487829985157217, 0.2487829985157217, 0.4142952003776506]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.366]
 [0.432]
 [0.407]
 [0.38 ]
 [0.422]
 [0.411]] [[6.381]
 [6.698]
 [6.46 ]
 [6.496]
 [6.26 ]
 [6.189]
 [6.498]] [[0.686]
 [0.763]
 [0.726]
 [0.726]
 [0.652]
 [0.65 ]
 [0.728]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.08813880259090591, 0.2487829985157217, 0.2487829985157217, 0.4142952003776506]
UNIT TEST: sample policy line 217 mcts : [0.102 0.02  0.061 0.143 0.265 0.163 0.245]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.09192786113445935, 0.09192786113445935, 0.4080721388655406, 0.4080721388655406]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [-0.005]
 [-0.008]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.007]] [[2.734]
 [3.639]
 [3.013]
 [3.639]
 [3.639]
 [3.639]
 [3.369]] [[-0.063]
 [ 0.411]
 [ 0.083]
 [ 0.411]
 [ 0.411]
 [ 0.411]
 [ 0.268]]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.09192786113445935, 0.09192786113445935, 0.4080721388655406, 0.4080721388655406]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.09192786113445935, 0.09192786113445935, 0.4080721388655406, 0.4080721388655406]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.09192786113445935, 0.09192786113445935, 0.4080721388655406, 0.4080721388655406]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.09192786113445935, 0.09192786113445935, 0.4080721388655406, 0.4080721388655406]
siam score:  -0.88442993
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[7.633]
 [7.633]
 [7.633]
 [7.633]
 [7.633]
 [7.633]
 [7.633]] [[0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.13439374591487147, 0.13439374591487147, 0.5968187622553855, 0.13439374591487147]
deleting a thread, now have 2 threads
Frames:  15431 train batches done:  1805 episodes:  344
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.3100075036853301, 0.3100075036853301, 0.3100075036853301, 0.06997748894400957]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.3100075036853301, 0.3100075036853301, 0.3100075036853301, 0.06997748894400957]
using another actor
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.3100075036853301, 0.3100075036853301, 0.3100075036853301, 0.06997748894400957]
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.659]
 [0.659]
 [0.644]
 [0.634]
 [0.632]
 [0.628]] [[3.474]
 [3.474]
 [3.474]
 [3.402]
 [3.444]
 [3.514]
 [3.737]] [[0.786]
 [0.786]
 [0.786]
 [0.735]
 [0.746]
 [0.779]
 [0.886]]
using another actor
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.31000750605569094, 0.31000750605569094, 0.31000750605569094, 0.06997748183292722]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.40795066885933656, 0.40795066885933656, 0.09204933114066349, 0.09204933114066349]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.40795066885933656, 0.40795066885933656, 0.09204933114066349, 0.09204933114066349]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.40795066885933656, 0.40795066885933656, 0.09204933114066349, 0.09204933114066349]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.40795066885933656, 0.40795066885933656, 0.09204933114066349, 0.09204933114066349]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.5964290734662838, 0.13452364217790533, 0.13452364217790533, 0.13452364217790533]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.5964290734662838, 0.13452364217790533, 0.13452364217790533, 0.13452364217790533]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.4141065369482208, 0.08825470818772403, 0.24881937743202753, 0.24881937743202753]
start point for exploration sampling:  11091
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.705]
 [0.723]
 [0.72 ]
 [0.71 ]
 [0.716]
 [0.718]] [[7.612]
 [6.86 ]
 [7.052]
 [7.664]
 [7.567]
 [7.468]
 [7.712]] [[0.604]
 [0.45 ]
 [0.5  ]
 [0.6  ]
 [0.573]
 [0.562]
 [0.605]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.41622666267341785, 0.19356088362165122, 0.08693220069545282, 0.303280253009478]
using explorer policy with actor:  1
siam score:  -0.8815338
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4617],
        [-0.5771],
        [-0.3483],
        [-0.5703],
        [-0.3447],
        [-0.5755],
        [-0.5755],
        [-0.5764],
        [-0.5755],
        [-0.5107]], dtype=torch.float64)
-0.032346567066 -0.4940361485794907
-0.032346567066 -0.6094655509622288
-0.09703970119800001 -0.4453064584360036
-0.09703970119800001 -0.6673486406231353
-0.09703970119800001 -0.4417013452990619
-0.032346567066 -0.6078105462735776
-0.032346567066 -0.6078105462735776
-0.032346567066 -0.6087935772188172
-0.032346567066 -0.6078105462735776
-0.032346567066 -0.5430403118753634
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.248836783265109, 0.248836783265109, 0.08831287385014512, 0.41401355961963693]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.248836783265109, 0.248836783265109, 0.08831287385014512, 0.41401355961963693]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.30993747548949957, 0.30993747548949957, 0.07018757353150111, 0.30993747548949957]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.30993747548949957, 0.30993747548949957, 0.07018757353150111, 0.30993747548949957]
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.6  ]
 [0.645]
 [0.643]
 [0.653]
 [0.657]
 [0.66 ]] [[3.624]
 [3.72 ]
 [3.588]
 [3.665]
 [3.364]
 [3.616]
 [3.729]] [[0.193]
 [0.185]
 [0.207]
 [0.219]
 [0.178]
 [0.224]
 [0.246]]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.30993747548949957, 0.30993747548949957, 0.07018757353150111, 0.30993747548949957]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.30993747548949957, 0.30993747548949957, 0.07018757353150111, 0.30993747548949957]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.30993747548949957, 0.30993747548949957, 0.07018757353150111, 0.30993747548949957]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.30993747548949957, 0.30993747548949957, 0.07018757353150111, 0.30993747548949957]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.07586547028993768, 0.3552677665478906, 0.21359899661428103, 0.3552677665478906]
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.594]
 [0.602]
 [0.617]
 [0.607]
 [0.608]
 [0.616]] [[0.829]
 [1.863]
 [0.728]
 [0.81 ]
 [0.736]
 [0.797]
 [1.126]] [[0.978]
 [1.254]
 [0.961]
 [0.995]
 [0.966]
 [0.983]
 [1.077]]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.07119302682853704, 0.340351226726216, 0.24810451971903094, 0.340351226726216]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.07119302682853704, 0.340351226726216, 0.24810451971903094, 0.340351226726216]
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.582]
 [0.557]
 [0.563]
 [0.535]
 [0.56 ]
 [0.564]] [[4.36 ]
 [4.709]
 [4.974]
 [4.841]
 [4.977]
 [5.22 ]
 [5.094]] [[0.654]
 [0.789]
 [0.853]
 [0.815]
 [0.831]
 [0.937]
 [0.9  ]]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.06898297609688883, 0.3332957513590152, 0.2644255211850807, 0.3332957513590152]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.06898297609688883, 0.3332957513590152, 0.2644255211850807, 0.3332957513590152]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.06898297609688883, 0.3332957513590152, 0.2644255211850807, 0.3332957513590152]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.06898297609688883, 0.3332957513590152, 0.2644255211850807, 0.3332957513590152]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.06769473174471727, 0.32918309631290055, 0.2739390756294816, 0.32918309631290055]
siam score:  -0.86641675
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.07597570224645576, 0.247197658386082, 0.36938421863595605, 0.3074424207315061]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.07597570224645576, 0.247197658386082, 0.36938421863595605, 0.3074424207315061]
Printing some Q and Qe and total Qs values:  [[0.855]
 [0.86 ]
 [0.855]
 [0.829]
 [0.825]
 [0.828]
 [0.829]] [[4.92 ]
 [4.718]
 [4.299]
 [3.79 ]
 [3.601]
 [4.22 ]
 [3.79 ]] [[0.877]
 [0.83 ]
 [0.727]
 [0.595]
 [0.549]
 [0.699]
 [0.595]]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.07597570224645576, 0.247197658386082, 0.36938421863595605, 0.3074424207315061]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.603]
 [0.603]
 [0.599]
 [0.603]
 [0.603]
 [0.603]] [[2.143]
 [2.143]
 [2.143]
 [2.1  ]
 [2.143]
 [2.143]
 [2.143]] [[0.618]
 [0.618]
 [0.618]
 [0.585]
 [0.618]
 [0.618]
 [0.618]]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.07597570224645576, 0.247197658386082, 0.36938421863595605, 0.3074424207315061]
from probs:  [0.07597570224645576, 0.247197658386082, 0.36938421863595605, 0.3074424207315061]
siam score:  -0.8690215
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.08083975974954496, 0.263045227124245, 0.393069786001965, 0.263045227124245]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.02 ]
 [0.031]
 [0.029]
 [0.02 ]
 [0.02 ]
 [0.011]] [[4.432]
 [3.351]
 [3.417]
 [3.513]
 [3.351]
 [3.351]
 [4.139]] [[0.532]
 [0.18 ]
 [0.207]
 [0.238]
 [0.18 ]
 [0.18 ]
 [0.434]]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.08083975974954496, 0.263045227124245, 0.393069786001965, 0.263045227124245]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8718918
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.08621119309693902, 0.2805458819754103, 0.4192260355412665, 0.21401688938638433]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.08621119309693902, 0.2805458819754103, 0.4192260355412665, 0.21401688938638433]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.08621119309693902, 0.2805458819754103, 0.4192260355412665, 0.21401688938638433]
siam score:  -0.87040037
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.08621119309693902, 0.2805458819754103, 0.4192260355412665, 0.21401688938638433]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.08621119309693902, 0.2805458819754103, 0.4192260355412665, 0.21401688938638433]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.047]
 [-0.052]
 [-0.054]
 [-0.047]
 [-0.05 ]
 [-0.047]] [[0.981]
 [0.933]
 [1.075]
 [1.088]
 [0.933]
 [1.075]
 [0.933]] [[-0.624]
 [-0.649]
 [-0.584]
 [-0.579]
 [-0.649]
 [-0.581]
 [-0.649]]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.07985656646698029, 0.30599764759052783, 0.3855658057636285, 0.22857998017886344]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.07852339787063567, 0.2733528446479681, 0.3747709128334281, 0.2733528446479681]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.07852339787063567, 0.2733528446479681, 0.3747709128334281, 0.2733528446479681]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.07852339787063567, 0.2733528446479681, 0.3747709128334281, 0.2733528446479681]
siam score:  -0.8776689
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
siam score:  -0.8774735
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.07852339484014222, 0.2733528450606809, 0.3747709150384961, 0.2733528450606809]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.07852339484014222, 0.2733528450606809, 0.3747709150384961, 0.2733528450606809]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.07852339484014222, 0.2733528450606809, 0.3747709150384961, 0.2733528450606809]
siam score:  -0.8778352
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.07852339185872295, 0.2733528454667108, 0.3747709172078554, 0.2733528454667108]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.449]
 [0.362]
 [0.352]
 [0.35 ]
 [0.351]
 [0.349]] [[1.907]
 [2.656]
 [1.872]
 [1.857]
 [1.818]
 [1.869]
 [1.897]] [[0.347]
 [0.449]
 [0.362]
 [0.352]
 [0.35 ]
 [0.351]
 [0.349]]
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.326]
 [0.34 ]
 [0.329]
 [0.326]
 [0.326]
 [0.317]] [[4.967]
 [4.743]
 [4.985]
 [4.884]
 [4.772]
 [4.743]
 [4.828]] [[0.953]
 [0.888]
 [0.97 ]
 [0.933]
 [0.897]
 [0.888]
 [0.908]]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.07852339185872295, 0.2733528454667108, 0.3747709172078554, 0.2733528454667108]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.07852339185872295, 0.2733528454667108, 0.3747709172078554, 0.2733528454667108]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.07852339185872295, 0.2733528454667108, 0.3747709172078554, 0.2733528454667108]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.06668354992791647, 0.31110548335736116, 0.31110548335736116, 0.31110548335736116]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.06668354992791647, 0.31110548335736116, 0.31110548335736116, 0.31110548335736116]
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.406]
 [0.43 ]
 [0.445]
 [0.444]
 [0.437]
 [0.435]] [[6.401]
 [5.123]
 [6.547]
 [6.284]
 [6.284]
 [6.319]
 [6.38 ]] [[0.632]
 [0.203]
 [0.658]
 [0.591]
 [0.591]
 [0.595]
 [0.612]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
line 256 mcts: sample exp_bonus -1.4420754256062616
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.274]
 [0.308]
 [0.308]] [[2.484]
 [2.484]
 [2.484]
 [2.484]
 [1.448]
 [2.484]
 [2.484]] [[0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.274]
 [0.308]
 [0.308]]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.10550522969254654, 0.10550522969254654, 0.296444747598824, 0.4925447930160829]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.10550522969254654, 0.10550522969254654, 0.296444747598824, 0.4925447930160829]
line 256 mcts: sample exp_bonus 5.916800015360001
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.10550522969254654, 0.10550522969254654, 0.296444747598824, 0.4925447930160829]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.10550522969254654, 0.10550522969254654, 0.296444747598824, 0.4925447930160829]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.10550522969254654, 0.10550522969254654, 0.296444747598824, 0.4925447930160829]
siam score:  -0.87618226
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.10550522486015967, 0.10550522486015967, 0.29644474915209185, 0.49254480112758875]
from probs:  [0.10550522486015967, 0.10550522486015967, 0.29644474915209185, 0.49254480112758875]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.21762857262300225, 0.21762857262300225, 0.09785429132810979, 0.4668885634258858]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.263696123133413
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.21762857262300225, 0.21762857262300225, 0.09785429132810979, 0.4668885634258858]
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.21762857262300225, 0.21762857262300225, 0.09785429132810979, 0.4668885634258858]
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.21762857262300225, 0.21762857262300225, 0.09785429132810979, 0.4668885634258858]
siam score:  -0.88023984
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.21762857262300225, 0.21762857262300225, 0.09785429132810979, 0.4668885634258858]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.401]
 [0.401]
 [0.419]
 [0.401]
 [0.401]
 [0.349]] [[5.211]
 [5.211]
 [5.211]
 [5.211]
 [5.211]
 [5.211]
 [5.227]] [[0.751]
 [0.751]
 [0.751]
 [0.761]
 [0.751]
 [0.751]
 [0.729]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.24893155135444767, 0.24893155135444767, 0.08866425452146712, 0.4134726427696375]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.13555350071932726, 0.13555350071932726, 0.13555350071932726, 0.5933394978420183]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.13555350071932726, 0.13555350071932726, 0.13555350071932726, 0.5933394978420183]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.13555350071932726, 0.13555350071932726, 0.13555350071932726, 0.5933394978420183]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.3096934477048831, 0.07091965688535058, 0.3096934477048831, 0.3096934477048831]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.3096587180475189, 0.3096587180475189, 0.3096587180475189, 0.07102384585744347]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.13580837338025226, 0.5925748798592433, 0.13580837338025226, 0.13580837338025226]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.08884126349319534, 0.41321171404192225, 0.24897351123244119, 0.24897351123244119]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.08884125871362034, 0.41321171888238517, 0.2489735112019972, 0.2489735112019972]
line 256 mcts: sample exp_bonus 6.882792818538844
first move QE:  0.17582457680983873
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.08884125871362034, 0.41321171888238517, 0.2489735112019972, 0.2489735112019972]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.10576355721828325, 0.4920238616167794, 0.10576355721828325, 0.296449023946654]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.10576355721828325, 0.4920238616167794, 0.10576355721828325, 0.296449023946654]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.09337511431987433, 0.4066248856801257, 0.09337511431987433, 0.4066248856801257]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.09337511431987433, 0.4066248856801257, 0.09337511431987433, 0.4066248856801257]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.13593541291028766, 0.5921937612691369, 0.13593541291028766, 0.13593541291028766]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.335]
 [0.335]
 [0.394]
 [0.335]
 [0.335]
 [0.335]] [[6.379]
 [6.379]
 [6.379]
 [4.668]
 [6.379]
 [6.379]
 [6.379]] [[0.881]
 [0.881]
 [0.881]
 [0.607]
 [0.881]
 [0.881]
 [0.881]]
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.697]
 [0.697]
 [0.693]
 [0.697]
 [0.697]
 [0.697]] [[5.459]
 [5.459]
 [5.459]
 [5.145]
 [5.459]
 [5.459]
 [5.459]] [[0.928]
 [0.928]
 [0.928]
 [0.815]
 [0.928]
 [0.928]
 [0.928]]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.27340974120040695, 0.37444125585479904, 0.078739261744387, 0.27340974120040695]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.3109964435658276, 0.3109964435658276, 0.06701066930251715, 0.3109964435658276]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.49176640260456395, 0.10589293243767277, 0.10589293243767277, 0.2964477325200905]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.49176640260456395, 0.10589293243767277, 0.10589293243767277, 0.2964477325200905]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.49176640260456395, 0.10589293243767277, 0.10589293243767277, 0.2964477325200905]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.4063858985371094, 0.0936141014628906, 0.0936141014628906, 0.4063858985371094]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.591]
 [0.678]
 [0.604]
 [0.596]
 [0.647]
 [0.622]] [[ 0.006]
 [-0.013]
 [ 0.006]
 [-0.008]
 [-0.007]
 [ 0.007]
 [ 0.006]] [[0.639]
 [0.591]
 [0.678]
 [0.604]
 [0.596]
 [0.647]
 [0.622]]
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
probs:  [0.40638590791066465, 0.0936140920893354, 0.0936140920893354, 0.40638590791066465]
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
probs:  [0.40638590791066465, 0.0936140920893354, 0.0936140920893354, 0.40638590791066465]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.40638591714057065, 0.09361408285942933, 0.09361408285942933, 0.40638591714057065]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.278]
 [0.368]
 [0.361]
 [0.364]
 [0.381]
 [0.365]] [[0.932]
 [4.316]
 [0.793]
 [0.873]
 [0.405]
 [0.409]
 [0.735]] [[0.354]
 [0.278]
 [0.368]
 [0.361]
 [0.364]
 [0.381]
 [0.365]]
siam score:  -0.8747479
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.40638591714057065, 0.09361408285942933, 0.09361408285942933, 0.40638591714057065]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[1.53]
 [1.53]
 [1.53]
 [1.53]
 [1.53]
 [1.53]
 [1.53]] [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.40638591714057065, 0.09361408285942933, 0.09361408285942933, 0.40638591714057065]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.40638591714057065, 0.09361408285942933, 0.09361408285942933, 0.40638591714057065]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.40638591714057065, 0.09361408285942933, 0.09361408285942933, 0.40638591714057065]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.40638591714057065, 0.09361408285942933, 0.09361408285942933, 0.40638591714057065]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.40638592623010544, 0.09361407376989456, 0.09361407376989456, 0.40638592623010544]
Printing some Q and Qe and total Qs values:  [[-0.115]
 [-0.115]
 [-0.115]
 [-0.115]
 [-0.11 ]
 [-0.115]
 [-0.115]] [[4.007]
 [4.007]
 [4.007]
 [4.007]
 [4.085]
 [4.007]
 [4.007]] [[0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.158]
 [0.115]
 [0.115]]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.40638592623010544, 0.09361407376989456, 0.09361407376989456, 0.40638592623010544]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.3547816849615864, 0.21394152541807793, 0.07649510465874926, 0.3547816849615864]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.3547816876625778, 0.21394152448858647, 0.07649510018625802, 0.3547816876625778]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.3547816876625778, 0.21394152448858647, 0.07649510018625802, 0.3547816876625778]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.4113309622465339, 0.08866903775346609, 0.08866903775346609, 0.4113309622465339]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.737]
 [0.727]
 [0.727]
 [0.733]
 [0.727]
 [0.727]] [[2.628]
 [2.2  ]
 [2.628]
 [2.628]
 [1.777]
 [2.628]
 [2.628]] [[0.244]
 [0.182]
 [0.244]
 [0.244]
 [0.108]
 [0.244]
 [0.244]]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.29644631235535673, 0.10595763889794023, 0.10595763889794023, 0.4916384098487629]
using another actor
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.09811965687503671, 0.21785014634581099, 0.21785014634581099, 0.4661800504333413]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.09811965343375006, 0.21785014561736343, 0.21785014561736343, 0.466180055331523]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.09811965343375006, 0.21785014561736343, 0.21785014561736343, 0.466180055331523]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.0890783006024223, 0.24902471697334902, 0.24902471697334902, 0.41287226545087974]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.07154323796154594, 0.30948558734615134, 0.30948558734615134, 0.30948558734615134]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.07154323796154594, 0.30948558734615134, 0.30948558734615134, 0.30948558734615134]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.07154323796154594, 0.30948558734615134, 0.30948558734615134, 0.30948558734615134]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.07154323796154594, 0.30948558734615134, 0.30948558734615134, 0.30948558734615134]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.07154323796154594, 0.30948558734615134, 0.30948558734615134, 0.30948558734615134]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.09385240792326409, 0.4061475920767359, 0.09385240792326409, 0.4061475920767359]
siam score:  -0.8870545
UNIT TEST: sample policy line 217 mcts : [0.082 0.    0.204 0.469 0.122 0.041 0.082]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.1364409953267476, 0.5906770140197571, 0.1364409953267476, 0.1364409953267476]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.1364409953267476, 0.5906770140197571, 0.1364409953267476, 0.1364409953267476]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.1364409953267476, 0.5906770140197571, 0.1364409953267476, 0.1364409953267476]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.1364409953267476, 0.5906770140197571, 0.1364409953267476, 0.1364409953267476]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[3.3]
 [3.3]
 [3.3]
 [3.3]
 [3.3]
 [3.3]
 [3.3]] [[0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.24903675286517613, 0.4127887657853165, 0.08913772848433135, 0.24903675286517613]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.10608715399293918, 0.49138369788435154, 0.10608715399293918, 0.29644199412977007]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.0939713472229668, 0.4060286527770332, 0.0939713472229668, 0.4060286527770332]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.13656676548752422, 0.13656676548752422, 0.13656676548752422, 0.5902997035374273]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.24904850422413283, 0.08919721387856933, 0.24904850422413283, 0.41270577767316513]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.24904850422413283, 0.08919721387856933, 0.24904850422413283, 0.41270577767316513]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.24904850422413283, 0.08919721387856933, 0.24904850422413283, 0.41270577767316513]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.24904850422413283, 0.08919721387856933, 0.24904850422413283, 0.41270577767316513]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.24904850422413283, 0.08919721387856933, 0.24904850422413283, 0.41270577767316513]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.30941656204041473, 0.07175031387875584, 0.30941656204041473, 0.30941656204041473]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.09409012915583652, 0.09409012915583652, 0.40590987084416347, 0.40590987084416347]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.13669228239222717, 0.13669228239222717, 0.5899231528233184, 0.13669228239222717]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.13669228239222717, 0.13669228239222717, 0.5899231528233184, 0.13669228239222717]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.006]
 [-0.001]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.002]] [[7.454]
 [3.006]
 [7.79 ]
 [7.164]
 [6.696]
 [6.155]
 [6.061]] [[ 0.436]
 [-0.359]
 [ 0.496]
 [ 0.385]
 [ 0.301]
 [ 0.205]
 [ 0.187]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 7.42868407533679
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.1366922723542228, 0.1366922723542228, 0.5899231829373316, 0.1366922723542228]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.1366922723542228, 0.1366922723542228, 0.5899231829373316, 0.1366922723542228]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.24905998098454166, 0.08925674835668133, 0.41262328967423534, 0.24905998098454166]
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.091]
 [0.074]] [[3.333]
 [2.692]
 [2.692]
 [2.692]
 [2.692]
 [2.726]
 [2.692]] [[0.407]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.109]
 [0.075]]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.24905998098454166, 0.08925674835668133, 0.41262328967423534, 0.24905998098454166]
Printing some Q and Qe and total Qs values:  [[0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]] [[5.581]
 [5.581]
 [5.581]
 [5.581]
 [5.581]
 [5.581]
 [5.581]] [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.822]
 [0.679]
 [0.705]
 [0.665]
 [0.715]
 [0.674]] [[3.121]
 [4.342]
 [3.177]
 [3.111]
 [3.016]
 [3.055]
 [3.258]] [[0.38 ]
 [0.748]
 [0.423]
 [0.416]
 [0.379]
 [0.407]
 [0.44 ]]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.4057912556845693, 0.0942087443154307, 0.4057912556845693, 0.0942087443154307]
Starting evaluation
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.22308514973960628
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.5895474206623356, 0.13681752644588818, 0.13681752644588818, 0.13681752644588818]
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.101]
 [0.105]
 [0.107]
 [0.106]
 [0.107]
 [0.106]] [[-0.126]
 [ 0.968]
 [-0.936]
 [-0.944]
 [-1.198]
 [-0.757]
 [-0.769]] [[0.1  ]
 [0.101]
 [0.105]
 [0.107]
 [0.106]
 [0.107]
 [0.106]]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.5895474505189425, 0.13681751649368584, 0.13681751649368584, 0.13681751649368584]
from probs:  [0.5895474505189425, 0.13681751649368584, 0.13681751649368584, 0.13681751649368584]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.637]
 [0.672]
 [0.662]
 [0.667]
 [0.665]
 [0.663]] [[2.765]
 [4.14 ]
 [2.11 ]
 [2.375]
 [2.178]
 [2.298]
 [2.368]] [[0.654]
 [0.637]
 [0.672]
 [0.662]
 [0.667]
 [0.665]
 [0.663]]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.4910048084008255, 0.2964321190496994, 0.10628153627473759, 0.10628153627473759]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.4910048171196326, 0.2964321207294703, 0.10628153107544855, 0.10628153107544855]
using explorer policy with actor:  0
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.567]
 [0.555]
 [0.532]
 [0.535]
 [0.576]
 [0.545]] [[5.403]
 [6.145]
 [4.565]
 [5.127]
 [4.655]
 [4.01 ]
 [4.841]] [[0.543]
 [0.567]
 [0.555]
 [0.532]
 [0.535]
 [0.576]
 [0.545]]
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.161]
 [0.176]
 [0.161]
 [0.166]
 [0.161]
 [0.161]] [[-0.072]
 [ 0.123]
 [ 0.217]
 [ 0.123]
 [ 0.41 ]
 [ 0.123]
 [ 0.123]] [[0.161]
 [0.161]
 [0.176]
 [0.161]
 [0.166]
 [0.161]
 [0.161]]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.546]
 [0.526]
 [0.546]
 [0.546]
 [0.514]
 [0.546]] [[3.479]
 [3.479]
 [4.377]
 [3.479]
 [3.479]
 [3.676]
 [3.479]] [[0.495]
 [0.495]
 [0.787]
 [0.495]
 [0.495]
 [0.54 ]
 [0.495]]
maxi score, test score, baseline:  -0.9930034482758621 -1.0 -0.9930034482758621
probs:  [0.4056728708589719, 0.4056728708589719, 0.0943271291410281, 0.0943271291410281]
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.4056729045280423, 0.4056729045280423, 0.09432709547195771, 0.09432709547195771]
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.4056729045280423, 0.4056729045280423, 0.09432709547195771, 0.09432709547195771]
siam score:  -0.8666354
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.354527656341943, 0.354527656341943, 0.07683909005995107, 0.21410559725616277]
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.354527656341943, 0.354527656341943, 0.07683909005995107, 0.21410559725616277]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.8806398998071905
siam score:  -0.8652437
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.888]
 [0.888]
 [0.951]
 [0.888]
 [0.888]
 [0.888]] [[7.751]
 [7.751]
 [7.751]
 [5.393]
 [7.751]
 [7.751]
 [7.751]] [[1.023]
 [1.023]
 [1.023]
 [0.594]
 [1.023]
 [1.023]
 [1.023]]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.35452765873994097, 0.35452765873994097, 0.07683908608741824, 0.21410559643269997]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.35452765873994097, 0.35452765873994097, 0.07683908608741824, 0.21410559643269997]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.47 ]
 [0.405]
 [0.416]
 [0.416]
 [0.423]
 [0.426]] [[2.376]
 [1.678]
 [2.177]
 [2.12 ]
 [1.923]
 [1.997]
 [1.627]] [[0.312]
 [0.275]
 [0.294]
 [0.295]
 [0.262]
 [0.281]
 [0.222]]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.2964280002242952, 0.4908793893989786, 0.10634630518836315, 0.10634630518836315]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.29642800593311613, 0.49087941901769155, 0.10634628752459616, 0.10634628752459616]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.29642800593311613, 0.49087941901769155, 0.10634628752459616, 0.10634628752459616]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.29642800593311613, 0.49087941901769155, 0.10634628752459616, 0.10634628752459616]
siam score:  -0.8658737
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.40555465905748217, 0.40555465905748217, 0.09444534094251787, 0.09444534094251787]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.35448594310448506, 0.35448594310448506, 0.07689642261794309, 0.2141316911730867]
siam score:  -0.8651398
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.35448594310448506, 0.35448594310448506, 0.07689642261794309, 0.2141316911730867]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.4123787248731551, 0.24909285628562522, 0.08943556255559457, 0.24909285628562522]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.49075428953346495, 0.10641111879842682, 0.10641111879842682, 0.2964234728696814]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
siam score:  -0.86546683
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.40543650763313516, 0.0945634923668649, 0.0945634923668649, 0.40543650763313516]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.24910332562475399, 0.0894952868309599, 0.24910332562475399, 0.4122980619195321]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.292]
 [0.235]
 [0.226]
 [0.223]
 [0.247]
 [0.247]] [[6.382]
 [4.595]
 [4.809]
 [4.298]
 [6.382]
 [4.254]
 [3.976]] [[0.413]
 [0.183]
 [0.162]
 [0.067]
 [0.413]
 [0.081]
 [0.035]]
line 256 mcts: sample exp_bonus 3.3771504192591992
first move QE:  0.45383399648572276
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.10647596170287567, 0.10647596170287567, 0.2964185551369879, 0.49062952145726085]
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.1064759572978819, 0.1064759572978819, 0.29641855656165245, 0.4906295288425838]
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.1064759572978819, 0.1064759572978819, 0.29641855656165245, 0.4906295288425838]
siam score:  -0.8650377
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.003]
 [0.021]
 [0.028]
 [0.026]
 [0.032]
 [0.018]] [[1.963]
 [2.285]
 [1.023]
 [1.181]
 [1.263]
 [1.531]
 [1.26 ]] [[0.011]
 [0.003]
 [0.021]
 [0.028]
 [0.026]
 [0.032]
 [0.018]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.405013421381451
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]] [[1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]] [[0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]]
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.24911356379426572, 0.08955504676212239, 0.4122178256493461, 0.24911356379426572]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.448]
 [0.1  ]
 [0.365]
 [0.365]
 [0.059]
 [0.112]] [[1.647]
 [2.666]
 [0.723]
 [1.183]
 [0.037]
 [0.431]
 [1.278]] [[0.206]
 [0.448]
 [0.1  ]
 [0.365]
 [0.365]
 [0.059]
 [0.112]]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.2964132730240844, 0.10654079247101114, 0.4905051420338933, 0.10654079247101114]
from probs:  [0.3032749043671573, 0.19434343995941172, 0.41462706798396204, 0.08775458768946888]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.11184065549089947, 0.24773509271296493, 0.5285835963052361, 0.11184065549089947]
using explorer policy with actor:  1
siam score:  -0.86751807
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.13156308801789612, 0.13156308801789612, 0.6053107359463116, 0.13156308801789612]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.748]
 [0.69 ]
 [0.671]
 [0.684]
 [0.679]
 [0.679]] [[5.645]
 [5.627]
 [5.529]
 [5.427]
 [5.667]
 [5.644]
 [5.721]] [[0.819]
 [0.84 ]
 [0.78 ]
 [0.741]
 [0.815]
 [0.806]
 [0.827]]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.3091418588714711, 0.3091418588714711, 0.0725744233855868, 0.3091418588714711]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.753]
 [0.659]
 [0.59 ]
 [0.674]
 [0.58 ]
 [0.652]] [[3.128]
 [3.619]
 [2.951]
 [2.81 ]
 [2.687]
 [2.9  ]
 [3.081]] [[0.325]
 [0.48 ]
 [0.31 ]
 [0.259]
 [0.259]
 [0.275]
 [0.335]]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.614]
 [0.487]
 [0.487]
 [0.484]
 [0.487]
 [0.487]] [[2.645]
 [3.315]
 [2.964]
 [2.964]
 [2.567]
 [2.964]
 [2.964]] [[0.21 ]
 [0.388]
 [0.275]
 [0.275]
 [0.191]
 [0.275]
 [0.275]]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.31160445491261995, 0.31160445491261995, 0.06518663526214023, 0.31160445491261995]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.31177593998048564, 0.31177593998048564, 0.06467218005854306, 0.31177593998048564]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.31177593998048564, 0.31177593998048564, 0.06467218005854306, 0.31177593998048564]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.31177593998048564, 0.31177593998048564, 0.06467218005854306, 0.31177593998048564]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.85 ]
 [0.747]
 [0.759]
 [0.732]
 [0.722]
 [0.788]] [[2.972]
 [3.943]
 [2.718]
 [2.965]
 [2.941]
 [2.524]
 [2.748]] [[0.699]
 [1.036]
 [0.767]
 [0.814]
 [0.786]
 [0.717]
 [0.808]]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.3115951718927724, 0.3115951718927724, 0.06521448432168273, 0.3115951718927724]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.3564262272722865, 0.21256732695940275, 0.07458021849602427, 0.3564262272722865]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.3564262272722865, 0.21256732695940275, 0.07458021849602427, 0.3564262272722865]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.4163339250643118, 0.2482852172673782, 0.08709564040093186, 0.2482852172673782]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.4163339250643118, 0.2482852172673782, 0.08709564040093186, 0.2482852172673782]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.13386972073835915, 0.6166148601862631, 0.12350180183554378, 0.12601361723983406]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.13154743469115432, 0.6182685971840179, 0.12383270724571911, 0.12635126087910856]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.13154743469115432, 0.6182685971840179, 0.12383270724571911, 0.12635126087910856]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.1336490719256204, 0.6168193437706359, 0.125990627534319, 0.12354095676942463]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.13535002649853003, 0.6139253677591754, 0.12776441361697355, 0.12296019212532103]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.13535002649853003, 0.6139253677591754, 0.12776441361697355, 0.12296019212532103]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.13698619135921838, 0.6111416238937958, 0.12947063582010546, 0.12240154892688042]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.1350627231178072, 0.6123470772451435, 0.12994405286793703, 0.1226461467691122]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
siam score:  -0.8660651
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.1350627231178072, 0.6123470772451435, 0.12994405286793703, 0.1226461467691122]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.13520942410410003, 0.6121635705789188, 0.13001585146822403, 0.12261115384875725]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.612]
 [0.695]
 [0.612]
 [0.612]
 [0.612]
 [0.612]] [[2.256]
 [2.256]
 [0.378]
 [2.256]
 [2.256]
 [2.256]
 [2.256]] [[0.612]
 [0.612]
 [0.695]
 [0.612]
 [0.612]
 [0.612]
 [0.612]]
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.734]
 [0.595]
 [0.596]
 [0.598]
 [0.593]
 [0.586]] [[-0.16 ]
 [ 3.037]
 [-1.178]
 [-1.02 ]
 [-0.899]
 [-0.864]
 [-1.245]] [[0.469]
 [0.94 ]
 [0.332]
 [0.353]
 [0.37 ]
 [0.374]
 [0.32 ]]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.13535770426173072, 0.6119780886435816, 0.1300884229198055, 0.12257578417488238]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.13570422663529583, 0.6135461354703284, 0.12786008996989062, 0.1228895479244853]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.13351298915857085, 0.61504724385558, 0.12824808165201382, 0.12319168533383529]
using explorer policy with actor:  1
from probs:  [0.13351298915857085, 0.61504724385558, 0.12824808165201382, 0.12319168533383529]
from probs:  [0.13351298915857085, 0.61504724385558, 0.12824808165201382, 0.12319168533383529]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.1336359303589307, 0.6149045058354334, 0.12829465280664046, 0.12316491099899539]
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.656]
 [0.651]
 [0.666]
 [0.632]
 [0.627]
 [0.657]] [[7.642]
 [4.975]
 [7.488]
 [6.789]
 [7.164]
 [7.35 ]
 [7.762]] [[1.069]
 [0.428]
 [1.046]
 [0.885]
 [0.952]
 [0.995]
 [1.119]]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.13363611767502714, 0.6149042884614895, 0.12829472372805836, 0.12316487013542494]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 7.978314192953054
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.328]
 [0.279]
 [0.295]
 [0.29 ]
 [0.286]
 [0.293]] [[7.434]
 [6.762]
 [7.242]
 [7.273]
 [7.632]
 [7.592]
 [7.567]] [[0.813]
 [0.659]
 [0.752]
 [0.774]
 [0.869]
 [0.854]
 [0.853]]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.13363611767502714, 0.6149042884614895, 0.12829472372805836, 0.12316487013542494]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.13363611767502714, 0.6149042884614895, 0.12829472372805836, 0.12316487013542494]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.13363611767502714, 0.6149042884614895, 0.12829472372805836, 0.12316487013542494]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.1355473508330941, 0.6117382110356612, 0.13018315374993464, 0.12253128438131006]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.13554757584735444, 0.6117379296102936, 0.1301832638887267, 0.12253123065362526]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.13554779817391543, 0.6117376515464296, 0.1301833727119585, 0.12253117756769649]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.13554779817391543, 0.6117376515464296, 0.1301833727119585, 0.12253117756769649]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.13554779817391543, 0.6117376515464296, 0.1301833727119585, 0.12253117756769649]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.1372663511975341, 0.608831807723221, 0.1319539044087096, 0.12194793667053526]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.1393078012686482, 0.6055091564367642, 0.13389828680640242, 0.12128475548818503]
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.023]
 [-0.023]
 [-0.031]
 [-0.023]
 [-0.023]
 [-0.023]] [[2.223]
 [2.223]
 [2.223]
 [1.303]
 [2.223]
 [2.223]
 [2.223]] [[-0.233]
 [-0.233]
 [-0.233]
 [-0.394]
 [-0.233]
 [-0.233]
 [-0.233]]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.1395044969933073, 0.6052420533889319, 0.13402042015787827, 0.12123302945988272]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.1347590700582802, 0.6085807368006962, 0.1347590700582802, 0.12190112308274342]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.1347590700582802, 0.6085807368006962, 0.1347590700582802, 0.12190112308274342]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 6.675130484920658
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.1366222814148312, 0.6054757058864311, 0.1366222814148312, 0.1212797312839065]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.1366222814148312, 0.6054757058864311, 0.1366222814148312, 0.1212797312839065]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.1366222814148312, 0.6054757058864311, 0.1366222814148312, 0.1212797312839065]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
line 256 mcts: sample exp_bonus 6.216576899736005
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.1367797492390544, 0.6052118480526655, 0.1367797492390544, 0.12122865346922589]
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.091]
 [-0.073]
 [-0.072]
 [-0.073]
 [-0.074]
 [-0.08 ]] [[6.654]
 [6.624]
 [6.493]
 [6.511]
 [6.608]
 [6.65 ]
 [6.955]] [[0.13 ]
 [0.114]
 [0.099]
 [0.103]
 [0.121]
 [0.129]
 [0.186]]
Printing some Q and Qe and total Qs values:  [[0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]] [[3.641]
 [3.641]
 [3.641]
 [3.641]
 [3.641]
 [3.641]
 [3.641]] [[0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.065]
 [-0.083]
 [-0.084]
 [-0.085]
 [-0.083]
 [-0.084]] [[3.73 ]
 [4.307]
 [3.769]
 [3.691]
 [3.72 ]
 [3.738]
 [3.868]] [[-0.365]
 [-0.171]
 [-0.353]
 [-0.378]
 [-0.371]
 [-0.363]
 [-0.324]]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.687]
 [0.589]
 [0.597]
 [0.58 ]
 [0.598]
 [0.608]] [[3.145]
 [4.155]
 [2.786]
 [2.129]
 [1.671]
 [2.997]
 [3.241]] [[0.965]
 [1.234]
 [0.908]
 [0.806]
 [0.713]
 [0.952]
 [1.003]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.08666666666666623  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.13408022736449385, 0.6097465837319341, 0.13408022736449385, 0.12209296153907807]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13364105128552153, 0.6089735258965251, 0.13544812420972047, 0.12193729860823284]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13373923533726542, 0.6087919148762406, 0.13556693061886943, 0.1219019191676245]
line 256 mcts: sample exp_bonus 4.714513170422417
siam score:  -0.874963
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.1339839900184843, 0.6099070337643011, 0.1339839900184843, 0.12212498619873025]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.1339839900184843, 0.6099070337643011, 0.1339839900184843, 0.12212498619873025]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.1339839900184843, 0.6099070337643011, 0.1339839900184843, 0.12212498619873025]
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.144]
 [0.123]
 [0.136]
 [0.137]
 [0.129]
 [0.122]] [[6.206]
 [6.672]
 [6.565]
 [6.222]
 [6.539]
 [6.45 ]
 [6.802]] [[0.345]
 [0.45 ]
 [0.411]
 [0.34 ]
 [0.415]
 [0.388]
 [0.465]]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13422477002587085, 0.6110040437694503, 0.13242675545383678, 0.1223444307508421]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13422477002587085, 0.6110040437694503, 0.13242675545383678, 0.1223444307508421]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13422477002587085, 0.6110040437694503, 0.13242675545383678, 0.1223444307508421]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13422477002587085, 0.6110040437694503, 0.13242675545383678, 0.1223444307508421]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13432937542699722, 0.6108459678387895, 0.13251088885351245, 0.12231376788070066]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.1344345559707846, 0.6106870227731664, 0.13259548483644715, 0.1222829364196018]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.669]
 [0.67 ]
 [0.67 ]] [[2.562]
 [2.562]
 [2.562]
 [2.562]
 [2.363]
 [2.562]
 [2.562]] [[0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.559]
 [0.593]
 [0.593]]
using explorer policy with actor:  1
start point for exploration sampling:  11091
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]] [[3.569]
 [3.728]
 [3.728]
 [3.728]
 [3.728]
 [3.728]
 [3.728]] [[0.534]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.34 ]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[6.993]
 [7.705]
 [6.993]
 [6.993]
 [6.993]
 [6.993]
 [6.993]] [[0.745]
 [0.883]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.475]
 [0.475]
 [0.376]
 [0.475]
 [0.375]
 [0.444]] [[3.803]
 [3.803]
 [3.803]
 [3.699]
 [3.803]
 [3.643]
 [4.212]] [[0.432]
 [0.432]
 [0.432]
 [0.303]
 [0.432]
 [0.283]
 [0.533]]
siam score:  -0.8687923
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 7.452744601434096
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13301609552887933, 0.6115169118176873, 0.13301609552887933, 0.12245089712455413]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.367]
 [0.401]
 [0.435]
 [0.439]
 [0.443]
 [0.426]] [[6.909]
 [5.947]
 [6.448]
 [6.824]
 [6.323]
 [6.954]
 [6.825]] [[0.55 ]
 [0.297]
 [0.433]
 [0.541]
 [0.431]
 [0.575]
 [0.535]]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13141152189426591, 0.6126489887757421, 0.13326212573110957, 0.12267736359888226]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.61 ]
 [0.534]
 [0.535]
 [0.535]
 [0.527]
 [0.601]] [[4.347]
 [4.399]
 [3.618]
 [3.516]
 [3.492]
 [3.658]
 [4.252]] [[0.718]
 [0.743]
 [0.5  ]
 [0.476]
 [0.47 ]
 [0.505]
 [0.701]]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13141152189426591, 0.6126489887757421, 0.13326212573110957, 0.12267736359888226]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.672]
 [0.581]
 [0.589]
 [0.582]
 [0.587]
 [0.587]] [[-0.181]
 [ 2.364]
 [-0.655]
 [-0.77 ]
 [-0.766]
 [-0.859]
 [-0.605]] [[0.237]
 [0.561]
 [0.182]
 [0.171]
 [0.17 ]
 [0.161]
 [0.19 ]]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13148272297184138, 0.6125122782237038, 0.13335399896153333, 0.12265099984292142]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13148272297184138, 0.6125122782237038, 0.13335399896153333, 0.12265099984292142]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13148272297184138, 0.6125122782237038, 0.13335399896153333, 0.12265099984292142]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13180357257754727, 0.6135360588550492, 0.13180357257754727, 0.1228567959898561]
siam score:  -0.8672693
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.13319268405219342, 0.6112202533077803, 0.13319268405219342, 0.12239437858783297]
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.13319280817388715, 0.6112200453469074, 0.13319280817388715, 0.12239433830531814]
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.13319280817388715, 0.6112200453469074, 0.13319280817388715, 0.12239433830531814]
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.13319280817388715, 0.6112200453469074, 0.13319280817388715, 0.12239433830531814]
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.13319280817388715, 0.6112200453469074, 0.13319280817388715, 0.12239433830531814]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
actor:  1 policy actor:  1  step number:  48 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.1017457255280298, 0.4659319589274701, 0.338991308120885, 0.09333100742361497]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.999]
 [0.636]
 [0.636]
 [0.66 ]
 [0.636]
 [0.636]] [[2.171]
 [3.116]
 [2.171]
 [2.171]
 [1.174]
 [2.171]
 [2.171]] [[0.658]
 [1.302]
 [0.658]
 [0.658]
 [0.292]
 [0.658]
 [0.658]]
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.1017457255280298, 0.4659319589274701, 0.338991308120885, 0.09333100742361497]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.1021271782988069, 0.4676801852779198, 0.33651189004224996, 0.09368074638102343]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.1021271782988069, 0.4676801852779198, 0.33651189004224996, 0.09368074638102343]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.1021271782988069, 0.4676801852779198, 0.33651189004224996, 0.09368074638102343]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.7373887398645254
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.10212725145856688, 0.4676799235873356, 0.3365121299098927, 0.09368069504420477]
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.633]
 [0.628]
 [0.621]
 [0.63 ]
 [0.628]
 [0.623]] [[6.226]
 [5.86 ]
 [6.143]
 [6.137]
 [6.147]
 [6.086]
 [6.175]] [[0.924]
 [0.805]
 [0.894]
 [0.885]
 [0.898]
 [0.875]
 [0.899]]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.451]
 [0.534]
 [0.533]
 [0.531]
 [0.451]
 [0.527]] [[5.449]
 [6.466]
 [5.67 ]
 [5.826]
 [5.954]
 [6.466]
 [5.241]] [[0.433]
 [0.606]
 [0.487]
 [0.52 ]
 [0.547]
 [0.606]
 [0.388]]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.10212725145856688, 0.4676799235873356, 0.3365121299098927, 0.09368069504420477]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.10212725145856688, 0.4676799235873356, 0.3365121299098927, 0.09368069504420477]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.10212725145856688, 0.4676799235873356, 0.3365121299098927, 0.09368069504420477]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.10193898995146844, 0.4663873320110586, 0.33825080379838945, 0.09342287423908352]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.10269473117336837, 0.46298808972762, 0.34157370849866, 0.09274347060035155]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.10269473117336837, 0.46298808972762, 0.34157370849866, 0.09274347060035155]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.10251615317110066, 0.4616940829705562, 0.3433044082747399, 0.09248535558360335]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.10251615317110066, 0.4616940829705562, 0.3433044082747399, 0.09248535558360335]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.241775409714505
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.10294136059338894, 0.46262534621926243, 0.3417600966005747, 0.092673196586774]
from probs:  [0.10294136059338894, 0.46262534621926243, 0.3417600966005747, 0.092673196586774]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.10276730830275833, 0.4613471989702869, 0.34346723126480755, 0.09241826146214724]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.10276730830275833, 0.4613471989702869, 0.34346723126480755, 0.09241826146214724]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.1011757514861719, 0.46079506486177785, 0.3457206075145852, 0.09230857613746501]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.1011757514861719, 0.46079506486177785, 0.3457206075145852, 0.09230857613746501]
from probs:  [0.1011757514861719, 0.46079506486177785, 0.3457206075145852, 0.09230857613746501]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.10099019094221796, 0.45951900153735176, 0.34743674742092273, 0.09205406009950756]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.10137722974137607, 0.46128149227480014, 0.3449346022704914, 0.09240667571333243]
maxi score, test score, baseline:  -0.9942502824858758 -1.0 -0.9942502824858758
maxi score, test score, baseline:  -0.9942502824858758 -1.0 -0.9942502824858758
probs:  [0.10137730380662607, 0.4612812061214772, 0.3449348705299161, 0.09240661954198057]
maxi score, test score, baseline:  -0.9942502824858758 -1.0 -0.9942502824858758
probs:  [0.10137730380662607, 0.4612812061214772, 0.3449348705299161, 0.09240661954198057]
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
probs:  [0.10176008181885998, 0.46302429433111064, 0.3424602704932195, 0.09275535335680997]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
probs:  [0.10176008181885998, 0.46302429433111064, 0.3424602704932195, 0.09275535335680997]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
probs:  [0.10176008181885998, 0.46302429433111064, 0.3424602704932195, 0.09275535335680997]
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
probs:  [0.10157896561028916, 0.46176487328343185, 0.3441519877019457, 0.09250417340433309]
line 256 mcts: sample exp_bonus 5.702114603866281
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
probs:  [0.10157896561028916, 0.46176487328343185, 0.3441519877019457, 0.09250417340433309]
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
first move QE:  0.8189526100821195
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
probs:  [0.1013977151014394, 0.4605045183568955, 0.3458449593433636, 0.09225280719830144]
Printing some Q and Qe and total Qs values:  [[-0.135]
 [-0.144]
 [-0.134]
 [-0.135]
 [-0.138]
 [-0.138]
 [-0.139]] [[4.281]
 [4.132]
 [4.093]
 [4.247]
 [4.328]
 [4.256]
 [4.234]] [[0.104]
 [0.045]
 [0.043]
 [0.093]
 [0.116]
 [0.093]
 [0.084]]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.10103488514985916, 0.4579807046417888, 0.3492349535419135, 0.0917494566664386]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.10103488514985916, 0.4579807046417888, 0.3492349535419135, 0.0917494566664386]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.10103488514985916, 0.4579807046417888, 0.3492349535419135, 0.0917494566664386]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.10103488514985916, 0.4579807046417888, 0.3492349535419135, 0.0917494566664386]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.10103488514985916, 0.4579807046417888, 0.3492349535419135, 0.0917494566664386]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.1008532306948203, 0.45671753462978065, 0.350931705650976, 0.091497529024423]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.1008532306948203, 0.45671753462978065, 0.350931705650976, 0.091497529024423]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]] [[4.942]
 [4.942]
 [4.942]
 [4.942]
 [4.942]
 [4.942]
 [4.942]] [[0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]]
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.044]
 [-0.044]
 [-0.046]
 [-0.044]
 [-0.044]
 [-0.044]] [[2.459]
 [2.55 ]
 [2.55 ]
 [2.672]
 [2.55 ]
 [2.55 ]
 [2.55 ]] [[-0.262]
 [-0.231]
 [-0.231]
 [-0.192]
 [-0.231]
 [-0.231]
 [-0.231]]
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
probs:  [0.10048959016705736, 0.45418806282693336, 0.35432929694277193, 0.09099305006323728]
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
probs:  [0.10030753035187692, 0.4529220679387549, 0.356029842676842, 0.09074055903252634]
deleting a thread, now have 3 threads
Frames:  26265 train batches done:  3077 episodes:  579
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
probs:  [0.10070400870957959, 0.45471438649662094, 0.3534824386815183, 0.09109916611228126]
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
probs:  [0.10070400870957959, 0.45471438649662094, 0.3534824386815183, 0.09109916611228126]
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.688]
 [0.646]
 [0.646]
 [0.646]
 [0.677]
 [0.698]] [[5.767]
 [5.599]
 [3.963]
 [3.963]
 [3.963]
 [4.422]
 [4.624]] [[0.939]
 [0.878]
 [0.416]
 [0.416]
 [0.416]
 [0.556]
 [0.621]]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.10046588852408511, 0.4546250745457999, 0.3538287833413469, 0.09108025358876808]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.10046588852408511, 0.4546250745457999, 0.3538287833413469, 0.09108025358876808]
start point for exploration sampling:  11091
deleting a thread, now have 2 threads
Frames:  26591 train batches done:  3107 episodes:  583
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
probs:  [0.10046595911540614, 0.4546247679360095, 0.3538290796052239, 0.09108019334336043]
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
probs:  [0.10046595911540614, 0.4546247679360095, 0.3538290796052239, 0.09108019334336043]
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
probs:  [0.10085568198826403, 0.45639038020755135, 0.35132048482226313, 0.09143345298192156]
siam score:  -0.867666
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
probs:  [0.10085575446397568, 0.45639008659950986, 0.35132076360648196, 0.0914333953300324]
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
probs:  [0.10085575446397568, 0.45639008659950986, 0.35132076360648196, 0.0914333953300324]
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
probs:  [0.10085575446397568, 0.45639008659950986, 0.35132076360648196, 0.0914333953300324]
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
probs:  [0.10085575446397568, 0.45639008659950986, 0.35132076360648196, 0.0914333953300324]
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
probs:  [0.10085575446397568, 0.45639008659950986, 0.35132076360648196, 0.0914333953300324]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.1012414903006543, 0.4581370159754131, 0.3488385756971512, 0.09178291802678132]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.1590028228598643
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.1012414903006543, 0.4581370159754131, 0.3488385756971512, 0.09178291802678132]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10162315368095642, 0.4598661105899619, 0.3463818644242447, 0.09212887130483688]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10144870661293959, 0.4586366285935937, 0.3480309771254331, 0.0918836876680336]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10144870661293959, 0.4586366285935937, 0.3480309771254331, 0.0918836876680336]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10127410098595864, 0.4574060290926787, 0.3496815887431953, 0.09163828117816726]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10127410098595864, 0.4574060290926787, 0.3496815887431953, 0.09163828117816726]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.1010993365837386, 0.45617431056293595, 0.3513337013220603, 0.09139265153126526]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10131077425159446, 0.4566851044914698, 0.3505084483338043, 0.09149567292313147]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10113783166603366, 0.4554590871678473, 0.352151892507914, 0.09125118865820507]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10113783166603366, 0.4554590871678473, 0.352151892507914, 0.09125118865820507]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10152245023817263, 0.4571931595478692, 0.3496862368580201, 0.09159815335593788]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10152245023817263, 0.4571931595478692, 0.3496862368580201, 0.09159815335593788]
using explorer policy with actor:  1
siam score:  -0.8613163
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10152245023817263, 0.4571931595478692, 0.3496862368580201, 0.09159815335593788]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10152245023817263, 0.4571931595478692, 0.3496862368580201, 0.09159815335593788]
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.045]
 [0.055]
 [0.056]
 [0.051]
 [0.052]
 [0.063]] [[2.501]
 [2.545]
 [2.138]
 [2.184]
 [2.319]
 [2.368]
 [2.231]] [[-0.216]
 [-0.221]
 [-0.347]
 [-0.33 ]
 [-0.29 ]
 [-0.273]
 [-0.307]]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10173439924368706, 0.45769860367727505, 0.348866878596527, 0.0917001184825108]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.10304704078034817, 0.456794409958662, 0.34864045528866405, 0.09151809397232574]
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.10304714175677797, 0.4567941188212654, 0.3486407025641459, 0.0915180368578107]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.10304714175677797, 0.4567941188212654, 0.3486407025641459, 0.0915180368578107]
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.10288863671311957, 0.45558251201603905, 0.3502524205226084, 0.09127643074823297]
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.10327256789809454, 0.4572837076066533, 0.34782690407232714, 0.09161682042292492]
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.10327256789809454, 0.4572837076066533, 0.34782690407232714, 0.09161682042292492]
UNIT TEST: sample policy line 217 mcts : [0.245 0.061 0.204 0.204 0.143 0.082 0.061]
from probs:  [0.10327256789809454, 0.4572837076066533, 0.34782690407232714, 0.09161682042292492]
from probs:  [0.10365264304194514, 0.45896854775232176, 0.34542487277122713, 0.09195393643450599]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.10349830661020507, 0.4577710949111461, 0.34701542913571237, 0.09171516934293648]
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
probs:  [0.10349841108536144, 0.4577708151422818, 0.34701565926255035, 0.09171511450980638]
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
probs:  [0.10349841108536144, 0.4577708151422818, 0.34701565926255035, 0.09171511450980638]
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.371]] [[0.986]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.921]] [[0.135]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.115]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.50261360534062
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
probs:  [0.1033439059180832, 0.4565720453082209, 0.34860796395150967, 0.09147608482218629]
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
probs:  [0.1033439059180832, 0.4565720453082209, 0.34860796395150967, 0.09147608482218629]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
probs:  [0.1033439059180832, 0.4565720453082209, 0.34860796395150967, 0.09147608482218629]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.10334400941044442, 0.4565717618151564, 0.3486081995313363, 0.09147602924306288]
from probs:  [0.10334400941044442, 0.4565717618151564, 0.3486081995313363, 0.09147602924306288]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.10372470517141288, 0.45825555832350723, 0.3462067939437381, 0.09181294256134169]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.10357228595346397, 0.4570627824284083, 0.34778981423304944, 0.09157511738507836]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.10357228595346397, 0.4570627824284083, 0.34778981423304944, 0.09157511738507836]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.10357228595346397, 0.4570627824284083, 0.34778981423304944, 0.09157511738507836]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.10395159039290253, 0.45873778538998106, 0.34540034880993936, 0.09191027540717706]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.10527546502415365, 0.4578095626430781, 0.34519156483311236, 0.09172340749965584]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.10527546502415365, 0.4578095626430781, 0.34519156483311236, 0.09172340749965584]
first move QE:  0.9344325818536108
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.10527546502415365, 0.4578095626430781, 0.34519156483311236, 0.09172340749965584]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.10527546502415365, 0.4578095626430781, 0.34519156483311236, 0.09172340749965584]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.10527546502415365, 0.4578095626430781, 0.34519156483311236, 0.09172340749965584]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.10527558721104466, 0.4578092892321522, 0.3451917696573198, 0.09172335389948323]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.816]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[2.936]
 [3.497]
 [2.936]
 [2.936]
 [2.936]
 [2.936]
 [2.936]] [[0.855]
 [0.985]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.10602794292749029, 0.4610846322256168, 0.3405087048578402, 0.09237871998905277]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.10639879090778041, 0.4626991003652652, 0.33820034847203423, 0.09270176025492023]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.10639879090778041, 0.4626991003652652, 0.33820034847203423, 0.09270176025492023]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.10626513586165104, 0.461536073703, 0.339728904387516, 0.09246988604783295]
line 256 mcts: sample exp_bonus 4.723268209968162
using explorer policy with actor:  1
siam score:  -0.86366534
actor:  1 policy actor:  1  step number:  44 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.11266556016600252, 0.5172306720840222, 0.26652997389190114, 0.1035737938580742]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.11266556016600252, 0.5172306720840222, 0.26652997389190114, 0.1035737938580742]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.628]
 [0.628]
 [0.641]
 [0.628]
 [0.628]
 [0.628]] [[3.35 ]
 [3.35 ]
 [3.35 ]
 [4.094]
 [3.35 ]
 [3.35 ]
 [3.35 ]] [[0.798]
 [0.798]
 [0.798]
 [0.954]
 [0.798]
 [0.798]
 [0.798]]
siam score:  -0.86070395
actor:  1 policy actor:  1  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.11611655751693088, 0.5472602298655672, 0.22706238499167541, 0.1095608276258266]
actor:  1 policy actor:  1  step number:  75 total reward:  0.02666666666666606  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.11708356667228698, 0.5556748573929505, 0.21600311257696825, 0.11123846335779412]
actor:  1 policy actor:  1  step number:  43 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.106]
 [0.117]
 [0.106]
 [0.106]
 [0.108]
 [0.106]] [[1.077]
 [1.087]
 [0.834]
 [1.087]
 [1.087]
 [0.895]
 [1.087]] [[-0.079]
 [-0.088]
 [-0.162]
 [-0.088]
 [-0.088]
 [-0.151]
 [-0.088]]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
from probs:  [0.11835000302016543, 0.5666950117477588, 0.20151941871585183, 0.11343556651622383]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.11826238358917222, 0.5659325733862548, 0.20252148488121183, 0.11328355814336127]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.11826238358917222, 0.5659325733862548, 0.20252148488121183, 0.11328355814336127]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.569]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]] [[4.645]
 [5.034]
 [4.645]
 [4.645]
 [4.645]
 [4.645]
 [4.645]] [[0.472]
 [0.592]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.11826238358917222, 0.5659325733862548, 0.20252148488121183, 0.11328355814336127]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.11826238358917222, 0.5659325733862548, 0.20252148488121183, 0.11328355814336127]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.075]
 [-0.067]
 [-0.065]
 [-0.065]
 [-0.068]
 [-0.067]] [[0.636]
 [0.441]
 [0.411]
 [0.468]
 [0.493]
 [0.658]
 [0.602]] [[-0.013]
 [-0.117]
 [-0.124]
 [-0.094]
 [-0.081]
 [-0.001]
 [-0.029]]
from probs:  [0.11841054344593643, 0.5666419477219449, 0.2015221040866199, 0.11342540474549864]
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.11885472950086483, 0.5660603772788894, 0.20177609324197485, 0.11330879997827101]
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.11885472950086483, 0.5660603772788894, 0.20177609324197485, 0.11330879997827101]
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.11885482189443068, 0.5660600749791991, 0.20177636276457006, 0.1133087403618]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.11827002181920755, 0.5664358510863546, 0.2019102469182664, 0.11338388017617136]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.11827002181920755, 0.5664358510863546, 0.2019102469182664, 0.11338388017617136]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[4.119]
 [4.119]
 [4.119]
 [4.119]
 [4.119]
 [4.119]
 [4.119]] [[-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.11822624900751867, 0.5660577783860992, 0.2024074694093239, 0.11330850319705824]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.11822624900751867, 0.5660577783860992, 0.2024074694093239, 0.11330850319705824]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.11818245721456756, 0.5656795417416784, 0.2029049075116281, 0.11323309353212588]
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]] [[4.981]
 [4.981]
 [4.981]
 [4.981]
 [4.981]
 [4.981]
 [4.981]] [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.11818245721456756, 0.5656795417416784, 0.2029049075116281, 0.11323309353212588]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.367 0.408 0.041 0.    0.    0.122 0.061]
siam score:  -0.84612435
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.11828699788145404, 0.5660117258794652, 0.20240153640882153, 0.1132997398302593]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.11834919471893213, 0.5659714631995935, 0.2023872063694252, 0.11329213571204907]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.11834919471893213, 0.5659714631995935, 0.2023872063694252, 0.11329213571204907]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.433]
 [0.433]
 [0.432]
 [0.433]
 [0.433]
 [0.433]] [[8.471]
 [8.471]
 [8.471]
 [7.959]
 [8.471]
 [8.471]
 [8.471]] [[0.732]
 [0.732]
 [0.732]
 [0.617]
 [0.732]
 [0.732]
 [0.732]]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.11834919471893213, 0.5659714631995935, 0.2023872063694252, 0.11329213571204907]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.636]
 [0.671]
 [0.672]
 [0.669]
 [0.67 ]
 [0.671]] [[3.268]
 [3.117]
 [2.494]
 [2.641]
 [2.475]
 [2.835]
 [2.756]] [[0.767]
 [0.719]
 [0.546]
 [0.595]
 [0.538]
 [0.658]
 [0.633]]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.11920512288072178, 0.5644302473928332, 0.20338109113877953, 0.11298353858766548]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.11916879118849698, 0.564053639414481, 0.2038691187611547, 0.11290845063586749]
using explorer policy with actor:  1
siam score:  -0.8194318
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.602]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[3.913]
 [4.094]
 [3.913]
 [3.913]
 [3.913]
 [3.913]
 [3.913]] [[0.35 ]
 [0.359]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.11909613691259008, 0.5632997990473546, 0.20484591347050654, 0.11275815056954888]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.5174675415039065
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.818]
 [0.845]
 [0.836]
 [0.834]
 [0.837]
 [0.832]] [[3.361]
 [2.407]
 [3.043]
 [3.237]
 [3.269]
 [3.255]
 [3.314]] [[1.167]
 [0.672]
 [1.017]
 [1.109]
 [1.125]
 [1.119]
 [1.146]]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.11909613691259008, 0.5632997990473546, 0.20484591347050654, 0.11275815056954888]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.11909613691259008, 0.5632997990473546, 0.20484591347050654, 0.11275815056954888]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.11953889686007359, 0.5627256917388351, 0.20509237575674996, 0.1126430356443413]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.11953889686007359, 0.5627256917388351, 0.20509237575674996, 0.1126430356443413]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.11953889686007359, 0.5627256917388351, 0.20509237575674996, 0.1126430356443413]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.11953889686007359, 0.5627256917388351, 0.20509237575674996, 0.1126430356443413]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.11893439873507357, 0.5631121367503898, 0.2052331550012624, 0.11272030951327396]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.11936927408128313, 0.5625491457921752, 0.20547415610835199, 0.11260742401818977]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.11951907403948735, 0.5632558043345607, 0.20447639396108486, 0.11274872766486692]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.11951907403948735, 0.5632558043345607, 0.20447639396108486, 0.11274872766486692]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.11994488135474367, 0.5627025658454934, 0.20471475430173414, 0.11263779849802881]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.11994488135474367, 0.5627025658454934, 0.20471475430173414, 0.11263779849802881]
actor:  1 policy actor:  1  step number:  46 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.10942392177218548, 0.5132961088582295, 0.27452147328968063, 0.10275849607990435]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.10942392177218548, 0.5132961088582295, 0.27452147328968063, 0.10275849607990435]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.10942392177218548, 0.5132961088582295, 0.27452147328968063, 0.10275849607990435]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.437]
 [0.411]
 [0.411]
 [0.413]
 [0.41 ]
 [0.41 ]] [[2.333]
 [3.399]
 [2.258]
 [2.395]
 [2.024]
 [2.284]
 [2.423]] [[0.41 ]
 [0.437]
 [0.411]
 [0.411]
 [0.413]
 [0.41 ]
 [0.41 ]]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.10942392177218548, 0.5132961088582295, 0.27452147328968063, 0.10275849607990435]
siam score:  -0.8567731
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.1093372290486241, 0.512675987146605, 0.2753520934227814, 0.10263469038198948]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.1093372290486241, 0.512675987146605, 0.2753520934227814, 0.10263469038198948]
from probs:  [0.10933726740414214, 0.5126757447525477, 0.27535234556355515, 0.10263464227975506]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.10878825254050417, 0.5129918312402518, 0.2755220692445068, 0.10269784697473709]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.10869849741965411, 0.5123737583555396, 0.2763532936498335, 0.10257445057497272]
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.123]
 [0.123]
 [0.112]
 [0.113]
 [0.111]
 [0.123]] [[1.35 ]
 [0.583]
 [0.583]
 [0.251]
 [0.608]
 [0.802]
 [0.583]] [[0.1  ]
 [0.123]
 [0.123]
 [0.112]
 [0.113]
 [0.111]
 [0.123]]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.10869849741965411, 0.5123737583555396, 0.2763532936498335, 0.10257445057497272]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.10869849741965411, 0.5123737583555396, 0.2763532936498335, 0.10257445057497272]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.10869849741965411, 0.5123737583555396, 0.2763532936498335, 0.10257445057497272]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.10869849741965411, 0.5123737583555396, 0.2763532936498335, 0.10257445057497272]
siam score:  -0.8497689
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.10869849741965411, 0.5123737583555396, 0.2763532936498335, 0.10257445057497272]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.10869849741965411, 0.5123737583555396, 0.2763532936498335, 0.10257445057497272]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.659]
 [0.602]
 [0.682]
 [0.692]
 [0.682]
 [0.717]] [[2.877]
 [4.119]
 [3.635]
 [3.571]
 [2.772]
 [2.877]
 [3.694]] [[0.547]
 [0.732]
 [0.594]
 [0.663]
 [0.539]
 [0.547]
 [0.718]]
from probs:  [0.10860886817970802, 0.5117556481897082, 0.2771844363159366, 0.10245104731464716]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10806332309190446, 0.5120684829581745, 0.27735459119903666, 0.10251360275088435]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10826174110276533, 0.5130097363399269, 0.2760267059438691, 0.10270181661343875]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.71 ]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]] [[4.449]
 [5.868]
 [4.449]
 [4.449]
 [4.449]
 [4.449]
 [4.449]] [[0.655]
 [1.111]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10836695709639224, 0.513331903290048, 0.2755347076429014, 0.10276643197065835]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10827617774774419, 0.5127244273151581, 0.27635423996408065, 0.10264515497301727]
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.865]
 [0.682]
 [0.78 ]
 [0.697]
 [0.653]
 [0.858]] [[3.813]
 [4.509]
 [2.9  ]
 [4.776]
 [2.726]
 [3.134]
 [3.786]] [[0.833]
 [1.027]
 [0.604]
 [1.048]
 [0.573]
 [0.642]
 [0.869]]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10827617774774419, 0.5127244273151581, 0.27635423996408065, 0.10264515497301727]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10827617774774419, 0.5127244273151581, 0.27635423996408065, 0.10264515497301727]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10818545882427982, 0.5121173556926667, 0.27717322678243556, 0.10252395870061792]
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.211]
 [0.223]
 [0.211]
 [0.211]
 [0.211]
 [0.207]] [[4.993]
 [5.549]
 [4.312]
 [5.549]
 [5.549]
 [5.549]
 [3.66 ]] [[0.508]
 [0.686]
 [0.343]
 [0.686]
 [0.686]
 [0.686]
 [0.15 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10838188732935458, 0.5130482067054899, 0.27585981240834445, 0.10271009355681109]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10838188732935458, 0.5130482067054899, 0.27585981240834445, 0.10271009355681109]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10829212170297221, 0.5124454468897472, 0.2766726716987816, 0.10258975970849891]
using another actor
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10820241403294475, 0.5118430762403617, 0.2774850061740591, 0.10246950355263432]
siam score:  -0.85130715
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10820241403294475, 0.5118430762403617, 0.2774850061740591, 0.10246950355263432]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.376]
 [0.364]
 [0.358]
 [0.362]
 [0.363]
 [0.364]] [[3.016]
 [3.012]
 [2.901]
 [2.884]
 [2.833]
 [2.86 ]
 [2.882]] [[0.371]
 [0.376]
 [0.364]
 [0.358]
 [0.362]
 [0.363]
 [0.364]]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10764854361186009, 0.512161034832595, 0.277657338115477, 0.10253308344006799]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10764854361186009, 0.512161034832595, 0.277657338115477, 0.10253308344006799]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.10764854361186009, 0.512161034832595, 0.277657338115477, 0.10253308344006799]
siam score:  -0.8473629
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.10764856285112548, 0.5121608130163896, 0.27765758467998036, 0.10253303945250448]
siam score:  -0.8477494
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.10807423721055387, 0.5117459630706488, 0.27773000221001276, 0.1024497975087847]
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.766]
 [0.797]
 [0.819]
 [0.829]
 [0.823]
 [0.816]] [[3.667]
 [4.864]
 [4.977]
 [4.53 ]
 [3.561]
 [4.737]
 [4.777]] [[0.345]
 [0.518]
 [0.568]
 [0.515]
 [0.364]
 [0.554]
 [0.553]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.10826961905692634, 0.5126721359185019, 0.27642324792806466, 0.1026349970965071]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.10763339137245684, 0.512388451639909, 0.27739969227731676, 0.10257846471031737]
Starting evaluation
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.10754117231527643, 0.5117923529708511, 0.27820701359087774, 0.10245946112299469]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.10754117231527643, 0.5117923529708511, 0.27820701359087774, 0.10245946112299469]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.10754117231527643, 0.5117923529708511, 0.27820701359087774, 0.10245946112299469]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.10744901179783202, 0.5111966326992717, 0.27901382242493616, 0.10234053307796005]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.555]
 [0.548]
 [0.55 ]
 [0.546]
 [0.531]
 [0.546]] [[4.894]
 [4.452]
 [3.999]
 [3.941]
 [4.212]
 [3.869]
 [4.07 ]] [[0.549]
 [0.555]
 [0.548]
 [0.55 ]
 [0.546]
 [0.531]
 [0.546]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.10744901179783202, 0.5111966326992717, 0.27901382242493616, 0.10234053307796005]
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.709]
 [0.686]
 [0.719]
 [0.686]
 [0.698]
 [0.706]] [[5.255]
 [5.178]
 [4.348]
 [4.212]
 [4.348]
 [4.15 ]
 [4.383]] [[0.945]
 [0.923]
 [0.665]
 [0.645]
 [0.665]
 [0.614]
 [0.687]]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.1074490302246337, 0.5111964101198057, 0.2790140707183172, 0.10234048893724355]
line 256 mcts: sample exp_bonus 4.274662318938859
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.708]
 [0.722]
 [0.691]
 [0.664]
 [0.716]
 [0.719]] [[3.455]
 [3.585]
 [3.263]
 [3.296]
 [3.376]
 [3.381]
 [3.337]] [[0.477]
 [0.57 ]
 [0.476]
 [0.456]
 [0.456]
 [0.509]
 [0.497]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.679]
 [0.686]
 [0.679]
 [0.694]
 [0.69 ]
 [0.682]] [[4.722]
 [5.234]
 [4.764]
 [5.021]
 [4.864]
 [5.011]
 [4.938]] [[0.683]
 [0.679]
 [0.686]
 [0.679]
 [0.694]
 [0.69 ]
 [0.682]]
start point for exploration sampling:  11091
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.673]
 [0.669]
 [0.666]
 [0.665]
 [0.673]
 [0.664]] [[4.377]
 [4.453]
 [4.544]
 [4.627]
 [4.659]
 [4.453]
 [4.433]] [[0.664]
 [0.673]
 [0.669]
 [0.666]
 [0.665]
 [0.673]
 [0.664]]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.10735708606045409, 0.5105991407904991, 0.2798225189664675, 0.10222125418257952]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.10735708606045409, 0.5105991407904991, 0.2798225189664675, 0.10222125418257952]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[8.726]
 [7.136]
 [7.136]
 [7.136]
 [7.136]
 [7.136]
 [7.136]] [[0.676]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]]
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.706]
 [0.693]
 [0.693]
 [0.695]
 [0.7  ]
 [0.694]] [[4.548]
 [5.783]
 [4.312]
 [4.59 ]
 [4.595]
 [4.619]
 [4.722]] [[0.678]
 [0.706]
 [0.693]
 [0.693]
 [0.695]
 [0.7  ]
 [0.694]]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.1073571027830433, 0.5105989369151609, 0.27982274655149936, 0.10222121375029647]
using explorer policy with actor:  0
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.10798149500673486, 0.5111142659066227, 0.2785802677949796, 0.10232397129166289]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.48 ]
 [0.452]
 [0.457]
 [0.457]
 [0.454]
 [0.463]] [[0.507]
 [0.499]
 [0.589]
 [0.473]
 [0.366]
 [0.347]
 [0.702]] [[0.484]
 [0.48 ]
 [0.452]
 [0.457]
 [0.457]
 [0.454]
 [0.463]]
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]] [[0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.10798149500673486, 0.5111142659066227, 0.2785802677949796, 0.10232397129166289]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.10798149500673486, 0.5111142659066227, 0.2785802677949796, 0.10232397129166289]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.10798149500673486, 0.5111142659066227, 0.2785802677949796, 0.10232397129166289]
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.65 ]
 [0.685]
 [0.691]
 [0.682]
 [0.65 ]
 [0.664]] [[9.521]
 [8.014]
 [7.312]
 [7.236]
 [6.933]
 [8.014]
 [7.77 ]] [[0.649]
 [0.65 ]
 [0.685]
 [0.691]
 [0.682]
 [0.65 ]
 [0.664]]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.10798151686316702, 0.5111140650027112, 0.27858048669002955, 0.10232393144409216]
siam score:  -0.8385849
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.457]
 [0.457]
 [0.465]
 [0.457]
 [0.457]
 [0.457]] [[8.599]
 [8.599]
 [8.599]
 [8.201]
 [8.599]
 [8.599]
 [8.599]] [[0.457]
 [0.457]
 [0.457]
 [0.465]
 [0.457]
 [0.457]
 [0.457]]
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.10798153851787172, 0.5111138659574422, 0.27858070355951614, 0.1023238919651699]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.1079815599736289, 0.5111136687451419, 0.27858091843142535, 0.10232385284980383]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.10798158123316767, 0.511113473340607, 0.27858113133323026, 0.10232381409299507]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.10817540646860872, 0.5120319199516333, 0.2772852039726664, 0.10250746960709155]
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]] [[5.294]
 [5.294]
 [5.294]
 [5.294]
 [5.294]
 [5.294]
 [5.294]] [[1.174]
 [1.174]
 [1.174]
 [1.174]
 [1.174]
 [1.174]
 [1.174]]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.10817540646860872, 0.5120319199516333, 0.2772852039726664, 0.10250746960709155]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]] [[8.7]
 [8.7]
 [8.7]
 [8.7]
 [8.7]
 [8.7]
 [8.7]] [[0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.04666666666666608  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.715]
 [0.596]
 [0.608]
 [0.617]
 [0.59 ]
 [0.61 ]] [[ 1.038]
 [ 2.529]
 [ 0.514]
 [-0.021]
 [ 0.192]
 [ 0.104]
 [ 0.318]] [[0.481]
 [0.867]
 [0.384]
 [0.283]
 [0.331]
 [0.297]
 [0.353]]
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.10200000926393606, 0.48260638426385966, 0.31876997787886435, 0.09662362859334]
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.10200000926393606, 0.48260638426385966, 0.31876997787886435, 0.09662362859334]
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.10200000926393606, 0.48260638426385966, 0.31876997787886435, 0.09662362859334]
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.102212795492945, 0.4836143370749456, 0.3173476849971364, 0.09682518243497296]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[5.465]
 [5.465]
 [5.465]
 [5.465]
 [5.465]
 [5.465]
 [5.465]] [[0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]]
first move QE:  1.0886665200222638
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.10242332390002412, 0.48461159474860815, 0.31594048371261374, 0.09702459763875416]
using another actor
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.10231227906836877, 0.4839227823104041, 0.31687789577603603, 0.09688704284519115]
siam score:  -0.84841394
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.10231227906836877, 0.4839227823104041, 0.31687789577603603, 0.09688704284519115]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10231225165959373, 0.48392238088522255, 0.3168784045896565, 0.09688696286552735]
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.909]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]] [[4.767]
 [5.212]
 [4.767]
 [4.767]
 [4.767]
 [4.767]
 [4.767]] [[1.115]
 [1.32 ]
 [1.115]
 [1.115]
 [1.115]
 [1.115]
 [1.115]]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10220133556779026, 0.4832343658924374, 0.31781473121827414, 0.09674956732149814]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10209054818886333, 0.48254714930849796, 0.31874997128414184, 0.09661233121849692]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10209054818886333, 0.48254714930849796, 0.31874997128414184, 0.09661233121849692]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10209054818886333, 0.48254714930849796, 0.31874997128414184, 0.09661233121849692]
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.842]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]] [[4.064]
 [4.034]
 [4.064]
 [4.064]
 [4.064]
 [4.064]
 [4.064]] [[1.14 ]
 [1.228]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.14 ]]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10219136844389819, 0.4828611403606646, 0.31827218972399834, 0.09667530147143873]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10208162304593002, 0.48217889134629166, 0.3192004269420474, 0.09653905866573081]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10208162304593002, 0.48217889134629166, 0.3192004269420474, 0.09653905866573081]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10208162304593002, 0.48217889134629166, 0.3192004269420474, 0.09653905866573081]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10208162304593002, 0.48217889134629166, 0.3192004269420474, 0.09653905866573081]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.707]
 [0.617]
 [0.633]
 [0.63 ]
 [0.614]
 [0.633]] [[2.127]
 [2.178]
 [1.534]
 [1.435]
 [1.402]
 [1.676]
 [1.649]] [[1.121]
 [1.197]
 [1.013]
 [1.013]
 [1.004]
 [1.032]
 [1.045]]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10271532272938176, 0.4828283065438217, 0.31778773186277554, 0.09666863886402115]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10271532272938176, 0.4828283065438217, 0.31778773186277554, 0.09666863886402115]
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.88 ]
 [0.517]
 [0.881]
 [0.517]
 [0.517]
 [0.517]] [[0.008]
 [0.006]
 [1.157]
 [0.007]
 [1.157]
 [1.157]
 [1.157]] [[ 0.094]
 [ 0.093]
 [-0.078]
 [ 0.094]
 [-0.078]
 [-0.078]
 [-0.078]]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10164820732523518, 0.48340263022936, 0.3181656790131229, 0.0967834834322819]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10164820732523518, 0.48340263022936, 0.3181656790131229, 0.0967834834322819]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10206584944626441, 0.48306415054954427, 0.31815447585880047, 0.09671552414539082]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10154681033625133, 0.48334343269852015, 0.31833838635692746, 0.09677137060830099]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.10103629220117043, 0.4836181299209568, 0.3185192776283447, 0.09682630024952794]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.10103625386499417, 0.48361773430590277, 0.3185197903985746, 0.09682622143052855]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.1009215960047345, 0.48294286515863843, 0.3194440872861272, 0.09669145155049974]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.1009215577611221, 0.4829424707919299, 0.319444598466919, 0.09669137298002896]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.1009215577611221, 0.4829424707919299, 0.319444598466919, 0.09669137298002896]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.1009215577611221, 0.4829424707919299, 0.319444598466919, 0.09669137298002896]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.1009215577611221, 0.4829424707919299, 0.319444598466919, 0.09669137298002896]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.1009215198557286, 0.482942079911372, 0.31944510512878727, 0.09669129510411216]
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.1009215198557286, 0.482942079911372, 0.31944510512878727, 0.09669129510411216]
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.1009215198557286, 0.482942079911372, 0.31944510512878727, 0.09669129510411216]
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.10101347106923687, 0.4832569483753231, 0.31897514259215815, 0.09675443796328188]
Printing some Q and Qe and total Qs values:  [[0.741]
 [0.772]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[3.543]
 [3.553]
 [3.123]
 [3.123]
 [3.123]
 [3.123]
 [3.123]] [[1.171]
 [1.2  ]
 [1.05 ]
 [1.05 ]
 [1.05 ]
 [1.05 ]
 [1.05 ]]
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.10131854979904106, 0.4822503433495367, 0.3198780498370948, 0.09655305701432758]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.10131854979904106, 0.4822503433495367, 0.3198780498370948, 0.09655305701432758]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [-0.02 ]
 [-0.016]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[5.536]
 [4.996]
 [5.215]
 [6.508]
 [6.508]
 [6.508]
 [6.508]] [[0.31 ]
 [0.145]
 [0.212]
 [0.598]
 [0.598]
 [0.598]
 [0.598]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.10141349181049976, 0.48256350013346644, 0.31940715197379155, 0.09661585608224225]
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.10130301482815643, 0.4818979256421647, 0.3203161164434199, 0.09648294308625899]
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5194],
        [-0.5454],
        [-0.2941],
        [-0.0000],
        [-0.4357],
        [-0.4407],
        [-0.4415],
        [-0.5802],
        [-0.2960],
        [-0.3968]], dtype=torch.float64)
-0.032346567066 -0.5517703911834276
-0.032346567066 -0.5777570478188655
-0.058614567066 -0.3526825238649356
-0.7867986733200001 -0.7867986733200001
-0.09703970119800001 -0.5326925381286299
-0.09703970119800001 -0.5377669921304559
-0.09703970119800001 -0.5385002506565773
-0.032346567066 -0.6125574606319475
-0.045414567066 -0.34138462137661596
-0.045026434398 -0.4418078685300215
from probs:  [0.10201990094872723, 0.4828419280765359, 0.31846655576911503, 0.0966716152056218]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.10170274440165815, 0.4840786552009708, 0.31729968255072616, 0.09691891784664491]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.10170274440165815, 0.4840786552009708, 0.31729968255072616, 0.09691891784664491]
UNIT TEST: sample policy line 217 mcts : [0.224 0.204 0.143 0.184 0.041 0.082 0.122]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.10170274440165815, 0.4840786552009708, 0.31729968255072616, 0.09691891784664491]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.10170274440165815, 0.4840786552009708, 0.31729968255072616, 0.09691891784664491]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.10170274440165815, 0.4840786552009708, 0.31729968255072616, 0.09691891784664491]
siam score:  -0.8514566
siam score:  -0.8514248
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.10170274440165815, 0.4840786552009708, 0.31729968255072616, 0.09691891784664491]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.10119199639294489, 0.48435394207980115, 0.31748009582823433, 0.09697396569901968]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10119196072501707, 0.48435356929970397, 0.31748057853854317, 0.09697389143673583]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10119196072501707, 0.48435356929970397, 0.31748057853854317, 0.09697389143673583]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10097045702397517, 0.4830481340328241, 0.3192682033114844, 0.09671320563171644]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10097045702397517, 0.4830481340328241, 0.3192682033114844, 0.09671320563171644]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10097045702397517, 0.4830481340328241, 0.3192682033114844, 0.09671320563171644]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10097045702397517, 0.4830481340328241, 0.3192682033114844, 0.09671320563171644]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.1008812881835284, 0.48298391444756494, 0.3194347083321706, 0.09670008903673598]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.1008812881835284, 0.48298391444756494, 0.3194347083321706, 0.09670008903673598]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.1008812881835284, 0.48298391444756494, 0.3194347083321706, 0.09670008903673598]
siam score:  -0.8427434
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.265]] [[6.61]
 [6.61]
 [6.61]
 [6.61]
 [6.61]
 [6.61]
 [6.61]] [[0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.1008812881835284, 0.48298391444756494, 0.3194347083321706, 0.09670008903673598]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10077046014557953, 0.48233286422720695, 0.32032659786448087, 0.09657007776273258]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10065975117803641, 0.4816825134761831, 0.3212175291767373, 0.09644020616904321]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]] [[5.388]
 [5.388]
 [5.388]
 [5.388]
 [5.388]
 [5.388]
 [5.388]] [[0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10065975117803641, 0.4816825134761831, 0.3212175291767373, 0.09644020616904321]
Printing some Q and Qe and total Qs values:  [[ 0.149]
 [-0.017]
 [-0.018]
 [-0.013]
 [-0.011]
 [-0.001]
 [ 0.059]] [[5.857]
 [5.813]
 [6.593]
 [6.281]
 [7.098]
 [5.65 ]
 [5.049]] [[ 0.135]
 [-0.029]
 [ 0.206]
 [ 0.115]
 [ 0.365]
 [-0.064]
 [-0.19 ]]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10065975117803641, 0.4816825134761831, 0.3212175291767373, 0.09644020616904321]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10065975117803641, 0.4816825134761831, 0.3212175291767373, 0.09644020616904321]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10086023470494472, 0.48264301001638027, 0.31986448216418345, 0.09663227311449155]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10086023470494472, 0.48264301001638027, 0.31986448216418345, 0.09663227311449155]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10086023470494472, 0.48264301001638027, 0.31986448216418345, 0.09663227311449155]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10086023470494472, 0.48264301001638027, 0.31986448216418345, 0.09663227311449155]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10064054994215874, 0.48135051607189794, 0.32163476268264224, 0.09637417130330103]
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[5.512]
 [5.512]
 [5.512]
 [5.512]
 [5.512]
 [5.512]
 [5.512]] [[1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.100840423369872, 0.4823076123213548, 0.3202864053006423, 0.09656555900813087]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.100840423369872, 0.4823076123213548, 0.3202864053006423, 0.09656555900813087]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10103833916020058, 0.48325533440126794, 0.3189512542505151, 0.09675507218801642]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10103833916020058, 0.48325533440126794, 0.3189512542505151, 0.09675507218801642]
using explorer policy with actor:  1
siam score:  -0.85679567
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.1009300297022538, 0.48261614063822983, 0.31982639754456693, 0.0966274321149494]
using another actor
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10061323305163111, 0.48383389444679104, 0.31868192931272776, 0.09687094318884995]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10061323305163111, 0.48383389444679104, 0.31868192931272776, 0.09687094318884995]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10061323305163111, 0.48383389444679104, 0.31868192931272776, 0.09687094318884995]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10061323305163111, 0.48383389444679104, 0.31868192931272776, 0.09687094318884995]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10010783180574359, 0.48410583172195554, 0.31886101469112527, 0.09692532178117569]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10010783180574359, 0.48410583172195554, 0.31886101469112527, 0.09692532178117569]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.10010783180574359, 0.48410583172195554, 0.31886101469112527, 0.09692532178117569]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.10010778760864969, 0.4841054641434838, 0.31886149968182875, 0.09692524856603774]
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.187]
 [0.158]
 [0.16 ]
 [0.154]
 [0.187]
 [0.151]] [[4.514]
 [3.953]
 [4.192]
 [4.311]
 [4.291]
 [3.953]
 [4.287]] [[0.743]
 [0.478]
 [0.551]
 [0.6  ]
 [0.587]
 [0.478]
 [0.582]]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.10010778760864969, 0.4841054641434838, 0.31886149968182875, 0.09692524856603774]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.10010778760864969, 0.4841054641434838, 0.31886149968182875, 0.09692524856603774]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.011]
 [-0.003]
 [ 0.   ]
 [-0.004]
 [-0.011]
 [-0.008]] [[4.206]
 [3.222]
 [4.387]
 [4.706]
 [4.842]
 [4.849]
 [4.472]] [[0.411]
 [0.056]
 [0.475]
 [0.591]
 [0.636]
 [0.633]
 [0.502]]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.10049195303669853, 0.48596543753569305, 0.3162454268004084, 0.0972971826271999]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.10086878915747288, 0.48778992539089516, 0.31367926471418345, 0.0976620207374485]
siam score:  -0.8460135
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.10086878915747288, 0.48778992539089516, 0.31367926471418345, 0.0976620207374485]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.10086878915747288, 0.48778992539089516, 0.31367926471418345, 0.0976620207374485]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.1010545239628866, 0.4886891780847965, 0.3124144559830946, 0.0978418419692224]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.10094552234229659, 0.48807016197824793, 0.31326607684263824, 0.09771823883681727]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]] [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.10094552234229659, 0.48807016197824793, 0.31326607684263824, 0.09771823883681727]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.10083657840860694, 0.48745135255302185, 0.31411739187561416, 0.09759467716275712]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.10083657840860694, 0.48745135255302185, 0.31411739187561416, 0.09759467716275712]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.10102183260515958, 0.48834794232686496, 0.31285625850837984, 0.09777396655959578]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.10102183260515958, 0.48834794232686496, 0.31285625850837984, 0.09777396655959578]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.10120535059814312, 0.48923612925535104, 0.31160694449149773, 0.09795157565500814]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.10120535059814312, 0.48923612925535104, 0.31160694449149773, 0.09795157565500814]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.10120535059814312, 0.48923612925535104, 0.31160694449149773, 0.09795157565500814]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.10120535059814312, 0.48923612925535104, 0.31160694449149773, 0.09795157565500814]
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.747]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[2.859]
 [2.814]
 [2.802]
 [2.802]
 [2.802]
 [2.802]
 [2.802]] [[0.576]
 [0.602]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]]
first move QE:  1.195111306682374
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.10109777010804812, 0.4886236646017808, 0.3124492820561898, 0.09782928323398124]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.10142660174185426, 0.4876500863617631, 0.31328882222193594, 0.0976344896744467]
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.663]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[2.488]
 [2.342]
 [2.488]
 [2.488]
 [2.488]
 [2.488]
 [2.488]] [[1.162]
 [1.135]
 [1.162]
 [1.162]
 [1.162]
 [1.162]
 [1.162]]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.10161007333980962, 0.48853323563194595, 0.3120455994990087, 0.09781109152923567]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.10150551298264422, 0.48792466426973985, 0.3128802470479255, 0.09768957569969032]
line 256 mcts: sample exp_bonus 0.6009278341182089
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.10140100861354685, 0.48731627482628626, 0.3137146201773804, 0.09756809638278663]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.2427190945727867
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.10129663046003717, 0.48670876330610824, 0.31454781405696974, 0.09744679217688487]
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.10129663046003717, 0.48670876330610824, 0.31454781405696974, 0.09744679217688487]
siam score:  -0.8562898
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.10129659525011751, 0.4867084150191289, 0.31454826691025894, 0.0974467228204947]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7865887665186078
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.10119227277375685, 0.4861010841601758, 0.3153811881906497, 0.09732545487541765]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.689]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.634]] [[2.339]
 [2.102]
 [2.339]
 [2.339]
 [2.339]
 [2.339]
 [2.344]] [[1.174]
 [1.166]
 [1.174]
 [1.174]
 [1.174]
 [1.174]
 [1.192]]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.10162881906561558, 0.4857424723156324, 0.31537525550103895, 0.09725345311771319]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.10162881906561558, 0.4857424723156324, 0.31537525550103895, 0.09725345311771319]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.10162881906561558, 0.4857424723156324, 0.31537525550103895, 0.09725345311771319]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.10162881906561558, 0.4857424723156324, 0.31537525550103895, 0.09725345311771319]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.1666666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.10373111469927275, 0.49823887040142895, 0.29828132144621616, 0.0997486934530822]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.10373111469927275, 0.49823887040142895, 0.29828132144621616, 0.0997486934530822]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.10390265643523079, 0.49906375987063456, 0.2971199438276362, 0.0999136398664986]
actor:  1 policy actor:  1  step number:  54 total reward:  0.17999999999999927  reward:  1.0 rdn_beta:  0.167
from probs:  [0.10381037901266939, 0.498514396933904, 0.29787127817385944, 0.09980394587956723]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.17585133582693288, 0.4584328343677938, 0.273926679476079, 0.09178915032919438]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.17585133582693288, 0.4584328343677938, 0.273926679476079, 0.09178915032919438]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.17605440753789223, 0.4577678268364318, 0.27452144645618315, 0.09165631916949303]
from probs:  [0.17605440753789223, 0.4577678268364318, 0.27452144645618315, 0.09165631916949303]
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
probs:  [0.17605457902219057, 0.4577674729137997, 0.2745216994413063, 0.09165624862270338]
siam score:  -0.8614088
line 256 mcts: sample exp_bonus 2.4100572951203802
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.17605474907283575, 0.45776712195018154, 0.27452195031125287, 0.09165617866572981]
Training Flag: False
Self play flag: True
add more workers flag:  True
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.584]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[4.172]
 [4.22 ]
 [4.172]
 [4.172]
 [4.172]
 [4.172]
 [4.172]] [[0.712]
 [0.736]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.17605474907283575, 0.45776712195018154, 0.27452195031125287, 0.09165617866572981]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.651]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[2.625]
 [3.127]
 [2.625]
 [2.625]
 [2.625]
 [2.625]
 [2.625]] [[0.187]
 [0.373]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[2.677]
 [2.677]
 [2.677]
 [2.677]
 [2.677]
 [2.677]
 [2.677]] [[0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.741]
 [0.667]
 [0.668]
 [0.653]
 [0.712]
 [0.644]] [[1.978]
 [2.083]
 [1.872]
 [1.47 ]
 [1.665]
 [1.69 ]
 [1.754]] [[0.17 ]
 [0.261]
 [0.145]
 [0.016]
 [0.069]
 [0.115]
 [0.093]]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.1765258183954913, 0.45780006240083526, 0.27401120851235045, 0.09166291069132312]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.1765258183954913, 0.45780006240083526, 0.27401120851235045, 0.09166291069132312]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.17134348115658385, 0.4746175360088138, 0.2590168969685168, 0.09502208586608567]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.17134348115658385, 0.4746175360088138, 0.2590168969685168, 0.09502208586608567]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
using another actor
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.17199339122022067, 0.4752981990331084, 0.2575500938606352, 0.09515831588603575]
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.182]
 [0.178]
 [0.182]
 [0.182]
 [0.182]
 [0.182]] [[-1.019]
 [-1.019]
 [-0.339]
 [-1.019]
 [-1.019]
 [-1.019]
 [-1.019]] [[0.182]
 [0.182]
 [0.178]
 [0.182]
 [0.182]
 [0.182]
 [0.182]]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.17199339122022067, 0.4752981990331084, 0.2575500938606352, 0.09515831588603575]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.386]
 [0.415]
 [0.393]
 [0.377]
 [0.418]
 [0.401]] [[2.109]
 [2.326]
 [1.927]
 [1.908]
 [1.962]
 [1.848]
 [1.741]] [[ 0.04 ]
 [ 0.077]
 [ 0.04 ]
 [ 0.015]
 [ 0.008]
 [ 0.029]
 [-0.006]]
line 256 mcts: sample exp_bonus 0.2370867421947614
from probs:  [0.1711240196928615, 0.4757971252915904, 0.257820775992141, 0.09525807902340705]
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.1713517558200373, 0.4764306835240557, 0.25683279832290634, 0.09538476233300068]
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.1713517558200373, 0.4764306835240557, 0.25683279832290634, 0.09538476233300068]
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.1713517558200373, 0.4764306835240557, 0.25683279832290634, 0.09538476233300068]
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.1713517558200373, 0.4764306835240557, 0.25683279832290634, 0.09538476233300068]
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.054]
 [0.378]
 [0.378]
 [0.667]
 [0.361]
 [0.378]] [[1.847]
 [3.273]
 [0.249]
 [0.249]
 [0.96 ]
 [0.855]
 [0.249]] [[ 0.421]
 [ 0.721]
 [-0.049]
 [-0.049]
 [ 0.369]
 [ 0.137]
 [-0.049]]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.17157741471047966, 0.4770577167942252, 0.25585472735330245, 0.09551014114199262]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.17157741471047966, 0.4770577167942252, 0.25585472735330245, 0.09551014114199262]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.17157741471047966, 0.4770577167942252, 0.25585472735330245, 0.09551014114199262]
line 256 mcts: sample exp_bonus 4.01038310300348
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[2.494]
 [2.494]
 [2.494]
 [2.494]
 [2.494]
 [2.494]
 [2.494]] [[0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.17157741471047966, 0.4770577167942252, 0.25585472735330245, 0.09551014114199262]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.1715775689977524, 0.4770574060561043, 0.2558549457395093, 0.09551007920663415]
actor:  1 policy actor:  1  step number:  38 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.1579863735758225, 0.4392470374359867, 0.3148168924083018, 0.08794969657988902]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.15665348266061505, 0.44082000757407713, 0.3142622895100775, 0.0882642202552303]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.15687939547493024, 0.4406340215641999, 0.31425974866594913, 0.0882268342949206]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.003]
 [ 0.024]
 [ 0.027]
 [ 0.008]
 [ 0.005]
 [-0.001]] [[6.278]
 [6.273]
 [5.796]
 [5.507]
 [5.804]
 [5.794]
 [5.721]] [[0.352]
 [0.352]
 [0.234]
 [0.151]
 [0.222]
 [0.216]
 [0.19 ]]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.15725909018561693, 0.4406855038308531, 0.3138181598568112, 0.08823724612671881]
Printing some Q and Qe and total Qs values:  [[-0.064]
 [-0.076]
 [-0.069]
 [-0.053]
 [-0.057]
 [-0.061]
 [-0.059]] [[4.973]
 [5.086]
 [5.24 ]
 [4.837]
 [4.993]
 [4.8  ]
 [4.988]] [[ 0.009]
 [ 0.034]
 [ 0.093]
 [-0.025]
 [ 0.023]
 [-0.046]
 [ 0.019]]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.657]
 [0.586]
 [0.598]
 [0.557]
 [0.672]
 [0.588]] [[1.856]
 [2.565]
 [0.551]
 [0.54 ]
 [1.012]
 [1.946]
 [0.578]] [[0.658]
 [0.775]
 [0.458]
 [0.462]
 [0.512]
 [0.693]
 [0.463]]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.593]
 [0.653]
 [0.653]] [[2.26 ]
 [2.26 ]
 [2.26 ]
 [2.26 ]
 [2.283]
 [2.26 ]
 [2.26 ]] [[0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.386]
 [0.439]
 [0.439]]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.15737797318941205, 0.44000588240772825, 0.31451467437631536, 0.08810147002654435]
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.56 ]
 [0.563]
 [0.64 ]
 [0.568]
 [0.545]
 [0.617]] [[6.208]
 [5.479]
 [5.739]
 [5.595]
 [5.87 ]
 [5.57 ]
 [5.449]] [[0.875]
 [0.614]
 [0.699]
 [0.727]
 [0.745]
 [0.628]
 [0.658]]
siam score:  -0.84633285
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.15737797318941205, 0.44000588240772825, 0.31451467437631536, 0.08810147002654435]
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.633]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]] [[1.811]
 [2.485]
 [1.811]
 [1.811]
 [1.811]
 [1.811]
 [1.811]] [[0.184]
 [0.339]
 [0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.15670668837719548, 0.4397395320716435, 0.31550544996517044, 0.08804832958599051]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.15604085972843434, 0.4394703371219502, 0.31649418280088915, 0.08799462034872625]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.15604085972843434, 0.4394703371219502, 0.31649418280088915, 0.08799462034872625]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.15615357634520471, 0.43879622980921207, 0.31719024745877555, 0.08785994638680768]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.15615357634520471, 0.43879622980921207, 0.31719024745877555, 0.08785994638680768]
start point for exploration sampling:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.15649065315015348, 0.43678121482600485, 0.31927074757299095, 0.08745738445085072]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.15672034093245515, 0.4365982642007633, 0.3192607936115936, 0.087420601255188]
line 256 mcts: sample exp_bonus 6.2845530958615345
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.15642645294525143, 0.436411335721214, 0.3197787543907607, 0.08738345694277388]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.15642645294525143, 0.436411335721214, 0.3197787543907607, 0.08738345694277388]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.15642645294525143, 0.436411335721214, 0.3197787543907607, 0.08738345694277388]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.15642645294525143, 0.436411335721214, 0.3197787543907607, 0.08738345694277388]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.15642645294525143, 0.436411335721214, 0.3197787543907607, 0.08738345694277388]
from probs:  [0.15642645294525143, 0.436411335721214, 0.3197787543907607, 0.08738345694277388]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.15565106519484115, 0.43681254816316056, 0.32007270444366037, 0.087463682198338]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.15565106519484115, 0.43681254816316056, 0.32007270444366037, 0.087463682198338]
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.15591434495340958, 0.43755121907662814, 0.318923051130442, 0.08761138483952023]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.465]
 [0.508]
 [0.508]
 [0.505]
 [0.508]
 [0.509]] [[3.429]
 [3.853]
 [2.731]
 [2.893]
 [2.955]
 [3.192]
 [3.254]] [[0.506]
 [0.465]
 [0.508]
 [0.508]
 [0.505]
 [0.508]
 [0.509]]
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.15591434495340958, 0.43755121907662814, 0.318923051130442, 0.08761138483952023]
from probs:  [0.15591451539389567, 0.43755105823500795, 0.31892307350871935, 0.08761135286237703]
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.1561439854675732, 0.43736782537694074, 0.3189136750422749, 0.08757451411321109]
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.15647965111457987, 0.43652680862085724, 0.3195872745692744, 0.08740626569528853]
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.15647965111457987, 0.43652680862085724, 0.3195872745692744, 0.08740626569528853]
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.15571396845557783, 0.4369231291859455, 0.31987738982796643, 0.08748551253051029]
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.15571396845557783, 0.4369231291859455, 0.31987738982796643, 0.08748551253051029]
siam score:  -0.84935594
siam score:  -0.85011065
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.15571413763994293, 0.4369229680896526, 0.31987741377226475, 0.08748548049813969]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.15571413763994293, 0.4369229680896526, 0.31987741377226475, 0.08748548049813969]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.1559290788731182, 0.4356059619302283, 0.3212425933233358, 0.08722236587331764]
using another actor
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.15619216774447894, 0.4363413852996285, 0.32009702808247376, 0.08736941887341884]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.15619216774447894, 0.4363413852996285, 0.32009702808247376, 0.08736941887341884]
siam score:  -0.85058594
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.15619233755615655, 0.4363412244292699, 0.3200970511281073, 0.08736938688646607]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.15619233755615655, 0.4363412244292699, 0.3200970511281073, 0.08736938688646607]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.15645316939335546, 0.4370703375539496, 0.3189613149429961, 0.0875151781096988]
actor:  1 policy actor:  1  step number:  57 total reward:  0.10666666666666624  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[2.658]
 [2.362]
 [2.362]
 [2.362]
 [2.362]
 [2.362]
 [2.362]] [[0.561]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]]
line 256 mcts: sample exp_bonus 2.5703888667649584
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.1499320505671925, 0.41884160683299454, 0.3473561255528516, 0.08387021704696147]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.1499320505671925, 0.41884160683299454, 0.3473561255528516, 0.08387021704696147]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.15015611955478445, 0.4186950565370225, 0.3473080987092447, 0.08384072519894827]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.14942114614134516, 0.41905722003307033, 0.3476084916879746, 0.08391314213760992]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.14942140582006688, 0.4190567520568962, 0.34760879322177285, 0.08391304890126412]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.14942140582006688, 0.4190567520568962, 0.34760879322177285, 0.08391304890126412]
siam score:  -0.8580589
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.316]
 [0.316]
 [0.314]] [[-0.362]
 [-0.362]
 [-0.362]
 [-0.362]
 [-0.92 ]
 [-0.902]
 [-0.362]] [[0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.316]
 [0.316]
 [0.314]]
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[5.325]
 [5.475]
 [5.475]
 [5.475]
 [5.475]
 [5.475]
 [5.475]] [[0.307]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]]
siam score:  -0.85494524
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.14788753366894022, 0.4209595903696184, 0.3468592323171625, 0.08429364364427899]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.14811303101532847, 0.42081103635602407, 0.34681218022961513, 0.0842637523990322]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.14811315298939878, 0.4208108094886306, 0.34681233031929976, 0.08426370720267093]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.559]
 [0.53 ]
 [0.543]
 [0.573]
 [0.606]
 [0.542]] [[1.801]
 [2.911]
 [1.211]
 [1.122]
 [0.787]
 [0.726]
 [0.838]] [[0.531]
 [0.559]
 [0.53 ]
 [0.543]
 [0.573]
 [0.606]
 [0.542]]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.14819003389816293, 0.42013088955837097, 0.3475512148509095, 0.08412786169255661]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.14826687837103916, 0.4194521425511781, 0.34828872841887515, 0.08399225065890756]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.14826687837103916, 0.4194521425511781, 0.34828872841887515, 0.08399225065890756]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.1486120190434914, 0.4195331337080819, 0.3478462928235853, 0.08400855442484145]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.1486120190434914, 0.4195331337080819, 0.3478462928235853, 0.08400855442484145]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.14887832717695745, 0.42028541430182037, 0.346677280266291, 0.08415897825493117]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.14940473064396934, 0.4217718566228249, 0.3443672096473318, 0.0844562030858739]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.14940473064396934, 0.4217718566228249, 0.3443672096473318, 0.0844562030858739]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.14940473064396934, 0.4217718566228249, 0.3443672096473318, 0.0844562030858739]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.14966475363215015, 0.4225063816317731, 0.34322578817469335, 0.08460307656138341]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.14974621558948334, 0.4218369461561235, 0.34394751046198546, 0.08446932779240766]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.15008748280128698, 0.42190009312210514, 0.3435303598460224, 0.08448206423058541]
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.612]
 [0.437]
 [0.602]
 [0.603]
 [0.359]
 [0.552]] [[3.684]
 [3.465]
 [3.866]
 [3.263]
 [2.098]
 [1.557]
 [3.492]] [[0.818]
 [1.038]
 [0.939]
 [0.998]
 [0.822]
 [0.515]
 [0.987]]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.1488907287577797, 0.4224543975020125, 0.3440621617071934, 0.08459271203301437]
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.1489691977034961, 0.42179005490247784, 0.34478076627681115, 0.08445998111721495]
using explorer policy with actor:  1
start point for exploration sampling:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.1489691977034961, 0.42179005490247784, 0.34478076627681115, 0.08445998111721495]
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.1489691977034961, 0.42179005490247784, 0.34478076627681115, 0.08445998111721495]
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.1489691977034961, 0.42179005490247784, 0.34478076627681115, 0.08445998111721495]
actor:  1 policy actor:  1  step number:  51 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.5
from probs:  [0.14161933778977343, 0.4009661681094133, 0.3771184034403931, 0.08029609066042015]
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.14161944148294872, 0.4009659393393453, 0.3771185741060606, 0.08029604507164527]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.141889677169795, 0.4017305448403683, 0.3759308440747306, 0.08044893391510609]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.369]
 [0.367]
 [0.367]
 [0.36 ]
 [0.367]
 [0.363]] [[3.64 ]
 [3.621]
 [2.946]
 [2.992]
 [3.286]
 [3.246]
 [3.379]] [[0.537]
 [0.529]
 [0.332]
 [0.345]
 [0.427]
 [0.419]
 [0.456]]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.141889677169795, 0.4017305448403683, 0.3759308440747306, 0.08044893391510609]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.672]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[4.552]
 [4.463]
 [3.431]
 [3.431]
 [3.431]
 [3.431]
 [3.431]] [[0.813]
 [0.79 ]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.14215772318199946, 0.40248997947133575, 0.3747515088492641, 0.08060078849740056]
actor:  1 policy actor:  1  step number:  63 total reward:  0.13333333333333297  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.1380091139579924, 0.38533158315720606, 0.39713089085569014, 0.07952841202911147]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.1380091139579924, 0.38533158315720606, 0.39713089085569014, 0.07952841202911147]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.1380091139579924, 0.38533158315720606, 0.39713089085569014, 0.07952841202911147]
line 256 mcts: sample exp_bonus 2.2603527020916934
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.1380091139579924, 0.38533158315720606, 0.39713089085569014, 0.07952841202911147]
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.259]
 [0.259]
 [0.258]
 [0.259]
 [0.259]
 [0.257]] [[4.344]
 [4.489]
 [4.591]
 [4.527]
 [4.489]
 [4.489]
 [4.401]] [[0.475]
 [0.505]
 [0.526]
 [0.512]
 [0.505]
 [0.505]
 [0.486]]
siam score:  -0.84639895
actor:  1 policy actor:  1  step number:  40 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.1359157528422186, 0.40823269625647957, 0.3741053467574394, 0.08174620414386242]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.1359494050691622, 0.40756246820355957, 0.374875845433832, 0.08161228129344623]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.13619654879205415, 0.4083039249841078, 0.3737389902281084, 0.08176053599572976]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.361]
 [0.122]
 [0.399]
 [0.404]
 [0.159]
 [0.308]] [[2.748]
 [3.123]
 [1.059]
 [1.7  ]
 [0.902]
 [0.689]
 [1.387]] [[0.344]
 [0.361]
 [0.122]
 [0.399]
 [0.404]
 [0.159]
 [0.308]]
line 256 mcts: sample exp_bonus 2.8178289534677106
line 256 mcts: sample exp_bonus 1.8009014727011237
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.13619654879205415, 0.4083039249841078, 0.3737389902281084, 0.08176053599572976]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.531]
 [0.82 ]
 [0.286]
 [0.356]
 [0.612]
 [0.822]] [[1.455]
 [1.499]
 [1.209]
 [1.268]
 [1.39 ]
 [1.413]
 [1.416]] [[0.756]
 [0.686]
 [0.819]
 [0.352]
 [0.473]
 [0.721]
 [0.917]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3199999999999992  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.1316994533747473, 0.38803839751289987, 0.4001355107604292, 0.08012663835192352]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.3050],
        [-0.5858],
        [-0.4776],
        [-0.5100],
        [-0.5479],
        [-0.5476],
        [-0.4151],
        [-0.4628],
        [-0.5168],
        [-0.5219]], dtype=torch.float64)
-0.032346567066 -0.3373574020750995
-0.09703970119800001 -0.6828642457701342
-0.032346567066 -0.5099600005137029
-0.032346567066 -0.5423162248017153
-0.032346567066 -0.580241937769782
-0.032346567066 -0.5799738913640978
-0.09703970119800001 -0.5121227903692948
-0.032346567066 -0.4951016770717305
-0.032346567066 -0.5490976494918862
-0.032346567066 -0.5542771885053538
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.121]
 [0.119]
 [0.121]
 [0.12 ]
 [0.117]
 [0.121]] [[0.282]
 [0.201]
 [0.25 ]
 [0.201]
 [0.203]
 [0.165]
 [0.201]] [[0.121]
 [0.121]
 [0.119]
 [0.121]
 [0.12 ]
 [0.117]
 [0.121]]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.13172526629817796, 0.38905185913413015, 0.39926932150347966, 0.07995355306421226]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.13172539836420147, 0.38905153509877927, 0.3992694813999018, 0.0799535851371176]
actor:  1 policy actor:  1  step number:  72 total reward:  0.05999999999999972  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.131576273097383, 0.3751549541805248, 0.4109758959139009, 0.08229287680819118]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.13175636070531593, 0.3742993734872956, 0.41153883220276216, 0.08240543360462628]
first move QE:  1.2727705237988676
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.575]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]] [[5.352]
 [5.194]
 [5.352]
 [5.352]
 [5.352]
 [5.352]
 [5.352]] [[1.159]
 [1.061]
 [1.159]
 [1.159]
 [1.159]
 [1.159]
 [1.159]]
siam score:  -0.84224284
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.13233134462163523, 0.3734993411981473, 0.4117264132825705, 0.082442900897647]
from probs:  [0.13235945529959403, 0.37445691854067853, 0.4109048857231079, 0.0822787404366194]
maxi score, test score, baseline:  -0.9961121212121212 -1.0 -0.9961121212121212
probs:  [0.13253819165439973, 0.37361206307048017, 0.4114600098738647, 0.0823897354012554]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.9961121212121212 -1.0 -0.9961121212121212
using explorer policy with actor:  0
from probs:  [0.13292290357430334, 0.3728853390371238, 0.41174494859943167, 0.08244680878914118]
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[1.159]
 [1.159]
 [1.159]
 [1.159]
 [1.159]
 [1.159]
 [1.159]] [[0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.1331301193377968, 0.3729972210238617, 0.41147893782476735, 0.08239372181357414]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.1331301193377968, 0.3729972210238617, 0.41147893782476735, 0.08239372181357414]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.13313024205883872, 0.37299685957804546, 0.4114791366970524, 0.08239376166606342]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.987558179807663
maxi score, test score, baseline:  -0.996168656716418 -1.0 -0.996168656716418
probs:  [0.13351384315107492, 0.3722791698861828, 0.41175747196315376, 0.08244951499958841]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996168656716418 -1.0 -0.996168656716418
probs:  [0.13295354817905208, 0.3725199243704072, 0.4120237675141002, 0.08250275993644045]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.03 ]
 [0.036]
 [0.014]
 [0.014]] [[3.322]
 [3.322]
 [3.322]
 [3.234]
 [3.231]
 [3.322]
 [3.322]] [[0.47 ]
 [0.47 ]
 [0.47 ]
 [0.435]
 [0.439]
 [0.47 ]
 [0.47 ]]
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.028]
 [0.039]
 [0.015]
 [0.019]
 [0.024]
 [0.039]] [[3.033]
 [2.65 ]
 [2.845]
 [3.151]
 [3.111]
 [3.046]
 [2.845]] [[0.323]
 [0.109]
 [0.227]
 [0.376]
 [0.358]
 [0.326]
 [0.227]]
maxi score, test score, baseline:  -0.996168656716418 -1.0 -0.996168656716418
probs:  [0.13295354817905208, 0.3725199243704072, 0.4120237675141002, 0.08250275993644045]
actor:  1 policy actor:  1  step number:  37 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.683]
 [0.706]
 [0.791]
 [0.601]
 [0.605]
 [0.628]] [[2.788]
 [1.555]
 [1.221]
 [2.134]
 [2.264]
 [2.405]
 [1.56 ]] [[1.336]
 [0.854]
 [0.766]
 [1.155]
 [1.008]
 [1.059]
 [0.801]]
maxi score, test score, baseline:  -0.996168656716418 -1.0 -0.996168656716418
probs:  [0.13187555442319554, 0.34919503758867987, 0.4323626211039265, 0.0865667868841982]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.13203207272734885, 0.3484225640762634, 0.4328759424397618, 0.08666942075662601]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.13203207272734885, 0.3484225640762634, 0.4328759424397618, 0.08666942075662601]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8822713132686912
actor:  1 policy actor:  1  step number:  49 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]] [[3.359]
 [2.563]
 [2.563]
 [2.563]
 [2.563]
 [2.563]
 [2.563]] [[0.777]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]]
Printing some Q and Qe and total Qs values:  [[0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]] [[3.023]
 [3.023]
 [3.023]
 [3.023]
 [3.023]
 [3.023]
 [3.023]] [[0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.13182251477218737, 0.3365181848229162, 0.44297250004507915, 0.08868680035981742]
line 256 mcts: sample exp_bonus 5.953538389739646
siam score:  -0.84352106
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
start point for exploration sampling:  11091
actor:  1 policy actor:  1  step number:  54 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.09051002641968288, 0.3654848217071847, 0.45195942672939815, 0.09204572514373426]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.424]
 [0.447]
 [0.449]
 [0.446]
 [0.442]
 [0.461]] [[1.214]
 [1.08 ]
 [1.007]
 [0.805]
 [0.782]
 [0.685]
 [0.889]] [[0.472]
 [0.424]
 [0.447]
 [0.449]
 [0.446]
 [0.442]
 [0.461]]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.09051002641968288, 0.3654848217071847, 0.45195942672939815, 0.09204572514373426]
maxi score, test score, baseline:  -0.9961962962962964 -1.0 -0.9961962962962964
probs:  [0.09064034253224133, 0.3645704230240608, 0.45261101947374816, 0.09217821496994982]
maxi score, test score, baseline:  -0.9961962962962964 -1.0 -0.9961962962962964
probs:  [0.09048463807110359, 0.3656547090928989, 0.4518311808892315, 0.09202947194676604]
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.733]
 [0.74 ]
 [0.741]
 [0.713]
 [0.684]
 [0.682]] [[2.815]
 [3.295]
 [2.383]
 [2.007]
 [2.615]
 [2.776]
 [2.897]] [[0.704]
 [0.733]
 [0.74 ]
 [0.741]
 [0.713]
 [0.684]
 [0.682]]
maxi score, test score, baseline:  -0.9961962962962964 -1.0 -0.9961962962962964
probs:  [0.09048463807110359, 0.3656547090928989, 0.4518311808892315, 0.09202947194676604]
maxi score, test score, baseline:  -0.9961962962962964 -1.0 -0.9961962962962964
maxi score, test score, baseline:  -0.9961962962962964 -1.0 -0.9961962962962964
probs:  [0.09048463807110359, 0.3656547090928989, 0.4518311808892315, 0.09202947194676604]
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.09048468534090337, 0.3656544190189801, 0.4518314165392351, 0.09202947910088152]
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.09048468534090337, 0.3656544190189801, 0.4518314165392351, 0.09202947910088152]
actor:  1 policy actor:  1  step number:  38 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  63 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.08535377858195405, 0.40165894213985737, 0.4261764493667775, 0.08681082991141106]
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.08518532875506984, 0.40283314777068485, 0.4253329584830363, 0.08664856499120896]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.08518532875506984, 0.40283314777068485, 0.4253329584830363, 0.08664856499120896]
siam score:  -0.8468217
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.08518532875506984, 0.40283314777068485, 0.4253329584830363, 0.08664856499120896]
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.08518532875506984, 0.40283314777068485, 0.4253329584830363, 0.08664856499120896]
siam score:  -0.84305495
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.08515102872832747, 0.4030659868554558, 0.42516022596537867, 0.08662275845083801]
actor:  1 policy actor:  1  step number:  61 total reward:  0.10666666666666591  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.356]
 [0.313]
 [0.312]
 [0.309]
 [0.315]
 [0.312]] [[3.271]
 [3.226]
 [3.281]
 [3.218]
 [3.113]
 [3.205]
 [3.021]] [[0.646]
 [0.648]
 [0.642]
 [0.602]
 [0.533]
 [0.596]
 [0.479]]
from probs:  [0.08338477829813376, 0.4163281824091688, 0.4154559925601906, 0.08483104673250676]
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.08347151699109279, 0.4160196530292211, 0.41676136538101743, 0.08374746459866868]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.5286438502192731
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.08360550745801555, 0.4150812705053585, 0.41743132321640536, 0.08388189882022058]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[-0.749]
 [-0.749]
 [-0.749]
 [-0.749]
 [-0.749]
 [-0.749]
 [-0.749]] [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.519]
 [0.519]
 [0.552]
 [0.543]
 [0.519]
 [0.519]] [[1.307]
 [1.025]
 [1.025]
 [1.111]
 [1.077]
 [1.025]
 [1.025]] [[0.532]
 [0.519]
 [0.519]
 [0.552]
 [0.543]
 [0.519]
 [0.519]]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.08360554436984712, 0.41508105436746245, 0.4174315071178491, 0.08388189414484132]
siam score:  -0.8485541
Starting evaluation
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.004]
 [-0.003]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[ 0.098]
 [ 2.773]
 [-0.093]
 [ 0.23 ]
 [ 0.23 ]
 [ 0.23 ]
 [ 0.23 ]] [[-0.007]
 [-0.004]
 [-0.003]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.08675280343752503, 0.39304876384017096, 0.43319030954959414, 0.08700812317270987]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[1.431]
 [1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]] [[0.52 ]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]]
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.667]
 [0.665]
 [0.692]
 [0.685]
 [0.671]
 [0.628]] [[2.982]
 [2.766]
 [2.642]
 [2.389]
 [2.44 ]
 [3.01 ]
 [2.731]] [[1.09 ]
 [1.006]
 [0.941]
 [0.841]
 [0.86 ]
 [1.132]
 [0.949]]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[3.161]
 [3.161]
 [3.161]
 [3.161]
 [3.161]
 [3.161]
 [3.161]] [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]] [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.08675283368165748, 0.39304859059856134, 0.4331904601473129, 0.08700811557246828]
Printing some Q and Qe and total Qs values:  [[0.953]
 [0.98 ]
 [0.972]
 [0.972]
 [0.972]
 [0.972]
 [0.972]] [[7.328]
 [6.934]
 [7.613]
 [7.613]
 [7.613]
 [7.613]
 [7.613]] [[0.953]
 [0.98 ]
 [0.972]
 [0.972]
 [0.972]
 [0.972]
 [0.972]]
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.08675298164133005, 0.393047743071033, 0.43319119689810326, 0.0870080783895337]
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.08675298164133005, 0.393047743071033, 0.43319119689810326, 0.0870080783895337]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4564750943460967
actor:  0 policy actor:  1  step number:  33 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9907920187793426 -1.0 -0.9907920187793426
probs:  [0.08658142173832838, 0.39423272091368194, 0.4323325199464464, 0.08685333740154329]
maxi score, test score, baseline:  -0.9907920187793426 -1.0 -0.9907920187793426
probs:  [0.08658142173832838, 0.39423272091368194, 0.4323325199464464, 0.08685333740154329]
maxi score, test score, baseline:  -0.990823976608187 -1.0 -0.990823976608187
probs:  [0.08658149456373317, 0.3942323041643494, 0.43233288256437863, 0.08685331870753879]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[4.109]
 [4.109]
 [4.109]
 [4.109]
 [4.109]
 [4.109]
 [4.109]] [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]]
actor:  0 policy actor:  1  step number:  50 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  50 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  51 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.976474074074074 -1.0 -0.976474074074074
probs:  [0.08654842984606247, 0.3944215708233085, 0.4321682432244486, 0.08686175610618037]
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[2.107]
 [1.865]
 [1.865]
 [1.865]
 [1.865]
 [1.865]
 [1.865]] [[0.548]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]]
actor:  0 policy actor:  1  step number:  52 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.972149134948097 -1.0 -0.972149134948097
probs:  [0.0865383191948336, 0.3944794659319178, 0.4321178988320387, 0.08686431604120998]
maxi score, test score, baseline:  -0.972149134948097 -1.0 -0.972149134948097
probs:  [0.0865383191948336, 0.3944794659319178, 0.4321178988320387, 0.08686431604120998]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.972149134948097 -1.0 -0.972149134948097
probs:  [0.0865383191948336, 0.3944794659319178, 0.4321178988320387, 0.08686431604120998]
line 256 mcts: sample exp_bonus 5.86553641177456
maxi score, test score, baseline:  -0.9723398625429553 -1.0 -0.9723398625429553
probs:  [0.08653876650732832, 0.3944769043508239, 0.4321201261576702, 0.08686420298417755]
maxi score, test score, baseline:  -0.9723398625429553 -1.0 -0.9723398625429553
probs:  [0.08653876650732832, 0.3944769043508239, 0.4321201261576702, 0.08686420298417755]
actor:  1 policy actor:  1  step number:  60 total reward:  0.09999999999999898  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8446088
maxi score, test score, baseline:  -0.972527986348123 -1.0 -0.972527986348123
probs:  [0.08497092849409713, 0.40545870057502126, 0.42428045635528105, 0.0852899145756006]
actor:  1 policy actor:  1  step number:  64 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9726210884353742 -1.0 -0.9726210884353742
probs:  [0.08215370596916183, 0.3919952421436173, 0.4101934939647213, 0.11565755792249972]
maxi score, test score, baseline:  -0.972713559322034 -0.649 -0.649
probs:  [0.08176668874460308, 0.3979172656721979, 0.408277753677492, 0.11203829190570704]
maxi score, test score, baseline:  -0.972713559322034 -0.649 -0.649
probs:  [0.08188580771889825, 0.39703918756732665, 0.40887341269385696, 0.11220159201991821]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.972713559322034 -0.649 -0.649
probs:  [0.08200435812243903, 0.3961653006463827, 0.4094662285502777, 0.11236411268090049]
maxi score, test score, baseline:  -0.972713559322034 -0.649 -0.649
actor:  1 policy actor:  1  step number:  48 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.336]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[2.808]
 [2.904]
 [2.808]
 [2.808]
 [2.808]
 [2.808]
 [2.808]] [[0.503]
 [0.561]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]]
siam score:  -0.83900434
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.972713559322034 -0.649 -0.649
probs:  [0.08186099589860903, 0.4087542510162134, 0.3981956342898479, 0.1111891187953297]
from probs:  [0.08186099589860903, 0.4087542510162134, 0.3981956342898479, 0.1111891187953297]
maxi score, test score, baseline:  -0.9728054054054055 -0.649 -0.649
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.331]
 [0.285]
 [0.29 ]
 [0.295]
 [0.261]
 [0.282]] [[1.901]
 [2.259]
 [1.861]
 [1.985]
 [1.283]
 [1.937]
 [2.005]] [[0.269]
 [0.331]
 [0.285]
 [0.29 ]
 [0.295]
 [0.261]
 [0.282]]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9728054054054055 -0.649 -0.649
probs:  [0.08175792600369272, 0.4082375134322941, 0.39869122242295396, 0.11131333814105923]
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.478]
 [0.447]
 [0.447]
 [0.445]
 [0.447]
 [0.447]] [[1.848]
 [1.901]
 [1.363]
 [1.363]
 [1.608]
 [1.363]
 [1.363]] [[0.447]
 [0.478]
 [0.447]
 [0.447]
 [0.445]
 [0.447]
 [0.447]]
maxi score, test score, baseline:  -0.9728054054054055 -0.649 -0.649
probs:  [0.08175792600369272, 0.4082375134322941, 0.39869122242295396, 0.11131333814105923]
maxi score, test score, baseline:  -0.9728054054054055 -0.649 -0.649
probs:  [0.08175792600369272, 0.4082375134322941, 0.39869122242295396, 0.11131333814105923]
actor:  1 policy actor:  1  step number:  51 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 1.9246528146047486
maxi score, test score, baseline:  -0.9728966329966331 -0.649 -0.649
probs:  [0.08359306471768668, 0.38784313213905763, 0.4174277112909041, 0.1111360918523516]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5249],
        [-0.5183],
        [-0.5349],
        [-0.2369],
        [-0.3474],
        [-0.2445],
        [ 0.5291],
        [-0.4667],
        [-0.5355],
        [-0.3754]], dtype=torch.float64)
-0.032346567066 -0.5572825048899913
-0.032346567066 -0.5506134928771644
-0.032346567066 -0.5672936100786746
-0.09703970119800001 -0.3339667329563529
-0.058614567066 -0.4060565141392521
-0.032346567066 -0.27686254675136956
-0.058226434398 0.470829576067464
-0.032346567066 -0.4990859080022733
-0.032346567066 -0.5678245600957621
-0.09703970119800001 -0.4724651673721578
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.222]
 [0.252]
 [0.136]
 [0.115]
 [0.252]
 [0.193]] [[2.066]
 [2.595]
 [1.366]
 [0.606]
 [0.075]
 [1.366]
 [2.373]] [[0.611]
 [0.814]
 [0.476]
 [0.189]
 [0.024]
 [0.476]
 [0.733]]
maxi score, test score, baseline:  -0.9729872483221477 -0.649 -0.649
probs:  [0.08343596291740916, 0.3888393394856341, 0.41664130124056964, 0.11108339635638709]
Printing some Q and Qe and total Qs values:  [[0.06 ]
 [0.056]
 [0.033]
 [0.012]
 [0.056]
 [0.056]
 [0.056]] [[5.482]
 [5.514]
 [5.345]
 [5.192]
 [5.514]
 [5.514]
 [5.514]] [[0.522]
 [0.528]
 [0.471]
 [0.42 ]
 [0.528]
 [0.528]
 [0.528]]
maxi score, test score, baseline:  -0.9729872483221477 -0.649 -0.649
probs:  [0.08343596291740916, 0.3888393394856341, 0.41664130124056964, 0.11108339635638709]
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]] [[4.732]
 [4.732]
 [4.732]
 [4.732]
 [4.732]
 [4.732]
 [4.732]] [[0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]]
maxi score, test score, baseline:  -0.9729872483221477 -0.649 -0.649
probs:  [0.08343596291740916, 0.3888393394856341, 0.41664130124056964, 0.11108339635638709]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08350653752370304, 0.38816597031889155, 0.41699343984108933, 0.11133405231631618]
actor:  1 policy actor:  1  step number:  59 total reward:  0.06666666666666621  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08486008335217572, 0.37948093080182016, 0.423768417706, 0.11189056814000423]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08486008335217572, 0.37948093080182016, 0.423768417706, 0.11189056814000423]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08486008335217572, 0.37948093080182016, 0.423768417706, 0.11189056814000423]
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]] [[2.745]
 [2.745]
 [2.745]
 [2.745]
 [2.745]
 [2.745]
 [2.745]] [[0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08486008335217572, 0.37948093080182016, 0.423768417706, 0.11189056814000423]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08470841994590712, 0.38044141490323063, 0.4230092245749642, 0.11184094057589808]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08455706108822211, 0.3813999702989641, 0.42225155594610353, 0.11179141266671018]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08455706108822211, 0.3813999702989641, 0.42225155594610353, 0.11179141266671018]
actor:  1 policy actor:  1  step number:  64 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.258]
 [0.196]
 [0.22 ]
 [0.187]
 [0.195]
 [0.183]] [[2.9  ]
 [3.092]
 [2.193]
 [2.373]
 [2.566]
 [2.61 ]
 [2.637]] [[0.298]
 [0.379]
 [0.031]
 [0.106]
 [0.157]
 [0.176]
 [0.18 ]]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.418]
 [0.253]
 [0.215]
 [0.118]
 [0.112]
 [0.122]] [[7.152]
 [5.904]
 [6.013]
 [5.934]
 [6.268]
 [6.336]
 [5.829]] [[0.852]
 [0.671]
 [0.63 ]
 [0.602]
 [0.628]
 [0.638]
 [0.548]]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08546830852125589, 0.37529473723205103, 0.42681435633349646, 0.11242259791319673]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08546830852125589, 0.37529473723205103, 0.42681435633349646, 0.11242259791319673]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.0854320726472236, 0.3751958052821722, 0.4266342440526026, 0.11273787801800157]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08781302795072134, 0.36015705985258417, 0.43855261956598307, 0.11347729263071143]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2733333333333332  reward:  1.0 rdn_beta:  0.667
siam score:  -0.830809
line 256 mcts: sample exp_bonus 5.7372246978633505
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08453564313314725, 0.34808749225576346, 0.4221618093723658, 0.14521505523872347]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08453564313314725, 0.34808749225576346, 0.4221618093723658, 0.14521505523872347]
actor:  1 policy actor:  1  step number:  52 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  68 total reward:  0.17999999999999983  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  62 total reward:  0.20666666666666655  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.149]
 [0.075]
 [0.075]
 [0.09 ]
 [0.085]
 [0.057]] [[2.522]
 [2.49 ]
 [2.208]
 [2.099]
 [2.456]
 [2.506]
 [2.219]] [[-0.093]
 [-0.052]
 [-0.22 ]
 [-0.256]
 [-0.122]
 [-0.111]
 [-0.234]]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08253908260929992, 0.34023780815703303, 0.4121842540532525, 0.16503885518041447]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08082168751443708, 0.3539811187469356, 0.40359543419559263, 0.16160175954303463]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.799]
 [0.525]
 [0.421]
 [0.479]
 [0.488]
 [0.49 ]] [[1.507]
 [1.478]
 [1.552]
 [1.493]
 [1.36 ]
 [1.264]
 [1.177]] [[0.431]
 [0.635]
 [0.385]
 [0.262]
 [0.275]
 [0.252]
 [0.225]]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08101038423304353, 0.35247108904039853, 0.4045391204380247, 0.16197940628853316]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.0811040223687329, 0.35172175769219116, 0.40500741167843224, 0.16216680826064372]
UNIT TEST: sample policy line 217 mcts : [0.163 0.429 0.082 0.041 0.061 0.122 0.102]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08117404569445617, 0.3520258587353423, 0.40535760350806443, 0.16144249206213704]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08111940582089874, 0.35204482553494165, 0.4050837856001956, 0.16175198304396401]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.505]
 [0.288]
 [0.293]
 [0.303]
 [0.307]
 [0.302]] [[1.86 ]
 [2.912]
 [1.669]
 [1.432]
 [1.45 ]
 [1.591]
 [1.55 ]] [[ 0.123]
 [ 0.539]
 [ 0.062]
 [-0.003]
 [ 0.008]
 [ 0.051]
 [ 0.036]]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08091782176032752, 0.3528219890194992, 0.4040745311747605, 0.1621856580454128]
maxi score, test score, baseline:  -0.9730772575250837 -0.649 -0.649
probs:  [0.08091782176032752, 0.3528219890194992, 0.4040745311747605, 0.1621856580454128]
maxi score, test score, baseline:  -0.9731666666666667 -0.649 -0.649
probs:  [0.08091782176032752, 0.3528219890194992, 0.4040745311747605, 0.1621856580454128]
maxi score, test score, baseline:  -0.9732554817275748 -0.649 -0.649
maxi score, test score, baseline:  -0.9732554817275748 -0.649 -0.649
probs:  [0.08091782176032752, 0.3528219890194992, 0.4040745311747605, 0.1621856580454128]
maxi score, test score, baseline:  -0.9732554817275748 -0.649 -0.649
probs:  [0.08091782176032752, 0.3528219890194992, 0.4040745311747605, 0.1621856580454128]
actor:  1 policy actor:  1  step number:  59 total reward:  0.09333333333333249  reward:  1.0 rdn_beta:  0.5
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.265]] [[4.661]
 [4.661]
 [4.661]
 [4.661]
 [4.661]
 [4.661]
 [4.661]] [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]]
maxi score, test score, baseline:  -0.9733437086092716 -0.649 -0.649
maxi score, test score, baseline:  -0.9733437086092716 -0.649 -0.649
probs:  [0.08213643854891481, 0.34692263064650875, 0.4101729385823666, 0.16076799222220986]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.264]
 [0.216]
 [0.209]
 [0.209]
 [0.222]
 [0.209]] [[4.745]
 [4.25 ]
 [4.618]
 [5.067]
 [5.067]
 [4.54 ]
 [5.067]] [[0.269]
 [0.185]
 [0.251]
 [0.357]
 [0.357]
 [0.235]
 [0.357]]
first move QE:  1.2929032931240716
maxi score, test score, baseline:  -0.9733437086092716 -0.649 -0.649
maxi score, test score, baseline:  -0.9733437086092716 -0.649 -0.649
probs:  [0.08213643854891481, 0.34692263064650875, 0.4101729385823666, 0.16076799222220986]
maxi score, test score, baseline:  -0.9733437086092716 -0.649 -0.649
probs:  [0.08213643854891481, 0.34692263064650875, 0.4101729385823666, 0.16076799222220986]
maxi score, test score, baseline:  -0.9733437086092716 -0.649 -0.649
probs:  [0.08213643854891481, 0.34692263064650875, 0.4101729385823666, 0.16076799222220986]
maxi score, test score, baseline:  -0.9733437086092716 -0.649 -0.649
probs:  [0.08213643854891481, 0.34692263064650875, 0.4101729385823666, 0.16076799222220986]
maxi score, test score, baseline:  -0.9733437086092716 -0.649 -0.649
probs:  [0.08213643854891481, 0.34692263064650875, 0.4101729385823666, 0.16076799222220986]
maxi score, test score, baseline:  -0.9733437086092716 -0.649 -0.649
probs:  [0.08213643854891481, 0.34692263064650875, 0.4101729385823666, 0.16076799222220986]
maxi score, test score, baseline:  -0.9733437086092716 -0.649 -0.649
probs:  [0.0819928568086651, 0.34766523845346764, 0.4094543292457, 0.1608875754921672]
maxi score, test score, baseline:  -0.9733437086092716 -0.649 -0.649
probs:  [0.08208448787312542, 0.3469353903517056, 0.40991258497946853, 0.16106753679570038]
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.357]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]] [[2.947]
 [2.277]
 [3.443]
 [3.443]
 [3.443]
 [3.443]
 [3.443]] [[0.28 ]
 [0.28 ]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9734313531353136 -0.649 -0.649
probs:  [0.08200805052085766, 0.34690347931850973, 0.40953063052105243, 0.16155783963958023]
maxi score, test score, baseline:  -0.9734313531353136 -0.649 -0.649
probs:  [0.08200805052085766, 0.34690347931850973, 0.40953063052105243, 0.16155783963958023]
line 256 mcts: sample exp_bonus 4.539469428568584
maxi score, test score, baseline:  -0.9734313531353136 -0.649 -0.649
Printing some Q and Qe and total Qs values:  [[0.17 ]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]] [[6.296]
 [5.947]
 [5.947]
 [5.947]
 [5.947]
 [5.947]
 [5.947]] [[0.82 ]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]]
maxi score, test score, baseline:  -0.9734313531353136 -0.649 -0.649
probs:  [0.08186551339551888, 0.34763842129787537, 0.4088172502128828, 0.16167881509372292]
maxi score, test score, baseline:  -0.9735184210526316 -0.649 -0.649
probs:  [0.08158141751485652, 0.34910326063967734, 0.40739538621489013, 0.1619199356305761]
siam score:  -0.83427995
maxi score, test score, baseline:  -0.9735184210526316 -0.649 -0.649
probs:  [0.08153114255749025, 0.3491035578161062, 0.40714341948174226, 0.16222188014466124]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9735184210526316 -0.649 -0.649
probs:  [0.08153114255749025, 0.3491035578161062, 0.40714341948174226, 0.16222188014466124]
line 256 mcts: sample exp_bonus 4.4232445098814575
maxi score, test score, baseline:  -0.9735184210526316 -0.649 -0.649
probs:  [0.08153114255749025, 0.3491035578161062, 0.40714341948174226, 0.16222188014466124]
maxi score, test score, baseline:  -0.9735184210526316 -0.649 -0.649
probs:  [0.08153114255749025, 0.3491035578161062, 0.40714341948174226, 0.16222188014466124]
Printing some Q and Qe and total Qs values:  [[-0.051]
 [-0.029]
 [-0.045]
 [-0.045]
 [-0.022]
 [-0.024]
 [-0.028]] [[4.009]
 [3.036]
 [3.868]
 [3.593]
 [4.556]
 [3.647]
 [2.799]] [[-0.007]
 [-0.148]
 [-0.025]
 [-0.071]
 [ 0.113]
 [-0.041]
 [-0.186]]
maxi score, test score, baseline:  -0.9735184210526316 -0.649 -0.649
probs:  [0.08168810246467671, 0.3476107435268767, 0.40792924868070995, 0.16277190532773655]
using explorer policy with actor:  1
siam score:  -0.835392
maxi score, test score, baseline:  -0.9735184210526316 -0.649 -0.649
probs:  [0.08182352002314792, 0.34714568426025816, 0.40860733209869304, 0.1624234636179008]
Printing some Q and Qe and total Qs values:  [[0.165]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]] [[1.527]
 [1.572]
 [1.572]
 [1.572]
 [1.572]
 [1.572]
 [1.572]] [[-0.045]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]]
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.451]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]] [[3.688]
 [3.519]
 [3.688]
 [3.688]
 [3.688]
 [3.688]
 [3.688]] [[0.625]
 [0.609]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]]
maxi score, test score, baseline:  -0.9735184210526316 -0.649 -0.649
probs:  [0.08182352002314792, 0.34714568426025816, 0.40860733209869304, 0.1624234636179008]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9735184210526316 -0.649 -0.649
maxi score, test score, baseline:  -0.9735184210526316 -0.649 -0.649
probs:  [0.0843996186315761, 0.333665322189157, 0.4215000472857987, 0.16043501189346818]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.134]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[0.423]
 [1.411]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[0.414]
 [0.806]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9735184210526316 -0.649 -0.649
probs:  [0.0867375528588503, 0.32166277010929434, 0.4332011173604629, 0.15839855967139257]
maxi score, test score, baseline:  -0.9735184210526316 -0.649 -0.649
probs:  [0.08661036845848491, 0.3223157129386192, 0.4325645752525975, 0.15850934335029848]
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.784]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.774]] [[3.18 ]
 [4.002]
 [3.514]
 [3.514]
 [3.514]
 [3.514]
 [2.648]] [[0.766]
 [0.784]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.774]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.9735184210526316 -0.649 -0.649
probs:  [0.08667532245640674, 0.3225577421537912, 0.4328894296633071, 0.15787750572649487]
maxi score, test score, baseline:  -0.9735184210526316 -0.649 -0.649
probs:  [0.08654850749686738, 0.32321069075813424, 0.4322547357930074, 0.157986065951991]
maxi score, test score, baseline:  -0.9736049180327869 -0.649 -0.649
probs:  [0.08654850749686738, 0.32321069075813424, 0.4322547357930074, 0.157986065951991]
maxi score, test score, baseline:  -0.9736049180327869 -0.649 -0.649
probs:  [0.08642194133261841, 0.3238623583579472, 0.4316212871137037, 0.15809441319573062]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7271479207707976
maxi score, test score, baseline:  -0.9736049180327869 -0.649 -0.649
probs:  [0.08642194133261841, 0.3238623583579472, 0.4316212871137037, 0.15809441319573062]
maxi score, test score, baseline:  -0.9736049180327869 -0.649 -0.649
probs:  [0.08642194133261841, 0.3238623583579472, 0.4316212871137037, 0.15809441319573062]
maxi score, test score, baseline:  -0.9736049180327869 -0.649 -0.649
probs:  [0.08642194133261841, 0.3238623583579472, 0.4316212871137037, 0.15809441319573062]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9736908496732026 -0.649 -0.649
probs:  [0.08642194133261841, 0.3238623583579472, 0.4316212871137037, 0.15809441319573062]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 4.772590208090149
maxi score, test score, baseline:  -0.9737762214983714 -0.649 -0.649
probs:  [0.08656337204197757, 0.32253513129390304, 0.4323293616243295, 0.15857213503978984]
maxi score, test score, baseline:  -0.973861038961039 -0.649 -0.649
probs:  [0.08656337204197757, 0.32253513129390304, 0.4323293616243295, 0.15857213503978984]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[1.643]
 [1.643]
 [1.643]
 [1.643]
 [1.643]
 [1.643]
 [1.643]] [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]]
using explorer policy with actor:  1
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.973861038961039 -0.649 -0.649
probs:  [0.08656337204197757, 0.32253513129390304, 0.4323293616243295, 0.15857213503978984]
maxi score, test score, baseline:  -0.973861038961039 -0.649 -0.649
probs:  [0.08656337204197757, 0.32253513129390304, 0.4323293616243295, 0.15857213503978984]
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.076]
 [0.045]
 [0.046]
 [0.048]
 [0.045]
 [0.045]] [[2.831]
 [2.191]
 [2.415]
 [2.448]
 [2.391]
 [2.45 ]
 [2.45 ]] [[ 0.277]
 [-0.032]
 [ 0.062]
 [ 0.081]
 [ 0.052]
 [ 0.081]
 [ 0.081]]
maxi score, test score, baseline:  -0.973861038961039 -0.649 -0.649
probs:  [0.08656337204197757, 0.32253513129390304, 0.4323293616243295, 0.15857213503978984]
maxi score, test score, baseline:  -0.973861038961039 -0.649 -0.649
probs:  [0.08656337204197757, 0.32253513129390304, 0.4323293616243295, 0.15857213503978984]
maxi score, test score, baseline:  -0.9739453074433657 -0.649 -0.649
probs:  [0.08656337204197757, 0.32253513129390304, 0.4323293616243295, 0.15857213503978984]
maxi score, test score, baseline:  -0.9739453074433657 -0.649 -0.649
probs:  [0.08656337204197757, 0.32253513129390304, 0.4323293616243295, 0.15857213503978984]
from probs:  [0.08656337204197757, 0.32253513129390304, 0.4323293616243295, 0.15857213503978984]
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.275]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[2.643]
 [2.144]
 [2.643]
 [2.643]
 [2.643]
 [2.643]
 [2.643]] [[1.107]
 [0.869]
 [1.107]
 [1.107]
 [1.107]
 [1.107]
 [1.107]]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.037]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[3.293]
 [2.455]
 [3.293]
 [3.293]
 [3.293]
 [3.293]
 [3.293]] [[ 0.308]
 [-0.032]
 [ 0.308]
 [ 0.308]
 [ 0.308]
 [ 0.308]
 [ 0.308]]
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.22 ]
 [0.22 ]
 [0.132]
 [0.178]
 [0.22 ]
 [0.22 ]] [[2.019]
 [2.019]
 [2.019]
 [2.246]
 [2.222]
 [2.019]
 [2.019]] [[0.006]
 [0.006]
 [0.006]
 [0.069]
 [0.099]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  -0.9740290322580646 -0.649 -0.649
probs:  [0.08683844113307967, 0.32259620000700995, 0.4337050640461693, 0.15686029481374106]
using explorer policy with actor:  1
siam score:  -0.83015907
maxi score, test score, baseline:  -0.9740290322580646 -0.649 -0.649
probs:  [0.08696258265044918, 0.32305795626480605, 0.43432593273560227, 0.1556535283491425]
maxi score, test score, baseline:  -0.9740290322580646 -0.649 -0.649
probs:  [0.08696258265044918, 0.32305795626480605, 0.43432593273560227, 0.1556535283491425]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9740290322580646 -0.649 -0.649
actor:  1 policy actor:  1  step number:  71 total reward:  0.15999999999999914  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  0.56  reward:  1.0 rdn_beta:  0.167
using another actor
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.9740290322580646 -0.649 -0.649
maxi score, test score, baseline:  -0.9740290322580646 -0.649 -0.649
first move QE:  1.2639812713776337
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.517]
 [0.473]
 [0.543]
 [0.542]
 [0.498]
 [0.532]] [[2.692]
 [2.137]
 [1.716]
 [2.312]
 [2.32 ]
 [1.999]
 [2.557]] [[0.502]
 [0.517]
 [0.473]
 [0.543]
 [0.542]
 [0.498]
 [0.532]]
maxi score, test score, baseline:  -0.9740290322580646 -0.649 -0.649
probs:  [0.09205169963983476, 0.33466763622546086, 0.45960787564299876, 0.11367278849170549]
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
probs:  [0.09205169963983476, 0.33466763622546086, 0.45960787564299876, 0.11367278849170549]
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
probs:  [0.09216523090549204, 0.33384627064327, 0.4601754689900348, 0.11381302946120318]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
probs:  [0.09176189084177051, 0.33640833826219724, 0.4581544646830189, 0.11367530621301335]
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
probs:  [0.09176189084177051, 0.33640833826219724, 0.4581544646830189, 0.11367530621301335]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.915757690181583
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.134]
 [0.13 ]
 [0.134]
 [0.132]
 [0.126]
 [0.136]] [[1.722]
 [1.888]
 [1.774]
 [1.511]
 [1.745]
 [1.873]
 [1.872]] [[0.281]
 [0.35 ]
 [0.304]
 [0.209]
 [0.295]
 [0.338]
 [0.346]]
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
probs:  [0.09176189084177051, 0.33640833826219724, 0.4581544646830189, 0.11367530621301335]
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]] [[1.647]
 [1.647]
 [1.647]
 [1.647]
 [1.647]
 [1.647]
 [1.647]] [[0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]]
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
probs:  [0.09162753193898722, 0.33726180317951654, 0.45748123644304306, 0.11362942843845308]
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
probs:  [0.09149321689294791, 0.3381149895131947, 0.45680822795477677, 0.11358356563908073]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.044]
 [-0.047]
 [-0.046]
 [-0.043]
 [-0.043]
 [-0.043]] [[2.219]
 [2.374]
 [2.045]
 [2.202]
 [2.035]
 [1.903]
 [2.039]] [[-0.245]
 [-0.169]
 [-0.337]
 [-0.256]
 [-0.338]
 [-0.404]
 [-0.336]]
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
probs:  [0.09149321689294791, 0.3381149895131947, 0.45680822795477677, 0.11358356563908073]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  61 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
probs:  [0.0923358843275797, 0.33276225830203177, 0.4610305571205418, 0.1138713002498468]
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
probs:  [0.09220593430564314, 0.3335877174448967, 0.4603794203313832, 0.11382692791807716]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.271]
 [0.211]
 [0.205]
 [0.215]
 [0.211]
 [0.25 ]] [[1.652]
 [1.759]
 [1.444]
 [2.32 ]
 [1.214]
 [1.444]
 [1.482]] [[-0.123]
 [ 0.088]
 [-0.077]
 [ 0.209]
 [-0.149]
 [-0.077]
 [-0.025]]
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.146]
 [0.146]
 [0.152]
 [0.146]
 [0.146]
 [0.146]] [[-0.513]
 [ 0.   ]
 [ 0.   ]
 [-0.483]
 [ 0.   ]
 [ 0.   ]
 [-0.423]] [[-0.088]
 [ 0.168]
 [ 0.168]
 [-0.068]
 [ 0.168]
 [ 0.168]
 [-0.043]]
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
probs:  [0.09207602528106036, 0.33441291616733176, 0.45972848896647095, 0.11378256958513687]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.447]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]] [[3.566]
 [3.766]
 [3.492]
 [3.492]
 [3.492]
 [3.492]
 [3.492]] [[0.307]
 [0.505]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]]
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
probs:  [0.09215652872923122, 0.3347056439141925, 0.46013096067123654, 0.11300686668533967]
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
probs:  [0.09215652872923122, 0.3347056439141925, 0.46013096067123654, 0.11300686668533967]
actor:  1 policy actor:  1  step number:  63 total reward:  0.05333333333333301  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9741948717948719 -0.649 -0.649
probs:  [0.09128283215356964, 0.341019020944892, 0.45576297199194415, 0.11193517490959431]
maxi score, test score, baseline:  -0.9742769968051119 -0.649 -0.649
probs:  [0.09128283215356964, 0.341019020944892, 0.45576297199194415, 0.11193517490959431]
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.201]
 [0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.141]] [[2.79 ]
 [3.564]
 [2.79 ]
 [2.79 ]
 [2.79 ]
 [2.79 ]
 [2.79 ]] [[0.184]
 [0.477]
 [0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]]
maxi score, test score, baseline:  -0.9742769968051119 -0.649 -0.649
maxi score, test score, baseline:  -0.9742769968051119 -0.649 -0.649
maxi score, test score, baseline:  -0.9742769968051119 -0.649 -0.649
probs:  [0.09113364029151409, 0.3418786637202491, 0.4550142007364758, 0.111973495251761]
maxi score, test score, baseline:  -0.9742769968051119 -0.649 -0.649
actor:  1 policy actor:  1  step number:  51 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  -0.9742769968051119 -0.649 -0.649
probs:  [0.1126548472864103, 0.336528564784115, 0.4588930510824896, 0.09192353684698513]
actor:  1 policy actor:  1  step number:  61 total reward:  0.013333333333332531  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8403926
maxi score, test score, baseline:  -0.9742769968051119 -0.649 -0.649
probs:  [0.11289685036874661, 0.33239523891639805, 0.46213720351645027, 0.09257070719840514]
maxi score, test score, baseline:  -0.9742769968051119 -0.649 -0.649
probs:  [0.11289685036874661, 0.33239523891639805, 0.46213720351645027, 0.09257070719840514]
siam score:  -0.83910155
maxi score, test score, baseline:  -0.9742769968051119 -0.649 -0.649
probs:  [0.11284670120476745, 0.33325176865444256, 0.4614649330196719, 0.09243659712111812]
maxi score, test score, baseline:  -0.9742769968051119 -0.649 -0.649
probs:  [0.11284670120476745, 0.33325176865444256, 0.4614649330196719, 0.09243659712111812]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9743585987261147 -0.649 -0.649
probs:  [0.11284670120476745, 0.33325176865444256, 0.4614649330196719, 0.09243659712111812]
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.028]
 [-0.034]
 [-0.035]
 [-0.035]
 [-0.033]
 [-0.035]] [[1.933]
 [2.025]
 [1.911]
 [1.724]
 [1.772]
 [1.693]
 [1.817]] [[-0.259]
 [-0.206]
 [-0.269]
 [-0.364]
 [-0.339]
 [-0.377]
 [-0.317]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9743585987261147 -0.649 -0.649
probs:  [0.11284670120476745, 0.33325176865444256, 0.4614649330196719, 0.09243659712111812]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.013]
 [-0.02 ]
 [-0.021]
 [-0.019]
 [-0.019]
 [-0.025]] [[5.721]
 [4.395]
 [3.234]
 [3.695]
 [5.422]
 [5.422]
 [4.302]] [[ 0.546]
 [ 0.071]
 [-0.352]
 [-0.187]
 [ 0.436]
 [ 0.436]
 [ 0.029]]
maxi score, test score, baseline:  -0.9743585987261147 -0.649 -0.649
probs:  [0.11284670120476745, 0.33325176865444256, 0.4614649330196719, 0.09243659712111812]
actor:  1 policy actor:  1  step number:  51 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9743585987261147 -0.649 -0.649
probs:  [0.11132953016596461, 0.34222175598834015, 0.45525451845984916, 0.09119419538584612]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.9743585987261147 -0.649 -0.649
maxi score, test score, baseline:  -0.9743585987261147 -0.649 -0.649
actor:  1 policy actor:  1  step number:  62 total reward:  0.07333333333333258  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9743585987261147 -0.649 -0.649
probs:  [0.11069724630942357, 0.3375876749693183, 0.4596447330844417, 0.09207034563681647]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9744396825396826 -0.649 -0.649
maxi score, test score, baseline:  -0.9744396825396826 -0.649 -0.649
probs:  [0.11069724630942357, 0.3375876749693183, 0.4596447330844417, 0.09207034563681647]
maxi score, test score, baseline:  -0.9744396825396826 -0.649 -0.649
maxi score, test score, baseline:  -0.9744396825396826 -0.649 -0.649
probs:  [0.11098508185932286, 0.3358641022993579, 0.46084113409728233, 0.09230968174403695]
maxi score, test score, baseline:  -0.9744396825396826 -0.649 -0.649
probs:  [0.11107159116603676, 0.33586065642309193, 0.46077161554487317, 0.09229613686599838]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9744396825396826 -0.649 -0.649
from probs:  [0.11107159116603676, 0.33586065642309193, 0.46077161554487317, 0.09229613686599838]
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.312]
 [0.325]
 [0.293]
 [0.326]
 [0.347]
 [0.336]] [[5.446]
 [4.863]
 [5.167]
 [5.379]
 [6.041]
 [6.106]
 [5.573]] [[0.324]
 [0.242]
 [0.312]
 [0.342]
 [0.496]
 [0.518]
 [0.401]]
maxi score, test score, baseline:  -0.9744396825396826 -0.649 -0.649
probs:  [0.11167474716126836, 0.3374776564690852, 0.45892144440376126, 0.09192615196588522]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.161]
 [0.083]
 [0.084]
 [0.156]
 [0.088]
 [0.089]] [[1.602]
 [2.247]
 [1.367]
 [1.203]
 [1.721]
 [1.406]
 [1.341]] [[-0.004]
 [ 0.274]
 [-0.085]
 [-0.138]
 [ 0.1  ]
 [-0.068]
 [-0.088]]
maxi score, test score, baseline:  -0.9744396825396826 -0.649 -0.649
probs:  [0.11167474716126836, 0.3374776564690852, 0.45892144440376126, 0.09192615196588522]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9744396825396826 -0.649 -0.649
probs:  [0.10907446207326855, 0.35291437521596075, 0.4482248425479717, 0.08978632016279889]
maxi score, test score, baseline:  -0.9744396825396826 -0.649 -0.649
probs:  [0.10907446207326855, 0.35291437521596075, 0.4482248425479717, 0.08978632016279889]
actor:  1 policy actor:  1  step number:  61 total reward:  0.07999999999999963  reward:  1.0 rdn_beta:  0.333
using another actor
from probs:  [0.10907446207326855, 0.35291437521596075, 0.4482248425479717, 0.08978632016279889]
maxi score, test score, baseline:  -0.974520253164557 -0.649 -0.649
probs:  [0.10813617434039004, 0.35848456592425354, 0.44436507718951535, 0.08901418254584108]
Printing some Q and Qe and total Qs values:  [[-0.011]
 [ 0.151]
 [-0.011]
 [ 0.007]
 [-0.011]
 [-0.011]
 [-0.011]] [[5.045]
 [4.986]
 [5.045]
 [3.522]
 [5.045]
 [5.045]
 [5.045]] [[ 0.144]
 [ 0.285]
 [ 0.144]
 [-0.074]
 [ 0.144]
 [ 0.144]
 [ 0.144]]
maxi score, test score, baseline:  -0.974520253164557 -0.649 -0.649
maxi score, test score, baseline:  -0.974520253164557 -0.649 -0.649
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.75 ]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[2.748]
 [2.879]
 [2.727]
 [2.727]
 [2.727]
 [2.727]
 [2.727]] [[0.732]
 [0.75 ]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.974520253164557 -0.649 -0.649
probs:  [0.10807072436915444, 0.3593885846362026, 0.4436660079159725, 0.08887468307867043]
maxi score, test score, baseline:  -0.974520253164557 -0.649 -0.649
maxi score, test score, baseline:  -0.974520253164557 -0.649 -0.649
maxi score, test score, baseline:  -0.974520253164557 -0.649 -0.649
probs:  [0.10714625484869753, 0.35976111725000015, 0.44412593671230827, 0.0889666911889941]
start point for exploration sampling:  11091
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.298]
 [0.124]
 [0.031]
 [0.099]
 [0.099]
 [0.138]] [[4.699]
 [4.265]
 [4.402]
 [3.969]
 [5.592]
 [5.592]
 [4.418]] [[0.289]
 [0.318]
 [0.208]
 [0.007]
 [0.525]
 [0.525]
 [0.225]]
maxi score, test score, baseline:  -0.9746003154574133 -0.649 -0.649
actor:  1 policy actor:  1  step number:  69 total reward:  0.09333333333333194  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9746003154574133 -0.649 -0.649
probs:  [0.10619652648443206, 0.3654400580262295, 0.4401850859628714, 0.08817832952646702]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]] [[-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]]
maxi score, test score, baseline:  -0.9746798742138365 -0.649 -0.649
probs:  [0.10612443308383535, 0.36635993878835144, 0.439478337158394, 0.08803729096941924]
siam score:  -0.82942015
maxi score, test score, baseline:  -0.9746798742138365 -0.649 -0.649
probs:  [0.10612443308383535, 0.36635993878835144, 0.439478337158394, 0.08803729096941924]
using explorer policy with actor:  1
siam score:  -0.8306923
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9746798742138365 -0.649 -0.649
probs:  [0.10612443308383535, 0.36635993878835144, 0.439478337158394, 0.08803729096941924]
from probs:  [0.10612443308383535, 0.36635993878835144, 0.439478337158394, 0.08803729096941924]
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.225]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]] [[2.123]
 [2.07 ]
 [2.123]
 [2.123]
 [2.123]
 [2.123]
 [2.123]] [[0.188]
 [0.277]
 [0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.188]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.203]
 [0.12 ]] [[4.189]
 [4.188]
 [4.188]
 [4.188]
 [4.188]
 [4.269]
 [4.188]] [[0.46 ]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.512]
 [0.432]]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.318]
 [0.285]
 [0.287]
 [0.283]
 [0.281]
 [0.283]] [[2.285]
 [1.536]
 [1.728]
 [1.669]
 [1.726]
 [1.808]
 [1.802]] [[0.284]
 [0.318]
 [0.285]
 [0.287]
 [0.283]
 [0.281]
 [0.283]]
maxi score, test score, baseline:  -0.9746798742138365 -0.649 -0.649
probs:  [0.10498737642939689, 0.36949841101477937, 0.4378096937976088, 0.08770451875821487]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7119095743570478
maxi score, test score, baseline:  -0.9746798742138365 -0.649 -0.649
maxi score, test score, baseline:  -0.9746798742138365 -0.649 -0.649
probs:  [0.10498737642939689, 0.36949841101477937, 0.4378096937976088, 0.08770451875821487]
maxi score, test score, baseline:  -0.9747589341692791 -0.649 -0.649
probs:  [0.10498737642939689, 0.36949841101477937, 0.4378096937976088, 0.08770451875821487]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9747589341692791 -0.649 -0.649
maxi score, test score, baseline:  -0.9748375 -0.649 -0.649
probs:  [0.10498737642939689, 0.36949841101477937, 0.4378096937976088, 0.08770451875821487]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.574]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]] [[4.541]
 [5.424]
 [4.541]
 [4.541]
 [4.541]
 [4.541]
 [4.541]] [[0.757]
 [0.939]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9748375 -0.649 -0.649
probs:  [0.10498737642939689, 0.36949841101477937, 0.4378096937976088, 0.08770451875821487]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2599999999999999  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.316]
 [0.034]
 [0.119]
 [0.119]
 [0.119]
 [0.125]] [[1.388]
 [2.07 ]
 [1.398]
 [1.757]
 [1.757]
 [1.757]
 [1.21 ]] [[0.344]
 [0.624]
 [0.328]
 [0.455]
 [0.455]
 [0.455]
 [0.327]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9748375 -0.649 -0.649
probs:  [0.1055364726175339, 0.3528262743887765, 0.45125033571370393, 0.09038691727998562]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.599]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[1.544]
 [1.557]
 [1.544]
 [1.544]
 [1.544]
 [1.544]
 [1.544]] [[0.456]
 [0.599]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]]
maxi score, test score, baseline:  -0.9748375 -0.649 -0.649
probs:  [0.1055364726175339, 0.3528262743887765, 0.45125033571370393, 0.09038691727998562]
maxi score, test score, baseline:  -0.9748375 -0.649 -0.649
maxi score, test score, baseline:  -0.9748375 -0.649 -0.649
maxi score, test score, baseline:  -0.9749155763239876 -0.649 -0.649
probs:  [0.1055364726175339, 0.3528262743887765, 0.45125033571370393, 0.09038691727998562]
actor:  1 policy actor:  1  step number:  43 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9749155763239876 -0.649 -0.649
maxi score, test score, baseline:  -0.9749155763239876 -0.649 -0.649
actor:  1 policy actor:  1  step number:  69 total reward:  0.22666666666666557  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9749155763239876 -0.649 -0.649
probs:  [0.10182708787623815, 0.3755895950835051, 0.4353725521041953, 0.08721076493606142]
using another actor
maxi score, test score, baseline:  -0.9749155763239876 -0.649 -0.649
probs:  [0.10166040732978045, 0.37738888162837353, 0.434011590659877, 0.08693912038196906]
maxi score, test score, baseline:  -0.9749155763239876 -0.649 -0.649
probs:  [0.10166040732978045, 0.37738888162837353, 0.434011590659877, 0.08693912038196906]
rdn beta is 0 so we're just using the maxi policy
using another actor
maxi score, test score, baseline:  -0.975146913580247 -0.649 -0.649
probs:  [0.10166040732978045, 0.37738888162837353, 0.434011590659877, 0.08693912038196906]
siam score:  -0.8317955
maxi score, test score, baseline:  -0.975146913580247 -0.649 -0.649
probs:  [0.10157712678306013, 0.3782878801638819, 0.43333159760995676, 0.08680339544310134]
maxi score, test score, baseline:  -0.975146913580247 -0.649 -0.649
probs:  [0.10149388601597363, 0.37918644928539147, 0.43265192936427493, 0.08666773533436001]
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.048]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.054]
 [-0.057]] [[2.271]
 [2.077]
 [1.035]
 [1.035]
 [1.035]
 [1.748]
 [1.987]] [[-0.468]
 [-0.492]
 [-0.673]
 [-0.673]
 [-0.673]
 [-0.552]
 [-0.516]]
maxi score, test score, baseline:  -0.975146913580247 -0.649 -0.649
probs:  [0.10149388601597363, 0.37918644928539147, 0.43265192936427493, 0.08666773533436001]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.10149388601597363, 0.37918644928539147, 0.43265192936427493, 0.08666773533436001]
line 256 mcts: sample exp_bonus 5.633415282725912
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.291]
 [0.139]
 [0.137]
 [0.134]
 [0.147]
 [0.136]] [[0.823]
 [0.908]
 [0.853]
 [0.676]
 [0.435]
 [0.57 ]
 [0.565]] [[0.363]
 [0.485]
 [0.372]
 [0.332]
 [0.277]
 [0.316]
 [0.307]]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.10067207840644195, 0.37953330402396046, 0.4330477101381883, 0.08674690743140916]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.10067207840644195, 0.37953330402396046, 0.4330477101381883, 0.08674690743140916]
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.448]
 [0.183]
 [0.104]
 [0.308]
 [0.323]
 [0.308]] [[4.997]
 [4.721]
 [4.575]
 [4.728]
 [4.61 ]
 [4.935]
 [4.61 ]] [[0.286]
 [0.401]
 [0.087]
 [0.059]
 [0.224]
 [0.347]
 [0.224]]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.10058589412164667, 0.3804334957100508, 0.43236914253941017, 0.08661146762889238]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.10058589412164667, 0.3804334957100508, 0.43236914253941017, 0.08661146762889238]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
siam score:  -0.8407167
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.10049975042680369, 0.3813332634355818, 0.43169089452356424, 0.08647609161405027]
line 256 mcts: sample exp_bonus 0.8736764119613283
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.09815755505282424, 0.3957626337976232, 0.4216185985064316, 0.08446121264312102]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.09829699245871623, 0.3949036132418277, 0.422218230306412, 0.0845811639930442]
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.09 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.064]] [[2.614]
 [2.316]
 [2.261]
 [2.261]
 [2.261]
 [2.261]
 [2.285]] [[-0.056]
 [-0.125]
 [-0.173]
 [-0.173]
 [-0.173]
 [-0.173]
 [-0.161]]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.09843590894367356, 0.3940478018807811, 0.42281562195601313, 0.08470066721953226]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.09843590894367356, 0.3940478018807811, 0.42281562195601313, 0.08470066721953226]
Printing some Q and Qe and total Qs values:  [[-0.025]
 [ 0.234]
 [ 0.175]
 [ 0.089]
 [ 0.141]
 [ 0.175]
 [ 0.141]] [[3.197]
 [4.118]
 [4.348]
 [3.993]
 [4.144]
 [4.733]
 [3.998]] [[0.407]
 [0.675]
 [0.704]
 [0.605]
 [0.652]
 [0.783]
 [0.622]]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.09843590894367356, 0.3940478018807811, 0.42281562195601313, 0.08470066721953226]
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.154]
 [0.12 ]
 [0.069]
 [0.12 ]
 [0.087]
 [0.12 ]] [[4.049]
 [2.77 ]
 [4.304]
 [3.475]
 [4.304]
 [3.917]
 [4.304]] [[ 0.253]
 [-0.125]
 [ 0.353]
 [ 0.025]
 [ 0.353]
 [ 0.191]
 [ 0.353]]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
from probs:  [0.09909615242890622, 0.3937963217938109, 0.4224754164141133, 0.08463210936316966]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.09900727852055717, 0.39471707805697626, 0.42178196116034594, 0.08449368226212065]
actor:  1 policy actor:  1  step number:  63 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  0.333
from probs:  [0.09813037911846494, 0.4000821598153244, 0.4180419375358746, 0.08374552353033599]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.0982695599870191, 0.39923061773739693, 0.41863555150712956, 0.08386427076845444]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.0982695599870191, 0.39923061773739693, 0.41863555150712956, 0.08386427076845444]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.0982695599870191, 0.39923061773739693, 0.41863555150712956, 0.08386427076845444]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.0982695599870191, 0.39923061773739693, 0.41863555150712956, 0.08386427076845444]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.0982695599870191, 0.39923061773739693, 0.41863555150712956, 0.08386427076845444]
maxi score, test score, baseline:  -0.975223076923077 -0.649 -0.649
probs:  [0.0982695599870191, 0.39923061773739693, 0.41863555150712956, 0.08386427076845444]
maxi score, test score, baseline:  -0.9753740061162081 -0.649 -0.649
probs:  [0.0982695599870191, 0.39923061773739693, 0.41863555150712956, 0.08386427076845444]
siam score:  -0.838514
maxi score, test score, baseline:  -0.9754487804878049 -0.649 -0.649
maxi score, test score, baseline:  -0.9754487804878049 -0.649 -0.649
probs:  [0.09831792028729695, 0.39930921139575193, 0.41852951806232175, 0.0838433502546294]
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.145]
 [0.102]
 [0.121]
 [0.088]
 [0.075]
 [0.144]] [[1.304]
 [1.114]
 [1.251]
 [1.328]
 [1.057]
 [0.771]
 [1.286]] [[0.174]
 [0.198]
 [0.201]
 [0.246]
 [0.123]
 [0.014]
 [0.254]]
maxi score, test score, baseline:  -0.9754487804878049 -0.649 -0.649
probs:  [0.0982276725301751, 0.4002355889247098, 0.41783252521594927, 0.08370421332916599]
Printing some Q and Qe and total Qs values:  [[0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]] [[1.606]
 [1.606]
 [1.606]
 [1.606]
 [1.606]
 [1.606]
 [1.606]] [[0.11]
 [0.11]
 [0.11]
 [0.11]
 [0.11]
 [0.11]
 [0.11]]
maxi score, test score, baseline:  -0.9754487804878049 -0.649 -0.649
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]] [[2.962]
 [2.962]
 [2.962]
 [2.962]
 [2.962]
 [2.962]
 [2.962]] [[1.023]
 [1.023]
 [1.023]
 [1.023]
 [1.023]
 [1.023]
 [1.023]]
maxi score, test score, baseline:  -0.9755231003039514 -0.649 -0.649
probs:  [0.0983665154188437, 0.3993871745704395, 0.4184238137952519, 0.08382249621546484]
maxi score, test score, baseline:  -0.9755231003039514 -0.649 -0.649
probs:  [0.09902016350796583, 0.3991219894202313, 0.41810052134790937, 0.08375732572389354]
maxi score, test score, baseline:  -0.9755231003039514 -0.649 -0.649
probs:  [0.09902016350796583, 0.3991219894202313, 0.41810052134790937, 0.08375732572389354]
maxi score, test score, baseline:  -0.9755231003039514 -0.649 -0.649
probs:  [0.09902016350796583, 0.3991219894202313, 0.41810052134790937, 0.08375732572389354]
actor:  1 policy actor:  1  step number:  55 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8393991
maxi score, test score, baseline:  -0.9755231003039514 -0.649 -0.649
probs:  [0.09797079972185331, 0.4054946613980721, 0.413664584141786, 0.0828699547382887]
maxi score, test score, baseline:  -0.9755231003039514 -0.649 -0.649
probs:  [0.09797079972185331, 0.4054946613980721, 0.413664584141786, 0.0828699547382887]
actor:  1 policy actor:  1  step number:  59 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]] [[1.097]
 [1.097]
 [1.097]
 [1.097]
 [1.097]
 [1.097]
 [1.097]] [[-0.493]
 [-0.493]
 [-0.493]
 [-0.493]
 [-0.493]
 [-0.493]
 [-0.493]]
maxi score, test score, baseline:  -0.9755231003039514 -0.649 -0.649
probs:  [0.09718895269170859, 0.41024273325446764, 0.4103595107473965, 0.0822088033064273]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9755231003039514 -0.649 -0.649
probs:  [0.09718895269170859, 0.41024273325446764, 0.4103595107473965, 0.0822088033064273]
maxi score, test score, baseline:  -0.9755969696969697 -0.649 -0.649
maxi score, test score, baseline:  -0.9755969696969697 -0.649 -0.649
probs:  [0.09718895269170859, 0.41024273325446764, 0.4103595107473965, 0.0822088033064273]
maxi score, test score, baseline:  -0.9755969696969697 -0.649 -0.649
probs:  [0.09718895269170859, 0.41024273325446764, 0.4103595107473965, 0.0822088033064273]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9755969696969697 -0.649 -0.649
probs:  [0.09718895269170859, 0.41024273325446764, 0.4103595107473965, 0.0822088033064273]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9755969696969697 -0.649 -0.649
probs:  [0.09718895269170859, 0.41024273325446764, 0.4103595107473965, 0.0822088033064273]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  69 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9755969696969697 -0.649 -0.649
probs:  [0.09775624753357516, 0.41476819373205964, 0.4043866183359217, 0.08308894039844351]
maxi score, test score, baseline:  -0.9755969696969697 -0.649 -0.649
maxi score, test score, baseline:  -0.9755969696969697 -0.649 -0.649
probs:  [0.09721676683876858, 0.4158642313779841, 0.4036105761121172, 0.08330842567113013]
maxi score, test score, baseline:  -0.9755969696969697 -0.649 -0.649
probs:  [0.09721676683876858, 0.4158642313779841, 0.4036105761121172, 0.08330842567113013]
maxi score, test score, baseline:  -0.9755969696969697 -0.649 -0.649
probs:  [0.09721676683876858, 0.4158642313779841, 0.4036105761121172, 0.08330842567113013]
maxi score, test score, baseline:  -0.9755969696969697 -0.649 -0.649
probs:  [0.09737195649638589, 0.41652887186733173, 0.40265779284403486, 0.08344137879224749]
Printing some Q and Qe and total Qs values:  [[-0.029]
 [ 0.079]
 [-0.031]
 [-0.019]
 [-0.031]
 [-0.001]
 [-0.031]] [[5.453]
 [5.037]
 [3.437]
 [4.648]
 [3.437]
 [4.727]
 [3.437]] [[ 0.179]
 [ 0.156]
 [-0.327]
 [-0.016]
 [-0.327]
 [ 0.018]
 [-0.327]]
maxi score, test score, baseline:  -0.9755969696969697 -0.649 -0.649
probs:  [0.0979985278933156, 0.416225922986537, 0.4023952424274597, 0.08338030669268774]
maxi score, test score, baseline:  -0.9755969696969697 -0.649 -0.649
probs:  [0.0979985278933156, 0.416225922986537, 0.4023952424274597, 0.08338030669268774]
maxi score, test score, baseline:  -0.9755969696969697 -0.649 -0.649
probs:  [0.09792176128450265, 0.41563350953063966, 0.40318269185229993, 0.08326203733255765]
siam score:  -0.83524436
maxi score, test score, baseline:  -0.9756703927492448 -0.649 -0.649
probs:  [0.09838607214283436, 0.41760660857978105, 0.4003505889665432, 0.0836567303108414]
siam score:  -0.8348076
maxi score, test score, baseline:  -0.9756703927492448 -0.649 -0.649
probs:  [0.09838607214283436, 0.41760660857978105, 0.4003505889665432, 0.0836567303108414]
maxi score, test score, baseline:  -0.9756703927492448 -0.649 -0.649
probs:  [0.09838607214283436, 0.41760660857978105, 0.4003505889665432, 0.0836567303108414]
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.026]
 [-0.039]
 [-0.04 ]
 [-0.04 ]
 [-0.039]
 [-0.04 ]] [[2.18 ]
 [1.936]
 [2.072]
 [2.276]
 [2.163]
 [2.301]
 [2.278]] [[-0.428]
 [-0.462]
 [-0.452]
 [-0.419]
 [-0.438]
 [-0.414]
 [-0.419]]
maxi score, test score, baseline:  -0.975743373493976 -0.649 -0.649
probs:  [0.09838607214283436, 0.41760660857978105, 0.4003505889665432, 0.0836567303108414]
maxi score, test score, baseline:  -0.975743373493976 -0.649 -0.649
probs:  [0.09761464320684554, 0.41796396651475987, 0.4006931751286488, 0.08372821514974571]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.239]
 [0.176]
 [0.227]
 [0.244]
 [0.195]
 [0.193]] [[2.997]
 [2.866]
 [2.057]
 [2.134]
 [2.161]
 [1.822]
 [2.493]] [[ 0.282]
 [ 0.191]
 [-0.141]
 [-0.065]
 [-0.039]
 [-0.201]
 [ 0.021]]
from probs:  [0.09761464320684554, 0.41796396651475987, 0.4006931751286488, 0.08372821514974571]
start point for exploration sampling:  11091
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.314]
 [0.318]
 [0.314]
 [0.314]
 [0.313]
 [0.314]] [[-0.631]
 [-0.631]
 [-0.02 ]
 [-0.631]
 [-0.631]
 [-0.248]
 [-0.631]] [[0.314]
 [0.314]
 [0.318]
 [0.314]
 [0.314]
 [0.313]
 [0.314]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.975815915915916 -0.649 -0.649
probs:  [0.09896751176378041, 0.4276973967299736, 0.38766348801196066, 0.08567160349428532]
siam score:  -0.82875407
maxi score, test score, baseline:  -0.975815915915916 -0.649 -0.649
probs:  [0.09896751176378041, 0.4276973967299736, 0.38766348801196066, 0.08567160349428532]
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.488]
 [0.506]
 [0.498]
 [0.497]
 [0.492]
 [0.493]] [[1.639]
 [2.048]
 [1.428]
 [1.36 ]
 [1.302]
 [1.772]
 [1.721]] [[0.52 ]
 [0.488]
 [0.506]
 [0.498]
 [0.497]
 [0.492]
 [0.493]]
maxi score, test score, baseline:  -0.975815915915916 -0.649 -0.649
probs:  [0.09889309792371112, 0.42713453948907354, 0.3884131246194138, 0.08555923796780157]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.975815915915916 -0.649 -0.649
probs:  [0.09889309792371112, 0.42713453948907354, 0.3884131246194138, 0.08555923796780157]
maxi score, test score, baseline:  -0.975815915915916 -0.649 -0.649
maxi score, test score, baseline:  -0.9758880239520958 -0.649 -0.649
probs:  [0.09830809666844822, 0.428118764623409, 0.3878170271260598, 0.08575611158208296]
maxi score, test score, baseline:  -0.9758880239520958 -0.649 -0.649
probs:  [0.09830809666844822, 0.428118764623409, 0.3878170271260598, 0.08575611158208296]
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.276]
 [0.197]
 [0.189]
 [0.189]
 [0.189]
 [0.189]] [[0.744]
 [0.992]
 [0.806]
 [1.366]
 [1.366]
 [1.366]
 [1.366]] [[-0.449]
 [-0.281]
 [-0.422]
 [-0.245]
 [-0.245]
 [-0.245]
 [-0.245]]
maxi score, test score, baseline:  -0.9758880239520958 -0.649 -0.649
probs:  [0.09845371084649342, 0.42875364119495607, 0.386909542621784, 0.0858831053367664]
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.356]
 [0.331]
 [0.363]
 [0.432]
 [0.388]
 [0.329]] [[5.598]
 [5.342]
 [3.458]
 [3.503]
 [3.515]
 [4.296]
 [2.975]] [[0.798]
 [0.767]
 [0.417]
 [0.434]
 [0.455]
 [0.585]
 [0.329]]
maxi score, test score, baseline:  -0.9758880239520958 -0.649 -0.649
probs:  [0.09845371084649342, 0.42875364119495607, 0.386909542621784, 0.0858831053367664]
maxi score, test score, baseline:  -0.9758880239520958 -0.649 -0.649
siam score:  -0.82078534
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.55 ]
 [0.366]
 [0.327]
 [0.356]
 [0.335]
 [0.374]] [[4.2  ]
 [3.168]
 [3.896]
 [4.114]
 [4.446]
 [4.225]
 [3.91 ]] [[0.613]
 [0.336]
 [0.504]
 [0.564]
 [0.704]
 [0.61 ]
 [0.514]]
maxi score, test score, baseline:  -0.9758880239520958 -0.649 -0.649
probs:  [0.09837780410270966, 0.4281936047253138, 0.38765728641375935, 0.08577130475821707]
actor:  1 policy actor:  1  step number:  60 total reward:  0.3133333333333327  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9758880239520958 -0.649 -0.649
probs:  [0.08529833826730464, 0.425790445093579, 0.3840439069263061, 0.10486730971281036]
maxi score, test score, baseline:  -0.9758880239520958 -0.649 -0.649
probs:  [0.08529833826730464, 0.425790445093579, 0.3840439069263061, 0.10486730971281036]
maxi score, test score, baseline:  -0.9758880239520958 -0.649 -0.649
probs:  [0.08529833826730464, 0.425790445093579, 0.3840439069263061, 0.10486730971281036]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.427594206112624
line 256 mcts: sample exp_bonus 4.612989289009022
maxi score, test score, baseline:  -0.9758880239520958 -0.649 -0.649
probs:  [0.08542806101831527, 0.4264389040528992, 0.38310618933906626, 0.10502684558971923]
actor:  1 policy actor:  1  step number:  67 total reward:  0.11999999999999911  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9759597014925374 -0.649 -0.649
probs:  [0.08600289033120358, 0.42931887723023504, 0.37936098365652243, 0.10531724878203888]
maxi score, test score, baseline:  -0.9759597014925374 -0.649 -0.649
siam score:  -0.8215895
from probs:  [0.08600289033120358, 0.42931887723023504, 0.37936098365652243, 0.10531724878203888]
maxi score, test score, baseline:  -0.9759597014925374 -0.649 -0.649
probs:  [0.08608059148163141, 0.4297072961572174, 0.3797041882439316, 0.10450792411721968]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9760309523809525 -0.649 -0.649
probs:  [0.08585292964382207, 0.4285666742210767, 0.38119244559744797, 0.10438795053765326]
maxi score, test score, baseline:  -0.9760309523809525 -0.649 -0.649
probs:  [0.08585292964382207, 0.4285666742210767, 0.38119244559744797, 0.10438795053765326]
using explorer policy with actor:  1
from probs:  [0.08585292964382207, 0.4285666742210767, 0.38119244559744797, 0.10438795053765326]
maxi score, test score, baseline:  -0.9760309523809525 -0.649 -0.649
probs:  [0.08573916431207422, 0.4279966918550703, 0.38193614552176647, 0.10432799831108913]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.594]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[2.192]
 [1.744]
 [2.192]
 [2.192]
 [2.192]
 [2.192]
 [2.192]] [[-0.027]
 [ 0.077]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]]
maxi score, test score, baseline:  -0.9761017804154304 -0.649 -0.649
probs:  [0.08586818058734826, 0.4286416232621077, 0.38100516096574216, 0.10448503518480194]
actor:  1 policy actor:  1  step number:  82 total reward:  0.0066666666666660435  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9761017804154304 -0.649 -0.649
probs:  [0.08720313792514978, 0.43526234341870135, 0.3860544359990538, 0.09148008265709506]
maxi score, test score, baseline:  -0.9761017804154304 -0.649 -0.649
probs:  [0.08720313792514978, 0.43526234341870135, 0.3860544359990538, 0.09148008265709506]
maxi score, test score, baseline:  -0.9761017804154304 -0.649 -0.649
probs:  [0.08720313792514978, 0.43526234341870135, 0.3860544359990538, 0.09148008265709506]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]] [[2.754]
 [2.754]
 [2.754]
 [2.754]
 [2.754]
 [2.754]
 [2.754]] [[0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]]
maxi score, test score, baseline:  -0.9761017804154304 -0.649 -0.649
probs:  [0.08720313792514978, 0.43526234341870135, 0.3860544359990538, 0.09148008265709506]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9761017804154304 -0.649 -0.649
probs:  [0.08734215887336409, 0.435957202696702, 0.3850747046940637, 0.09162593373587015]
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.126]
 [0.078]
 [0.097]
 [0.125]
 [0.125]
 [0.125]] [[3.403]
 [3.871]
 [2.945]
 [3.033]
 [4.588]
 [4.588]
 [4.588]] [[ 0.096]
 [ 0.301]
 [-0.055]
 [-0.008]
 [ 0.538]
 [ 0.538]
 [ 0.538]]
maxi score, test score, baseline:  -0.9761017804154304 -0.649 -0.649
probs:  [0.08722628717623555, 0.43537644963878436, 0.3858740332481192, 0.09152322993686095]
maxi score, test score, baseline:  -0.9761017804154304 -0.649 -0.649
UNIT TEST: sample policy line 217 mcts : [0.245 0.061 0.143 0.184 0.082 0.204 0.082]
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.494]
 [0.393]
 [0.461]
 [0.444]
 [0.383]
 [0.383]] [[6.263]
 [5.264]
 [6.039]
 [6.236]
 [6.158]
 [5.599]
 [5.599]] [[0.728]
 [0.532]
 [0.685]
 [0.774]
 [0.744]
 [0.562]
 [0.562]]
maxi score, test score, baseline:  -0.9761017804154304 -0.649 -0.649
maxi score, test score, baseline:  -0.9761017804154304 -0.649 -0.649
probs:  [0.08736500742281716, 0.43606980339195317, 0.3848963935980278, 0.0916687955872019]
actor:  1 policy actor:  1  step number:  56 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9761017804154304 -0.649 -0.649
probs:  [0.08820515479138055, 0.4402806945699985, 0.3791011879892611, 0.09241296264935979]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.79573496541786
maxi score, test score, baseline:  -0.9761017804154304 -0.649 -0.649
probs:  [0.08820515479138055, 0.4402806945699985, 0.3791011879892611, 0.09241296264935979]
maxi score, test score, baseline:  -0.9761017804154304 -0.649 -0.649
probs:  [0.08834155112806837, 0.4409624509702055, 0.3781401210804375, 0.09255587682128871]
maxi score, test score, baseline:  -0.9761017804154304 -0.649 -0.649
probs:  [0.08847740905608655, 0.4416415162163436, 0.3771828478714181, 0.09269822685615174]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2652578499758076
siam score:  -0.81950814
Printing some Q and Qe and total Qs values:  [[0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]] [[5.143]
 [5.143]
 [5.143]
 [5.143]
 [5.143]
 [5.143]
 [5.143]] [[0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.11333333333333295  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.147]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]] [[2.26 ]
 [2.843]
 [2.26 ]
 [2.26 ]
 [2.26 ]
 [2.26 ]
 [2.26 ]] [[0.113]
 [0.335]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]]
maxi score, test score, baseline:  -0.9761721893491125 -0.649 -0.649
probs:  [0.08924569403034055, 0.44549046509710155, 0.3718643211074128, 0.09339951976514503]
maxi score, test score, baseline:  -0.9761721893491125 -0.649 -0.649
probs:  [0.08924569403034055, 0.44549046509710155, 0.3718643211074128, 0.09339951976514503]
maxi score, test score, baseline:  -0.9762421828908555 -0.649 -0.649
probs:  [0.08924569403034055, 0.44549046509710155, 0.3718643211074128, 0.09339951976514503]
siam score:  -0.8163389
maxi score, test score, baseline:  -0.9762421828908555 -0.649 -0.649
probs:  [0.08924569403034055, 0.44549046509710155, 0.3718643211074128, 0.09339951976514503]
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.661]
 [0.772]
 [0.382]
 [0.363]
 [0.349]
 [0.385]] [[1.643]
 [1.335]
 [1.414]
 [1.943]
 [1.225]
 [1.93 ]
 [1.975]] [[0.068]
 [0.451]
 [0.588]
 [0.375]
 [0.118]
 [0.337]
 [0.388]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9762421828908555 -0.649 -0.649
maxi score, test score, baseline:  -0.9762421828908555 -0.649 -0.649
probs:  [0.08995489741933513, 0.4490451627601186, 0.3669735234440135, 0.0940264163765328]
maxi score, test score, baseline:  -0.9762421828908555 -0.649 -0.649
probs:  [0.08995489741933513, 0.4490451627601186, 0.3669735234440135, 0.0940264163765328]
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.437]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]] [[1.844]
 [2.808]
 [2.854]
 [2.854]
 [2.854]
 [2.854]
 [2.854]] [[0.201]
 [0.804]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]]
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.23 ]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]] [[1.574]
 [2.805]
 [1.574]
 [1.574]
 [1.574]
 [1.574]
 [1.574]] [[0.118]
 [0.709]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]]
maxi score, test score, baseline:  -0.9762421828908555 -0.649 -0.649
probs:  [0.08993019260907463, 0.44892020647013214, 0.36805053996349224, 0.09309906095730085]
maxi score, test score, baseline:  -0.9762421828908555 -0.649 -0.649
probs:  [0.08993019260907463, 0.44892020647013214, 0.36805053996349224, 0.09309906095730085]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.153]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]] [[2.594]
 [3.005]
 [2.594]
 [2.594]
 [2.594]
 [2.594]
 [2.594]] [[0.5  ]
 [0.716]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]]
siam score:  -0.82354325
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.264]
 [0.195]
 [0.173]
 [0.144]
 [0.18 ]
 [0.161]] [[1.485]
 [1.945]
 [1.938]
 [1.14 ]
 [0.927]
 [1.809]
 [1.495]] [[ 0.207]
 [ 0.574]
 [ 0.5  ]
 [ 0.081]
 [-0.054]
 [ 0.421]
 [ 0.246]]
using explorer policy with actor:  1
Starting evaluation
maxi score, test score, baseline:  -0.9762421828908555 -0.649 -0.649
probs:  [0.0901942097419222, 0.4502399065601182, 0.366193486907626, 0.09337239679033357]
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.485]
 [0.334]
 [0.292]
 [0.349]
 [0.286]
 [0.333]] [[5.092]
 [3.424]
 [3.506]
 [3.51 ]
 [3.598]
 [3.467]
 [3.505]] [[0.617]
 [0.298]
 [0.189]
 [0.152]
 [0.229]
 [0.135]
 [0.188]]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.448]
 [0.447]
 [0.441]
 [0.422]
 [0.442]
 [0.439]] [[1.218]
 [1.54 ]
 [1.887]
 [1.399]
 [0.945]
 [1.939]
 [1.38 ]] [[0.423]
 [0.448]
 [0.447]
 [0.441]
 [0.422]
 [0.442]
 [0.439]]
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]] [[1.801]
 [1.801]
 [1.801]
 [1.801]
 [1.801]
 [1.801]
 [1.801]] [[0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]]
maxi score, test score, baseline:  -0.9764497076023393 -0.649 -0.649
probs:  [0.0901942097419222, 0.4502399065601182, 0.366193486907626, 0.09337239679033357]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]] [[2.653]
 [2.653]
 [2.653]
 [2.653]
 [2.653]
 [2.653]
 [2.653]] [[0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]]
maxi score, test score, baseline:  -0.9764497076023393 -0.649 -0.649
probs:  [0.09008767806020916, 0.4497059296796834, 0.3669308099945295, 0.09327558226557797]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9764497076023393 -0.649 -0.649
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9694641760006548
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.591]
 [0.646]
 [0.589]
 [0.646]
 [0.646]
 [0.646]] [[1.581]
 [2.219]
 [1.361]
 [1.234]
 [1.361]
 [1.361]
 [1.361]] [[0.629]
 [0.591]
 [0.646]
 [0.589]
 [0.646]
 [0.646]
 [0.646]]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.714]
 [0.586]
 [0.566]
 [0.575]
 [0.569]
 [0.573]] [[4.175]
 [3.993]
 [3.989]
 [3.848]
 [3.695]
 [4.015]
 [4.011]] [[0.571]
 [0.714]
 [0.586]
 [0.566]
 [0.575]
 [0.569]
 [0.573]]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  27 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  27 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9622188405797102 -0.649 -0.649
probs:  [0.09008767806020916, 0.4497059296796834, 0.3669308099945295, 0.09327558226557797]
actor:  0 policy actor:  1  step number:  28 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  30 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  31 total reward:  0.56  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  34 total reward:  0.54  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9445227316141355 -0.649 -0.649
probs:  [0.09008767806020916, 0.4497059296796834, 0.3669308099945295, 0.09327558226557797]
actor:  1 policy actor:  1  step number:  54 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.5
from probs:  [0.09008767806020916, 0.4497059296796834, 0.3669308099945295, 0.09327558226557797]
actor:  0 policy actor:  1  step number:  41 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  42 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9370212121212121 -0.649 -0.649
actor:  0 policy actor:  1  step number:  47 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  0.667
from probs:  [0.0879309868423266, 0.4389256585992878, 0.38210090887013803, 0.09104244568824761]
actor:  0 policy actor:  1  step number:  50 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9302098591549296 -0.649 -0.649
probs:  [0.08782059562200109, 0.43837242514225105, 0.3828656682299873, 0.0909413110057606]
maxi score, test score, baseline:  -0.9304056179775281 -0.649 -0.649
probs:  [0.08782059562200109, 0.43837242514225105, 0.3828656682299873, 0.0909413110057606]
maxi score, test score, baseline:  -0.9304056179775281 -0.649 -0.649
probs:  [0.08782059562200109, 0.43837242514225105, 0.3828656682299873, 0.0909413110057606]
actor:  1 policy actor:  1  step number:  68 total reward:  0.019999999999998908  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.313]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]] [[1.943]
 [2.225]
 [1.943]
 [1.943]
 [1.943]
 [1.943]
 [1.943]] [[1.001]
 [1.12 ]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]]
maxi score, test score, baseline:  -0.9304056179775281 -0.649 -0.649
maxi score, test score, baseline:  -0.9304056179775281 -0.649 -0.649
probs:  [0.09905714189286619, 0.4346954243296562, 0.3791603527009129, 0.08708708107656467]
maxi score, test score, baseline:  -0.9306002801120448 -0.649 -0.649
probs:  [0.09905714189286619, 0.4346954243296562, 0.3791603527009129, 0.08708708107656467]
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.642]
 [0.642]
 [0.641]
 [0.642]
 [0.642]
 [0.642]] [[0.969]
 [0.969]
 [0.969]
 [1.165]
 [0.969]
 [0.969]
 [0.969]] [[0.642]
 [0.642]
 [0.642]
 [0.641]
 [0.642]
 [0.642]
 [0.642]]
line 256 mcts: sample exp_bonus 4.512887217860377
actor:  0 policy actor:  1  step number:  55 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.424]
 [0.454]
 [0.482]
 [0.467]
 [0.464]
 [0.469]] [[1.128]
 [0.655]
 [1.036]
 [1.044]
 [0.889]
 [1.069]
 [0.944]] [[0.476]
 [0.424]
 [0.454]
 [0.482]
 [0.467]
 [0.464]
 [0.469]]
using explorer policy with actor:  0
from probs:  [0.09905714189286619, 0.4346954243296562, 0.3791603527009129, 0.08708708107656467]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9279555555555556 -0.649 -0.649
maxi score, test score, baseline:  -0.9281548476454294 -0.16400000000000003 -0.16400000000000003
probs:  [0.11371413775324458, 0.4178802686703145, 0.38469864640269463, 0.08370694717374638]
maxi score, test score, baseline:  -0.9281548476454294 -0.16400000000000003 -0.16400000000000003
probs:  [0.11371413775324458, 0.4178802686703145, 0.38469864640269463, 0.08370694717374638]
maxi score, test score, baseline:  -0.9281548476454294 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9281548476454294 -0.16400000000000003 -0.16400000000000003
probs:  [0.11371413775324458, 0.4178802686703145, 0.38469864640269463, 0.08370694717374638]
maxi score, test score, baseline:  -0.9287461538461539 -0.16400000000000003 -0.16400000000000003
probs:  [0.11371413775324458, 0.4178802686703145, 0.38469864640269463, 0.08370694717374638]
Printing some Q and Qe and total Qs values:  [[-0.107]
 [-0.109]
 [-0.119]
 [-0.114]
 [-0.119]
 [-0.12 ]
 [-0.114]] [[2.237]
 [2.87 ]
 [1.866]
 [2.024]
 [1.86 ]
 [1.976]
 [2.257]] [[-0.561]
 [-0.457]
 [-0.635]
 [-0.604]
 [-0.636]
 [-0.618]
 [-0.565]]
Printing some Q and Qe and total Qs values:  [[-0.099]
 [-0.099]
 [-0.102]
 [-0.103]
 [-0.102]
 [-0.102]
 [-0.103]] [[1.998]
 [2.245]
 [1.891]
 [1.805]
 [1.854]
 [1.918]
 [1.869]] [[-0.642]
 [-0.6  ]
 [-0.663]
 [-0.678]
 [-0.669]
 [-0.658]
 [-0.668]]
line 256 mcts: sample exp_bonus 1.872730859367974
Printing some Q and Qe and total Qs values:  [[-0.12]
 [-0.12]
 [-0.12]
 [-0.12]
 [-0.12]
 [-0.12]
 [-0.12]] [[2.85]
 [2.85]
 [2.85]
 [2.85]
 [2.85]
 [2.85]
 [2.85]] [[0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]]
siam score:  -0.8071512
maxi score, test score, baseline:  -0.9287461538461539 -0.16400000000000003 -0.16400000000000003
probs:  [0.11384324618026909, 0.41792050614646703, 0.3845210512594273, 0.08371519641383648]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9287461538461539 -0.16400000000000003 -0.16400000000000003
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9287461538461539 -0.16400000000000003 -0.16400000000000003
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.648]
 [0.271]
 [0.291]
 [0.451]
 [0.168]
 [0.302]] [[3.73 ]
 [4.045]
 [4.108]
 [4.989]
 [4.474]
 [4.737]
 [3.874]] [[0.358]
 [0.64 ]
 [0.425]
 [0.711]
 [0.651]
 [0.557]
 [0.371]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9287461538461539 -0.16400000000000003 -0.16400000000000003
probs:  [0.1138139184587404, 0.4173782746099218, 0.38520087051586627, 0.08360693641547162]
maxi score, test score, baseline:  -0.9287461538461539 -0.16400000000000003 -0.16400000000000003
probs:  [0.1138139184587404, 0.4173782746099218, 0.38520087051586627, 0.08360693641547162]
maxi score, test score, baseline:  -0.9287461538461539 -0.16400000000000003 -0.16400000000000003
probs:  [0.1130504148159505, 0.41773792494573286, 0.3855327839690226, 0.08367887626929403]
maxi score, test score, baseline:  -0.9287461538461539 -0.16400000000000003 -0.16400000000000003
probs:  [0.1130504148159505, 0.41773792494573286, 0.3855327839690226, 0.08367887626929403]
maxi score, test score, baseline:  -0.9287461538461539 -0.16400000000000003 -0.16400000000000003
actor:  1 policy actor:  1  step number:  70 total reward:  0.1533333333333321  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9287461538461539 -0.16400000000000003 -0.16400000000000003
probs:  [0.11251920544198886, 0.4157731271050838, 0.3884218042170512, 0.08328586323587624]
maxi score, test score, baseline:  -0.9287461538461539 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9287461538461539 -0.16400000000000003 -0.16400000000000003
probs:  [0.11251920544198886, 0.4157731271050838, 0.3884218042170512, 0.08328586323587624]
maxi score, test score, baseline:  -0.9289410958904111 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9289410958904111 -0.16400000000000003 -0.16400000000000003
probs:  [0.11251920544198886, 0.4157731271050838, 0.3884218042170512, 0.08328586323587624]
maxi score, test score, baseline:  -0.9289410958904111 -0.16400000000000003 -0.16400000000000003
probs:  [0.11251920544198886, 0.4157731271050838, 0.3884218042170512, 0.08328586323587624]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.5465],
        [-0.5425],
        [ 0.2111],
        [ 0.1836],
        [-0.5296],
        [-0.0789],
        [-0.4093],
        [-0.5412],
        [-0.5530],
        [-0.5406]], dtype=torch.float64)
-0.070771701198 0.47575913607082865
-0.032346567066 -0.5748539823910649
-0.032346567066 0.17880047728874487
-0.09703970119800001 0.08656044990430808
-0.084359833866 -0.613912676347244
-0.070771701198 -0.14967755179354975
-0.083839701198 -0.49309722637366143
-0.032346567066 -0.573570488267675
-0.032346567066 -0.5853751722980776
-0.032346567066 -0.5729276955554319
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9289410958904111 -0.16400000000000003 -0.16400000000000003
probs:  [0.10962140983589758, 0.40297830918707295, 0.40605815135695195, 0.08134212962007736]
actor:  1 policy actor:  1  step number:  66 total reward:  0.1666666666666654  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  84 total reward:  0.006666666666665377  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9291349726775957 -0.16400000000000003 -0.16400000000000003
probs:  [0.11202161986609267, 0.4041158495450407, 0.40290909591979934, 0.08095343466906736]
maxi score, test score, baseline:  -0.9291349726775957 -0.16400000000000003 -0.16400000000000003
probs:  [0.11218205887827203, 0.4046951907957902, 0.4020534325824845, 0.08106931774345343]
maxi score, test score, baseline:  -0.9291349726775957 -0.16400000000000003 -0.16400000000000003
probs:  [0.11218205887827203, 0.4046951907957902, 0.4020534325824845, 0.08106931774345343]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9291349726775957 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9291349726775957 -0.16400000000000003 -0.16400000000000003
probs:  [0.11234187002142387, 0.4052722648272222, 0.4012011178353917, 0.08118474731596219]
actor:  1 policy actor:  1  step number:  67 total reward:  0.026666666666666394  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  57 total reward:  0.3599999999999993  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9291349726775957 -0.16400000000000003 -0.16400000000000003
probs:  [0.11240218152233061, 0.39485819088204993, 0.4105094937904737, 0.08223013380514585]
maxi score, test score, baseline:  -0.9291349726775957 -0.16400000000000003 -0.16400000000000003
probs:  [0.11240218152233061, 0.39485819088204993, 0.4105094937904737, 0.08223013380514585]
maxi score, test score, baseline:  -0.9291349726775957 -0.16400000000000003 -0.16400000000000003
probs:  [0.11240218152233061, 0.39485819088204993, 0.4105094937904737, 0.08223013380514585]
maxi score, test score, baseline:  -0.9291349726775957 -0.16400000000000003 -0.16400000000000003
probs:  [0.11240218152233061, 0.39485819088204993, 0.4105094937904737, 0.08223013380514585]
maxi score, test score, baseline:  -0.9291349726775957 -0.16400000000000003 -0.16400000000000003
probs:  [0.11240218152233061, 0.39485819088204993, 0.4105094937904737, 0.08223013380514585]
maxi score, test score, baseline:  -0.9293277929155314 -0.16400000000000003 -0.16400000000000003
probs:  [0.11240218152233061, 0.39485819088204993, 0.4105094937904737, 0.08223013380514585]
maxi score, test score, baseline:  -0.9293277929155314 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9293277929155314 -0.16400000000000003 -0.16400000000000003
probs:  [0.11236609390416964, 0.3956288240207852, 0.40989721007577434, 0.08210787199927086]
maxi score, test score, baseline:  -0.9295195652173914 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9295195652173914 -0.16400000000000003 -0.16400000000000003
probs:  [0.11236609390416964, 0.3956288240207852, 0.40989721007577434, 0.08210787199927086]
maxi score, test score, baseline:  -0.9295195652173914 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9295195652173914 -0.16400000000000003 -0.16400000000000003
probs:  [0.11236609390416964, 0.3956288240207852, 0.40989721007577434, 0.08210787199927086]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11233004220708181, 0.3963986900828676, 0.4092855358190893, 0.08198573189096131]
siam score:  -0.7947452
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11246633459217772, 0.39566589572970184, 0.40978261198750665, 0.0820851576906138]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11246633459217772, 0.39566589572970184, 0.40978261198750665, 0.0820851576906138]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11246633459217772, 0.39566589572970184, 0.40978261198750665, 0.0820851576906138]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11246633459217772, 0.39566589572970184, 0.40978261198750665, 0.0820851576906138]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.577]
 [0.459]
 [0.444]
 [0.442]
 [0.428]
 [0.442]] [[5.445]
 [3.932]
 [5.436]
 [5.124]
 [4.688]
 [4.313]
 [4.919]] [[0.609]
 [0.248]
 [0.615]
 [0.516]
 [0.389]
 [0.272]
 [0.455]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11246633459217772, 0.39566589572970184, 0.40978261198750665, 0.0820851576906138]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11243066163588136, 0.3964338534852763, 0.4091722123078587, 0.08196327257098356]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.616]
 [0.435]
 [0.407]
 [0.483]
 [0.38 ]
 [0.385]] [[2.886]
 [2.932]
 [2.859]
 [3.122]
 [2.676]
 [3.092]
 [2.514]] [[0.328]
 [0.476]
 [0.27 ]
 [0.33 ]
 [0.257]
 [0.293]
 [0.106]]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.112566825941299, 0.3957024464830819, 0.4096682388921313, 0.08206248868348781]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.112566825941299, 0.3957024464830819, 0.4096682388921313, 0.08206248868348781]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.112566825941299, 0.3957024464830819, 0.4096682388921313, 0.08206248868348781]
actor:  1 policy actor:  1  step number:  82 total reward:  0.16666666666666474  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus -2.059264722576497
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.068]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]] [[1.806]
 [2.308]
 [1.779]
 [1.779]
 [1.779]
 [1.779]
 [1.779]] [[-0.359]
 [-0.186]
 [-0.366]
 [-0.366]
 [-0.366]
 [-0.366]
 [-0.366]]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11281442451578133, 0.3925246967929148, 0.412110543177594, 0.08255033551370984]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11281442451578133, 0.3925246967929148, 0.412110543177594, 0.08255033551370984]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11291443458755435, 0.39255501369383716, 0.4120017685757839, 0.08252878314282455]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11291443458755435, 0.39255501369383716, 0.4120017685757839, 0.08252878314282455]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11291443458755435, 0.39255501369383716, 0.4120017685757839, 0.08252878314282455]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11291443458755435, 0.39255501369383716, 0.4120017685757839, 0.08252878314282455]
using another actor
Printing some Q and Qe and total Qs values:  [[-0.088]
 [-0.073]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]] [[1.736]
 [1.727]
 [1.71 ]
 [1.71 ]
 [1.71 ]
 [1.71 ]
 [1.71 ]] [[-0.405]
 [-0.394]
 [-0.412]
 [-0.412]
 [-0.412]
 [-0.412]
 [-0.412]]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11246328510003731, 0.391431803079514, 0.4133137127598544, 0.08279119906059436]
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11242781133358064, 0.39218334249722353, 0.4127168304028711, 0.08267201576632467]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9297102981029811 -0.16400000000000003 -0.16400000000000003
probs:  [0.11242781133358064, 0.39218334249722353, 0.4127168304028711, 0.08267201576632467]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3859597204270309
actor:  0 policy actor:  1  step number:  67 total reward:  0.11999999999999911  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.926872972972973 -0.16400000000000003 -0.16400000000000003
probs:  [0.11242781133358064, 0.39218334249722353, 0.4127168304028711, 0.08267201576632467]
maxi score, test score, baseline:  -0.926872972972973 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.926872972972973 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.926872972972973 -0.16400000000000003 -0.16400000000000003
probs:  [0.11233529528598785, 0.3915874762769782, 0.4132907897065263, 0.08278643873050774]
Printing some Q and Qe and total Qs values:  [[-0.081]
 [-0.073]
 [-0.089]
 [-0.077]
 [-0.086]
 [-0.085]
 [-0.083]] [[0.9  ]
 [1.336]
 [0.991]
 [0.962]
 [1.055]
 [1.089]
 [1.143]] [[-0.464]
 [-0.311]
 [-0.441]
 [-0.439]
 [-0.418]
 [-0.405]
 [-0.385]]
maxi score, test score, baseline:  -0.926872972972973 -0.16400000000000003 -0.16400000000000003
probs:  [0.11229961076859106, 0.39233630030420735, 0.4126963468874223, 0.0826677420397792]
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]] [[2.179]
 [2.179]
 [2.179]
 [2.179]
 [2.179]
 [2.179]
 [2.179]] [[-0.265]
 [-0.265]
 [-0.265]
 [-0.265]
 [-0.265]
 [-0.265]
 [-0.265]]
maxi score, test score, baseline:  -0.926872972972973 -0.16400000000000003 -0.16400000000000003
probs:  [0.11229961076859106, 0.39233630030420735, 0.4126963468874223, 0.0826677420397792]
using another actor
siam score:  -0.7993679
first move QE:  1.1802605931566739
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.032]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[1.955]
 [3.181]
 [1.955]
 [1.955]
 [1.955]
 [1.955]
 [1.955]] [[0.291]
 [0.669]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.034]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]] [[1.893]
 [2.218]
 [1.893]
 [1.893]
 [1.893]
 [1.893]
 [1.893]] [[0.019]
 [0.178]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]]
siam score:  -0.796024
first move QE:  1.1786411065194113
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9270698113207547 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9270698113207547 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9270698113207547 -0.16400000000000003 -0.16400000000000003
probs:  [0.11059377275717583, 0.4032122390304685, 0.40505422595644863, 0.08113976225590706]
maxi score, test score, baseline:  -0.9270698113207547 -0.16400000000000003 -0.16400000000000003
probs:  [0.11037566896994262, 0.40331315819411373, 0.405152190778411, 0.08115898205753258]
actor:  1 policy actor:  1  step number:  50 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9270698113207547 -0.16400000000000003 -0.16400000000000003
probs:  [0.11061637079330089, 0.398744730671473, 0.4087595668400679, 0.08187933169515818]
line 256 mcts: sample exp_bonus 0.10206277707226918
maxi score, test score, baseline:  -0.9270698113207547 -0.16400000000000003 -0.16400000000000003
probs:  [0.11061637079330089, 0.398744730671473, 0.4087595668400679, 0.08187933169515818]
maxi score, test score, baseline:  -0.9270698113207547 -0.16400000000000003 -0.16400000000000003
probs:  [0.11061637079330089, 0.398744730671473, 0.4087595668400679, 0.08187933169515818]
maxi score, test score, baseline:  -0.9270698113207547 -0.16400000000000003 -0.16400000000000003
probs:  [0.11061637079330089, 0.398744730671473, 0.4087595668400679, 0.08187933169515818]
maxi score, test score, baseline:  -0.9270698113207547 -0.16400000000000003 -0.16400000000000003
probs:  [0.11061637079330089, 0.398744730671473, 0.4087595668400679, 0.08187933169515818]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.9270698113207547 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9270698113207547 -0.16400000000000003 -0.16400000000000003
probs:  [0.11057709663335025, 0.3994901390391536, 0.40817096862144603, 0.0817617957060502]
maxi score, test score, baseline:  -0.9270698113207547 -0.16400000000000003 -0.16400000000000003
probs:  [0.11057709663335025, 0.3994901390391536, 0.40817096862144603, 0.0817617957060502]
Printing some Q and Qe and total Qs values:  [[-0.084]
 [-0.07 ]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]] [[2.47 ]
 [2.091]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]] [[ 0.093]
 [-0.02 ]
 [-0.353]
 [-0.353]
 [-0.353]
 [-0.353]
 [-0.353]]
using another actor
maxi score, test score, baseline:  -0.9272655913978495 -0.16400000000000003 -0.16400000000000003
probs:  [0.11104671851488673, 0.3992878530576802, 0.4079484941514046, 0.08171693427602841]
actor:  1 policy actor:  1  step number:  53 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9272655913978495 -0.16400000000000003 -0.16400000000000003
probs:  [0.11131716203008689, 0.39398655575216474, 0.41214195565855277, 0.08255432655919549]
using another actor
maxi score, test score, baseline:  -0.9274603217158177 -0.16400000000000003 -0.16400000000000003
probs:  [0.11137216547682183, 0.3947395004387896, 0.4114683618470019, 0.0824199722373865]
maxi score, test score, baseline:  -0.9274603217158177 -0.16400000000000003 -0.16400000000000003
probs:  [0.11137216547682183, 0.3947395004387896, 0.4114683618470019, 0.0824199722373865]
maxi score, test score, baseline:  -0.9274603217158177 -0.16400000000000003 -0.16400000000000003
probs:  [0.11137216547682183, 0.3947395004387896, 0.4114683618470019, 0.0824199722373865]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.365]
 [0.207]
 [0.108]
 [0.207]
 [0.168]
 [0.207]] [[3.711]
 [4.032]
 [3.716]
 [2.91 ]
 [3.716]
 [3.033]
 [3.716]] [[0.54 ]
 [0.591]
 [0.463]
 [0.26 ]
 [0.463]
 [0.309]
 [0.463]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9274603217158177 -0.16400000000000003 -0.16400000000000003
probs:  [0.11133538825681341, 0.39546534424209645, 0.4108939909609368, 0.08230527654015332]
maxi score, test score, baseline:  -0.9274603217158177 -0.16400000000000003 -0.16400000000000003
probs:  [0.11133538825681341, 0.39546534424209645, 0.4108939909609368, 0.08230527654015332]
line 256 mcts: sample exp_bonus 0.39587631312570903
actor:  0 policy actor:  1  step number:  65 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9243741532976827 -0.16400000000000003 -0.16400000000000003
probs:  [0.11129864719796732, 0.39619047436025795, 0.4103201848244326, 0.08219069361734208]
maxi score, test score, baseline:  -0.9243741532976827 -0.16400000000000003 -0.16400000000000003
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9243741532976827 -0.16400000000000003 -0.16400000000000003
probs:  [0.11071287065587068, 0.396513874595615, 0.41053867218036455, 0.08223458256814975]
actor:  1 policy actor:  1  step number:  53 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9243741532976827 -0.16400000000000003 -0.16400000000000003
probs:  [0.1099113803075554, 0.40139749372545475, 0.40713679043497686, 0.08155433553201284]
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.065]
 [-0.088]
 [-0.076]
 [-0.084]
 [-0.076]
 [-0.082]] [[0.997]
 [0.814]
 [0.436]
 [0.864]
 [0.839]
 [0.864]
 [0.928]] [[-0.488]
 [-0.51 ]
 [-0.596]
 [-0.513]
 [-0.525]
 [-0.513]
 [-0.508]]
first move QE:  1.1600295477898732
maxi score, test score, baseline:  -0.9243741532976827 -0.16400000000000003 -0.16400000000000003
probs:  [0.1092584271242891, 0.4011208366101584, 0.4079115951312657, 0.08170914113428672]
using explorer policy with actor:  1
start point for exploration sampling:  11091
Printing some Q and Qe and total Qs values:  [[-0.126]
 [-0.11 ]
 [-0.121]
 [-0.115]
 [-0.125]
 [-0.115]
 [-0.115]] [[ 0.115]
 [ 0.537]
 [ 0.2  ]
 [ 0.151]
 [-0.012]
 [ 0.151]
 [ 0.151]] [[-0.642]
 [-0.556]
 [-0.623]
 [-0.625]
 [-0.663]
 [-0.625]
 [-0.625]]
Printing some Q and Qe and total Qs values:  [[-0.125]
 [-0.105]
 [-0.122]
 [-0.123]
 [-0.116]
 [-0.119]
 [-0.124]] [[ 0.141]
 [ 0.63 ]
 [ 0.249]
 [-0.033]
 [-0.001]
 [ 0.369]
 [ 0.376]] [[-0.634]
 [-0.532]
 [-0.612]
 [-0.66 ]
 [-0.648]
 [-0.59 ]
 [-0.593]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9245755555555556 -0.16400000000000003 -0.16400000000000003
probs:  [0.10963895306378475, 0.3990337591902156, 0.40933370132856983, 0.08199358641742972]
maxi score, test score, baseline:  -0.9245755555555556 -0.16400000000000003 -0.16400000000000003
probs:  [0.1089806474694509, 0.3993288306927645, 0.4096363921403725, 0.08205412969741208]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]] [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
maxi score, test score, baseline:  -0.9245755555555556 -0.16400000000000003 -0.16400000000000003
probs:  [0.10893818279232469, 0.4000542344672679, 0.4090671288545596, 0.08194045388584772]
from probs:  [0.10893818279232469, 0.4000542344672679, 0.4090671288545596, 0.08194045388584772]
maxi score, test score, baseline:  -0.9247758865248227 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9247758865248227 -0.16400000000000003 -0.16400000000000003
probs:  [0.10889575959290243, 0.4007789296979887, 0.40849842160109184, 0.0818268891080171]
maxi score, test score, baseline:  -0.9247758865248227 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9249751547303271 -0.16400000000000003 -0.16400000000000003
probs:  [0.10889575959290243, 0.4007789296979887, 0.40849842160109184, 0.0818268891080171]
maxi score, test score, baseline:  -0.9249751547303271 -0.16400000000000003 -0.16400000000000003
probs:  [0.10889575959290243, 0.4007789296979887, 0.40849842160109184, 0.0818268891080171]
actor:  0 policy actor:  1  step number:  45 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9215754850088184 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9215754850088184 -0.16400000000000003 -0.16400000000000003
probs:  [0.10889575959290243, 0.4007789296979887, 0.40849842160109184, 0.0818268891080171]
maxi score, test score, baseline:  -0.9217821459982409 -0.16400000000000003 -0.16400000000000003
probs:  [0.1088533778104432, 0.40150291742253325, 0.40793026956570355, 0.08171343520132002]
maxi score, test score, baseline:  -0.9217821459982409 -0.16400000000000003 -0.16400000000000003
probs:  [0.1088533778104432, 0.40150291742253325, 0.40793026956570355, 0.08171343520132002]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.9217821459982409 -0.16400000000000003 -0.16400000000000003
probs:  [0.1088533778104432, 0.40150291742253325, 0.40793026956570355, 0.08171343520132002]
Printing some Q and Qe and total Qs values:  [[-0.164]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]] [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]]
maxi score, test score, baseline:  -0.9217821459982409 -0.16400000000000003 -0.16400000000000003
probs:  [0.10881103738432478, 0.4022261986764834, 0.4073626719357181, 0.08160009200347364]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9217821459982409 -0.16400000000000003 -0.16400000000000003
probs:  [0.10920838980710457, 0.4014212975183794, 0.4077028816656048, 0.08166743100891112]
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.574]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]] [[3.616]
 [3.881]
 [3.616]
 [3.616]
 [3.616]
 [3.616]
 [3.616]] [[0.33 ]
 [0.612]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]]
maxi score, test score, baseline:  -0.9217821459982409 -0.16400000000000003 -0.16400000000000003
probs:  [0.10920838980710457, 0.4014212975183794, 0.4077028816656048, 0.08166743100891112]
maxi score, test score, baseline:  -0.9217821459982409 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9217821459982409 -0.16400000000000003 -0.16400000000000003
probs:  [0.10920838980710457, 0.4014212975183794, 0.4077028816656048, 0.08166743100891112]
maxi score, test score, baseline:  -0.9217821459982409 -0.16400000000000003 -0.16400000000000003
actor:  1 policy actor:  1  step number:  48 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -1.1360143927960815
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[4.547]
 [4.547]
 [4.547]
 [4.547]
 [4.547]
 [4.547]
 [4.547]] [[0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]]
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.65 ]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]] [[4.505]
 [3.652]
 [4.505]
 [4.505]
 [4.505]
 [4.505]
 [4.505]] [[0.204]
 [0.179]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]]
maxi score, test score, baseline:  -0.9219877192982456 -0.16400000000000003 -0.16400000000000003
probs:  [0.10857733142359086, 0.40582444725924927, 0.4043067233719486, 0.08129149794521141]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9219877192982456 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9219877192982456 -0.16400000000000003 -0.16400000000000003
probs:  [0.1086918013331526, 0.4133756736660124, 0.3951326622526643, 0.08279986274817072]
maxi score, test score, baseline:  -0.9219877192982456 -0.16400000000000003 -0.16400000000000003
probs:  [0.1086918013331526, 0.4133756736660124, 0.3951326622526643, 0.08279986274817072]
maxi score, test score, baseline:  -0.9219877192982456 -0.16400000000000003 -0.16400000000000003
probs:  [0.10882623636622266, 0.4138874762386587, 0.3943840581873926, 0.082902229207726]
maxi score, test score, baseline:  -0.9221922134733158 -0.16400000000000003 -0.16400000000000003
probs:  [0.10896017953463347, 0.41439740625180743, 0.3936381930799286, 0.08300422113363037]
maxi score, test score, baseline:  -0.9221922134733158 -0.16400000000000003 -0.16400000000000003
Printing some Q and Qe and total Qs values:  [[-0.034]
 [-0.036]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]] [[2.018]
 [2.083]
 [2.018]
 [2.018]
 [2.018]
 [2.018]
 [2.018]] [[-0.035]
 [-0.005]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]]
siam score:  -0.7664372
maxi score, test score, baseline:  -0.9221922134733158 -0.16400000000000003 -0.16400000000000003
probs:  [0.10932257551519421, 0.41542944615261396, 0.39203718073975835, 0.08321079759243345]
Printing some Q and Qe and total Qs values:  [[-0.066]
 [-0.071]
 [-0.091]
 [-0.091]
 [-0.093]
 [-0.091]
 [-0.095]] [[1.378]
 [1.603]
 [0.4  ]
 [0.4  ]
 [1.015]
 [0.4  ]
 [1.156]] [[ 0.048]
 [ 0.173]
 [-0.536]
 [-0.536]
 [-0.184]
 [-0.536]
 [-0.104]]
maxi score, test score, baseline:  -0.9221922134733158 -0.16400000000000003 -0.16400000000000003
probs:  [0.10928609680241186, 0.4149432352966434, 0.39265696176745685, 0.08311370613348797]
maxi score, test score, baseline:  -0.9221922134733158 -0.16400000000000003 -0.16400000000000003
probs:  [0.10928609680241186, 0.4149432352966434, 0.39265696176745685, 0.08311370613348797]
from probs:  [0.1104296436403531, 0.4155008473439923, 0.3908449396322478, 0.08322456938340674]
maxi score, test score, baseline:  -0.9221922134733158 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9221922134733158 -0.16400000000000003 -0.16400000000000003
probs:  [0.11069327805767169, 0.41649378411282684, 0.38938977040188477, 0.08342316742761677]
actor:  1 policy actor:  1  step number:  56 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9221922134733158 -0.16400000000000003 -0.16400000000000003
probs:  [0.11005533505097202, 0.4140910740485813, 0.3929109913544312, 0.08294259954601539]
maxi score, test score, baseline:  -0.9221922134733158 -0.16400000000000003 -0.16400000000000003
probs:  [0.11005533505097202, 0.4140910740485813, 0.3929109913544312, 0.08294259954601539]
maxi score, test score, baseline:  -0.9223956369982548 -0.16400000000000003 -0.16400000000000003
probs:  [0.11002085109247739, 0.4136058903753123, 0.3935275474870806, 0.0828457110451298]
maxi score, test score, baseline:  -0.9223956369982548 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9223956369982548 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9223956369982548 -0.16400000000000003 -0.16400000000000003
probs:  [0.10995197194335977, 0.4126367719782546, 0.3947590726267112, 0.08265218345167448]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9223956369982548 -0.16400000000000003 -0.16400000000000003
probs:  [0.11008385400098974, 0.4131322054442555, 0.394032664523327, 0.08275127603142768]
maxi score, test score, baseline:  -0.9223956369982548 -0.16400000000000003 -0.16400000000000003
probs:  [0.11008385400098974, 0.4131322054442555, 0.394032664523327, 0.08275127603142768]
maxi score, test score, baseline:  -0.9223956369982548 -0.16400000000000003 -0.16400000000000003
probs:  [0.11008385400098974, 0.4131322054442555, 0.394032664523327, 0.08275127603142768]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.435]
 [0.371]
 [0.378]
 [0.37 ]
 [0.379]
 [0.366]] [[-0.348]
 [-0.037]
 [-0.118]
 [-0.207]
 [-0.306]
 [-0.035]
 [-0.277]] [[0.401]
 [0.435]
 [0.371]
 [0.378]
 [0.37 ]
 [0.379]
 [0.366]]
maxi score, test score, baseline:  -0.9223956369982548 -0.16400000000000003 -0.16400000000000003
probs:  [0.11004971838374412, 0.4126488180300172, 0.39464671699371634, 0.08265474659252234]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]] [[1.796]
 [1.796]
 [1.796]
 [1.796]
 [1.796]
 [1.796]
 [1.796]] [[0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]]
maxi score, test score, baseline:  -0.922597998259356 -0.16400000000000003 -0.16400000000000003
probs:  [0.11004971838374412, 0.4126488180300172, 0.39464671699371634, 0.08265474659252234]
maxi score, test score, baseline:  -0.922597998259356 -0.16400000000000003 -0.16400000000000003
probs:  [0.11001561184505833, 0.4121658423912489, 0.3952602463810936, 0.0825582993825992]
maxi score, test score, baseline:  -0.922597998259356 -0.16400000000000003 -0.16400000000000003
probs:  [0.11001561184505833, 0.4121658423912489, 0.3952602463810936, 0.0825582993825992]
maxi score, test score, baseline:  -0.922597998259356 -0.16400000000000003 -0.16400000000000003
probs:  [0.11001561184505833, 0.4121658423912489, 0.3952602463810936, 0.0825582993825992]
maxi score, test score, baseline:  -0.922597998259356 -0.16400000000000003 -0.16400000000000003
probs:  [0.11014754407386415, 0.4126606117266338, 0.39453458470196995, 0.08265725949753204]
actor:  0 policy actor:  0  step number:  70 total reward:  0.07333333333333214  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9200041666666666 -0.16400000000000003 -0.16400000000000003
using explorer policy with actor:  1
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9200041666666666 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9200041666666666 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9200041666666666 -0.16400000000000003 -0.16400000000000003
probs:  [0.11037671011447331, 0.41316398098933294, 0.39370121475037106, 0.08275809414582254]
Printing some Q and Qe and total Qs values:  [[0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]] [[4.754]
 [4.754]
 [4.754]
 [4.754]
 [4.754]
 [4.754]
 [4.754]] [[0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9202116883116883 -0.16400000000000003 -0.16400000000000003
actor:  1 policy actor:  1  step number:  46 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9202116883116883 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9202116883116883 -0.16400000000000003 -0.16400000000000003
probs:  [0.11084283476387463, 0.4185289646774237, 0.38679861082425226, 0.08382958973444944]
maxi score, test score, baseline:  -0.9202116883116883 -0.16400000000000003 -0.16400000000000003
probs:  [0.11084283476387463, 0.4185289646774237, 0.38679861082425226, 0.08382958973444944]
actor:  0 policy actor:  1  step number:  43 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9168948186528498 -0.16400000000000003 -0.16400000000000003
probs:  [0.11081053062434772, 0.41805933463131606, 0.38739432578964406, 0.08373580895469225]
maxi score, test score, baseline:  -0.9168948186528498 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9168948186528498 -0.16400000000000003 -0.16400000000000003
probs:  [0.11025091755074429, 0.41789850769081766, 0.38814663306128616, 0.08370394169715192]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9168948186528498 -0.16400000000000003 -0.16400000000000003
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.45 ]
 [0.406]
 [0.292]
 [0.174]
 [0.176]
 [0.276]] [[5.001]
 [4.188]
 [4.171]
 [3.887]
 [4.886]
 [4.732]
 [4.157]] [[0.655]
 [0.546]
 [0.51 ]
 [0.358]
 [0.514]
 [0.479]
 [0.412]]
maxi score, test score, baseline:  -0.9168948186528498 -0.16400000000000003 -0.16400000000000003
probs:  [0.11031202182463169, 0.4174480959299294, 0.3886257273338705, 0.0836141549115684]
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]] [[3.515]
 [3.515]
 [3.515]
 [3.515]
 [3.515]
 [3.515]
 [3.515]] [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.11031202182463169, 0.4174480959299294, 0.3886257273338705, 0.0836141549115684]
actor:  1 policy actor:  1  step number:  56 total reward:  0.17999999999999927  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.10968723435443933, 0.41508137770277637, 0.3920906028092449, 0.08314078513353931]
actor:  1 policy actor:  1  step number:  46 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.333
siam score:  -0.77028704
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.10999151503196242, 0.4192402637803855, 0.3867969379377876, 0.08397128324986451]
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.11011677032711283, 0.4197181636249953, 0.38609819906323345, 0.08406686698465851]
actor:  1 policy actor:  1  step number:  43 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  49 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.10877966980240383, 0.416541379253776, 0.39124830814287703, 0.08343064280094303]
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.309]
 [0.274]
 [0.222]
 [0.19 ]
 [0.219]
 [0.24 ]] [[2.905]
 [3.621]
 [3.279]
 [3.425]
 [2.703]
 [3.841]
 [3.546]] [[0.405]
 [0.658]
 [0.573]
 [0.582]
 [0.418]
 [0.666]
 [0.614]]
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.10874377129744933, 0.41608133504479416, 0.39183612194789097, 0.08333877170986562]
Printing some Q and Qe and total Qs values:  [[-0.121]
 [-0.087]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]] [[-0.403]
 [-0.077]
 [-0.403]
 [-0.403]
 [-0.403]
 [-0.403]
 [-0.403]] [[-0.319]
 [-0.189]
 [-0.319]
 [-0.319]
 [-0.319]
 [-0.319]
 [-0.319]]
Printing some Q and Qe and total Qs values:  [[-0.106]
 [-0.082]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]] [[ 0.358]
 [ 0.492]
 [-0.156]
 [-0.156]
 [-0.156]
 [-0.156]
 [-0.156]] [[-0.122]
 [-0.058]
 [-0.288]
 [-0.288]
 [-0.288]
 [-0.288]
 [-0.288]]
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.10874377129744933, 0.41608133504479416, 0.39183612194789097, 0.08333877170986562]
actor:  1 policy actor:  1  step number:  40 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.10721747718251727, 0.4102353905419513, 0.40037758568133425, 0.08216954659419716]
siam score:  -0.7747178
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.51 ]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]] [[3.882]
 [3.384]
 [3.882]
 [3.882]
 [3.882]
 [3.882]
 [3.882]] [[0.73 ]
 [0.566]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]]
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.10663331800030094, 0.4105038500235398, 0.40063959182256215, 0.08222324015359718]
Printing some Q and Qe and total Qs values:  [[0.129]
 [0.411]
 [0.127]
 [0.263]
 [0.106]
 [0.322]
 [0.322]] [[3.926]
 [4.119]
 [3.983]
 [4.987]
 [4.26 ]
 [4.01 ]
 [4.01 ]] [[0.2  ]
 [0.501]
 [0.215]
 [0.624]
 [0.277]
 [0.392]
 [0.392]]
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.10663331800030094, 0.4105038500235398, 0.40063959182256215, 0.08222324015359718]
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.10659334841847841, 0.4100382584843891, 0.4012381355416398, 0.08213025755549272]
using another actor
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.10671774114888884, 0.41051726528469623, 0.40053893152149195, 0.08222606204492294]
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.10671774114888884, 0.41051726528469623, 0.40053893152149195, 0.08222606204492294]
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.10671774114888884, 0.41051726528469623, 0.40053893152149195, 0.08222606204492294]
maxi score, test score, baseline:  -0.9171093023255815 -0.16400000000000003 -0.16400000000000003
probs:  [0.10671774114888884, 0.41051726528469623, 0.40053893152149195, 0.08222606204492294]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.606]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[1.217]
 [1.258]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.   ]] [[0.586]
 [0.606]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]]
actor:  0 policy actor:  1  step number:  66 total reward:  0.1266666666666657  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9146386461011141 -0.16400000000000003 -0.16400000000000003
probs:  [0.10671774114888884, 0.41051726528469623, 0.40053893152149195, 0.08222606204492294]
maxi score, test score, baseline:  -0.9146386461011141 -0.16400000000000003 -0.16400000000000003
siam score:  -0.7743059
maxi score, test score, baseline:  -0.9146386461011141 -0.16400000000000003 -0.16400000000000003
probs:  [0.10671774114888884, 0.41051726528469623, 0.40053893152149195, 0.08222606204492294]
actor:  0 policy actor:  1  step number:  60 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9117290598290598 -0.16400000000000003 -0.16400000000000003
probs:  [0.10684170754982117, 0.4109946303916464, 0.399842123873576, 0.08232153818495654]
first move QE:  1.0313214038499612
Printing some Q and Qe and total Qs values:  [[-0.076]
 [-0.064]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]] [[-0.003]
 [ 0.448]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[-0.153]
 [ 0.031]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.153]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9121789115646259 -0.16400000000000003 -0.16400000000000003
probs:  [0.106802219665498, 0.41053044706637587, 0.4004384958304435, 0.08222883743768275]
maxi score, test score, baseline:  -0.9121789115646259 -0.16400000000000003 -0.16400000000000003
probs:  [0.106802219665498, 0.41053044706637587, 0.4004384958304435, 0.08222883743768275]
maxi score, test score, baseline:  -0.9121789115646259 -0.16400000000000003 -0.16400000000000003
probs:  [0.106802219665498, 0.41053044706637587, 0.4004384958304435, 0.08222883743768275]
line 256 mcts: sample exp_bonus 7.264212649839548
maxi score, test score, baseline:  -0.9121789115646259 -0.16400000000000003 -0.16400000000000003
probs:  [0.1069259845196129, 0.41100667385865003, 0.3997432555531474, 0.0823240860685896]
maxi score, test score, baseline:  -0.9121789115646259 -0.16400000000000003 -0.16400000000000003
probs:  [0.1069259845196129, 0.41100667385865003, 0.3997432555531474, 0.0823240860685896]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -4.111816667329986
maxi score, test score, baseline:  -0.9121789115646259 -0.16400000000000003 -0.16400000000000003
probs:  [0.10642604267433502, 0.4112874560628325, 0.3999061174267522, 0.0823803838360803]
maxi score, test score, baseline:  -0.9121789115646259 -0.16400000000000003 -0.16400000000000003
probs:  [0.10654859433575262, 0.41176155676721665, 0.39921464150148606, 0.08247520739554452]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9126241962774958 -0.16400000000000003 -0.16400000000000003
probs:  [0.10691376319725221, 0.41317424111970524, 0.3971542412305888, 0.08275775445245381]
actor:  1 policy actor:  1  step number:  49 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  43 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.167
from probs:  [0.1318739902405101, 0.39802337260982007, 0.3903750305623875, 0.07972760658728237]
maxi score, test score, baseline:  -0.9128451476793249 -0.16400000000000003 -0.16400000000000003
probs:  [0.1320198552431159, 0.3984639588228367, 0.3897004587776499, 0.07981572715639741]
actor:  1 policy actor:  1  step number:  56 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9128451476793249 -0.16400000000000003 -0.16400000000000003
probs:  [0.13142833994400283, 0.3966772829175687, 0.3924359986528357, 0.07945837848559271]
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.302]
 [0.29 ]
 [0.291]
 [0.285]
 [0.286]
 [0.291]] [[-0.316]
 [ 0.177]
 [-0.131]
 [-0.092]
 [-0.301]
 [-0.169]
 [-0.144]] [[0.288]
 [0.302]
 [0.29 ]
 [0.291]
 [0.285]
 [0.286]
 [0.291]]
maxi score, test score, baseline:  -0.9128451476793249 -0.16400000000000003 -0.16400000000000003
probs:  [0.1318628117198498, 0.3979896078136258, 0.39042672709341164, 0.07972085337311274]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
siam score:  -0.7684378
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.521]
 [0.445]
 [0.407]
 [0.405]
 [0.404]
 [0.412]] [[ 0.305]
 [ 0.881]
 [-0.114]
 [ 0.18 ]
 [-0.023]
 [ 0.039]
 [ 0.322]] [[0.429]
 [0.521]
 [0.445]
 [0.407]
 [0.405]
 [0.404]
 [0.412]]
maxi score, test score, baseline:  -0.9128451476793249 -0.16400000000000003 -0.16400000000000003
probs:  [0.1318628117198498, 0.3979896078136258, 0.39042672709341164, 0.07972085337311274]
UNIT TEST: sample policy line 217 mcts : [0.184 0.245 0.061 0.041 0.041 0.143 0.286]
maxi score, test score, baseline:  -0.9128451476793249 -0.16400000000000003 -0.16400000000000003
probs:  [0.132219080850989, 0.397821024658857, 0.39027305936727436, 0.07968683512287959]
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.891]
 [0.772]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]] [[3.661]
 [2.814]
 [3.621]
 [4.036]
 [4.036]
 [4.036]
 [4.036]] [[0.782]
 [0.891]
 [0.772]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]]
maxi score, test score, baseline:  -0.9128451476793249 -0.16400000000000003 -0.16400000000000003
line 256 mcts: sample exp_bonus 1.2705438482978206
first move QE:  0.9998754072991762
maxi score, test score, baseline:  -0.9128451476793249 -0.16400000000000003 -0.16400000000000003
probs:  [0.132219080850989, 0.397821024658857, 0.39027305936727436, 0.07968683512287959]
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.006]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]] [[1.778]
 [2.086]
 [1.778]
 [1.778]
 [1.778]
 [1.778]
 [1.778]] [[0.121]
 [0.207]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]]
line 256 mcts: sample exp_bonus 2.1373605769700807
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.1323783311739357, 0.39778142202711814, 0.39016119847875247, 0.07967904832019364]
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.1325221249839262, 0.39821383246955283, 0.3894985090539287, 0.07976553349259231]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.1325221249839262, 0.39821383246955283, 0.3894985090539287, 0.07976553349259231]
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.1325221249839262, 0.39821383246955283, 0.3894985090539287, 0.07976553349259231]
line 256 mcts: sample exp_bonus 3.7324017650195693
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.1325221249839262, 0.39821383246955283, 0.3894985090539287, 0.07976553349259231]
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.1325221249839262, 0.39821383246955283, 0.3894985090539287, 0.07976553349259231]
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.1325221249839262, 0.39821383246955283, 0.3894985090539287, 0.07976553349259231]
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.132537717021346, 0.3977416245060093, 0.39004943577870044, 0.07967122269394421]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.15064856079210912, 0.3904087848753142, 0.38073805433627833, 0.07820459999629828]
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.15064856079210912, 0.3904087848753142, 0.38073805433627833, 0.07820459999629828]
Printing some Q and Qe and total Qs values:  [[-0.132]
 [-0.097]
 [-0.103]
 [-0.101]
 [-0.125]
 [-0.107]
 [-0.107]] [[0.464]
 [0.223]
 [0.434]
 [0.006]
 [0.14 ]
 [0.342]
 [0.342]] [[-0.251]
 [-0.296]
 [-0.231]
 [-0.372]
 [-0.352]
 [-0.266]
 [-0.266]]
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.15064856079210912, 0.3904087848753142, 0.38073805433627833, 0.07820459999629828]
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.15064856079210912, 0.3904087848753142, 0.38073805433627833, 0.07820459999629828]
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.15064856079210912, 0.3904087848753142, 0.38073805433627833, 0.07820459999629828]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.15070076618053474, 0.3899308254152996, 0.38125927216451755, 0.07810913623964806]
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.15070076618053474, 0.3899308254152996, 0.38125927216451755, 0.07810913623964806]
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.1508050066873696, 0.3889764653796606, 0.38230000784766294, 0.07791852008530695]
maxi score, test score, baseline:  -0.9130649831649832 -0.16400000000000003 -0.16400000000000003
probs:  [0.1508050066873696, 0.3889764653796606, 0.38230000784766294, 0.07791852008530695]
maxi score, test score, baseline:  -0.9132837111670865 -0.16400000000000003 -0.16400000000000003
probs:  [0.1508050066873696, 0.3889764653796606, 0.38230000784766294, 0.07791852008530695]
maxi score, test score, baseline:  -0.9132837111670865 -0.16400000000000003 -0.16400000000000003
probs:  [0.1509646550630679, 0.3893885027376609, 0.38164591114646784, 0.07800093105280331]
maxi score, test score, baseline:  -0.9132837111670865 -0.16400000000000003 -0.16400000000000003
probs:  [0.1509646550630679, 0.3893885027376609, 0.38164591114646784, 0.07800093105280331]
maxi score, test score, baseline:  -0.9132837111670865 -0.16400000000000003 -0.16400000000000003
probs:  [0.151277310017425, 0.3892397102575711, 0.38151210374048283, 0.0779708759845211]
maxi score, test score, baseline:  -0.9132837111670865 -0.16400000000000003 -0.16400000000000003
start point for exploration sampling:  11091
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.499]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[1.176]
 [2.165]
 [1.176]
 [1.176]
 [1.176]
 [1.176]
 [1.176]] [[0.431]
 [0.499]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]]
siam score:  -0.7668773
Printing some Q and Qe and total Qs values:  [[-0.079]
 [-0.077]
 [-0.084]
 [-0.075]
 [-0.07 ]
 [-0.06 ]
 [-0.077]] [[0.079]
 [0.34 ]
 [0.536]
 [0.254]
 [0.266]
 [0.259]
 [0.425]] [[-0.557]
 [-0.512]
 [-0.486]
 [-0.524]
 [-0.518]
 [-0.508]
 [-0.497]]
maxi score, test score, baseline:  -0.9135013400335009 -0.16400000000000003 -0.16400000000000003
probs:  [0.15133025249133825, 0.38876425813379784, 0.38202957680086397, 0.07787591257399995]
maxi score, test score, baseline:  -0.9135013400335009 -0.16400000000000003 -0.16400000000000003
probs:  [0.15133025249133825, 0.38876425813379784, 0.38202957680086397, 0.07787591257399995]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[4.144]
 [4.144]
 [4.144]
 [4.144]
 [4.144]
 [4.144]
 [4.144]] [[0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]]
maxi score, test score, baseline:  -0.9135013400335009 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9135013400335009 -0.16400000000000003 -0.16400000000000003
probs:  [0.15056478637134538, 0.3891149585548145, 0.38237419958838464, 0.07794605548545548]
maxi score, test score, baseline:  -0.9137178780284044 -0.16400000000000003 -0.16400000000000003
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.054]
 [-0.069]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.053]
 [-0.054]] [[-0.002]
 [ 0.182]
 [-0.027]
 [-0.441]
 [-0.22 ]
 [-0.161]
 [ 0.037]] [[-0.457]
 [-0.411]
 [-0.465]
 [-0.603]
 [-0.529]
 [-0.509]
 [-0.444]]
Printing some Q and Qe and total Qs values:  [[-0.08]
 [-0.08]
 [-0.08]
 [-0.08]
 [-0.08]
 [-0.08]
 [-0.08]] [[-0.392]
 [-0.392]
 [-0.392]
 [-0.392]
 [-0.392]
 [-0.392]
 [-0.392]] [[-0.289]
 [-0.289]
 [-0.289]
 [-0.289]
 [-0.289]
 [-0.289]
 [-0.289]]
maxi score, test score, baseline:  -0.9137178780284044 -0.16400000000000003 -0.16400000000000003
probs:  [0.1510901762834428, 0.3880421704522465, 0.3831356404970742, 0.07773201276723662]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9137178780284044 -0.16400000000000003 -0.16400000000000003
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.454]
 [0.349]
 [0.241]
 [0.196]
 [0.299]
 [0.3  ]] [[3.364]
 [3.335]
 [3.281]
 [3.338]
 [2.94 ]
 [3.425]
 [3.565]] [[ 0.159]
 [ 0.234]
 [ 0.118]
 [ 0.034]
 [-0.133]
 [ 0.116]
 [ 0.161]]
maxi score, test score, baseline:  -0.9137178780284044 -0.16400000000000003 -0.16400000000000003
probs:  [0.15064161011197158, 0.38921081913417505, 0.3821818172515373, 0.07796575350231603]
maxi score, test score, baseline:  -0.9137178780284044 -0.16400000000000003 -0.16400000000000003
probs:  [0.15085072686390016, 0.389147446511397, 0.38204861653640565, 0.07795321008829716]
maxi score, test score, baseline:  -0.9137178780284044 -0.16400000000000003 -0.16400000000000003
probs:  [0.15085072686390016, 0.389147446511397, 0.38204861653640565, 0.07795321008829716]
maxi score, test score, baseline:  -0.9137178780284044 -0.16400000000000003 -0.16400000000000003
probs:  [0.15085072686390016, 0.389147446511397, 0.38204861653640565, 0.07795321008829716]
actor:  1 policy actor:  1  step number:  60 total reward:  0.19333333333333247  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9137178780284044 -0.16400000000000003 -0.16400000000000003
probs:  [0.15059717340022943, 0.3914630302977818, 0.3795240955378279, 0.0784157007641609]
using another actor
maxi score, test score, baseline:  -0.9141477140482128 -0.16400000000000003 -0.16400000000000003
probs:  [0.15096067768434637, 0.39084582851926875, 0.37990140332474803, 0.07829209047163686]
maxi score, test score, baseline:  -0.9143610281923715 -0.16400000000000003 -0.16400000000000003
probs:  [0.14976924946717432, 0.39234637871294853, 0.37929215979995057, 0.0785922120199266]
maxi score, test score, baseline:  -0.9143610281923715 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9143610281923715 -0.16400000000000003 -0.16400000000000003
probs:  [0.14997258581909087, 0.39228621909043704, 0.3791608863238765, 0.07858030876659558]
maxi score, test score, baseline:  -0.9143610281923715 -0.16400000000000003 -0.16400000000000003
siam score:  -0.78037024
actor:  1 policy actor:  1  step number:  54 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.914784488448845 -0.16400000000000003 -0.16400000000000003
probs:  [0.14930624916397964, 0.3910656034400682, 0.3812915843045848, 0.07833656309136736]
Printing some Q and Qe and total Qs values:  [[-0.1  ]
 [-0.061]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]] [[-1.046]
 [ 0.45 ]
 [-1.046]
 [-1.046]
 [-1.046]
 [-1.046]
 [-1.046]] [[-0.371]
 [ 0.049]
 [-0.371]
 [-0.371]
 [-0.371]
 [-0.371]
 [-0.371]]
maxi score, test score, baseline:  -0.9149946502057613 -0.16400000000000003 -0.16400000000000003
using another actor
maxi score, test score, baseline:  -0.9152037766830871 -0.16400000000000003 -0.16400000000000003
probs:  [0.149129673385005, 0.39073325117568675, 0.38186702393115024, 0.078270051508158]
maxi score, test score, baseline:  -0.9152037766830871 -0.16400000000000003 -0.16400000000000003
probs:  [0.149129673385005, 0.39073325117568675, 0.38186702393115024, 0.078270051508158]
maxi score, test score, baseline:  -0.9152037766830871 -0.16400000000000003 -0.16400000000000003
probs:  [0.14928167814798302, 0.3911317652212383, 0.38123679876975725, 0.07834975786102129]
actor:  0 policy actor:  1  step number:  57 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9124307125307126 -0.16400000000000003 -0.16400000000000003
probs:  [0.14928167814798302, 0.3911317652212383, 0.38123679876975725, 0.07834975786102129]
Printing some Q and Qe and total Qs values:  [[-0.141]
 [-0.048]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]] [[ 0.092]
 [ 0.145]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]] [[0.015]
 [0.116]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]]
Printing some Q and Qe and total Qs values:  [[ 0.054]
 [ 0.237]
 [-0.033]
 [-0.023]
 [-0.003]
 [-0.003]
 [-0.003]] [[1.925]
 [2.302]
 [1.378]
 [1.379]
 [2.349]
 [2.349]
 [1.871]] [[ 0.008]
 [ 0.282]
 [-0.23 ]
 [-0.222]
 [ 0.083]
 [ 0.083]
 [-0.058]]
maxi score, test score, baseline:  -0.9124307125307126 -0.16400000000000003 -0.16400000000000003
probs:  [0.14854667150473108, 0.39146974578849386, 0.38156622572683857, 0.07841735697993645]
maxi score, test score, baseline:  -0.9124307125307126 -0.16400000000000003 -0.16400000000000003
probs:  [0.14854667150473108, 0.39146974578849386, 0.38156622572683857, 0.07841735697993645]
siam score:  -0.7790811
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.548]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[0.481]
 [0.67 ]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.509]
 [0.548]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]]
maxi score, test score, baseline:  -0.9126450980392158 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9126450980392158 -0.16400000000000003 -0.16400000000000003
probs:  [0.14854667150473108, 0.39146974578849386, 0.38156622572683857, 0.07841735697993645]
maxi score, test score, baseline:  -0.9128584352078241 -0.16400000000000003 -0.16400000000000003
probs:  [0.14854667150473108, 0.39146974578849386, 0.38156622572683857, 0.07841735697993645]
maxi score, test score, baseline:  -0.9128584352078241 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9128584352078241 -0.16400000000000003 -0.16400000000000003
probs:  [0.1485928306963969, 0.3910114407709945, 0.3820699080225178, 0.07832582051009081]
maxi score, test score, baseline:  -0.9128584352078241 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9128584352078241 -0.16400000000000003 -0.16400000000000003
probs:  [0.1485928306963969, 0.3910114407709945, 0.3820699080225178, 0.07832582051009081]
maxi score, test score, baseline:  -0.9130707317073171 -0.16400000000000003 -0.16400000000000003
probs:  [0.1485928306963969, 0.3910114407709945, 0.3820699080225178, 0.07832582051009081]
maxi score, test score, baseline:  -0.9130707317073171 -0.16400000000000003 -0.16400000000000003
probs:  [0.14863894278079479, 0.39055360347176443, 0.3825730762906332, 0.07823437745680754]
actor:  1 policy actor:  1  step number:  54 total reward:  0.19333333333333258  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9130707317073171 -0.16400000000000003 -0.16400000000000003
probs:  [0.14875492215944466, 0.39317348865631063, 0.37931372448727196, 0.07875786469697268]
line 256 mcts: sample exp_bonus 1.505373169951982
maxi score, test score, baseline:  -0.9130707317073171 -0.16400000000000003 -0.16400000000000003
probs:  [0.14875492215944466, 0.39317348865631063, 0.37931372448727196, 0.07875786469697268]
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.041]
 [-0.05 ]
 [-0.053]
 [-0.054]
 [-0.054]
 [-0.054]] [[ 0.408]
 [ 0.565]
 [-0.057]
 [-0.377]
 [-0.299]
 [-0.326]
 [-0.345]] [[ 0.044]
 [ 0.111]
 [-0.168]
 [-0.311]
 [-0.277]
 [-0.289]
 [-0.298]]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.9130707317073171 -0.16400000000000003 -0.16400000000000003
probs:  [0.14875492215944466, 0.39317348865631063, 0.37931372448727196, 0.07875786469697268]
maxi score, test score, baseline:  -0.9130707317073171 -0.16400000000000003 -0.16400000000000003
probs:  [0.14875492215944466, 0.39317348865631063, 0.37931372448727196, 0.07875786469697268]
maxi score, test score, baseline:  -0.9130707317073171 -0.16400000000000003 -0.16400000000000003
probs:  [0.14890417254251376, 0.3935682190818225, 0.3786907943406001, 0.07883681403506378]
first move QE:  0.8857212960670224
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.817]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]] [[2.561]
 [1.931]
 [2.561]
 [2.561]
 [2.561]
 [2.561]
 [2.561]] [[0.811]
 [0.817]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]]
maxi score, test score, baseline:  -0.91328199513382 -0.16400000000000003 -0.16400000000000003
probs:  [0.14890417254251376, 0.3935682190818225, 0.3786907943406001, 0.07883681403506378]
siam score:  -0.79613006
actor:  0 policy actor:  1  step number:  56 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.5
siam score:  -0.79489136
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.05]
 [-0.05]
 [-0.05]
 [-0.05]
 [-0.05]
 [-0.05]
 [-0.05]] [[0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]]
Printing some Q and Qe and total Qs values:  [[-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]] [[1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]] [[-0.39]
 [-0.39]
 [-0.39]
 [-0.39]
 [-0.39]
 [-0.39]
 [-0.39]]
maxi score, test score, baseline:  -0.9106281553398059 -0.16400000000000003 -0.16400000000000003
probs:  [0.1482222406043836, 0.3934532300181509, 0.3795105869231811, 0.07881394245428423]
actor:  0 policy actor:  0  step number:  56 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.5
from probs:  [0.1482222406043836, 0.3934532300181509, 0.3795105869231811, 0.07881394245428423]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.545]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]] [[0.684]
 [1.122]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]] [[0.473]
 [0.545]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.608]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]] [[1.233]
 [1.395]
 [1.153]
 [1.153]
 [1.153]
 [1.153]
 [1.153]] [[0.479]
 [0.608]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]]
maxi score, test score, baseline:  -0.9075351896690881 -0.16400000000000003 -0.16400000000000003
probs:  [0.1482222406043836, 0.3934532300181509, 0.3795105869231811, 0.07881394245428423]
Printing some Q and Qe and total Qs values:  [[-0.131]
 [-0.089]
 [-0.128]
 [-0.119]
 [-0.089]
 [-0.121]
 [-0.106]] [[ 0.66 ]
 [ 1.235]
 [ 0.653]
 [ 0.645]
 [ 1.235]
 [-0.208]
 [ 0.426]] [[-0.193]
 [ 0.04 ]
 [-0.192]
 [-0.186]
 [ 0.04 ]
 [-0.472]
 [-0.246]]
first move QE:  0.8771832524131442
maxi score, test score, baseline:  -0.9075351896690881 -0.16400000000000003 -0.16400000000000003
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.8245057910981766
actor:  1 policy actor:  1  step number:  76 total reward:  0.11333333333333173  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9077582930756845 -0.16400000000000003 -0.16400000000000003
probs:  [0.14757779715130165, 0.39490263134868087, 0.37841594039145104, 0.07910363110856658]
maxi score, test score, baseline:  -0.9077582930756845 -0.16400000000000003 -0.16400000000000003
from probs:  [0.14757779715130165, 0.39490263134868087, 0.37841594039145104, 0.07910363110856658]
maxi score, test score, baseline:  -0.9077582930756845 -0.16400000000000003 -0.16400000000000003
probs:  [0.1476216083196725, 0.3944556140717415, 0.37890842726475754, 0.0790143503438285]
maxi score, test score, baseline:  -0.9077582930756845 -0.16400000000000003 -0.16400000000000003
probs:  [0.1476216083196725, 0.3944556140717415, 0.37890842726475754, 0.0790143503438285]
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]] [[4.511]
 [4.511]
 [4.511]
 [4.511]
 [4.511]
 [4.511]
 [4.511]] [[0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]]
maxi score, test score, baseline:  -0.9077582930756845 -0.16400000000000003 -0.16400000000000003
probs:  [0.1476216083196725, 0.3944556140717415, 0.37890842726475754, 0.0790143503438285]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9077582930756845 -0.16400000000000003 -0.16400000000000003
probs:  [0.1476216083196725, 0.3944556140717415, 0.37890842726475754, 0.0790143503438285]
actor:  0 policy actor:  1  step number:  52 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9049763052208836 -0.16400000000000003 -0.16400000000000003
probs:  [0.1476216083196725, 0.3944556140717415, 0.37890842726475754, 0.0790143503438285]
siam score:  -0.79324746
maxi score, test score, baseline:  -0.9052044871794872 -0.16400000000000003 -0.16400000000000003
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.604]
 [0.355]
 [0.462]
 [0.488]
 [0.474]
 [0.455]] [[0.76 ]
 [0.232]
 [0.883]
 [1.213]
 [0.922]
 [1.044]
 [0.684]] [[-0.095]
 [ 0.225]
 [ 0.085]
 [ 0.247]
 [ 0.225]
 [ 0.231]
 [ 0.152]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9052044871794872 -0.16400000000000003 -0.16400000000000003
probs:  [0.1664691284432031, 0.38573224904484804, 0.3705290181692055, 0.07726960434274328]
maxi score, test score, baseline:  -0.9054315747402079 -0.16400000000000003 -0.16400000000000003
probs:  [0.1664691284432031, 0.38573224904484804, 0.3705290181692055, 0.07726960434274328]
Printing some Q and Qe and total Qs values:  [[-0.11 ]
 [-0.116]
 [-0.121]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]] [[-0.426]
 [-0.317]
 [-0.07 ]
 [-0.426]
 [-0.426]
 [-0.426]
 [-0.426]] [[0.52 ]
 [0.532]
 [0.569]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]]
maxi score, test score, baseline:  -0.9054315747402079 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9054315747402079 -0.16400000000000003 -0.16400000000000003
probs:  [0.16662607605080965, 0.38482630252522343, 0.37145896708066356, 0.07708865434330328]
maxi score, test score, baseline:  -0.9054315747402079 -0.16400000000000003 -0.16400000000000003
probs:  [0.16662607605080965, 0.38482630252522343, 0.37145896708066356, 0.07708865434330328]
maxi score, test score, baseline:  -0.9054315747402079 -0.16400000000000003 -0.16400000000000003
probs:  [0.16662607605080965, 0.38482630252522343, 0.37145896708066356, 0.07708865434330328]
actor:  0 policy actor:  0  step number:  52 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9025475279106858 -0.16400000000000003 -0.16400000000000003
probs:  [0.16591960430625274, 0.38473611814336345, 0.37227353724303797, 0.0770707403073459]
maxi score, test score, baseline:  -0.9025475279106858 -0.16400000000000003 -0.16400000000000003
probs:  [0.16591960430625274, 0.38473611814336345, 0.37227353724303797, 0.0770707403073459]
line 256 mcts: sample exp_bonus 1.694634931331249
actor:  1 policy actor:  1  step number:  57 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.9027798727128084 -0.16400000000000003 -0.16400000000000003
probs:  [0.17716435143704168, 0.37958547260780523, 0.3672094906617598, 0.07604068529339343]
siam score:  -0.8055231
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.586]
 [0.583]
 [0.582]
 [0.583]
 [0.582]
 [0.589]] [[-1.201]
 [-0.15 ]
 [-1.316]
 [-1.304]
 [-0.958]
 [-1.04 ]
 [-1.105]] [[0.593]
 [0.586]
 [0.583]
 [0.582]
 [0.583]
 [0.582]
 [0.589]]
maxi score, test score, baseline:  -0.9030111111111111 -0.16400000000000003 -0.16400000000000003
probs:  [0.1774325429224507, 0.37949859645450584, 0.36704542911421467, 0.07602343150882875]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.9032412509897071 -0.16400000000000003 -0.16400000000000003
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.900473926415296
maxi score, test score, baseline:  -0.9032412509897071 -0.16400000000000003 -0.16400000000000003
probs:  [0.175204294273406, 0.37324047373720515, 0.3761931862386277, 0.07536204575076128]
maxi score, test score, baseline:  -0.9032412509897071 -0.16400000000000003 -0.16400000000000003
probs:  [0.175204294273406, 0.37324047373720515, 0.3761931862386277, 0.07536204575076128]
maxi score, test score, baseline:  -0.9032412509897071 -0.16400000000000003 -0.16400000000000003
probs:  [0.175204294273406, 0.37324047373720515, 0.3761931862386277, 0.07536204575076128]
first move QE:  0.835827163806737
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9032412509897071 -0.16400000000000003 -0.16400000000000003
probs:  [0.175204294273406, 0.37324047373720515, 0.3761931862386277, 0.07536204575076128]
maxi score, test score, baseline:  -0.9032412509897071 -0.16400000000000003 -0.16400000000000003
probs:  [0.17536268294350288, 0.3726737905540595, 0.3765334290419104, 0.07543009746052722]
actor:  1 policy actor:  1  step number:  85 total reward:  0.06666666666666565  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9034703001579779 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9034703001579779 -0.16400000000000003 -0.16400000000000003
probs:  [0.1740959174041769, 0.3755930108941189, 0.3750686186665212, 0.0752424530351831]
maxi score, test score, baseline:  -0.9034703001579779 -0.16400000000000003 -0.16400000000000003
probs:  [0.17452350449270274, 0.37583915158107967, 0.37434594671360916, 0.07529139721260832]
maxi score, test score, baseline:  -0.9034703001579779 -0.16400000000000003 -0.16400000000000003
probs:  [0.17452350449270274, 0.37583915158107967, 0.37434594671360916, 0.07529139721260832]
maxi score, test score, baseline:  -0.9034703001579779 -0.16400000000000003 -0.16400000000000003
probs:  [0.17452350449270274, 0.37583915158107967, 0.37434594671360916, 0.07529139721260832]
maxi score, test score, baseline:  -0.9034703001579779 -0.16400000000000003 -0.16400000000000003
probs:  [0.17452350449270274, 0.37583915158107967, 0.37434594671360916, 0.07529139721260832]
maxi score, test score, baseline:  -0.9036982663514579 -0.16400000000000003 -0.16400000000000003
probs:  [0.17452350449270274, 0.37583915158107967, 0.37434594671360916, 0.07529139721260832]
from probs:  [0.17452350449270274, 0.37583915158107967, 0.37434594671360916, 0.07529139721260832]
maxi score, test score, baseline:  -0.9036982663514579 -0.16400000000000003 -0.16400000000000003
probs:  [0.1746919276329538, 0.3762020205235947, 0.37374207732766174, 0.0753639745157896]
maxi score, test score, baseline:  -0.9036982663514579 -0.16400000000000003 -0.16400000000000003
probs:  [0.1746919276329538, 0.3762020205235947, 0.37374207732766174, 0.0753639745157896]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3533333333333327  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9039251572327044 -0.16400000000000003 -0.16400000000000003
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9039251572327044 -0.16400000000000003 -0.16400000000000003
probs:  [0.17365777119512982, 0.3813814109323733, 0.36856228963508086, 0.07639852823741598]
maxi score, test score, baseline:  -0.9039251572327044 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.9039251572327044 -0.16400000000000003 -0.16400000000000003
actor:  0 policy actor:  1  step number:  63 total reward:  0.1999999999999993  reward:  1.0 rdn_beta:  0.167
siam score:  -0.80169207
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.274]
 [0.22 ]
 [0.22 ]
 [0.227]
 [0.22 ]
 [0.251]] [[7.438]
 [5.282]
 [7.26 ]
 [7.26 ]
 [7.632]
 [7.26 ]
 [8.106]] [[0.609]
 [0.157]
 [0.575]
 [0.575]
 [0.661]
 [0.575]
 [0.777]]
maxi score, test score, baseline:  -0.9013274509803922 -0.16400000000000003 -0.16400000000000003
probs:  [0.17399890995777395, 0.3808546904395726, 0.3688529864444051, 0.07629341315824842]
maxi score, test score, baseline:  -0.9013274509803922 -0.16400000000000003 -0.16400000000000003
probs:  [0.17425166597219632, 0.38077115006561485, 0.36870036313727816, 0.07627682082491066]
maxi score, test score, baseline:  -0.9013274509803922 -0.16400000000000003 -0.16400000000000003
probs:  [0.17425166597219632, 0.38077115006561485, 0.36870036313727816, 0.07627682082491066]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.832]
 [0.758]
 [0.614]
 [0.758]
 [0.758]
 [0.65 ]] [[1.727]
 [2.246]
 [1.939]
 [0.918]
 [1.939]
 [1.939]
 [0.893]] [[0.671]
 [0.832]
 [0.758]
 [0.614]
 [0.758]
 [0.758]
 [0.65 ]]
actor:  0 policy actor:  0  step number:  54 total reward:  0.40666666666666607  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  46 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.091]
 [-0.036]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]] [[-0.233]
 [ 0.188]
 [-0.233]
 [-0.233]
 [-0.233]
 [-0.233]
 [-0.233]] [[-0.604]
 [-0.478]
 [-0.604]
 [-0.604]
 [-0.604]
 [-0.604]
 [-0.604]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.504]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.436]] [[ 0.   ]
 [-0.344]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.957]] [[0.444]
 [0.504]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.436]]
maxi score, test score, baseline:  -0.895387900078064 -0.16400000000000003 -0.16400000000000003
probs:  [0.17448568739462947, 0.3794854287351777, 0.3700093078984598, 0.076019575971733]
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.6  ]
 [0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]] [[-0.77 ]
 [-0.781]
 [-0.77 ]
 [-0.77 ]
 [-0.77 ]
 [-0.77 ]
 [-0.77 ]] [[0.461]
 [0.6  ]
 [0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]]
maxi score, test score, baseline:  -0.895387900078064 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.895387900078064 -0.16400000000000003 -0.16400000000000003
probs:  [0.17464949656747972, 0.37984185951775007, 0.3694177793790488, 0.07609086453572138]
maxi score, test score, baseline:  -0.895387900078064 -0.16400000000000003 -0.16400000000000003
probs:  [0.17464949656747972, 0.37984185951775007, 0.3694177793790488, 0.07609086453572138]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0068230664829259
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.693]
 [0.429]
 [0.463]
 [0.477]
 [0.462]
 [0.46 ]] [[2.546]
 [2.136]
 [0.827]
 [0.321]
 [0.802]
 [0.381]
 [1.21 ]] [[0.705]
 [0.693]
 [0.429]
 [0.463]
 [0.477]
 [0.462]
 [0.46 ]]
maxi score, test score, baseline:  -0.895387900078064 -0.16400000000000003 -0.16400000000000003
probs:  [0.17395440376114404, 0.37976451842686204, 0.37020556729632736, 0.07607551051566658]
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.767]
 [0.613]
 [0.567]
 [0.618]
 [0.607]
 [0.652]] [[3.912]
 [4.164]
 [2.663]
 [2.227]
 [2.9  ]
 [2.871]
 [3.181]] [[0.685]
 [0.767]
 [0.613]
 [0.567]
 [0.618]
 [0.607]
 [0.652]]
Printing some Q and Qe and total Qs values:  [[0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]] [[4.827]
 [4.827]
 [4.827]
 [4.827]
 [4.827]
 [4.827]
 [4.827]] [[0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]]
Printing some Q and Qe and total Qs values:  [[0.95 ]
 [0.999]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]] [[2.882]
 [2.229]
 [3.839]
 [3.839]
 [3.839]
 [3.839]
 [3.839]] [[0.95 ]
 [0.999]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]]
maxi score, test score, baseline:  -0.8956320872274144 -0.16400000000000003 -0.16400000000000003
maxi score, test score, baseline:  -0.8956320872274144 -0.16400000000000003 -0.16400000000000003
probs:  [0.17411749293306739, 0.3801207291945413, 0.36961502268955326, 0.07614675518283809]
actor:  0 policy actor:  1  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.9956861966067966
actor:  0 policy actor:  1  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  27 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  28 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  28 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  28 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  28 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  28 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  28 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  29 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  29 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  29 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  29 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  29 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.4325270142616584
actor:  0 policy actor:  1  step number:  30 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  30 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  31 total reward:  0.56  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  32 total reward:  0.6066666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  38 total reward:  0.5933333333333336  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]] [[-0.755]
 [-0.73 ]
 [-0.73 ]
 [-0.73 ]
 [-0.73 ]
 [-0.73 ]
 [-0.73 ]] [[-0.486]
 [-0.481]
 [-0.481]
 [-0.481]
 [-0.481]
 [-0.481]
 [-0.481]]
maxi score, test score, baseline:  -0.8288680772086118 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8288680772086118 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8292481481481483 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8292481481481483 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.351]
 [0.729]
 [0.35 ]
 [0.729]
 [0.729]
 [0.729]] [[-0.003]
 [ 0.001]
 [ 0.546]
 [-0.006]
 [ 0.546]
 [ 0.546]
 [ 0.546]] [[0.348]
 [0.351]
 [0.729]
 [0.35 ]
 [0.729]
 [0.729]
 [0.729]]
actor:  0 policy actor:  0  step number:  52 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8273631268436578 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8273631268436578 0.6083333333333335 0.6083333333333335
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8273631268436578 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8277440029433407 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.326]
 [0.326]
 [0.299]
 [0.326]
 [0.326]
 [0.326]] [[1.868]
 [2.924]
 [2.924]
 [4.365]
 [2.924]
 [2.924]
 [2.924]] [[0.111]
 [0.395]
 [0.395]
 [0.734]
 [0.395]
 [0.395]
 [0.395]]
siam score:  -0.810378
siam score:  -0.80976295
Printing some Q and Qe and total Qs values:  [[-0.179]
 [-0.16 ]
 [-0.214]
 [-0.108]
 [-0.142]
 [-0.134]
 [-0.142]] [[ 0.017]
 [ 0.014]
 [-0.111]
 [ 0.13 ]
 [ 0.131]
 [-0.015]
 [-0.197]] [[-0.34 ]
 [-0.323]
 [-0.418]
 [-0.232]
 [-0.266]
 [-0.307]
 [-0.375]]
siam score:  -0.8078169
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8277440029433407 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8277440029433407 0.6083333333333335 0.6083333333333335
siam score:  -0.80981386
maxi score, test score, baseline:  -0.8277440029433407 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.841]
 [0.678]
 [0.527]
 [0.52 ]
 [0.57 ]
 [0.599]] [[1.784]
 [3.537]
 [3.6  ]
 [2.111]
 [1.421]
 [1.909]
 [2.708]] [[0.59 ]
 [0.841]
 [0.678]
 [0.527]
 [0.52 ]
 [0.57 ]
 [0.599]]
maxi score, test score, baseline:  -0.8277440029433407 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.810474413355673
maxi score, test score, baseline:  -0.8277440029433407 0.6083333333333335 0.6083333333333335
using explorer policy with actor:  1
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.8277440029433407 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  42 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8250541850220264 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8095514
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8250541850220264 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8254384615384616 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8254384615384616 0.6083333333333335 0.6083333333333335
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  54 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8231456140350877 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8231456140350877 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8235323851203501 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  51 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  52 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.167
siam score:  -0.811806
maxi score, test score, baseline:  -0.8210353711790394 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8210353711790394 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.784]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]] [[1.159]
 [1.616]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]] [[0.631]
 [0.784]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]]
actor:  0 policy actor:  0  step number:  41 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8189310918293566 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8189310918293566 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]] [[8.975]
 [9.575]
 [9.575]
 [9.575]
 [9.575]
 [9.575]
 [9.575]] [[0.686]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]]
maxi score, test score, baseline:  -0.8189310918293566 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8193227994227995 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]] [[0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]] [[-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]]
maxi score, test score, baseline:  -0.8193227994227995 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8193227994227995 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8193227994227995 0.6083333333333335 0.6083333333333335
actor:  0 policy actor:  0  step number:  53 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8170058315334774 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  48 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8174000000000001 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8174000000000001 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8174000000000001 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  0.167
siam score:  -0.81833106
maxi score, test score, baseline:  -0.8177924731182797 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8177924731182797 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8177924731182797 0.6083333333333335 0.6083333333333335
Printing some Q and Qe and total Qs values:  [[ 0.019]
 [ 0.162]
 [ 0.006]
 [-0.012]
 [-0.01 ]
 [ 0.006]
 [ 0.006]] [[ 0.225]
 [ 0.969]
 [ 1.442]
 [-0.069]
 [ 0.162]
 [ 1.442]
 [ 1.442]] [[-0.473]
 [-0.206]
 [-0.283]
 [-0.553]
 [-0.512]
 [-0.283]
 [-0.283]]
maxi score, test score, baseline:  -0.8177924731182797 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8177924731182797 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.481]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[2.663]
 [3.9  ]
 [4.114]
 [4.114]
 [4.114]
 [4.114]
 [4.114]] [[0.117]
 [0.366]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]]
maxi score, test score, baseline:  -0.8181832618025753 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.233]
 [-0.283]
 [-0.282]
 [-0.286]
 [-0.233]
 [-0.284]
 [-0.285]] [[0.022]
 [0.567]
 [1.033]
 [0.933]
 [0.022]
 [0.696]
 [0.7  ]] [[-0.427]
 [-0.296]
 [-0.14 ]
 [-0.177]
 [-0.427]
 [-0.254]
 [-0.254]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.8181832618025753 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.745570425022443
Printing some Q and Qe and total Qs values:  [[-0.137]
 [-0.141]
 [-0.142]
 [-0.14 ]
 [-0.139]
 [-0.144]
 [-0.136]] [[-4.165]
 [-1.122]
 [-3.961]
 [-4.333]
 [-4.222]
 [-2.836]
 [-4.138]] [[-0.014]
 [ 0.522]
 [ 0.021]
 [-0.044]
 [-0.024]
 [ 0.219]
 [-0.009]]
maxi score, test score, baseline:  -0.8185723768736618 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  52 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8185723768736618 0.6083333333333335 0.6083333333333335
Printing some Q and Qe and total Qs values:  [[-0.148]
 [-0.166]
 [-0.013]
 [-0.101]
 [-0.099]
 [-0.043]
 [-0.162]] [[0.923]
 [0.486]
 [0.484]
 [0.535]
 [0.916]
 [0.974]
 [0.902]] [[ 0.089]
 [-0.104]
 [ 0.02 ]
 [-0.032]
 [ 0.126]
 [ 0.196]
 [ 0.07 ]]
maxi score, test score, baseline:  -0.8185723768736618 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8185723768736618 0.6083333333333335 0.6083333333333335
siam score:  -0.8131057
maxi score, test score, baseline:  -0.8185723768736618 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  53 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8169496453900709 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8169496453900709 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8169496453900709 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8169496453900709 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8173380750176928 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  63 total reward:  0.053333333333332344  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.886]
 [0.814]
 [0.724]
 [0.751]
 [0.806]
 [0.787]] [[2.026]
 [2.125]
 [2.519]
 [1.73 ]
 [1.546]
 [1.725]
 [2.308]] [[0.798]
 [0.886]
 [0.814]
 [0.724]
 [0.751]
 [0.806]
 [0.787]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.31333333333333246  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.8154932203389832 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.8158830866807611 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.079]
 [-0.064]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]] [[-1.983]
 [-0.922]
 [-1.983]
 [-1.983]
 [-1.983]
 [-1.983]
 [-1.983]] [[-0.666]
 [-0.473]
 [-0.666]
 [-0.666]
 [-0.666]
 [-0.666]
 [-0.666]]
actor:  0 policy actor:  0  step number:  55 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8143561403508772 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.469]
 [0.318]
 [0.278]
 [0.318]
 [0.341]
 [0.318]] [[2.295]
 [1.82 ]
 [1.517]
 [0.908]
 [1.517]
 [1.302]
 [1.517]] [[-0.004]
 [ 0.045]
 [-0.156]
 [-0.299]
 [-0.156]
 [-0.17 ]
 [-0.156]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  52 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.8147459383753501 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[-0.178]
 [-0.121]
 [-0.279]
 [-0.136]
 [-0.13 ]
 [-0.166]
 [-0.161]] [[-0.572]
 [-0.978]
 [ 0.045]
 [-1.155]
 [-2.549]
 [-0.933]
 [-0.911]] [[ 0.166]
 [ 0.081]
 [ 0.288]
 [ 0.02 ]
 [-0.384]
 [ 0.068]
 [ 0.077]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.8151341020265549 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8151341020265549 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.224 0.388 0.02  0.122 0.041 0.061 0.143]
start point for exploration sampling:  11091
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.725]
 [0.587]
 [0.578]
 [0.465]
 [0.579]
 [0.514]] [[0.611]
 [1.141]
 [0.185]
 [0.518]
 [0.512]
 [0.609]
 [0.634]] [[0.573]
 [0.725]
 [0.587]
 [0.578]
 [0.465]
 [0.579]
 [0.514]]
maxi score, test score, baseline:  -0.8159055671537927 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8159055671537927 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8159055671537927 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  49 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8136222222222224 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8136222222222224 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8140094941094942 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8275665
maxi score, test score, baseline:  -0.8140094941094942 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8140094941094942 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8140094941094942 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.09 ]
 [0.424]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.349]] [[-0.778]
 [-1.352]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-1.363]] [[0.09 ]
 [0.424]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.349]]
maxi score, test score, baseline:  -0.8143951590594745 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8143951590594745 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -1.5599245131313002
Printing some Q and Qe and total Qs values:  [[-0.113]
 [-0.083]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]] [[-1.633]
 [-1.652]
 [-1.633]
 [-1.633]
 [-1.633]
 [-1.633]
 [-1.633]] [[0.35 ]
 [0.354]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8151617079889808 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8151617079889808 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.728]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]] [[-0.881]
 [-0.252]
 [-0.881]
 [-0.881]
 [-0.881]
 [-0.881]
 [-0.881]] [[0.581]
 [0.728]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]]
maxi score, test score, baseline:  -0.8155426116838489 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8155426116838489 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8271
actor:  1 policy actor:  1  step number:  45 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.8155426116838489 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8155426116838489 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  52 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  68 total reward:  0.019999999999998908  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.8115632443531827 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8273645
line 256 mcts: sample exp_bonus 2.5072184891616947
maxi score, test score, baseline:  -0.8115632443531827 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8115632443531827 0.6083333333333335 0.6083333333333335
actor:  1 policy actor:  1  step number:  50 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8115632443531827 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.098]
 [-0.093]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]] [[0.223]
 [0.795]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[-0.684]
 [-0.584]
 [-0.655]
 [-0.655]
 [-0.655]
 [-0.655]
 [-0.655]]
maxi score, test score, baseline:  -0.8119491803278688 0.6083333333333335 0.6083333333333335
siam score:  -0.82721543
maxi score, test score, baseline:  -0.8119491803278688 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8119491803278688 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8119491803278688 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8119491803278688 0.6083333333333335 0.6083333333333335
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8127163265306123 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8127163265306123 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8127163265306123 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.551]
 [0.456]
 [0.473]
 [0.456]
 [0.456]
 [0.46 ]] [[-1.646]
 [-0.413]
 [-1.646]
 [-1.93 ]
 [-1.646]
 [-1.646]
 [-0.693]] [[0.456]
 [0.551]
 [0.456]
 [0.473]
 [0.456]
 [0.456]
 [0.46 ]]
maxi score, test score, baseline:  -0.8127163265306123 0.6083333333333335 0.6083333333333335
first move QE:  0.44620281433871734
maxi score, test score, baseline:  -0.8127163265306123 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8127163265306123 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8130975560081467 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8130975560081467 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  58 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.028]
 [ 0.34 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[ 1.259]
 [ 1.942]
 [-0.352]
 [-0.352]
 [-0.352]
 [-0.352]
 [-0.352]] [[0.293]
 [0.614]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]]
first move QE:  0.4368745767947009
maxi score, test score, baseline:  -0.811546423751687 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.811546423751687 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.811546423751687 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.811546423751687 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8119269360269361 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.493]
 [0.473]
 [0.399]
 [0.39 ]
 [0.455]
 [0.463]] [[1.066]
 [1.387]
 [1.178]
 [0.823]
 [0.488]
 [1.153]
 [1.389]] [[-0.087]
 [ 0.039]
 [-0.017]
 [-0.15 ]
 [-0.215]
 [-0.04 ]
 [ 0.008]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.8123059139784947 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8123059139784947 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8123059139784947 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8123059139784947 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8123059139784947 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8123059139784947 0.6083333333333335 0.6083333333333335
Printing some Q and Qe and total Qs values:  [[-0.143]
 [-0.186]
 [-0.107]
 [-0.102]
 [-0.137]
 [-0.079]
 [-0.15 ]] [[ 0.334]
 [ 0.911]
 [-0.499]
 [-0.791]
 [ 0.23 ]
 [-1.126]
 [-0.061]] [[ 0.285]
 [ 0.62 ]
 [-0.223]
 [-0.409]
 [ 0.223]
 [-0.605]
 [ 0.021]]
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.032]
 [-0.061]
 [-0.041]
 [-0.049]
 [-0.053]
 [-0.053]] [[ 0.334]
 [ 0.567]
 [-0.301]
 [ 0.301]
 [-0.397]
 [ 0.053]
 [-0.108]] [[-0.401]
 [-0.317]
 [-0.634]
 [-0.414]
 [-0.654]
 [-0.509]
 [-0.562]]
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.029]
 [-0.043]
 [-0.048]
 [-0.048]
 [-0.049]
 [-0.049]] [[-1.099]
 [-0.808]
 [-1.114]
 [-1.181]
 [-1.052]
 [-1.009]
 [-0.948]] [[-0.555]
 [-0.38 ]
 [-0.578]
 [-0.623]
 [-0.545]
 [-0.52 ]
 [-0.483]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8126833668678739 0.6083333333333335 0.6083333333333335
Printing some Q and Qe and total Qs values:  [[-0.165]
 [-0.272]
 [-0.151]
 [-0.165]
 [-0.165]
 [-0.165]
 [-0.28 ]] [[-0.472]
 [ 0.009]
 [ 1.441]
 [-0.472]
 [-0.472]
 [-0.472]
 [ 0.257]] [[-0.245]
 [-0.272]
 [ 0.088]
 [-0.245]
 [-0.245]
 [-0.245]
 [-0.239]]
maxi score, test score, baseline:  -0.8126833668678739 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8126833668678739 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8126833668678739 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8126833668678739 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.8126833668678739 0.6083333333333335 0.6083333333333335
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.005]
 [-0.021]
 [-0.05 ]
 [-0.031]
 [-0.055]
 [-0.052]] [[ 0.157]
 [ 0.404]
 [ 0.089]
 [ 0.33 ]
 [-0.282]
 [ 0.181]
 [ 0.134]] [[-0.616]
 [-0.535]
 [-0.604]
 [-0.592]
 [-0.675]
 [-0.621]
 [-0.627]]
maxi score, test score, baseline:  -0.8126833668678739 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999994  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8126833668678739 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.8126833668678739 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8126833668678739 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8126833668678739 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8126833668678739 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8126833668678739 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8211734
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  64 total reward:  0.03333333333333244  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.466]
 [0.713]
 [0.717]
 [0.709]
 [0.706]
 [0.704]] [[-0.249]
 [ 0.144]
 [-0.295]
 [-0.279]
 [-0.254]
 [-0.323]
 [-0.241]] [[0.727]
 [0.466]
 [0.713]
 [0.717]
 [0.709]
 [0.706]
 [0.704]]
maxi score, test score, baseline:  -0.8130593038821954 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8130593038821954 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8130593038821954 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8130593038821954 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8130593038821954 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8130593038821954 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8130593038821954 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.164]
 [-0.061]
 [-0.091]
 [-0.079]
 [-0.089]
 [-0.099]
 [-0.084]] [[-0.182]
 [-0.589]
 [-0.326]
 [-0.808]
 [-0.902]
 [-1.025]
 [-0.655]] [[-0.269]
 [-0.301]
 [-0.244]
 [-0.393]
 [-0.434]
 [-0.485]
 [-0.347]]
start point for exploration sampling:  11091
Printing some Q and Qe and total Qs values:  [[-0.212]
 [-0.124]
 [-0.282]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.129]] [[ 0.139]
 [-0.469]
 [ 0.033]
 [-1.299]
 [-1.299]
 [-1.299]
 [-1.299]] [[0.873]
 [0.788]
 [0.836]
 [0.638]
 [0.638]
 [0.638]
 [0.638]]
Printing some Q and Qe and total Qs values:  [[-0.03 ]
 [-0.023]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]] [[-0.316]
 [-0.362]
 [-0.399]
 [-0.399]
 [-0.399]
 [-0.399]
 [-0.399]] [[-0.293]
 [-0.306]
 [-0.324]
 [-0.324]
 [-0.324]
 [-0.324]
 [-0.324]]
actor:  0 policy actor:  0  step number:  50 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.8113133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8113133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.25 ]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]] [[2.058]
 [3.076]
 [1.93 ]
 [1.93 ]
 [1.93 ]
 [1.93 ]
 [1.93 ]] [[0.052]
 [0.411]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]]
maxi score, test score, baseline:  -0.8113133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.317]
 [0.334]
 [0.291]
 [0.304]
 [0.312]
 [0.284]] [[4.634]
 [4.356]
 [5.098]
 [4.559]
 [4.55 ]
 [4.738]
 [4.705]] [[0.367]
 [0.286]
 [0.494]
 [0.326]
 [0.331]
 [0.386]
 [0.362]]
maxi score, test score, baseline:  -0.8113133333333334 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8113133333333334 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8113133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.0601201923132852
line 256 mcts: sample exp_bonus 3.2097173415578526
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  73 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  54 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
Printing some Q and Qe and total Qs values:  [[0.894]
 [0.973]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]] [[4.651]
 [3.936]
 [4.651]
 [4.651]
 [4.651]
 [4.651]
 [4.651]] [[0.894]
 [0.973]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  64 total reward:  0.0733333333333328  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]] [[0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]] [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]]
Printing some Q and Qe and total Qs values:  [[-0.122]
 [-0.112]
 [-0.11 ]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]] [[-0.204]
 [ 1.235]
 [-0.561]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]] [[ 0.085]
 [ 0.566]
 [-0.026]
 [ 0.128]
 [ 0.128]
 [ 0.128]
 [ 0.128]]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.434]
 [0.609]
 [0.435]
 [0.609]
 [0.383]
 [0.609]] [[ 2.395]
 [ 0.3  ]
 [ 1.395]
 [ 0.054]
 [ 1.395]
 [-0.137]
 [ 1.395]] [[0.729]
 [0.434]
 [0.609]
 [0.435]
 [0.609]
 [0.383]
 [0.609]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.720789039164083
maxi score, test score, baseline:  -0.8133133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.585]
 [0.456]
 [0.499]
 [0.477]
 [0.411]
 [0.405]] [[-0.294]
 [-0.468]
 [-0.247]
 [-0.422]
 [-0.669]
 [-0.132]
 [-0.101]] [[0.347]
 [0.585]
 [0.456]
 [0.499]
 [0.477]
 [0.411]
 [0.405]]
actor:  0 policy actor:  0  step number:  43 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8104333333333333 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  42 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  85 total reward:  0.14666666666666495  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8075666666666668 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.8075666666666668 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8075666666666668 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0053473343076185
maxi score, test score, baseline:  -0.8075666666666668 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.8075666666666668 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.288]
 [0.161]
 [0.155]
 [0.153]
 [0.164]
 [0.161]] [[5.095]
 [4.748]
 [5.216]
 [5.375]
 [5.264]
 [5.364]
 [4.573]] [[0.438]
 [0.416]
 [0.479]
 [0.523]
 [0.489]
 [0.525]
 [0.289]]
actor:  0 policy actor:  0  step number:  42 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.231]
 [0.122]
 [0.088]
 [0.088]
 [0.122]
 [0.098]] [[-0.677]
 [-1.514]
 [ 0.   ]
 [-2.303]
 [-3.038]
 [ 0.   ]
 [-1.432]] [[0.056]
 [0.231]
 [0.122]
 [0.088]
 [0.088]
 [0.122]
 [0.098]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8047800000000002 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8047800000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8047800000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8174175
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.009]
 [-0.042]
 [-0.041]
 [-0.048]
 [-0.049]
 [-0.044]] [[-0.86 ]
 [-0.701]
 [-0.933]
 [-1.191]
 [-1.012]
 [-0.881]
 [-0.743]] [[-0.17 ]
 [-0.092]
 [-0.194]
 [-0.288]
 [-0.226]
 [-0.179]
 [-0.126]]
maxi score, test score, baseline:  -0.8047800000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8047800000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8047800000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8047800000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.062]
 [-0.01 ]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]] [[ 0.797]
 [-0.021]
 [ 0.29 ]
 [ 0.29 ]
 [ 0.29 ]
 [ 0.29 ]
 [ 0.29 ]] [[-0.029]
 [-0.218]
 [-0.167]
 [-0.167]
 [-0.167]
 [-0.167]
 [-0.167]]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.805]
 [0.611]
 [0.63 ]
 [0.643]
 [0.643]
 [0.643]] [[-0.076]
 [ 0.347]
 [ 0.205]
 [-0.02 ]
 [ 0.205]
 [ 0.205]
 [ 0.205]] [[0.696]
 [0.805]
 [0.611]
 [0.63 ]
 [0.643]
 [0.643]
 [0.643]]
Printing some Q and Qe and total Qs values:  [[-0.125]
 [-0.12 ]
 [-0.126]
 [-0.126]
 [-0.125]
 [-0.126]
 [-0.126]] [[-0.41 ]
 [-0.084]
 [-0.657]
 [-0.588]
 [ 0.   ]
 [-0.554]
 [-0.51 ]] [[-0.311]
 [-0.142]
 [-0.434]
 [-0.401]
 [-0.105]
 [-0.384]
 [-0.361]]
maxi score, test score, baseline:  -0.8047800000000002 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8047800000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.251]
 [0.095]
 [0.141]
 [0.249]
 [0.249]
 [0.262]] [[1.659]
 [1.845]
 [0.929]
 [1.067]
 [1.956]
 [1.956]
 [1.794]] [[ 0.174]
 [ 0.319]
 [-0.044]
 [ 0.024]
 [ 0.351]
 [ 0.351]
 [ 0.31 ]]
maxi score, test score, baseline:  -0.8047800000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  61 total reward:  0.13333333333333297  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8025133333333335 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8025133333333335 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.722]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]] [[2.482]
 [2.524]
 [2.482]
 [2.482]
 [2.482]
 [2.482]
 [2.482]] [[0.655]
 [0.722]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]]
maxi score, test score, baseline:  -0.8025133333333333 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8025133333333333 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8025133333333333 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8025133333333333 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8025133333333333 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8025133333333333 0.6083333333333335 0.6083333333333335
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8025133333333333 0.6083333333333335 0.6083333333333335
actor:  0 policy actor:  0  step number:  57 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8003000000000001 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80585694
maxi score, test score, baseline:  -0.8003000000000001 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80345947
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.8003000000000001 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.8003000000000001 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.688]
 [0.676]
 [0.676]
 [0.439]
 [0.674]
 [0.674]] [[0.536]
 [0.977]
 [0.844]
 [0.834]
 [1.077]
 [0.955]
 [0.309]] [[0.446]
 [0.578]
 [0.522]
 [0.519]
 [0.363]
 [0.557]
 [0.341]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8003 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8003 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.8003 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8003 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.584]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]] [[1.895]
 [2.668]
 [2.183]
 [2.183]
 [2.183]
 [2.183]
 [2.183]] [[-0.113]
 [ 0.25 ]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]]
maxi score, test score, baseline:  -0.8003 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.8003 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8003 0.6083333333333335 0.6083333333333335
actor:  0 policy actor:  0  step number:  59 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7978466666666667 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  73 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.7957133333333333 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.517]
 [0.505]
 [0.506]
 [0.503]
 [0.506]
 [0.507]] [[ 0.483]
 [ 1.32 ]
 [ 0.625]
 [ 0.35 ]
 [-0.007]
 [ 0.417]
 [ 0.551]] [[0.506]
 [0.517]
 [0.505]
 [0.506]
 [0.503]
 [0.506]
 [0.507]]
maxi score, test score, baseline:  -0.7957133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7957133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7957133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7957133333333333 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7957133333333333 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7957133333333333 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7957133333333334 0.6083333333333335 0.6083333333333335
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]] [[5.215]
 [5.215]
 [5.215]
 [5.215]
 [5.215]
 [5.215]
 [5.215]] [[0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]]
maxi score, test score, baseline:  -0.7957133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.601]
 [0.462]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[1.794]
 [2.524]
 [0.807]
 [0.983]
 [0.983]
 [0.983]
 [0.983]] [[0.376]
 [0.601]
 [0.462]
 [0.454]
 [0.454]
 [0.454]
 [0.454]]
maxi score, test score, baseline:  -0.7957133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
start point for exploration sampling:  11091
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.399]
 [0.415]
 [0.407]
 [0.378]
 [0.398]
 [0.387]] [[3.692]
 [4.159]
 [3.565]
 [3.664]
 [3.601]
 [3.661]
 [3.731]] [[0.335]
 [0.495]
 [0.355]
 [0.374]
 [0.336]
 [0.367]
 [0.376]]
maxi score, test score, baseline:  -0.7957133333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  44 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  82 total reward:  0.04666666666666497  reward:  1.0 rdn_beta:  0.333
siam score:  -0.80751634
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
actor:  1 policy actor:  1  step number:  54 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.286]
 [0.229]
 [0.27 ]
 [0.294]
 [0.26 ]
 [0.29 ]] [[-0.686]
 [-0.603]
 [-0.561]
 [-0.922]
 [-1.61 ]
 [-0.569]
 [-1.139]] [[0.238]
 [0.286]
 [0.229]
 [0.27 ]
 [0.294]
 [0.26 ]
 [0.29 ]]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]] [[-1.682]
 [-1.682]
 [-1.682]
 [-1.682]
 [-1.682]
 [-1.682]
 [-1.682]] [[0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80983806
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81039083
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.7819184611468024
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.776]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]] [[1.261]
 [1.26 ]
 [1.261]
 [1.261]
 [1.261]
 [1.261]
 [1.261]] [[0.794]
 [0.776]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.7874937721669824
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8027556
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.505]
 [0.431]
 [0.412]
 [0.48 ]
 [0.463]
 [0.47 ]] [[0.967]
 [0.594]
 [1.107]
 [1.742]
 [1.415]
 [1.121]
 [1.184]] [[ 0.14 ]
 [-0.03 ]
 [ 0.153]
 [ 0.451]
 [ 0.357]
 [ 0.192]
 [ 0.23 ]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.19999999999999907  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.395]
 [0.267]
 [0.317]
 [0.36 ]
 [0.145]
 [0.293]] [[ 0.221]
 [ 0.144]
 [ 0.186]
 [-0.316]
 [-0.442]
 [-0.176]
 [-0.046]] [[0.388]
 [0.395]
 [0.267]
 [0.317]
 [0.36 ]
 [0.145]
 [0.293]]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.413]
 [0.463]
 [0.463]
 [0.472]
 [0.468]
 [0.459]] [[-0.451]
 [ 0.263]
 [-0.413]
 [-0.413]
 [-0.501]
 [-0.405]
 [-0.457]] [[0.466]
 [0.413]
 [0.463]
 [0.463]
 [0.472]
 [0.468]
 [0.459]]
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.501]
 [0.476]
 [0.47 ]
 [0.471]
 [0.477]
 [0.472]] [[0.206]
 [0.732]
 [0.202]
 [0.095]
 [0.289]
 [0.345]
 [0.432]] [[0.485]
 [0.501]
 [0.476]
 [0.47 ]
 [0.471]
 [0.477]
 [0.472]]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.558]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]] [[2.067]
 [2.93 ]
 [2.067]
 [2.067]
 [2.067]
 [2.067]
 [2.067]] [[0.389]
 [0.544]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  48 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.607]
 [0.609]
 [0.604]
 [0.606]
 [0.592]
 [0.585]] [[-1.17 ]
 [-0.265]
 [-1.328]
 [-1.433]
 [-1.401]
 [-1.249]
 [-1.346]] [[0.597]
 [0.607]
 [0.609]
 [0.604]
 [0.606]
 [0.592]
 [0.585]]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.2095247100182901
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.082]
 [-0.057]
 [-0.084]
 [-0.065]
 [-0.083]
 [-0.065]
 [-0.065]] [[-1.031]
 [-0.323]
 [-1.12 ]
 [ 0.279]
 [-0.998]
 [ 0.279]
 [ 0.279]] [[-0.749]
 [-0.607]
 [-0.766]
 [-0.514]
 [-0.745]
 [-0.514]
 [-0.514]]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[ 0.227]
 [-0.041]
 [-0.014]
 [-0.02 ]
 [-0.012]
 [ 0.099]
 [-0.039]] [[3.377]
 [1.229]
 [1.589]
 [1.172]
 [1.13 ]
 [2.124]
 [1.54 ]] [[ 0.275]
 [-0.351]
 [-0.264]
 [-0.34 ]
 [-0.339]
 [-0.062]
 [-0.297]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.763]
 [0.68 ]
 [0.666]
 [0.68 ]
 [0.623]
 [0.667]] [[1.369]
 [1.393]
 [0.488]
 [0.637]
 [0.488]
 [0.473]
 [0.811]] [[0.672]
 [0.763]
 [0.68 ]
 [0.666]
 [0.68 ]
 [0.623]
 [0.667]]
siam score:  -0.79274726
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.926]
 [0.992]
 [0.955]
 [0.955]
 [0.955]
 [0.955]
 [0.955]] [[2.936]
 [3.936]
 [4.197]
 [4.197]
 [4.197]
 [4.197]
 [4.197]] [[0.926]
 [0.992]
 [0.955]
 [0.955]
 [0.955]
 [0.955]
 [0.955]]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.79298 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.923]
 [0.838]
 [0.838]
 [0.838]
 [0.838]
 [0.838]] [[1.98 ]
 [2.145]
 [2.865]
 [2.865]
 [2.865]
 [2.865]
 [2.865]] [[0.727]
 [0.923]
 [0.838]
 [0.838]
 [0.838]
 [0.838]
 [0.838]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  0 policy actor:  0  step number:  53 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7905266666666667 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  53 total reward:  0.2266666666666659  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  57 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.134]
 [-0.181]
 [-0.136]
 [-0.134]
 [-0.134]
 [-0.136]
 [-0.133]] [[-4.644]
 [-1.426]
 [-4.721]
 [-4.788]
 [-4.845]
 [-4.516]
 [-4.329]] [[-0.159]
 [ 0.496]
 [-0.176]
 [-0.19 ]
 [-0.201]
 [-0.133]
 [-0.093]]
maxi score, test score, baseline:  -0.7880733333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7880733333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7880733333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7880733333333334 0.6083333333333335 0.6083333333333335
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]] [[1.136]
 [1.136]
 [1.136]
 [1.136]
 [1.136]
 [1.136]
 [1.136]] [[0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]]
using explorer policy with actor:  1
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.7880733333333334 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  52 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.178]
 [-0.193]
 [-0.178]
 [-0.178]
 [-0.178]
 [-0.178]
 [-0.178]] [[0.46 ]
 [0.322]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]] [[0.856]
 [0.828]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]]
Printing some Q and Qe and total Qs values:  [[-0.054]
 [-0.047]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]] [[0.041]
 [0.322]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]] [[-0.031]
 [ 0.067]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]]
line 256 mcts: sample exp_bonus 3.210521928752298
maxi score, test score, baseline:  -0.78522 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.78522 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  51 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8050602
maxi score, test score, baseline:  -0.78522 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.042]
 [-0.085]
 [-0.082]
 [-0.081]
 [-0.089]
 [-0.079]] [[ 0.367]
 [ 0.146]
 [-0.372]
 [-0.586]
 [-0.667]
 [-0.802]
 [-0.589]] [[-0.428]
 [-0.44 ]
 [-0.569]
 [-0.602]
 [-0.614]
 [-0.645]
 [-0.599]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  51 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
start point for exploration sampling:  11091
Printing some Q and Qe and total Qs values:  [[-0.158]
 [-0.187]
 [-0.158]
 [-0.158]
 [-0.158]
 [-0.158]
 [-0.158]] [[1.452]
 [0.984]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]] [[ 0.044]
 [-0.064]
 [ 0.044]
 [ 0.044]
 [ 0.044]
 [ 0.044]
 [ 0.044]]
maxi score, test score, baseline:  -0.7852199999999999 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7852199999999999 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7852199999999999 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.7852199999999999 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7852199999999999 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  81 total reward:  0.0533333333333319  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7831133333333333 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7831133333333333 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7831133333333333 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  43 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]] [[-0.639]
 [-0.639]
 [-0.639]
 [-0.639]
 [-0.639]
 [-0.639]
 [-0.639]] [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]]
maxi score, test score, baseline:  -0.7800200000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]] [[5.296]
 [5.296]
 [5.296]
 [5.296]
 [5.296]
 [5.296]
 [5.296]] [[0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.7800200000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7800200000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.1586127748966086
actor:  1 policy actor:  1  step number:  52 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7800200000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.728]
 [0.659]
 [0.633]
 [0.662]
 [0.634]
 [0.642]] [[-0.943]
 [-0.822]
 [-1.292]
 [-1.173]
 [-1.085]
 [-1.076]
 [-1.001]] [[0.661]
 [0.728]
 [0.659]
 [0.633]
 [0.662]
 [0.634]
 [0.642]]
maxi score, test score, baseline:  -0.7800200000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7800200000000002 0.6083333333333335 0.6083333333333335
maxi score, test score, baseline:  -0.7800200000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7800200000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.705]
 [0.53 ]
 [0.585]
 [0.579]
 [0.53 ]
 [0.571]] [[ 0.359]
 [ 0.102]
 [ 0.   ]
 [-0.948]
 [-1.007]
 [ 0.   ]
 [-0.779]] [[0.612]
 [0.705]
 [0.53 ]
 [0.585]
 [0.579]
 [0.53 ]
 [0.571]]
siam score:  -0.79833055
Printing some Q and Qe and total Qs values:  [[0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]] [[-0.096]
 [-0.096]
 [-0.096]
 [-0.096]
 [-0.096]
 [-0.096]
 [-0.096]] [[0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]]
Printing some Q and Qe and total Qs values:  [[0.823]
 [0.839]
 [0.748]
 [0.661]
 [0.677]
 [0.624]
 [0.73 ]] [[ 1.842]
 [ 1.352]
 [ 1.44 ]
 [ 0.231]
 [-0.075]
 [ 0.114]
 [ 1.133]] [[0.823]
 [0.839]
 [0.748]
 [0.661]
 [0.677]
 [0.624]
 [0.73 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.7800200000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7800200000000002 0.6083333333333335 0.6083333333333335
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  57 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.71238 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.549146556535363
line 256 mcts: sample exp_bonus -0.43694334629557957
siam score:  -0.7921548
UNIT TEST: sample policy line 217 mcts : [0.102 0.796 0.02  0.02  0.02  0.02  0.02 ]
maxi score, test score, baseline:  -0.71238 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.71238 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.71238 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.71238 0.6910000000000003 0.6910000000000003
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.71238 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  47 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.645]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]] [[-0.02 ]
 [ 0.247]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[0.545]
 [0.645]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]]
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.81 ]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]] [[2.678]
 [2.752]
 [2.678]
 [2.678]
 [2.678]
 [2.678]
 [2.678]] [[0.728]
 [0.81 ]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]]
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7097666666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  59 total reward:  0.17333333333333256  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7097666666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7097666666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
using explorer policy with actor:  1
first move QE:  0.1252971751803126
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  52 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7913733
maxi score, test score, baseline:  -0.7097666666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7097666666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.123]
 [-0.15 ]
 [-0.127]
 [-0.12 ]
 [-0.122]
 [-0.136]
 [-0.125]] [[-2.373]
 [ 0.237]
 [-2.336]
 [-2.49 ]
 [-2.516]
 [ 0.002]
 [-2.506]] [[-0.432]
 [ 0.481]
 [-0.421]
 [-0.471]
 [-0.481]
 [ 0.407]
 [-0.48 ]]
maxi score, test score, baseline:  -0.7097666666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7097666666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7097666666666668 0.6910000000000003 0.6910000000000003
actor:  0 policy actor:  0  step number:  51 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.7073400000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7953638
maxi score, test score, baseline:  -0.7073400000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.03 ]
 [-0.041]
 [-0.05 ]
 [-0.055]
 [-0.05 ]
 [-0.052]] [[-2.062]
 [-1.322]
 [-1.668]
 [-2.064]
 [-1.963]
 [-1.904]
 [-1.935]] [[-0.471]
 [-0.33 ]
 [-0.398]
 [-0.473]
 [-0.461]
 [-0.446]
 [-0.454]]
maxi score, test score, baseline:  -0.7073400000000001 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.7073400000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  76 total reward:  0.01999999999999902  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.071]
 [ 0.006]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]] [[ 1.254]
 [ 4.278]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]] [[-0.052]
 [ 0.385]
 [-0.207]
 [-0.207]
 [-0.207]
 [-0.207]
 [-0.207]]
actor:  0 policy actor:  0  step number:  48 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.7047933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7047933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7047933333333334 0.6910000000000003 0.6910000000000003
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.189]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]] [[2.179]
 [1.398]
 [2.179]
 [2.179]
 [2.179]
 [2.179]
 [2.179]] [[-0.073]
 [-0.287]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]]
Printing some Q and Qe and total Qs values:  [[-0.158]
 [-0.159]
 [-0.16 ]
 [-0.158]
 [-0.158]
 [-0.158]
 [-0.158]] [[-0.638]
 [-0.412]
 [-1.075]
 [-0.638]
 [-0.638]
 [-0.638]
 [-0.638]] [[-0.155]
 [-0.118]
 [-0.23 ]
 [-0.155]
 [-0.155]
 [-0.155]
 [-0.155]]
maxi score, test score, baseline:  -0.7047933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7047933333333334 0.6910000000000003 0.6910000000000003
actor:  1 policy actor:  1  step number:  65 total reward:  0.19999999999999896  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  41 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.7047933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7047933333333334 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.7047933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[9.506]
 [9.523]
 [9.523]
 [9.523]
 [9.523]
 [9.523]
 [9.523]] [[0.834]
 [0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]]
maxi score, test score, baseline:  -0.7047933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7047933333333335 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.7047933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7047933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  64 total reward:  0.0599999999999995  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  53 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.7047933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80850893
maxi score, test score, baseline:  -0.7047933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.6  ]
 [0.485]
 [0.473]
 [0.482]
 [0.481]
 [0.494]] [[2.868]
 [1.425]
 [1.699]
 [2.323]
 [2.288]
 [2.148]
 [1.69 ]] [[ 0.512]
 [-0.008]
 [ 0.034]
 [ 0.282]
 [ 0.273]
 [ 0.215]
 [ 0.036]]
actor:  0 policy actor:  0  step number:  54 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.529]
 [0.516]
 [0.426]
 [0.516]
 [0.422]
 [0.516]] [[-0.761]
 [-0.011]
 [ 0.681]
 [-0.925]
 [ 0.681]
 [-0.906]
 [ 0.681]] [[0.432]
 [0.529]
 [0.516]
 [0.426]
 [0.516]
 [0.422]
 [0.516]]
line 256 mcts: sample exp_bonus 3.155192881078001
maxi score, test score, baseline:  -0.7023 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.7023000000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8056621
maxi score, test score, baseline:  -0.7023000000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7023000000000001 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.7023000000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80293655
maxi score, test score, baseline:  -0.7023000000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.7023000000000001 0.6910000000000003 0.6910000000000003
actor:  0 policy actor:  0  step number:  42 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6993266666666668 0.6910000000000003 0.6910000000000003
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.667]
 [0.39 ]
 [0.456]
 [0.47 ]
 [0.433]
 [0.427]] [[2.271]
 [1.671]
 [1.861]
 [2.248]
 [2.095]
 [2.445]
 [2.772]] [[-0.063]
 [ 0.074]
 [-0.172]
 [-0.041]
 [-0.053]
 [-0.031]
 [ 0.017]]
siam score:  -0.7994241
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7988657
maxi score, test score, baseline:  -0.6993266666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6993266666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6993266666666668 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6993266666666668 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6993266666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.065]
 [-0.075]
 [-0.1  ]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]] [[-0.029]
 [-0.546]
 [-0.977]
 [-0.623]
 [-0.623]
 [-0.623]
 [-0.623]] [[ 0.139]
 [-0.172]
 [-0.445]
 [-0.233]
 [-0.233]
 [-0.233]
 [-0.233]]
maxi score, test score, baseline:  -0.6993266666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6993266666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7998033
maxi score, test score, baseline:  -0.6993266666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.572]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]] [[3.569]
 [3.926]
 [3.569]
 [3.569]
 [3.569]
 [3.569]
 [3.569]] [[0.698]
 [0.821]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]]
maxi score, test score, baseline:  -0.6993266666666668 0.6910000000000003 0.6910000000000003
actor:  0 policy actor:  0  step number:  39 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 2.2863216142190224
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.6964200000000002 0.6910000000000003 0.6910000000000003
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6964200000000002 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6964200000000002 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  53 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.6964200000000002 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.79942733
maxi score, test score, baseline:  -0.6964200000000002 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6964200000000002 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6964200000000002 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6964200000000002 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6964200000000002 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.3012793793278397
siam score:  -0.79616135
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.757]
 [0.725]
 [0.674]
 [0.651]
 [0.683]
 [0.684]] [[4.052]
 [4.527]
 [2.873]
 [2.272]
 [1.385]
 [2.342]
 [2.632]] [[0.711]
 [0.757]
 [0.725]
 [0.674]
 [0.651]
 [0.683]
 [0.684]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  72 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  60 total reward:  0.11333333333333273  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.384]
 [0.619]
 [0.48 ]
 [0.464]
 [0.51 ]
 [0.511]] [[ 1.25 ]
 [ 2.154]
 [ 3.644]
 [ 0.353]
 [-0.2  ]
 [ 0.652]
 [ 0.519]] [[0.403]
 [0.384]
 [0.619]
 [0.48 ]
 [0.464]
 [0.51 ]
 [0.511]]
maxi score, test score, baseline:  -0.6941666666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6941666666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6941666666666667 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6941666666666667 0.6910000000000003 0.6910000000000003
Printing some Q and Qe and total Qs values:  [[0.17]
 [0.22]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]] [[1.932]
 [2.13 ]
 [1.932]
 [1.932]
 [1.932]
 [1.932]
 [1.932]] [[0.289]
 [0.385]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]]
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.422]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[2.134]
 [2.867]
 [2.134]
 [2.134]
 [2.134]
 [2.134]
 [2.134]] [[0.318]
 [0.602]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]]
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.33 ]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[1.697]
 [2.37 ]
 [2.134]
 [2.134]
 [2.134]
 [2.134]
 [2.134]] [[0.205]
 [0.435]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]]
maxi score, test score, baseline:  -0.6941666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  67 total reward:  0.19999999999999885  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.6941666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  56 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.10775013857308391
maxi score, test score, baseline:  -0.6941666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6941666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]] [[1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.951]] [[0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6941666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6941666666666668 0.6910000000000003 0.6910000000000003
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6941666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.879]
 [0.937]
 [0.879]
 [0.879]
 [0.879]
 [0.879]
 [0.879]] [[1.933]
 [1.564]
 [1.933]
 [1.933]
 [1.933]
 [1.933]
 [1.933]] [[0.879]
 [0.937]
 [0.879]
 [0.879]
 [0.879]
 [0.879]
 [0.879]]
actor:  0 policy actor:  0  step number:  51 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.7219907830737456
maxi score, test score, baseline:  -0.6918466666666667 0.6910000000000003 0.6910000000000003
actor:  1 policy actor:  1  step number:  61 total reward:  0.23999999999999921  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
siam score:  -0.7981668
maxi score, test score, baseline:  -0.6918466666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.6888733333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6888733333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6888733333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.79334486
maxi score, test score, baseline:  -0.6888733333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  67 total reward:  0.06666666666666576  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6867400000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6867400000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6737218671256793
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.798]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[0.977]
 [0.517]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]] [[0.692]
 [0.798]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]]
maxi score, test score, baseline:  -0.68674 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  48 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.68414 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.68414 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.204]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[-1.802]
 [-1.343]
 [-1.802]
 [-1.802]
 [-1.802]
 [-1.802]
 [-1.802]] [[0.095]
 [0.204]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.68414 0.6910000000000003 0.6910000000000003
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]] [[2.998]
 [2.998]
 [2.998]
 [2.998]
 [2.998]
 [2.998]
 [2.998]] [[0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]]
actor:  0 policy actor:  0  step number:  42 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  54 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6789133333333334 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6789133333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6789133333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  63 total reward:  0.13333333333333275  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.193]
 [-0.193]
 [-0.193]
 [-0.193]
 [-0.193]
 [-0.193]
 [-0.193]] [[-0.476]
 [-0.476]
 [-0.476]
 [-0.476]
 [-0.476]
 [-0.476]
 [-0.476]] [[-0.375]
 [-0.375]
 [-0.375]
 [-0.375]
 [-0.375]
 [-0.375]
 [-0.375]]
maxi score, test score, baseline:  -0.6766466666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6766466666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.6766466666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6766466666666667 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6766466666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6766466666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6766466666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6766466666666668 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6766466666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  43 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6737666666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.102]
 [-0.102]
 [-0.13 ]
 [-0.102]
 [-0.102]
 [-0.102]
 [-0.102]] [[ 0.   ]
 [ 0.   ]
 [-0.125]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.493]
 [-0.493]
 [-0.542]
 [-0.493]
 [-0.493]
 [-0.493]
 [-0.493]]
maxi score, test score, baseline:  -0.6737666666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.007]
 [ 0.129]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[1.385]
 [2.243]
 [1.385]
 [1.385]
 [1.385]
 [1.385]
 [1.385]] [[-0.341]
 [-0.062]
 [-0.341]
 [-0.341]
 [-0.341]
 [-0.341]
 [-0.341]]
actor:  0 policy actor:  0  step number:  57 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6709933333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.266]
 [-0.266]
 [-0.266]
 [-0.266]
 [-0.266]
 [-0.266]
 [-0.266]] [[-0.635]
 [-0.635]
 [-0.635]
 [-0.635]
 [-0.635]
 [-0.635]
 [-0.635]] [[-0.3]
 [-0.3]
 [-0.3]
 [-0.3]
 [-0.3]
 [-0.3]
 [-0.3]]
maxi score, test score, baseline:  -0.6709933333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6709933333333333 0.6910000000000003 0.6910000000000003
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  65 total reward:  0.10666666666666569  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.078]
 [-0.068]
 [-0.078]
 [-0.078]
 [-0.078]
 [-0.078]
 [-0.078]] [[-0.251]
 [ 0.221]
 [-0.251]
 [-0.251]
 [-0.251]
 [-0.251]
 [-0.251]] [[-0.042]
 [ 0.162]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]]
actor:  1 policy actor:  1  step number:  75 total reward:  0.1999999999999993  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -3.139495849609375e-07
actor:  0 policy actor:  0  step number:  48 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.092]
 [-0.083]
 [-0.116]
 [-0.106]
 [-0.108]
 [-0.101]
 [-0.103]] [[-0.657]
 [-0.629]
 [-1.009]
 [-1.231]
 [-1.222]
 [-0.04 ]
 [-1.223]] [[-0.575]
 [-0.562]
 [-0.658]
 [-0.685]
 [-0.686]
 [-0.481]
 [-0.682]]
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.36 ]
 [0.379]
 [0.369]
 [0.369]
 [0.369]
 [0.369]] [[2.634]
 [2.846]
 [3.145]
 [2.245]
 [2.245]
 [2.245]
 [2.245]] [[-0.086]
 [-0.066]
 [ 0.004]
 [-0.156]
 [-0.156]
 [-0.156]
 [-0.156]]
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[ 1.5  ]
 [-0.128]
 [ 1.5  ]
 [ 1.5  ]
 [ 1.5  ]
 [ 1.5  ]
 [ 1.5  ]] [[ 0.   ]
 [-0.798]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[2.187]
 [0.294]
 [2.187]
 [2.187]
 [2.187]
 [2.187]
 [2.187]]
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  59 total reward:  0.05333333333333268  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.104]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[0.524]
 [1.127]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[0.082]
 [0.41 ]
 [0.082]
 [0.082]
 [0.082]
 [0.082]
 [0.082]]
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.3360549314467702
line 256 mcts: sample exp_bonus -0.36902121865210524
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
Printing some Q and Qe and total Qs values:  [[-0.014]
 [ 0.22 ]
 [ 0.169]
 [ 0.169]
 [ 0.169]
 [ 0.169]
 [ 0.169]] [[1.438]
 [1.785]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]] [[0.115]
 [0.39 ]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]]
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.773]
 [0.525]
 [0.555]
 [0.582]
 [0.547]
 [0.538]] [[1.512]
 [1.455]
 [0.295]
 [0.698]
 [1.295]
 [0.647]
 [0.7  ]] [[0.73 ]
 [0.773]
 [0.525]
 [0.555]
 [0.582]
 [0.547]
 [0.538]]
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.01 ]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]] [[0.293]
 [1.099]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[0.173]
 [0.451]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78970355
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  48 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.79256845
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]] [[3.824]
 [3.824]
 [3.824]
 [3.824]
 [3.824]
 [3.824]
 [3.824]] [[0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]]
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6682333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  51 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6657266666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  69 total reward:  0.1466666666666654  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7980521
maxi score, test score, baseline:  -0.6657266666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6657266666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]] [[3.266]
 [2.734]
 [2.734]
 [2.734]
 [2.734]
 [2.734]
 [2.734]] [[0.752]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4999999999999999  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.247434762995672
siam score:  -0.7966382
actor:  1 policy actor:  1  step number:  44 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.964186096938833
actor:  0 policy actor:  0  step number:  52 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]] [[4.27]
 [4.27]
 [4.27]
 [4.27]
 [4.27]
 [4.27]
 [4.27]] [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]]
siam score:  -0.7983827
Printing some Q and Qe and total Qs values:  [[-0.188]
 [-0.122]
 [-0.287]
 [-0.115]
 [-0.188]
 [-0.28 ]
 [-0.167]] [[ 0.243]
 [-0.102]
 [ 0.068]
 [-2.565]
 [ 0.   ]
 [ 0.077]
 [-0.487]] [[ 0.502]
 [ 0.426]
 [ 0.403]
 [-0.322]
 [ 0.428]
 [ 0.409]
 [ 0.288]]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.03322246138701343
maxi score, test score, baseline:  -0.6602333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6602333333333333 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6570600000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.727]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[0.52 ]
 [0.019]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]] [[0.727]
 [0.727]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]]
actor:  0 policy actor:  0  step number:  31 total reward:  0.6133333333333334  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6538333333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6538333333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6538333333333334 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6538333333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6538333333333334 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6538333333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6538333333333334 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6538333333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.039]
 [-0.039]
 [-0.04 ]] [[1.042]
 [1.033]
 [0.87 ]
 [0.702]
 [0.953]
 [0.664]
 [0.912]] [[-0.114]
 [-0.115]
 [-0.143]
 [-0.171]
 [-0.128]
 [-0.176]
 [-0.135]]
actor:  0 policy actor:  0  step number:  56 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.6513666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6513666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6513666666666668 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6513666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6513666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6513666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6513666666666668 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6513666666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  46 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6486333333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6486333333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7916053
maxi score, test score, baseline:  -0.6486333333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6486333333333334 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6486333333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -4.259705791991877
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6486333333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6486333333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  11091
actor:  0 policy actor:  0  step number:  49 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.015]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[ 0.396]
 [ 0.083]
 [-0.303]
 [-0.303]
 [-0.303]
 [-0.303]
 [-0.303]] [[-0.644]
 [-0.666]
 [-0.752]
 [-0.752]
 [-0.752]
 [-0.752]
 [-0.752]]
maxi score, test score, baseline:  -0.6461800000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6461800000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6461800000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6461800000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  55 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6438600000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6438600000000001 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6438600000000001 0.6910000000000003 0.6910000000000003
actor:  1 policy actor:  1  step number:  55 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  63 total reward:  0.09333333333333249  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.6438600000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6438600000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6438600000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6438600000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  52 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.6438600000000001 0.6910000000000003 0.6910000000000003
siam score:  -0.7898048
maxi score, test score, baseline:  -0.6438600000000001 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  71 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
first move QE:  -0.10296754570460882
maxi score, test score, baseline:  -0.6413266666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6413266666666667 0.6910000000000003 0.6910000000000003
using explorer policy with actor:  1
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.6413266666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6413266666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6413266666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7889998
maxi score, test score, baseline:  -0.6413266666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.447]
 [0.294]
 [0.323]
 [0.289]
 [0.244]
 [0.371]] [[1.658]
 [0.675]
 [1.507]
 [1.927]
 [1.762]
 [2.068]
 [1.614]] [[-0.097]
 [-0.223]
 [-0.1  ]
 [ 0.069]
 [-0.02 ]
 [ 0.038]
 [ 0.013]]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.6413266666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  51 total reward:  0.3599999999999992  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6386066666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6386066666666668 0.6910000000000003 0.6910000000000003
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.303]
 [0.328]
 [0.347]
 [0.349]
 [0.343]
 [0.321]] [[-2.73 ]
 [-1.092]
 [-2.784]
 [-3.029]
 [-2.902]
 [-2.881]
 [-3.001]] [[0.343]
 [0.303]
 [0.328]
 [0.347]
 [0.349]
 [0.343]
 [0.321]]
first move QE:  -0.10998775372519067
maxi score, test score, baseline:  -0.6386066666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[1.484]
 [1.181]
 [1.181]
 [1.181]
 [1.181]
 [1.181]
 [1.181]] [[-0.461]
 [-0.55 ]
 [-0.55 ]
 [-0.55 ]
 [-0.55 ]
 [-0.55 ]
 [-0.55 ]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6386066666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  55 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.6358066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  65 total reward:  0.1333333333333323  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  68 total reward:  0.09999999999999898  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6358066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6358066666666667 0.6910000000000003 0.6910000000000003
Printing some Q and Qe and total Qs values:  [[-0.131]
 [-0.09 ]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]] [[-0.141]
 [-0.2  ]
 [-0.505]
 [-0.505]
 [-0.505]
 [-0.505]
 [-0.505]] [[-0.161]
 [-0.131]
 [-0.202]
 [-0.202]
 [-0.202]
 [-0.202]
 [-0.202]]
maxi score, test score, baseline:  -0.6358066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 8.29054414970465
maxi score, test score, baseline:  -0.6358066666666667 0.6910000000000003 0.6910000000000003
Printing some Q and Qe and total Qs values:  [[-0.111]
 [-0.096]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.115]] [[-0.138]
 [-0.067]
 [ 0.174]
 [ 0.174]
 [ 0.174]
 [ 0.174]
 [-0.232]] [[-0.344]
 [-0.317]
 [-0.293]
 [-0.293]
 [-0.293]
 [-0.293]
 [-0.364]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1751543130968347
maxi score, test score, baseline:  -0.6358066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6358066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  72 total reward:  0.08666666666666523  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6358066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  11091
line 256 mcts: sample exp_bonus -0.7105515757080888
maxi score, test score, baseline:  -0.6358066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6358066666666667 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6358066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6358066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6358066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6358066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  59 total reward:  0.19999999999999907  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.6334066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.403]
 [0.364]
 [0.369]
 [0.377]
 [0.376]
 [0.38 ]] [[-3.911]
 [-2.451]
 [-3.871]
 [-4.469]
 [-4.485]
 [-4.418]
 [-4.039]] [[0.377]
 [0.403]
 [0.364]
 [0.369]
 [0.377]
 [0.376]
 [0.38 ]]
maxi score, test score, baseline:  -0.6334066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.401]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]] [[1.842]
 [1.793]
 [1.842]
 [1.842]
 [1.842]
 [1.842]
 [1.842]] [[-0.167]
 [-0.012]
 [-0.167]
 [-0.167]
 [-0.167]
 [-0.167]
 [-0.167]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.6334066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6334066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6334066666666667 0.6910000000000003 0.6910000000000003
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6334066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6334066666666667 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6334066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6334066666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6334066666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.293746612718233
maxi score, test score, baseline:  -0.6334066666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  56 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.261]
 [0.276]
 [0.265]
 [0.338]
 [0.276]
 [0.276]] [[1.116]
 [0.046]
 [0.428]
 [0.095]
 [0.291]
 [0.428]
 [0.428]] [[0.466]
 [0.261]
 [0.276]
 [0.265]
 [0.338]
 [0.276]
 [0.276]]
maxi score, test score, baseline:  -0.6309933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6309933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6309933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78387934
maxi score, test score, baseline:  -0.6309933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6309933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6309933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6309933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]] [[0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[-0.319]
 [-0.319]
 [-0.319]
 [-0.319]
 [-0.319]
 [-0.319]
 [-0.319]]
maxi score, test score, baseline:  -0.6309933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]] [[1.912]
 [1.912]
 [1.912]
 [1.912]
 [1.912]
 [1.912]
 [1.912]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6278466666666668 0.6910000000000003 0.6910000000000003
actor:  1 policy actor:  1  step number:  63 total reward:  0.05333333333333257  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.6278466666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78430754
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.369]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[3.526]
 [3.408]
 [3.413]
 [3.413]
 [3.413]
 [3.413]
 [3.413]] [[0.665]
 [0.655]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]]
maxi score, test score, baseline:  -0.6278466666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6278466666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6278466666666668 0.6910000000000003 0.6910000000000003
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6278466666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6278466666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6278466666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6278466666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6278466666666668 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  48 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6252733333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6252733333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.702]
 [0.583]
 [0.58 ]
 [0.579]
 [0.618]
 [0.577]] [[-0.427]
 [-0.826]
 [-0.481]
 [-0.637]
 [-0.875]
 [-1.209]
 [-0.567]] [[0.58 ]
 [0.702]
 [0.583]
 [0.58 ]
 [0.579]
 [0.618]
 [0.577]]
maxi score, test score, baseline:  -0.6252733333333335 0.6910000000000003 0.6910000000000003
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.6252733333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6252733333333335 0.6910000000000003 0.6910000000000003
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6252733333333335 0.6910000000000003 0.6910000000000003
actor:  0 policy actor:  0  step number:  58 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[-1.541]
 [-1.541]
 [-1.541]
 [-1.541]
 [-1.541]
 [-1.541]
 [-1.541]] [[0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]]
maxi score, test score, baseline:  -0.6228866666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.6228866666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6228866666666667 0.6910000000000003 0.6910000000000003
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.218]
 [0.581]
 [0.398]
 [0.56 ]
 [0.552]
 [0.236]] [[0.196]
 [1.28 ]
 [1.438]
 [0.672]
 [0.868]
 [1.055]
 [0.723]] [[0.451]
 [0.218]
 [0.581]
 [0.398]
 [0.56 ]
 [0.552]
 [0.236]]
siam score:  -0.7848859
maxi score, test score, baseline:  -0.6228866666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.619]
 [0.604]
 [0.608]
 [0.602]
 [0.597]
 [0.606]] [[1.144]
 [1.962]
 [1.024]
 [1.235]
 [1.294]
 [1.367]
 [1.115]] [[0.605]
 [0.619]
 [0.604]
 [0.608]
 [0.602]
 [0.597]
 [0.606]]
maxi score, test score, baseline:  -0.6228866666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6228866666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6228866666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6228866666666667 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.143 0.571 0.122 0.041 0.02  0.041 0.061]
actor:  0 policy actor:  0  step number:  80 total reward:  0.2466666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6203933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7804679
maxi score, test score, baseline:  -0.6203933333333334 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6203933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6203933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6203933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[2.506]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]] [[ 0.168]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6203933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  53 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.6203933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  84 total reward:  0.08666666666666456  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.6203933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6203933333333334 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.17 ]
 [0.036]
 [0.14 ]
 [0.094]
 [0.109]
 [0.112]] [[3.597]
 [3.069]
 [3.44 ]
 [3.692]
 [3.384]
 [3.697]
 [3.567]] [[0.512]
 [0.326]
 [0.404]
 [0.59 ]
 [0.417]
 [0.571]
 [0.514]]
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.066]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]] [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]] [[-0.436]
 [-0.436]
 [-0.436]
 [-0.436]
 [-0.436]
 [-0.436]
 [-0.436]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3599999999999992  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 2.455748647579779
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.1900570114061313
actor:  1 policy actor:  1  step number:  62 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.77463806
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.12 ]
 [-0.182]
 [-0.12 ]
 [-0.119]
 [-0.119]
 [-0.12 ]
 [-0.119]] [[-4.472]
 [-2.256]
 [-4.529]
 [-4.106]
 [-4.236]
 [-4.344]
 [-4.052]] [[-0.22 ]
 [ 0.26 ]
 [-0.233]
 [-0.137]
 [-0.166]
 [-0.191]
 [-0.125]]
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.055]
 [-0.052]
 [-0.089]
 [-0.091]
 [-0.055]
 [-0.084]
 [-0.055]] [[1.362]
 [1.609]
 [1.301]
 [1.189]
 [1.362]
 [1.123]
 [1.362]] [[-0.438]
 [-0.394]
 [-0.483]
 [-0.504]
 [-0.438]
 [-0.508]
 [-0.438]]
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  0.119999999999999  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.075]
 [-0.045]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]] [[-0.195]
 [ 0.098]
 [ 0.218]
 [ 0.218]
 [ 0.218]
 [ 0.218]
 [ 0.218]] [[-0.091]
 [ 0.077]
 [ 0.098]
 [ 0.098]
 [ 0.098]
 [ 0.098]
 [ 0.098]]
Starting evaluation
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[-0.631]
 [-0.631]
 [-0.631]
 [-0.631]
 [-0.631]
 [-0.631]
 [-0.631]] [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]]
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]] [[0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]] [[0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.12]
 [0.12]
 [0.12]
 [0.12]
 [0.12]
 [0.12]
 [0.12]] [[3.001]
 [3.001]
 [3.001]
 [3.001]
 [3.001]
 [3.001]
 [3.001]] [[0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]]
maxi score, test score, baseline:  -0.6203933333333335 0.6910000000000003 0.6910000000000003
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.819684861621822
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.872]
 [0.744]
 [0.752]
 [0.711]
 [0.653]
 [0.73 ]] [[3.83 ]
 [1.41 ]
 [1.979]
 [2.881]
 [2.561]
 [3.176]
 [2.394]] [[0.721]
 [0.872]
 [0.744]
 [0.752]
 [0.711]
 [0.653]
 [0.73 ]]
siam score:  -0.7701593
line 256 mcts: sample exp_bonus 0.1747249677151605
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5527666666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5527666666666667 0.6906666666666668 0.6906666666666668
actor:  0 policy actor:  0  step number:  46 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.5500333333333335 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.143]
 [-0.141]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]] [[-0.614]
 [ 0.276]
 [-0.614]
 [-0.614]
 [-0.614]
 [-0.614]
 [-0.614]] [[0.559]
 [0.706]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]]
maxi score, test score, baseline:  -0.5500333333333335 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.5500333333333335 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  65 total reward:  0.07999999999999918  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  54 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.5474333333333333 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5474333333333333 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5474333333333333 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5474333333333333 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5474333333333333 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5474333333333333 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5474333333333333 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5474333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.27]
 [-0.27]
 [-0.27]
 [-0.27]
 [-0.27]
 [-0.27]
 [-0.27]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 2.581528226664617
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.6918175991538913
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.613]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[4.477]
 [3.938]
 [4.2  ]
 [4.2  ]
 [4.2  ]
 [4.2  ]
 [4.2  ]] [[0.784]
 [0.76 ]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]]
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.024]
 [-0.024]
 [-0.033]
 [-0.024]
 [-0.024]
 [-0.04 ]] [[4.882]
 [5.351]
 [5.351]
 [4.97 ]
 [5.351]
 [5.351]
 [4.688]] [[0.624]
 [0.734]
 [0.734]
 [0.642]
 [0.734]
 [0.734]
 [0.573]]
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.316]
 [0.364]
 [0.351]
 [0.353]
 [0.369]
 [0.348]] [[1.481]
 [0.97 ]
 [1.421]
 [1.189]
 [1.247]
 [1.899]
 [1.297]] [[0.372]
 [0.316]
 [0.364]
 [0.351]
 [0.353]
 [0.369]
 [0.348]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.255]
 [0.363]
 [0.366]
 [0.36 ]
 [0.361]
 [0.359]] [[1.386]
 [1.088]
 [1.504]
 [1.499]
 [1.618]
 [1.739]
 [1.304]] [[0.377]
 [0.255]
 [0.363]
 [0.366]
 [0.36 ]
 [0.361]
 [0.359]]
actor:  1 policy actor:  1  step number:  100 total reward:  0.07333333333333103  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.3577921790531835
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.54462 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  0 policy actor:  0  step number:  57 total reward:  0.03999999999999937  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5425400000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5425400000000001 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5425400000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5425400000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7662391
maxi score, test score, baseline:  -0.5425400000000001 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5425400000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5425400000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5425400000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7682182
actor:  1 policy actor:  1  step number:  46 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.5425400000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  57 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7684772
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[-0.208]
 [-0.155]
 [-0.284]
 [-0.126]
 [-0.144]
 [-0.254]
 [-0.138]] [[-0.493]
 [-1.376]
 [-0.096]
 [-1.718]
 [-2.471]
 [-0.288]
 [-1.318]] [[-0.28 ]
 [-0.375]
 [-0.29 ]
 [-0.403]
 [-0.547]
 [-0.292]
 [-0.348]]
maxi score, test score, baseline:  -0.5399266666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.5399266666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5399266666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5399266666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5399266666666668 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5399266666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5399266666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5399266666666668 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5399266666666668 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5399266666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.752]
 [0.682]
 [0.643]
 [0.646]
 [0.654]
 [0.545]] [[1.328]
 [1.047]
 [1.408]
 [1.409]
 [1.426]
 [1.477]
 [2.548]] [[0.13 ]
 [0.093]
 [0.173]
 [0.145]
 [0.153]
 [0.177]
 [0.486]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.5369666666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.559]
 [0.267]
 [0.35 ]
 [0.392]
 [0.252]
 [0.54 ]] [[ 0.217]
 [-0.314]
 [ 0.141]
 [-0.823]
 [-1.805]
 [-0.635]
 [-0.497]] [[0.212]
 [0.559]
 [0.267]
 [0.35 ]
 [0.392]
 [0.252]
 [0.54 ]]
siam score:  -0.76682276
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5369666666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5369666666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  50 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.123]
 [-0.109]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]] [[-3.754]
 [-2.969]
 [-3.754]
 [-3.754]
 [-3.754]
 [-3.754]
 [-3.754]] [[0.229]
 [0.343]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.731]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[0.35 ]
 [1.076]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[0.695]
 [0.731]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]]
siam score:  -0.7638389
actor:  0 policy actor:  0  step number:  29 total reward:  0.6133333333333334  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  49 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.5282466666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  68 total reward:  0.11333333333333229  reward:  1.0 rdn_beta:  0.5
siam score:  -0.76502556
Printing some Q and Qe and total Qs values:  [[-0.148]
 [-0.095]
 [-0.163]
 [-0.121]
 [-0.146]
 [-0.146]
 [-0.126]] [[ 1.873]
 [ 0.36 ]
 [ 0.176]
 [-0.12 ]
 [-0.375]
 [-0.375]
 [-0.129]] [[-0.056]
 [-0.256]
 [-0.354]
 [-0.361]
 [-0.429]
 [-0.429]
 [-0.368]]
maxi score, test score, baseline:  -0.5282466666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5282466666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  45 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.5256333333333334 0.6906666666666668 0.6906666666666668
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5256333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5256333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.5256333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5256333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5256333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.187]
 [-0.094]
 [-0.12 ]
 [-0.12 ]
 [-0.12 ]
 [-0.12 ]
 [-0.12 ]] [[ 0.028]
 [-2.248]
 [-2.388]
 [-2.388]
 [-2.388]
 [-2.388]
 [-2.388]] [[-0.024]
 [-0.312]
 [-0.36 ]
 [-0.36 ]
 [-0.36 ]
 [-0.36 ]
 [-0.36 ]]
maxi score, test score, baseline:  -0.5256333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5256333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  58 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.058]
 [-0.038]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]] [[2.281]
 [2.189]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[ 0.225]
 [ 0.23 ]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]]
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[2.38]
 [2.38]
 [2.38]
 [2.38]
 [2.38]
 [2.38]
 [2.38]] [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
actor:  0 policy actor:  0  step number:  51 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.5208733333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5208733333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5208733333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5208733333333334 0.6906666666666668 0.6906666666666668
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.256]
 [0.259]] [[3.793]
 [4.034]
 [4.034]
 [4.034]
 [4.034]
 [3.886]
 [4.034]] [[0.405]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.444]
 [0.504]]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.363]
 [0.276]
 [0.231]
 [0.25 ]
 [0.246]
 [0.251]] [[3.438]
 [3.445]
 [3.625]
 [3.713]
 [3.666]
 [4.044]
 [3.819]] [[0.268]
 [0.358]
 [0.36 ]
 [0.359]
 [0.356]
 [0.498]
 [0.415]]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.5208733333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]] [[1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.232]] [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]]
maxi score, test score, baseline:  -0.5208733333333334 0.6906666666666668 0.6906666666666668
actor:  1 policy actor:  1  step number:  63 total reward:  0.09333333333333238  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  52 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[2.125]
 [2.125]
 [2.125]
 [2.125]
 [2.125]
 [2.125]
 [2.125]] [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]]
maxi score, test score, baseline:  -0.5184066666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.668]
 [0.488]
 [0.665]
 [0.665]
 [0.485]
 [0.665]] [[0.566]
 [1.104]
 [0.542]
 [1.147]
 [1.147]
 [0.473]
 [1.147]] [[0.532]
 [0.668]
 [0.488]
 [0.665]
 [0.665]
 [0.485]
 [0.665]]
maxi score, test score, baseline:  -0.5184066666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.5184066666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  43 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.154]
 [-0.15 ]
 [-0.124]
 [-0.154]
 [-0.154]
 [-0.154]
 [-0.154]] [[ 0.183]
 [-0.119]
 [-0.66 ]
 [ 0.183]
 [ 0.183]
 [ 0.183]
 [ 0.183]] [[-0.43 ]
 [-0.526]
 [-0.68 ]
 [-0.43 ]
 [-0.43 ]
 [-0.43 ]
 [-0.43 ]]
maxi score, test score, baseline:  -0.5155000000000002 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5155000000000002 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5155000000000002 0.6906666666666668 0.6906666666666668
siam score:  -0.7778476
maxi score, test score, baseline:  -0.5155000000000002 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5155000000000002 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.5155000000000002 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5155000000000002 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.723]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]] [[5.479]
 [4.675]
 [5.479]
 [5.479]
 [5.479]
 [5.479]
 [5.479]] [[0.706]
 [0.723]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]]
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.762]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]] [[2.563]
 [3.153]
 [2.563]
 [2.563]
 [2.563]
 [2.563]
 [2.563]] [[0.686]
 [0.762]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]]
siam score:  -0.7678348
actor:  0 policy actor:  0  step number:  50 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5130333333333335 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5130333333333335 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5130333333333335 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.11 ]
 [-0.224]
 [-0.116]
 [-0.114]
 [-0.113]
 [-0.113]
 [-0.109]] [[-3.698]
 [-1.504]
 [-3.4  ]
 [-3.236]
 [-3.305]
 [-3.657]
 [-3.817]] [[-0.024]
 [ 0.572]
 [ 0.062]
 [ 0.111]
 [ 0.091]
 [-0.013]
 [-0.058]]
maxi score, test score, baseline:  -0.5130333333333335 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.767632
maxi score, test score, baseline:  -0.5130333333333335 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5130333333333335 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  61 total reward:  0.2666666666666657  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5130333333333335 0.6906666666666668 0.6906666666666668
actor:  1 policy actor:  1  step number:  58 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5130333333333335 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5130333333333335 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.347]
 [0.3  ]
 [0.213]
 [0.213]
 [0.065]
 [0.036]] [[4.03 ]
 [5.131]
 [4.013]
 [4.03 ]
 [4.03 ]
 [4.012]
 [3.762]] [[0.262]
 [0.647]
 [0.306]
 [0.262]
 [0.262]
 [0.174]
 [0.087]]
maxi score, test score, baseline:  -0.5130333333333335 0.6906666666666668 0.6906666666666668
actor:  1 policy actor:  1  step number:  55 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5130333333333335 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  67 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.5130333333333335 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5130333333333335 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5130333333333335 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.29844117768571005
maxi score, test score, baseline:  -0.5130333333333333 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.951]
 [0.992]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.917]] [[2.678]
 [1.971]
 [4.003]
 [4.003]
 [4.003]
 [4.003]
 [2.684]] [[0.951]
 [0.992]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.917]]
maxi score, test score, baseline:  -0.5130333333333333 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.667
siam score:  -0.77015686
maxi score, test score, baseline:  -0.5101666666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5101666666666668 0.6906666666666668 0.6906666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5101666666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  52 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.5101666666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.978]
 [0.978]
 [0.978]
 [0.978]
 [0.978]
 [0.978]
 [0.978]] [[1.624]
 [1.624]
 [1.624]
 [1.624]
 [1.624]
 [1.624]
 [1.624]] [[0.978]
 [0.978]
 [0.978]
 [0.978]
 [0.978]
 [0.978]
 [0.978]]
siam score:  -0.767051
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]] [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]]
siam score:  -0.7699363
maxi score, test score, baseline:  -0.5101666666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5101666666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5101666666666668 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5101666666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.13947987629370984
maxi score, test score, baseline:  -0.5101666666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5101666666666668 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5101666666666668 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5101666666666667 0.6906666666666668 0.6906666666666668
actor:  0 policy actor:  0  step number:  46 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5074333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.811]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]] [[-0.525]
 [-0.272]
 [-0.525]
 [-0.525]
 [-0.525]
 [-0.525]
 [-0.525]] [[0.763]
 [0.811]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]]
maxi score, test score, baseline:  -0.5074333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7727976187601686
maxi score, test score, baseline:  -0.5074333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5074333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.829]
 [0.745]
 [0.737]
 [0.852]
 [0.733]
 [0.852]] [[3.651]
 [2.731]
 [3.577]
 [3.531]
 [2.454]
 [3.824]
 [2.454]] [[0.717]
 [0.829]
 [0.745]
 [0.737]
 [0.852]
 [0.733]
 [0.852]]
maxi score, test score, baseline:  -0.5074333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.5074333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5074333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5074333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.689]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]] [[0.   ]
 [0.337]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.587]
 [0.689]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]]
maxi score, test score, baseline:  -0.5074333333333334 0.6906666666666668 0.6906666666666668
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.657]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[-0.701]
 [ 0.413]
 [-0.701]
 [-0.701]
 [-0.701]
 [-0.701]
 [-0.701]] [[0.608]
 [0.657]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]]
actor:  0 policy actor:  0  step number:  48 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5049133333333334 0.6906666666666668 0.6906666666666668
actor:  0 policy actor:  0  step number:  52 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.156]
 [-0.171]
 [-0.147]
 [-0.211]
 [-0.211]
 [-0.083]
 [-0.201]] [[0.713]
 [1.209]
 [1.09 ]
 [1.356]
 [1.356]
 [1.344]
 [1.165]] [[-0.108]
 [ 0.023]
 [-0.   ]
 [ 0.047]
 [ 0.047]
 [ 0.097]
 [-0.002]]
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.268]
 [0.174]] [[2.274]
 [2.274]
 [2.274]
 [2.274]
 [2.274]
 [1.932]
 [2.274]] [[0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.17]
 [0.19]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  56 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]] [[4.423]
 [4.423]
 [4.423]
 [4.423]
 [4.423]
 [4.423]
 [4.423]] [[0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.528]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]] [[4.806]
 [4.303]
 [4.806]
 [4.806]
 [4.806]
 [4.806]
 [4.806]] [[0.438]
 [0.411]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]]
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]] [[0.246]
 [0.199]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]] [[-0.518]
 [-0.532]
 [-0.518]
 [-0.518]
 [-0.518]
 [-0.518]
 [-0.518]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.49033543093664006
actor:  1 policy actor:  1  step number:  44 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0811],
        [-0.2740],
        [ 0.7574],
        [-0.0000],
        [-0.0425],
        [ 0.2816],
        [ 0.2654],
        [-0.4474],
        [ 0.3398],
        [-0.5938]], dtype=torch.float64)
-0.071422513866 -0.15247370409762764
-0.083839701198 -0.3578806126872565
-0.084359833866 0.6730335367857101
-0.20443304138399937 -0.20443304138399937
-0.08423175439800001 -0.12674323963803144
-0.057834381198 0.2237562248973448
-0.058094434398 0.20726724442331834
-0.070771701198 -0.5181382955352531
-0.09703970119800001 0.24273760789613893
-0.04528388706599999 -0.6390755780026712
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]] [[1.22]
 [1.22]
 [1.22]
 [1.22]
 [1.22]
 [1.22]
 [1.22]] [[-0.356]
 [-0.356]
 [-0.356]
 [-0.356]
 [-0.356]
 [-0.356]
 [-0.356]]
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
line 256 mcts: sample exp_bonus 1.3866242105876212
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.16 ]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]] [[1.943]
 [2.497]
 [2.228]
 [2.228]
 [2.228]
 [2.228]
 [2.228]] [[0.299]
 [0.45 ]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.167
siam score:  -0.76366377
using explorer policy with actor:  1
siam score:  -0.7603827
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.5023133333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  51 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4998600000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4998600000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  76 total reward:  0.12666666666666582  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.4998600000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.4998600000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]] [[0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]] [[0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]]
actor:  0 policy actor:  0  step number:  51 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75566375
maxi score, test score, baseline:  -0.49711333333333335 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  45 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  51 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4919666666666667 0.6906666666666668 0.6906666666666668
actor:  0 policy actor:  0  step number:  73 total reward:  0.05333333333333168  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.48986000000000013 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.48986000000000013 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.48986000000000013 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76390696
maxi score, test score, baseline:  -0.48986000000000013 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  88 total reward:  0.04666666666666508  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.559]
 [0.592]
 [0.553]
 [0.344]
 [0.441]
 [0.498]] [[0.783]
 [0.846]
 [1.845]
 [1.099]
 [0.623]
 [1.319]
 [1.815]] [[0.527]
 [0.559]
 [0.592]
 [0.553]
 [0.344]
 [0.441]
 [0.498]]
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.942]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]] [[4.243]
 [2.947]
 [4.243]
 [4.243]
 [4.243]
 [4.243]
 [4.243]] [[0.786]
 [0.942]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  58 total reward:  0.11333333333333262  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4876333333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  49 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.4851800000000001 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.4851800000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4851800000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4851800000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]] [[2.096]
 [2.096]
 [2.096]
 [2.096]
 [2.096]
 [2.096]
 [2.096]] [[0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]]
maxi score, test score, baseline:  -0.4851800000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  48 total reward:  0.4466666666666663  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.142]
 [-0.064]
 [-0.119]
 [-0.093]
 [-0.082]
 [-0.096]
 [-0.093]] [[-0.066]
 [-0.397]
 [-0.398]
 [ 0.   ]
 [-1.593]
 [-0.915]
 [ 0.   ]] [[-0.363]
 [-0.395]
 [-0.45 ]
 [-0.291]
 [-0.811]
 [-0.599]
 [-0.291]]
Printing some Q and Qe and total Qs values:  [[-0.111]
 [-0.083]
 [-0.19 ]
 [-0.119]
 [-0.119]
 [-0.119]
 [-0.162]] [[-0.131]
 [-0.238]
 [-0.155]
 [-0.398]
 [-0.398]
 [-0.398]
 [-0.055]] [[-0.294]
 [-0.302]
 [-0.381]
 [-0.39 ]
 [-0.39 ]
 [-0.39 ]
 [-0.319]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.4822866666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4822866666666668 0.6906666666666668 0.6906666666666668
actor:  0 policy actor:  0  step number:  68 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.48011333333333345 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.48011333333333345 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.48011333333333345 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.48011333333333345 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.48011333333333345 0.6906666666666668 0.6906666666666668
Printing some Q and Qe and total Qs values:  [[-0.197]
 [-0.107]
 [-0.284]
 [-0.138]
 [-0.138]
 [-0.138]
 [-0.138]] [[-0.066]
 [-1.175]
 [-0.452]
 [-1.308]
 [-1.308]
 [-1.308]
 [-1.308]] [[0.682]
 [0.455]
 [0.561]
 [0.413]
 [0.413]
 [0.413]
 [0.413]]
Printing some Q and Qe and total Qs values:  [[-0.176]
 [-0.075]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]] [[-0.095]
 [-1.03 ]
 [-1.037]
 [-1.037]
 [-1.037]
 [-1.037]
 [-1.037]] [[0.599]
 [0.411]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]]
maxi score, test score, baseline:  -0.48011333333333345 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.48011333333333345 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.48011333333333345 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.156]
 [0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[1.801]
 [2.444]
 [1.801]
 [1.801]
 [1.801]
 [1.801]
 [1.801]] [[-0.029]
 [ 0.247]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]]
maxi score, test score, baseline:  -0.48011333333333345 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75485533
Printing some Q and Qe and total Qs values:  [[0.892]
 [0.938]
 [0.853]
 [0.785]
 [0.893]
 [0.746]
 [0.746]] [[1.779]
 [0.535]
 [2.155]
 [1.956]
 [0.842]
 [1.46 ]
 [1.46 ]] [[0.892]
 [0.938]
 [0.853]
 [0.785]
 [0.893]
 [0.746]
 [0.746]]
actor:  0 policy actor:  0  step number:  40 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4772200000000001 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.375762453259143
actor:  0 policy actor:  0  step number:  44 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  43 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.47155333333333344 0.6906666666666668 0.6906666666666668
Printing some Q and Qe and total Qs values:  [[-0.03 ]
 [-0.004]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]] [[0.553]
 [1.372]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]] [[-0.514]
 [-0.274]
 [-0.514]
 [-0.514]
 [-0.514]
 [-0.514]
 [-0.514]]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  77 total reward:  0.013333333333331976  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.47155333333333344 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.47155333333333344 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.47155333333333344 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.47155333333333344 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.47155333333333344 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  -0.37842309762747645
maxi score, test score, baseline:  -0.47155333333333344 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.47155333333333344 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7600427
maxi score, test score, baseline:  -0.47155333333333344 0.6906666666666668 0.6906666666666668
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.913]
 [0.921]
 [0.883]
 [0.918]
 [0.918]
 [0.895]] [[3.122]
 [2.766]
 [3.303]
 [2.973]
 [2.992]
 [2.992]
 [3.019]] [[0.863]
 [0.913]
 [0.921]
 [0.883]
 [0.918]
 [0.918]
 [0.895]]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.788]
 [0.699]
 [0.696]
 [0.698]
 [0.695]
 [0.705]] [[-0.398]
 [-0.343]
 [-0.693]
 [-0.764]
 [-0.758]
 [-0.794]
 [-0.671]] [[0.739]
 [0.788]
 [0.699]
 [0.696]
 [0.698]
 [0.695]
 [0.705]]
maxi score, test score, baseline:  -0.47155333333333344 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.47155333333333344 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  49 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.46902000000000005 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.047]
 [-0.022]
 [-0.051]
 [-0.054]
 [-0.054]
 [-0.056]
 [-0.052]] [[-0.168]
 [ 0.123]
 [-0.546]
 [-0.305]
 [-0.305]
 [-0.678]
 [-0.405]] [[-0.065]
 [ 0.057]
 [-0.208]
 [-0.12 ]
 [-0.12 ]
 [-0.26 ]
 [-0.156]]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.585]
 [0.555]
 [0.553]
 [0.554]
 [0.554]
 [0.554]] [[-1.581]
 [-0.647]
 [-0.967]
 [-1.009]
 [-0.94 ]
 [-0.94 ]
 [-0.94 ]] [[0.555]
 [0.585]
 [0.555]
 [0.553]
 [0.554]
 [0.554]
 [0.554]]
maxi score, test score, baseline:  -0.46902000000000005 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.46902000000000005 0.6906666666666668 0.6906666666666668
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.619]
 [0.467]
 [0.499]
 [0.485]
 [0.368]
 [0.544]] [[ 1.551]
 [ 1.541]
 [ 0.303]
 [ 0.405]
 [ 0.842]
 [-0.466]
 [ 0.755]] [[0.588]
 [0.619]
 [0.467]
 [0.499]
 [0.485]
 [0.368]
 [0.544]]
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]] [[1.605]
 [1.605]
 [1.605]
 [1.605]
 [1.605]
 [1.605]
 [1.605]] [[0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.783]
 [0.631]
 [0.634]
 [0.628]
 [0.636]
 [0.633]] [[2.113]
 [1.864]
 [1.856]
 [2.381]
 [1.994]
 [1.702]
 [1.96 ]] [[0.634]
 [0.783]
 [0.631]
 [0.634]
 [0.628]
 [0.636]
 [0.633]]
maxi score, test score, baseline:  -0.46902000000000005 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.46902000000000005 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.46902000000000005 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.46902000000000005 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.46902000000000005 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  53 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.46664666666666677 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  42 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.46386000000000016 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  11091
actor:  0 policy actor:  0  step number:  42 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.46115333333333347 0.6906666666666668 0.6906666666666668
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.855]
 [0.794]
 [0.728]
 [0.794]
 [0.794]
 [0.794]] [[2.471]
 [3.374]
 [2.471]
 [3.704]
 [2.471]
 [2.471]
 [2.471]] [[0.794]
 [0.855]
 [0.794]
 [0.728]
 [0.794]
 [0.794]
 [0.794]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2819703734510586
actor:  0 policy actor:  0  step number:  51 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4585933333333334 0.6906666666666668 0.6906666666666668
start point for exploration sampling:  11091
Printing some Q and Qe and total Qs values:  [[ 0.034]
 [-0.002]
 [ 0.034]
 [ 0.034]
 [ 0.034]
 [ 0.034]
 [ 0.034]] [[1.715]
 [2.288]
 [1.715]
 [1.715]
 [1.715]
 [1.715]
 [1.715]] [[-0.36]
 [-0.3 ]
 [-0.36]
 [-0.36]
 [-0.36]
 [-0.36]
 [-0.36]]
siam score:  -0.759097
maxi score, test score, baseline:  -0.4585933333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.161]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]] [[0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]] [[0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]]
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.718]
 [0.549]
 [0.549]
 [0.549]
 [0.509]
 [0.549]] [[1.45 ]
 [0.995]
 [1.623]
 [1.623]
 [1.623]
 [1.883]
 [1.623]] [[0.31 ]
 [0.168]
 [0.104]
 [0.104]
 [0.104]
 [0.107]
 [0.104]]
maxi score, test score, baseline:  -0.4585933333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  71 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.4585933333333334 0.6906666666666668 0.6906666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4585933333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4585933333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4585933333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  69 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.4585933333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4585933333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4585933333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  56 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.45618000000000003 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.45618000000000003 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.368]
 [0.345]
 [0.354]
 [0.354]
 [0.358]
 [0.368]] [[1.665]
 [2.439]
 [2.117]
 [1.798]
 [1.833]
 [1.82 ]
 [2.01 ]] [[-0.167]
 [-0.039]
 [-0.115]
 [-0.16 ]
 [-0.155]
 [-0.152]
 [-0.111]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
siam score:  -0.75923365
actor:  1 policy actor:  1  step number:  69 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.45618000000000003 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.45618000000000003 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  50 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.4534466666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.009]
 [-0.048]
 [-0.031]
 [-0.029]
 [-0.031]
 [-0.031]] [[1.337]
 [1.474]
 [0.951]
 [1.365]
 [1.229]
 [1.365]
 [1.126]] [[-0.093]
 [-0.044]
 [-0.23 ]
 [-0.095]
 [-0.132]
 [-0.095]
 [-0.164]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.4534466666666668 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.4534466666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.242]
 [0.141]
 [0.094]
 [0.253]
 [0.128]
 [0.12 ]] [[1.745]
 [1.345]
 [1.537]
 [1.81 ]
 [1.22 ]
 [1.736]
 [1.362]] [[-0.298]
 [-0.249]
 [-0.318]
 [-0.319]
 [-0.259]
 [-0.298]
 [-0.368]]
maxi score, test score, baseline:  -0.4534466666666668 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.4534466666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4534466666666668 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.4534466666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4534466666666668 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.027]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]] [[-0.066]
 [ 3.141]
 [ 0.123]
 [ 0.123]
 [ 0.123]
 [ 0.123]
 [ 0.123]] [[-0.329]
 [ 0.178]
 [-0.304]
 [-0.304]
 [-0.304]
 [-0.304]
 [-0.304]]
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
actor:  1 policy actor:  1  step number:  80 total reward:  0.07333333333333159  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.4939243769667447
actor:  1 policy actor:  1  step number:  50 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  50 total reward:  0.23333333333333306  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.288]
 [0.232]
 [0.236]
 [0.236]
 [0.223]
 [0.236]] [[2.191]
 [3.45 ]
 [1.909]
 [2.191]
 [2.191]
 [1.122]
 [2.191]] [[-0.286]
 [-0.024]
 [-0.337]
 [-0.286]
 [-0.286]
 [-0.478]
 [-0.286]]
siam score:  -0.7588064
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4502466666666667 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  43 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.44750000000000006 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  0.17999999999999972  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.44750000000000006 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.44750000000000006 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2466666666666658  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.44750000000000006 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.76 ]
 [0.7  ]
 [0.686]
 [0.672]
 [0.671]
 [0.708]] [[1.471]
 [0.934]
 [1.157]
 [1.057]
 [1.29 ]
 [1.457]
 [1.198]] [[0.692]
 [0.76 ]
 [0.7  ]
 [0.686]
 [0.672]
 [0.671]
 [0.708]]
maxi score, test score, baseline:  -0.44750000000000006 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.44750000000000006 0.6906666666666668 0.6906666666666668
actor:  0 policy actor:  0  step number:  41 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4446733333333334 0.6906666666666668 0.6906666666666668
actor:  1 policy actor:  1  step number:  48 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.4446733333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4446733333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4446733333333334 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.4446733333333334 0.6906666666666668 0.6906666666666668
actor:  1 policy actor:  1  step number:  60 total reward:  0.17999999999999916  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.628]
 [0.505]
 [0.521]
 [0.516]
 [0.513]
 [0.566]] [[1.031]
 [0.081]
 [0.535]
 [0.892]
 [0.763]
 [0.77 ]
 [0.611]] [[0.536]
 [0.628]
 [0.505]
 [0.521]
 [0.516]
 [0.513]
 [0.566]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.014]
 [ 0.044]
 [-0.002]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.023]] [[1.793]
 [1.552]
 [1.196]
 [1.793]
 [1.793]
 [1.793]
 [1.243]] [[-0.473]
 [-0.456]
 [-0.561]
 [-0.473]
 [-0.473]
 [-0.473]
 [-0.575]]
maxi score, test score, baseline:  -0.4446733333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.4446733333333334 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.4446733333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76175505
first move QE:  -0.4141243074485052
siam score:  -0.7588466
actor:  1 policy actor:  1  step number:  57 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.4446733333333334 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.4446733333333334 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  55 total reward:  0.17333333333333256  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]] [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]]
maxi score, test score, baseline:  -0.44232666666666676 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.44232666666666676 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.44232666666666676 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  65 total reward:  0.13333333333333253  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.713]
 [0.603]
 [0.636]
 [0.578]
 [0.603]
 [0.603]] [[1.43 ]
 [1.093]
 [3.143]
 [1.379]
 [1.629]
 [3.143]
 [3.143]] [[0.159]
 [0.122]
 [0.832]
 [0.186]
 [0.248]
 [0.832]
 [0.832]]
maxi score, test score, baseline:  -0.44232666666666676 0.6906666666666668 0.6906666666666668
actor:  1 policy actor:  1  step number:  52 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.44232666666666676 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.44232666666666676 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.44232666666666676 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.8460860813897326
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  0.167
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.792]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[0.65 ]
 [0.003]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]] [[0.549]
 [0.792]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.786]
 [0.657]
 [0.495]
 [0.504]
 [0.657]
 [0.612]] [[1.284]
 [0.565]
 [1.488]
 [0.48 ]
 [1.222]
 [1.488]
 [1.335]] [[0.666]
 [0.786]
 [0.657]
 [0.495]
 [0.504]
 [0.657]
 [0.612]]
maxi score, test score, baseline:  -0.44232666666666676 0.6906666666666668 0.6906666666666668
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]] [[4.19 ]
 [4.204]
 [4.204]
 [4.204]
 [4.204]
 [4.204]
 [4.204]] [[0.654]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]]
rdn beta is 0 so we're just using the maxi policy
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.610830245402921
line 256 mcts: sample exp_bonus 1.9281731028414741
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  37 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.44232666666666676 0.6906666666666668 0.6906666666666668
maxi score, test score, baseline:  -0.44232666666666676 0.6906666666666668 0.6906666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  40 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.013]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[1.512]
 [0.665]
 [1.632]
 [1.632]
 [1.632]
 [1.632]
 [1.632]] [[-0.139]
 [-0.28 ]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]]
maxi score, test score, baseline:  -0.37166000000000005 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.345]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]] [[1.864]
 [1.91 ]
 [1.686]
 [1.686]
 [1.686]
 [1.686]
 [1.686]] [[0.256]
 [0.154]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]]
maxi score, test score, baseline:  -0.37166000000000005 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  50 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.51  0.265 0.082 0.02  0.02  0.041 0.061]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
siam score:  -0.75204635
maxi score, test score, baseline:  -0.37166000000000005 0.6956666666666668 0.6956666666666668
actor:  0 policy actor:  0  step number:  42 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.112]] [[-0.253]
 [-0.253]
 [-0.253]
 [-0.253]
 [-0.253]
 [-0.253]
 [-0.253]]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75453657
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  67 total reward:  0.2533333333333321  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
actor:  1 policy actor:  1  step number:  43 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  75 total reward:  0.0666666666666651  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.224]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[1.756]
 [2.158]
 [1.756]
 [1.756]
 [1.756]
 [1.756]
 [1.756]] [[-0.048]
 [ 0.238]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  11091
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.407]
 [0.278]
 [0.239]
 [0.257]
 [0.231]
 [0.242]] [[2.427]
 [1.864]
 [1.419]
 [1.261]
 [1.304]
 [1.065]
 [1.767]] [[ 0.13 ]
 [ 0.026]
 [-0.177]
 [-0.243]
 [-0.217]
 [-0.284]
 [-0.156]]
Printing some Q and Qe and total Qs values:  [[-0.156]
 [-0.082]
 [-0.28 ]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.075]] [[-0.533]
 [-1.094]
 [-0.414]
 [-2.371]
 [-2.371]
 [-2.371]
 [-0.713]] [[ 0.286]
 [ 0.219]
 [ 0.236]
 [-0.054]
 [-0.054]
 [-0.054]
 [ 0.299]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.738]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[-0.503]
 [ 0.561]
 [-0.503]
 [-0.503]
 [-0.503]
 [-0.503]
 [-0.503]] [[0.614]
 [0.738]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]]
Printing some Q and Qe and total Qs values:  [[ 0.035]
 [ 0.173]
 [ 0.115]
 [ 0.035]
 [-0.022]
 [ 0.035]
 [ 0.03 ]] [[1.436]
 [1.724]
 [1.843]
 [1.436]
 [0.825]
 [1.436]
 [1.681]] [[-0.131]
 [ 0.12 ]
 [ 0.121]
 [-0.131]
 [-0.452]
 [-0.131]
 [-0.026]]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.557]
 [0.404]
 [0.441]
 [0.441]
 [0.315]
 [0.441]] [[3.301]
 [3.473]
 [2.816]
 [3.021]
 [3.021]
 [2.643]
 [3.021]] [[0.535]
 [0.627]
 [0.377]
 [0.449]
 [0.449]
 [0.285]
 [0.449]]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36895333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3659000000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  91288 train batches done:  10693 episodes:  2803
maxi score, test score, baseline:  -0.3659000000000001 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3659000000000001 0.6956666666666668 0.6956666666666668
actor:  0 policy actor:  0  step number:  44 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.36324666666666683 0.6956666666666668 0.6956666666666668
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.915]
 [0.767]
 [0.798]
 [0.767]
 [0.767]
 [0.767]] [[1.248]
 [2.992]
 [1.248]
 [1.213]
 [1.248]
 [1.248]
 [1.248]] [[0.767]
 [0.915]
 [0.767]
 [0.798]
 [0.767]
 [0.767]
 [0.767]]
maxi score, test score, baseline:  -0.36324666666666683 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.36324666666666683 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.89 ]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.697]] [[3.944]
 [2.118]
 [4.067]
 [4.067]
 [4.067]
 [4.067]
 [3.406]] [[0.711]
 [0.89 ]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.697]]
actor:  0 policy actor:  0  step number:  43 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.36034000000000005 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36034000000000005 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36034000000000005 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.386]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[3.17 ]
 [3.367]
 [3.17 ]
 [3.17 ]
 [3.17 ]
 [3.17 ]
 [3.17 ]] [[0.44 ]
 [0.532]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]]
maxi score, test score, baseline:  -0.36034000000000005 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36034000000000005 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36034000000000005 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36034000000000005 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.36034000000000005 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.36034000000000005 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.36034000000000005 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7654297
actor:  0 policy actor:  0  step number:  64 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.35800666666666675 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.35800666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.512]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[3.138]
 [2.672]
 [2.893]
 [2.893]
 [2.893]
 [2.893]
 [2.893]] [[0.562]
 [0.456]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.35800666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35800666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35800666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35800666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35800666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35800666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35800666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]]
maxi score, test score, baseline:  -0.35800666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.35800666666666675 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.35800666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  57 total reward:  0.11999999999999955  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.35800666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.785]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]] [[4.124]
 [3.531]
 [3.798]
 [3.798]
 [3.798]
 [3.798]
 [3.798]] [[0.634]
 [0.785]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]]
maxi score, test score, baseline:  -0.35800666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  46 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.35543333333333343 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35543333333333343 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35543333333333343 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35543333333333343 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  0.09333333333333238  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  56 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.3561666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3561666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  59 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.35616666666666674 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35616666666666674 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35616666666666674 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  44 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.35356666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35356666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  52 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.35110000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3108700265950415
maxi score, test score, baseline:  -0.35110000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35110000000000013 0.6956666666666668 0.6956666666666668
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.35110000000000013 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.35110000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.224]
 [-0.103]
 [-0.173]
 [-0.156]
 [-0.104]
 [-0.213]
 [-0.171]] [[ 0.74 ]
 [ 0.745]
 [-0.148]
 [-0.748]
 [ 0.502]
 [ 0.201]
 [-0.143]] [[-0.054]
 [ 0.069]
 [-0.299]
 [-0.482]
 [-0.014]
 [-0.222]
 [-0.296]]
maxi score, test score, baseline:  -0.35110000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  75 total reward:  0.0533333333333319  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.35110000000000013 0.6956666666666668 0.6956666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.35415333333333343 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  57 total reward:  0.30666666666666587  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.138]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]] [[1.253]
 [1.354]
 [1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]] [[-0.608]
 [-0.57 ]
 [-0.608]
 [-0.608]
 [-0.608]
 [-0.608]
 [-0.608]]
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.630807978646585
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
actor:  1 policy actor:  1  step number:  68 total reward:  0.15333333333333232  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  81 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.3569933333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  63 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  54 total reward:  0.31333333333333324  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3570466666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3570466666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.764]
 [0.657]
 [0.587]
 [0.575]
 [0.561]
 [0.622]] [[2.165]
 [3.217]
 [3.174]
 [2.121]
 [2.13 ]
 [2.392]
 [2.96 ]] [[0.553]
 [0.764]
 [0.657]
 [0.587]
 [0.575]
 [0.561]
 [0.622]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  0 policy actor:  0  step number:  39 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3541933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35419333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35419333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35419333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35419333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74621475
maxi score, test score, baseline:  -0.35419333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35419333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35419333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.35419333333333347 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.35419333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7477403
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  0 policy actor:  0  step number:  72 total reward:  0.03333333333333277  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.35212666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.7  ]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]] [[1.735]
 [1.166]
 [1.735]
 [1.735]
 [1.735]
 [1.735]
 [1.735]] [[0.728]
 [0.7  ]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]]
actor:  0 policy actor:  0  step number:  48 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.231]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]] [[2.99 ]
 [5.093]
 [2.99 ]
 [2.99 ]
 [2.99 ]
 [2.99 ]
 [2.99 ]] [[0.134]
 [0.733]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]]
maxi score, test score, baseline:  -0.34958000000000006 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.46267902382119913
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.34958000000000006 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  45 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.433]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]] [[-0.578]
 [-0.163]
 [-0.578]
 [-0.578]
 [-0.578]
 [-0.578]
 [-0.578]] [[0.314]
 [0.433]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]]
start point for exploration sampling:  11091
actor:  1 policy actor:  1  step number:  63 total reward:  0.15999999999999892  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.34958000000000006 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.34958000000000006 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.34958000000000006 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.34958000000000006 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
Printing some Q and Qe and total Qs values:  [[-0.11]
 [-0.11]
 [-0.11]
 [-0.11]
 [-0.11]
 [-0.11]
 [-0.11]] [[-1.829]
 [-1.829]
 [-1.829]
 [-1.829]
 [-1.829]
 [-1.829]
 [-1.829]] [[-0.465]
 [-0.465]
 [-0.465]
 [-0.465]
 [-0.465]
 [-0.465]
 [-0.465]]
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.451]
 [0.321]
 [0.246]
 [0.353]
 [0.172]
 [0.24 ]] [[3.74 ]
 [3.478]
 [3.067]
 [2.336]
 [3.782]
 [3.047]
 [3.107]] [[ 0.055]
 [ 0.14 ]
 [-0.059]
 [-0.256]
 [ 0.093]
 [-0.211]
 [-0.133]]
siam score:  -0.76023257
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  80 total reward:  0.07333333333333159  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.8666305899344612
first move QE:  -0.4695569256432452
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.305]
 [0.286]
 [0.275]
 [0.287]
 [0.285]
 [0.284]] [[2.493]
 [2.806]
 [2.919]
 [2.493]
 [2.945]
 [2.774]
 [2.811]] [[-0.213]
 [-0.131]
 [-0.132]
 [-0.213]
 [-0.127]
 [-0.156]
 [-0.152]]
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  57 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.252]
 [0.123]
 [0.123]
 [0.123]
 [0.123]
 [0.147]] [[-2.27 ]
 [-2.816]
 [-2.27 ]
 [-2.27 ]
 [-2.27 ]
 [-2.27 ]
 [-1.31 ]] [[0.123]
 [0.252]
 [0.123]
 [0.123]
 [0.123]
 [0.123]
 [0.147]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.09999999999999909  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.0439],
        [ 0.1889],
        [-0.1658],
        [ 0.9249],
        [ 0.7685],
        [ 0.4735],
        [ 0.5948],
        [-0.3067],
        [-0.0000]], dtype=torch.float64)
-0.05880797999999959 -0.05880797999999959
-0.045414567066 -0.08935292129201133
-0.05809183386599999 0.13078153872792092
-0.057834381198 -0.2236680671633288
-0.084359833866 0.8405456457376501
-0.032346567066 0.7361053733243114
-0.084359833866 0.3890972924797649
-0.057834381198 0.5369393361702574
-0.032346567066 -0.339049855300768
-0.9452970149699998 -0.9452970149699998
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.683]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]] [[0.738]
 [0.467]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]] [[0.637]
 [0.683]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]]
maxi score, test score, baseline:  -0.3465933333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  60 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.34426000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76327837
maxi score, test score, baseline:  -0.34426000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 4.527881344356924
maxi score, test score, baseline:  -0.34426000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  0.4999999999999999  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.34126000000000006 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.34126000000000006 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3381533333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3381533333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3381533333333335 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3381533333333335 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3381533333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3381533333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  49 total reward:  0.4  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.3353533333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3353533333333335 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3353533333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3353533333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.758]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[3.735]
 [3.535]
 [3.735]
 [3.735]
 [3.735]
 [3.735]
 [3.735]] [[0.676]
 [0.758]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
maxi score, test score, baseline:  -0.3353533333333335 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.171]
 [-0.179]
 [-0.092]
 [-0.142]
 [-0.219]
 [-0.106]
 [-0.155]] [[-1.16 ]
 [ 0.325]
 [-1.559]
 [-3.682]
 [ 0.731]
 [-2.063]
 [-1.7  ]] [[ 0.366]
 [ 0.715]
 [ 0.299]
 [-0.222]
 [ 0.798]
 [ 0.175]
 [ 0.244]]
actor:  0 policy actor:  0  step number:  52 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.33014000000000016 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.33014000000000016 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.269]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]] [[0.774]
 [1.491]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]] [[-0.074]
 [ 0.32 ]
 [ 0.014]
 [ 0.014]
 [ 0.014]
 [ 0.014]
 [ 0.014]]
maxi score, test score, baseline:  -0.33014000000000016 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  11091
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.33014000000000016 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.33014000000000016 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]] [[1.972]
 [1.972]
 [1.972]
 [1.972]
 [1.972]
 [1.972]
 [1.972]] [[0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]]
maxi score, test score, baseline:  -0.33014000000000016 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.33014000000000016 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.324]
 [0.293]
 [0.287]
 [0.301]
 [0.288]
 [0.281]] [[2.944]
 [2.627]
 [2.66 ]
 [2.82 ]
 [2.847]
 [2.869]
 [2.83 ]] [[0.358]
 [0.243]
 [0.236]
 [0.306]
 [0.329]
 [0.329]
 [0.306]]
maxi score, test score, baseline:  -0.33014000000000016 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.33014000000000016 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  51 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.179]
 [0.176]
 [0.186]] [[3.617]
 [3.732]
 [3.732]
 [3.732]
 [3.818]
 [3.842]
 [3.835]] [[0.543]
 [0.584]
 [0.584]
 [0.584]
 [0.615]
 [0.623]
 [0.626]]
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.764]
 [0.711]
 [0.71 ]
 [0.649]
 [0.672]
 [0.727]] [[1.48 ]
 [1.615]
 [2.13 ]
 [1.956]
 [1.702]
 [2.021]
 [1.845]] [[0.51 ]
 [0.764]
 [0.711]
 [0.71 ]
 [0.649]
 [0.672]
 [0.727]]
maxi score, test score, baseline:  -0.32774000000000014 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.32774000000000014 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  58 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3250600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.48141923366993333
maxi score, test score, baseline:  -0.3250600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3250600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.761361
maxi score, test score, baseline:  -0.3250600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3250600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3250600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.48253564626837714
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]] [[3.204]
 [3.204]
 [3.204]
 [3.204]
 [3.204]
 [3.204]
 [3.204]] [[0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]]
maxi score, test score, baseline:  -0.3250600000000001 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3250600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.32506000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.186]
 [-0.184]
 [-0.248]
 [-0.23 ]
 [-0.247]
 [-0.247]
 [-0.251]] [[0.135]
 [0.259]
 [0.258]
 [0.336]
 [0.145]
 [0.501]
 [0.035]] [[-0.256]
 [-0.192]
 [-0.257]
 [-0.2  ]
 [-0.312]
 [-0.134]
 [-0.371]]
maxi score, test score, baseline:  -0.32506000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.32506000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.32506000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.32506000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.163 0.163 0.082 0.163 0.061 0.265 0.102]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.32506000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.32506000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.32506000000000007 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.32506000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.32506000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.32506000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  43 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.91 ]
 [0.976]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]] [[2.68 ]
 [1.102]
 [2.68 ]
 [2.68 ]
 [2.68 ]
 [2.68 ]
 [2.68 ]] [[0.91 ]
 [0.976]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]]
maxi score, test score, baseline:  -0.3222866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  56 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3197400000000001 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3197400000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]] [[3.728]
 [3.728]
 [3.728]
 [3.728]
 [3.728]
 [3.728]
 [3.728]] [[0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]]
siam score:  -0.75844467
maxi score, test score, baseline:  -0.3197400000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.007]
 [-0.023]
 [-0.016]
 [-0.024]
 [-0.024]
 [-0.024]] [[0.438]
 [0.524]
 [0.313]
 [0.776]
 [0.345]
 [0.356]
 [0.326]] [[-0.546]
 [-0.496]
 [-0.617]
 [-0.379]
 [-0.601]
 [-0.596]
 [-0.611]]
siam score:  -0.76151955
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.48 ]
 [0.283]
 [0.283]
 [0.283]
 [0.323]
 [0.34 ]] [[5.024]
 [4.543]
 [4.302]
 [4.302]
 [4.302]
 [4.888]
 [3.969]] [[0.683]
 [0.601]
 [0.398]
 [0.398]
 [0.398]
 [0.612]
 [0.328]]
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.664]
 [0.664]
 [0.632]
 [0.522]
 [0.669]
 [0.664]] [[2.84 ]
 [3.114]
 [3.114]
 [3.611]
 [3.236]
 [3.234]
 [3.114]] [[0.493]
 [0.581]
 [0.581]
 [0.723]
 [0.527]
 [0.623]
 [0.581]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  87 total reward:  0.06666666666666454  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.167]
 [-0.098]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.096]] [[-4.996]
 [-1.164]
 [-4.824]
 [-4.63 ]
 [-4.812]
 [-4.893]
 [-5.001]] [[-0.432]
 [ 0.28 ]
 [-0.399]
 [-0.36 ]
 [-0.396]
 [-0.412]
 [-0.433]]
Printing some Q and Qe and total Qs values:  [[-0.085]
 [-0.08 ]
 [ 1.5  ]
 [ 1.5  ]
 [ 1.5  ]
 [ 1.5  ]
 [ 1.5  ]] [[-4.509]
 [-1.424]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.032]
 [ 0.739]
 [ 1.684]
 [ 1.684]
 [ 1.684]
 [ 1.684]
 [ 1.684]]
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[1.711]
 [1.711]
 [1.711]
 [1.711]
 [1.711]
 [1.711]
 [1.711]] [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.254]
 [0.255]
 [0.24 ]
 [0.271]
 [0.271]
 [0.248]] [[4.115]
 [3.97 ]
 [4.087]
 [3.858]
 [3.842]
 [4.012]
 [3.956]] [[0.215]
 [0.172]
 [0.212]
 [0.12 ]
 [0.147]
 [0.202]
 [0.161]]
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  51 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.263]
 [ 0.   ]
 [-0.263]
 [-0.263]
 [-0.263]
 [-0.263]
 [-0.263]] [[0.386]
 [0.   ]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[-0.264]
 [-0.129]
 [-0.264]
 [-0.264]
 [-0.264]
 [-0.264]
 [-0.264]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.4552031749113512
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75607896
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3166866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  50 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3113800000000001 0.6956666666666668 0.6956666666666668
line 256 mcts: sample exp_bonus -0.05723042490464618
maxi score, test score, baseline:  -0.3113800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3113800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3085400000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3085400000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3085400000000001 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3085400000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3085400000000001 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3085400000000001 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3085400000000001 0.6956666666666668 0.6956666666666668
siam score:  -0.74447054
maxi score, test score, baseline:  -0.3085400000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3085400000000001 0.6956666666666668 0.6956666666666668
actor:  1 policy actor:  1  step number:  46 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3085400000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7462803
maxi score, test score, baseline:  -0.3085400000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3085400000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.1703381498409495
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  40 total reward:  0.5  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3055400000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.093]
 [-0.166]
 [-0.1  ]
 [-0.096]
 [-0.095]
 [-0.099]
 [-0.097]] [[-3.084]
 [-0.978]
 [-3.356]
 [-3.193]
 [-3.144]
 [-2.995]
 [-3.33 ]] [[-0.001]
 [ 0.383]
 [-0.061]
 [-0.026]
 [-0.015]
 [ 0.013]
 [-0.054]]
maxi score, test score, baseline:  -0.3055400000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3055400000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.331]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]] [[3.23 ]
 [4.245]
 [3.23 ]
 [3.23 ]
 [3.23 ]
 [3.23 ]
 [3.23 ]] [[0.147]
 [0.498]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]]
siam score:  -0.7347877
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.425]
 [0.268]
 [0.278]
 [0.275]
 [0.322]
 [0.401]] [[2.908]
 [2.763]
 [2.668]
 [2.549]
 [2.418]
 [2.816]
 [2.689]] [[ 0.095]
 [ 0.153]
 [-0.036]
 [-0.065]
 [-0.111]
 [ 0.067]
 [ 0.104]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  0 policy actor:  0  step number:  37 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3025266666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.3025266666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  54 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2999533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.123]
 [-0.09 ]
 [-0.272]
 [-0.076]
 [-0.076]
 [-0.235]
 [-0.073]] [[ 0.357]
 [ 0.159]
 [-0.107]
 [-0.771]
 [-0.771]
 [ 0.141]
 [-0.746]] [[0.62 ]
 [0.595]
 [0.473]
 [0.42 ]
 [0.42 ]
 [0.535]
 [0.426]]
maxi score, test score, baseline:  -0.2999533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.033]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[1.642]
 [1.032]
 [1.211]
 [1.211]
 [1.211]
 [1.211]
 [1.211]] [[0.319]
 [0.143]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]]
maxi score, test score, baseline:  -0.2999533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2999533333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.2999533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2999533333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.2999533333333334 0.6956666666666668 0.6956666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2999533333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.2999533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2999533333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.29995333333333346 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.29995333333333346 0.6956666666666668 0.6956666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.29995333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]] [[-0.915]
 [-0.915]
 [-0.915]
 [-0.915]
 [-0.915]
 [-0.915]
 [-0.915]] [[0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]]
siam score:  -0.74416995
maxi score, test score, baseline:  -0.29995333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2999533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2999533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.685]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]] [[1.494]
 [1.328]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]] [[0.497]
 [0.685]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.2999533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2999533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  51 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74434876
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.383]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]] [[1.802]
 [2.002]
 [1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.802]] [[0.256]
 [0.432]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  64 total reward:  0.15333333333333232  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.9549090831004505
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.5119668171642474
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.5153686952590552
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.388]
 [0.564]] [[2.268]
 [1.892]
 [1.892]
 [1.892]
 [1.892]
 [3.277]
 [2.187]] [[ 0.157]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [ 0.56 ]
 [ 0.235]]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  78 total reward:  0.19333333333333158  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.226]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]] [[1.751]
 [1.821]
 [1.751]
 [1.751]
 [1.751]
 [1.751]
 [1.751]] [[-0.234]
 [-0.06 ]
 [-0.234]
 [-0.234]
 [-0.234]
 [-0.234]
 [-0.234]]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.57 ]
 [0.604]
 [0.601]
 [0.566]
 [0.59 ]
 [0.57 ]] [[3.023]
 [2.895]
 [3.692]
 [3.883]
 [3.142]
 [3.334]
 [2.895]] [[0.277]
 [0.259]
 [0.516]
 [0.568]
 [0.326]
 [0.402]
 [0.259]]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.599]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[2.426]
 [3.097]
 [2.426]
 [2.426]
 [2.426]
 [2.426]
 [2.426]] [[0.431]
 [0.697]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
siam score:  -0.7388234
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  57 total reward:  0.17333333333333256  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29720666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.29423333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.29423333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29423333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29423333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29423333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29423333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29423333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.29423333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  53 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  50 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2892866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2892866666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  59 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[ 0.046]
 [ 0.21 ]
 [ 0.046]
 [ 0.046]
 [ 0.046]
 [ 0.046]
 [-0.006]] [[-1.272]
 [-1.17 ]
 [-1.272]
 [-1.272]
 [-1.272]
 [-1.272]
 [-0.571]] [[ 0.046]
 [ 0.21 ]
 [ 0.046]
 [ 0.046]
 [ 0.046]
 [ 0.046]
 [-0.006]]
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  52 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.853]
 [0.63 ]
 [0.814]
 [0.692]
 [0.806]
 [0.694]] [[1.082]
 [0.128]
 [0.146]
 [0.723]
 [0.775]
 [0.98 ]
 [0.212]] [[0.677]
 [0.853]
 [0.63 ]
 [0.814]
 [0.692]
 [0.806]
 [0.694]]
maxi score, test score, baseline:  -0.28638000000000013 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  53 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.314]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[2.286]
 [1.762]
 [2.286]
 [2.286]
 [2.286]
 [2.286]
 [2.286]] [[-0.149]
 [-0.275]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]]
maxi score, test score, baseline:  -0.28406000000000015 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.382277921238808
actor:  0 policy actor:  0  step number:  62 total reward:  0.2599999999999999  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.2815400000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.737617
actor:  0 policy actor:  0  step number:  57 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.27895333333333344 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27895333333333344 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.27895333333333344 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.27895333333333344 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27895333333333344 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27895333333333344 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27895333333333344 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  59 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  51 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.667
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.4816],
        [-0.0790],
        [-0.0000],
        [-0.0000],
        [-0.2703],
        [ 0.4199],
        [-0.2483],
        [-0.1690],
        [-0.2454]], dtype=torch.float64)
0.99 0.99
-0.032346567066 -0.5139199683441042
-0.083839701198 -0.16285963500847309
0.9734999999999999 0.9734999999999999
-0.62791938 -0.62791938
-0.09703970119800001 -0.36733855750239264
-0.09703970119800001 0.32281402675746174
-0.032346567066 -0.28060314309908885
-0.045026434398 -0.2140721600329544
-0.032346567066 -0.2777440392882103
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.021]
 [-0.001]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]] [[0.771]
 [1.203]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]] [[-0.356]
 [-0.192]
 [-0.356]
 [-0.356]
 [-0.356]
 [-0.356]
 [-0.356]]
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7438852
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.111]
 [0.21 ]
 [0.111]
 [0.111]
 [0.111]
 [0.111]] [[1.643]
 [1.643]
 [2.114]
 [1.643]
 [1.643]
 [1.643]
 [1.643]] [[-0.302]
 [-0.302]
 [-0.124]
 [-0.302]
 [-0.302]
 [-0.302]
 [-0.302]]
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  62 total reward:  0.19333333333333236  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.2775225007899014
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74451673
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.292]
 [0.231]
 [0.231]
 [0.231]
 [0.245]
 [0.272]] [[2.202]
 [1.777]
 [2.202]
 [2.202]
 [2.202]
 [1.273]
 [1.776]] [[-0.231]
 [-0.241]
 [-0.231]
 [-0.231]
 [-0.231]
 [-0.372]
 [-0.261]]
actor:  1 policy actor:  1  step number:  70 total reward:  0.0733333333333328  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.387]
 [0.246]
 [0.266]
 [0.25 ]
 [0.273]
 [0.267]] [[3.631]
 [3.451]
 [2.695]
 [2.89 ]
 [3.598]
 [3.052]
 [2.881]] [[0.552]
 [0.487]
 [0.155]
 [0.23 ]
 [0.446]
 [0.287]
 [0.228]]
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.406]
 [0.773]
 [0.425]
 [0.43 ]
 [0.497]
 [0.41 ]] [[-2.381]
 [ 0.246]
 [ 0.122]
 [-2.521]
 [-1.776]
 [-1.264]
 [-2.334]] [[0.383]
 [0.406]
 [0.773]
 [0.425]
 [0.43 ]
 [0.497]
 [0.41 ]]
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.59 ]
 [0.517]
 [0.555]
 [0.44 ]
 [0.444]
 [0.412]] [[3.421]
 [3.017]
 [3.418]
 [3.224]
 [3.368]
 [3.494]
 [3.573]] [[0.513]
 [0.431]
 [0.539]
 [0.489]
 [0.453]
 [0.509]
 [0.515]]
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.44824134273617
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.719]
 [0.673]
 [0.673]
 [0.672]
 [0.67 ]
 [0.67 ]] [[-0.502]
 [ 0.182]
 [-0.565]
 [-0.624]
 [-0.719]
 [-0.552]
 [-0.365]] [[0.67 ]
 [0.719]
 [0.673]
 [0.673]
 [0.672]
 [0.67 ]
 [0.67 ]]
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27447333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.623]
 [0.593]
 [0.578]
 [0.655]
 [0.655]
 [0.655]] [[1.326]
 [3.062]
 [2.151]
 [1.71 ]
 [1.326]
 [1.326]
 [1.326]] [[0.655]
 [0.623]
 [0.593]
 [0.578]
 [0.655]
 [0.655]
 [0.655]]
actor:  0 policy actor:  0  step number:  59 total reward:  0.1599999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.27215333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27215333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27215333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.079]
 [-0.122]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]] [[-3.053]
 [-1.653]
 [-3.053]
 [-3.053]
 [-3.053]
 [-3.053]
 [-3.053]] [[0.162]
 [0.332]
 [0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]]
siam score:  -0.74774843
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.27215333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.27215333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.402]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[1.261]
 [1.321]
 [1.202]
 [1.202]
 [1.202]
 [1.202]
 [1.202]] [[-0.271]
 [-0.189]
 [-0.298]
 [-0.298]
 [-0.298]
 [-0.298]
 [-0.298]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27215333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27215333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7437378
actor:  1 policy actor:  1  step number:  66 total reward:  0.04666666666666586  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.27215333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27215333333333347 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  49 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.2693000000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2693000000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2693000000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7472446
maxi score, test score, baseline:  -0.2693000000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  53 total reward:  0.27999999999999925  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7486283
maxi score, test score, baseline:  -0.27014000000000016 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.27014000000000016 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.504]
 [0.489]
 [0.504]
 [0.504]
 [0.504]] [[-2.895]
 [-2.895]
 [-2.895]
 [-2.99 ]
 [-2.895]
 [-2.895]
 [-2.895]] [[0.504]
 [0.504]
 [0.504]
 [0.489]
 [0.504]
 [0.504]
 [0.504]]
Printing some Q and Qe and total Qs values:  [[-0.165]
 [-0.165]
 [-0.165]
 [-0.165]
 [-0.165]
 [-0.165]
 [-0.165]] [[-0.379]
 [-0.379]
 [-0.379]
 [-0.379]
 [-0.379]
 [-0.379]
 [-0.379]] [[0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]]
maxi score, test score, baseline:  -0.27014000000000016 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2734200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2734200000000001 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.2734200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2734200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2734200000000001 0.6956666666666668 0.6956666666666668
actor:  1 policy actor:  1  step number:  58 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.2734200000000001 0.6956666666666668 0.6956666666666668
actor:  0 policy actor:  0  step number:  51 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  0.667
Starting evaluation
maxi score, test score, baseline:  -0.2742466666666668 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.2742466666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[ 0.006]
 [ 0.016]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[1.599]
 [1.271]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[-0.163]
 [-0.262]
 [-0.521]
 [-0.521]
 [-0.521]
 [-0.521]
 [-0.521]]
maxi score, test score, baseline:  -0.2742466666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2742466666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.797]
 [0.708]
 [0.737]
 [0.692]
 [0.698]
 [0.705]] [[-0.696]
 [-0.45 ]
 [-0.263]
 [-1.154]
 [-0.456]
 [-0.374]
 [-0.365]] [[0.733]
 [0.797]
 [0.708]
 [0.737]
 [0.692]
 [0.698]
 [0.705]]
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.736]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]] [[-0.259]
 [ 0.533]
 [-0.259]
 [-0.259]
 [-0.259]
 [-0.259]
 [-0.259]] [[0.57 ]
 [0.736]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]]
maxi score, test score, baseline:  -0.2742466666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.707]
 [0.64 ]
 [0.508]
 [0.584]
 [0.238]
 [0.678]] [[1.744]
 [2.11 ]
 [1.416]
 [0.708]
 [0.374]
 [0.53 ]
 [2.041]] [[0.651]
 [0.707]
 [0.64 ]
 [0.508]
 [0.584]
 [0.238]
 [0.678]]
maxi score, test score, baseline:  -0.2742466666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.972]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]] [[2.406]
 [1.838]
 [2.406]
 [2.406]
 [2.406]
 [2.406]
 [2.406]] [[0.852]
 [0.972]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.766]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[4.581]
 [3.171]
 [3.299]
 [3.299]
 [3.299]
 [3.299]
 [3.299]] [[0.603]
 [0.766]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]]
maxi score, test score, baseline:  -0.2742466666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.666]
 [0.55 ]
 [0.447]
 [0.478]
 [0.583]
 [0.7  ]] [[2.541]
 [1.377]
 [1.968]
 [0.166]
 [1.007]
 [2.618]
 [1.978]] [[0.694]
 [0.666]
 [0.55 ]
 [0.447]
 [0.478]
 [0.583]
 [0.7  ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2742466666666668 0.6956666666666668 0.6956666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2742466666666668 0.6956666666666668 0.6956666666666668
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.23260666666666677 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.23260666666666677 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.23260666666666677 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.23260666666666677 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.23260666666666677 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.23260666666666677 0.6860000000000002 0.6860000000000002
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.879]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.702]] [[3.729]
 [1.367]
 [2.93 ]
 [2.93 ]
 [2.93 ]
 [2.93 ]
 [2.728]] [[0.513]
 [0.879]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.702]]
actor:  0 policy actor:  0  step number:  61 total reward:  0.06666666666666565  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.23047333333333345 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.23047333333333345 0.6860000000000002 0.6860000000000002
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.841]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[3.781]
 [3.091]
 [3.781]
 [3.781]
 [3.781]
 [3.781]
 [3.781]] [[0.618]
 [0.841]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]]
maxi score, test score, baseline:  -0.23047333333333345 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.23047333333333345 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.23047333333333345 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.83 ]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]] [[2.348]
 [2.124]
 [2.348]
 [2.348]
 [2.348]
 [2.348]
 [2.348]] [[0.798]
 [0.83 ]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]]
maxi score, test score, baseline:  -0.23047333333333345 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  0.56  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.22735333333333343 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.22735333333333343 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22735333333333343 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22735333333333343 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22735333333333343 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.22735333333333343 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75243646
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.22735333333333343 0.6860000000000002 0.6860000000000002
siam score:  -0.75031847
maxi score, test score, baseline:  -0.22735333333333343 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22735333333333343 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22735333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22735333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22735333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.053]
 [-0.034]
 [-0.051]
 [-0.053]
 [-0.054]
 [-0.054]
 [-0.054]] [[-0.076]
 [-0.239]
 [ 0.045]
 [-0.013]
 [-0.033]
 [ 0.035]
 [ 0.079]] [[-0.584]
 [-0.674]
 [-0.502]
 [-0.542]
 [-0.557]
 [-0.511]
 [-0.482]]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.22735333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.22735333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22735333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.535]
 [0.303]
 [0.299]
 [0.299]
 [0.3  ]
 [0.299]] [[ 0.099]
 [-0.24 ]
 [-0.152]
 [-0.127]
 [-0.066]
 [ 0.031]
 [ 0.04 ]] [[0.301]
 [0.535]
 [0.303]
 [0.299]
 [0.299]
 [0.3  ]
 [0.299]]
maxi score, test score, baseline:  -0.22959333333333343 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22959333333333343 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22959333333333343 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  59 total reward:  0.039999999999999036  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.22751333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22751333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22751333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22751333333333346 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.22751333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22751333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[2.665]
 [2.523]
 [2.523]
 [2.523]
 [2.523]
 [2.523]
 [2.523]] [[0.413]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]] [[3.071]
 [3.071]
 [3.071]
 [3.071]
 [3.071]
 [3.071]
 [3.071]] [[0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]]
maxi score, test score, baseline:  -0.22751333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.922851931240929
siam score:  -0.74430007
actor:  1 policy actor:  1  step number:  76 total reward:  0.05999999999999983  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.5520807137356359
first move QE:  -0.5522200721794692
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.134]
 [0.119]
 [0.103]
 [0.115]
 [0.11 ]
 [0.111]] [[1.582]
 [2.15 ]
 [2.136]
 [2.011]
 [1.757]
 [1.732]
 [1.868]] [[-0.309]
 [-0.202]
 [-0.219]
 [-0.256]
 [-0.286]
 [-0.295]
 [-0.272]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.220079122344777
maxi score, test score, baseline:  -0.22751333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.831]
 [0.719]
 [0.805]
 [0.702]
 [0.693]
 [0.694]] [[3.277]
 [2.353]
 [3.543]
 [2.794]
 [3.247]
 [3.905]
 [3.177]] [[0.793]
 [0.831]
 [0.719]
 [0.805]
 [0.702]
 [0.693]
 [0.694]]
maxi score, test score, baseline:  -0.22751333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  50 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.352]
 [0.312]
 [0.357]
 [0.345]
 [0.331]
 [0.286]] [[1.567]
 [1.512]
 [1.406]
 [1.297]
 [1.255]
 [1.316]
 [1.589]] [[-0.196]
 [-0.144]
 [-0.202]
 [-0.175]
 [-0.194]
 [-0.197]
 [-0.197]]
maxi score, test score, baseline:  -0.22512666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22512666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22512666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22512666666666678 0.6860000000000002 0.6860000000000002
UNIT TEST: sample policy line 217 mcts : [0.061 0.163 0.082 0.041 0.02  0.122 0.51 ]
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.1925],
        [-0.0269],
        [ 0.3117],
        [ 0.1409],
        [-0.0000],
        [ 0.2412],
        [ 0.4356],
        [ 0.7627],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.058351887066 0.13414757446697692
-0.09703970119800001 -0.12397574627150534
-0.032346567066 0.2793964361786424
-0.058614567066 0.08228470397764594
-0.947496 -0.947496
-0.045026434398 0.1961439509193946
-0.083839701198 0.35178328233654693
-0.071551887066 0.6911532043527655
-0.0065999999999994926 -0.0065999999999994926
0.99 0.99
maxi score, test score, baseline:  -0.22512666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  48 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.2226600000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2226600000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7595462
maxi score, test score, baseline:  -0.2226600000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2226600000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  43 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.667
first move QE:  -0.5535146160984495
actor:  1 policy actor:  1  step number:  54 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.22244666666666676 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.309]
 [0.272]
 [0.263]
 [0.26 ]
 [0.282]
 [0.278]] [[0.903]
 [1.398]
 [1.135]
 [0.523]
 [0.582]
 [1.08 ]
 [1.175]] [[-0.326]
 [-0.182]
 [-0.263]
 [-0.374]
 [-0.367]
 [-0.262]
 [-0.25 ]]
maxi score, test score, baseline:  -0.22244666666666676 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.097219158349454
maxi score, test score, baseline:  -0.22244666666666676 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  55 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.22244666666666674 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.2523140942771778
actor:  1 policy actor:  1  step number:  57 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.22244666666666674 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.346]
 [0.335]
 [0.324]
 [0.347]
 [0.356]
 [0.33 ]] [[3.721]
 [3.75 ]
 [3.768]
 [3.616]
 [3.749]
 [3.766]
 [3.774]] [[0.308]
 [0.344]
 [0.339]
 [0.278]
 [0.345]
 [0.36 ]
 [0.336]]
maxi score, test score, baseline:  -0.22244666666666674 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76367164
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.22244666666666674 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.22244666666666674 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.112]
 [-0.006]
 [-0.009]
 [-0.022]
 [-0.021]
 [-0.017]] [[ 0.571]
 [ 0.122]
 [ 0.543]
 [-0.023]
 [ 0.   ]
 [ 0.266]
 [ 0.179]] [[-0.359]
 [-0.447]
 [-0.374]
 [-0.601]
 [-0.602]
 [-0.496]
 [-0.527]]
maxi score, test score, baseline:  -0.22244666666666674 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.22244666666666674 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.22244666666666674 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.22244666666666674 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22244666666666674 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.22244666666666674 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22244666666666674 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22244666666666674 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22244666666666676 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7565747
maxi score, test score, baseline:  -0.22244666666666676 0.6860000000000002 0.6860000000000002
actor:  0 policy actor:  0  step number:  40 total reward:  0.5133333333333333  reward:  1.0 rdn_beta:  0.5
siam score:  -0.7601733
maxi score, test score, baseline:  -0.22214000000000014 0.6860000000000002 0.6860000000000002
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22214000000000014 0.6860000000000002 0.6860000000000002
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.524]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]] [[-0.525]
 [ 0.195]
 [-0.525]
 [-0.525]
 [-0.525]
 [-0.525]
 [-0.525]] [[0.28 ]
 [0.524]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]]
maxi score, test score, baseline:  -0.22214000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.22214000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.22214000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  39 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.21632666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.21632666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.21632666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.901103579661703
maxi score, test score, baseline:  -0.21632666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.21632666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.21632666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.21632666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.21632666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2132866666666668 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2132866666666668 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.2132866666666668 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2132866666666668 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2132866666666668 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.2132866666666668 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  42 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2105800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.103]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]] [[0.014]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[0.327]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]]
maxi score, test score, baseline:  -0.2105800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2105800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2105800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2105800000000001 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.2105800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2105800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.4275],
        [ 0.0614],
        [-0.3211],
        [ 0.7036],
        [-0.0000],
        [ 0.3705],
        [-0.0000],
        [-0.2802],
        [ 0.5268],
        [ 0.8040]], dtype=torch.float64)
-0.058614567066 0.36887892288173924
-0.084359833866 -0.02293187690271454
-0.09703970119800001 -0.41809094181573847
-0.08423175439800001 0.6193784404667949
0.9147253499999999 0.9147253499999999
-0.058226434398 0.31230353111185716
-0.973632 -0.973632
-0.032346567066 -0.31252977864776327
-0.032346567066 0.4944386237457347
-0.09703970119800001 0.706956002224813
first move QE:  -0.5651284163769645
actor:  1 policy actor:  1  step number:  78 total reward:  0.07333333333333159  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2105800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.825]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.664]] [[0.103]
 [0.593]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.017]] [[0.684]
 [0.825]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.664]]
maxi score, test score, baseline:  -0.2105800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.031]
 [ 0.102]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]] [[-0.428]
 [ 1.381]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]] [[-0.489]
 [-0.054]
 [-0.422]
 [-0.422]
 [-0.422]
 [-0.422]
 [-0.422]]
maxi score, test score, baseline:  -0.2105800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.15370637138438603
maxi score, test score, baseline:  -0.2105800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2105800000000001 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.2105800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.423]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]] [[3.848]
 [3.6  ]
 [3.848]
 [3.848]
 [3.848]
 [3.848]
 [3.848]] [[0.517]
 [0.471]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]]
siam score:  -0.7571185
actor:  1 policy actor:  1  step number:  53 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.2105800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7611966
actor:  0 policy actor:  0  step number:  44 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.22 ]
 [0.129]
 [0.146]
 [0.146]
 [0.119]
 [0.146]] [[1.707]
 [2.034]
 [2.141]
 [1.707]
 [1.707]
 [3.084]
 [1.707]] [[-0.093]
 [ 0.091]
 [ 0.078]
 [-0.093]
 [-0.093]
 [ 0.467]
 [-0.093]]
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
siam score:  -0.76404655
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.299]
 [0.295]
 [0.299]
 [0.307]
 [0.301]
 [0.299]] [[3.389]
 [3.016]
 [3.311]
 [3.016]
 [3.366]
 [3.274]
 [3.016]] [[0.245]
 [0.126]
 [0.22 ]
 [0.126]
 [0.25 ]
 [0.214]
 [0.126]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  11091
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.729]
 [0.692]
 [0.707]
 [0.686]
 [0.687]
 [0.634]] [[2.408]
 [1.443]
 [2.667]
 [1.875]
 [2.84 ]
 [2.604]
 [2.552]] [[0.665]
 [0.425]
 [0.727]
 [0.527]
 [0.77 ]
 [0.708]
 [0.667]]
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  65 total reward:  0.039999999999999036  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
siam score:  -0.76720065
maxi score, test score, baseline:  -0.20980666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  54 total reward:  0.11333333333333273  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  73 total reward:  0.1999999999999985  reward:  1.0 rdn_beta:  0.333
siam score:  -0.764503
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]] [[3.273]
 [3.273]
 [3.273]
 [3.273]
 [3.273]
 [3.273]
 [3.273]] [[0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]]
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  57 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.487]
 [0.262]
 [0.27 ]
 [0.254]
 [0.258]
 [0.202]] [[2.252]
 [1.996]
 [2.306]
 [3.074]
 [2.227]
 [2.277]
 [2.252]] [[0.158]
 [0.262]
 [0.206]
 [0.52 ]
 [0.168]
 [0.192]
 [0.136]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20758000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2103000000000001 0.6860000000000002 0.6860000000000002
Printing some Q and Qe and total Qs values:  [[-0.295]
 [-0.183]
 [-0.295]
 [-0.183]
 [-0.183]
 [-0.295]
 [-0.295]] [[ 0.171]
 [-0.159]
 [-0.013]
 [-0.159]
 [-0.159]
 [-0.028]
 [-0.054]] [[0.204]
 [0.173]
 [0.15 ]
 [0.173]
 [0.173]
 [0.146]
 [0.138]]
maxi score, test score, baseline:  -0.2103000000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2103000000000001 0.6860000000000002 0.6860000000000002
siam score:  -0.7610669
actor:  1 policy actor:  1  step number:  94 total reward:  0.0066666666666646  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.816]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]] [[0.44 ]
 [0.965]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]] [[0.664]
 [0.816]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]]
maxi score, test score, baseline:  -0.2103000000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2103000000000001 0.6860000000000002 0.6860000000000002
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.751]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[2.007]
 [2.765]
 [2.007]
 [2.007]
 [2.007]
 [2.007]
 [2.007]] [[0.54 ]
 [0.751]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.2103000000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2103000000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  50 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.2078866666666668 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2078866666666668 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2071666666666668 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2071666666666668 0.6860000000000002 0.6860000000000002
actor:  0 policy actor:  0  step number:  46 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.2045933333333335 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.2045933333333335 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2045933333333335 0.6860000000000002 0.6860000000000002
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2045933333333335 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  67 total reward:  0.10666666666666558  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.2045933333333335 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2045933333333335 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.2045933333333335 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  79 total reward:  0.22666666666666546  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  38 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.20400666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20400666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20400666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20400666666666678 0.6860000000000002 0.6860000000000002
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.20400666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  43 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.20126000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.20126000000000013 0.6860000000000002 0.6860000000000002
actor:  0 policy actor:  0  step number:  65 total reward:  0.03999999999999948  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.1991800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1991800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8960840887558328
maxi score, test score, baseline:  -0.1991800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1991800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[2.582]
 [2.582]
 [2.582]
 [2.582]
 [2.582]
 [2.582]
 [2.582]] [[0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]]
maxi score, test score, baseline:  -0.1991800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1991800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1991800000000001 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.1991800000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.723]
 [0.671]
 [0.656]
 [0.656]
 [0.673]
 [0.688]] [[-0.544]
 [ 0.384]
 [-0.378]
 [-0.7  ]
 [-0.724]
 [-0.547]
 [-0.381]] [[0.672]
 [0.723]
 [0.671]
 [0.656]
 [0.656]
 [0.673]
 [0.688]]
maxi score, test score, baseline:  -0.19918000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19918000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19918000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19918000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19918000000000013 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  47 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1966200000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1966200000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1966200000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1966200000000001 0.6860000000000002 0.6860000000000002
Printing some Q and Qe and total Qs values:  [[0.877]
 [0.917]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]] [[2.002]
 [1.061]
 [2.002]
 [2.002]
 [2.002]
 [2.002]
 [2.002]] [[0.877]
 [0.917]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]]
maxi score, test score, baseline:  -0.1966200000000001 0.6860000000000002 0.6860000000000002
actor:  0 policy actor:  0  step number:  53 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.19411333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19411333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.015]
 [-0.028]
 [-0.028]
 [-0.029]
 [-0.028]
 [-0.028]] [[ 0.387]
 [ 1.202]
 [ 0.327]
 [ 0.149]
 [ 0.214]
 [-0.082]
 [ 0.098]] [[-0.388]
 [-0.11 ]
 [-0.414]
 [-0.474]
 [-0.454]
 [-0.55 ]
 [-0.491]]
maxi score, test score, baseline:  -0.19411333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19411333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19411333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19411333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.5781499268515029
maxi score, test score, baseline:  -0.19411333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.19411333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74053866
maxi score, test score, baseline:  -0.19411333333333347 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.19411333333333347 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.19411333333333347 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.19411333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19411333333333347 0.6860000000000002 0.6860000000000002
actor:  0 policy actor:  0  step number:  49 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.19155333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  0.153333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.19155333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.7795418049749077
maxi score, test score, baseline:  -0.19155333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333346 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.19155333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.337]
 [0.214]
 [0.303]
 [0.257]
 [0.273]
 [0.273]] [[2.743]
 [2.958]
 [3.415]
 [3.079]
 [2.827]
 [2.743]
 [2.743]] [[0.182]
 [0.314]
 [0.341]
 [0.32 ]
 [0.194]
 [0.182]
 [0.182]]
maxi score, test score, baseline:  -0.19155333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.0223713420441154
actor:  1 policy actor:  1  step number:  82 total reward:  0.16666666666666508  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.006]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[0.548]
 [1.036]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[-0.256]
 [-0.003]
 [-0.256]
 [-0.256]
 [-0.256]
 [-0.256]
 [-0.256]]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73832977
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
siam score:  -0.7373918
actor:  1 policy actor:  1  step number:  57 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.403]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]] [[2.185]
 [2.086]
 [2.115]
 [2.115]
 [2.115]
 [2.115]
 [2.115]] [[-0.223]
 [-0.084]
 [-0.232]
 [-0.232]
 [-0.232]
 [-0.232]
 [-0.232]]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.4574563933470084
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5351554124835878
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7348976
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.765]
 [0.767]
 [0.669]
 [0.669]
 [0.763]
 [0.765]] [[1.415]
 [1.092]
 [1.032]
 [0.043]
 [0.043]
 [0.391]
 [0.387]] [[0.51 ]
 [0.454]
 [0.445]
 [0.183]
 [0.183]
 [0.335]
 [0.336]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.391]
 [0.328]
 [0.24 ]
 [0.24 ]
 [0.082]
 [0.119]] [[2.697]
 [3.869]
 [4.4  ]
 [4.002]
 [4.002]
 [2.115]
 [2.598]] [[0.33 ]
 [0.616]
 [0.693]
 [0.584]
 [0.584]
 [0.166]
 [0.272]]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.643]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]] [[2.153]
 [2.032]
 [2.153]
 [2.153]
 [2.153]
 [2.153]
 [2.153]] [[-0.069]
 [ 0.129]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  65 total reward:  0.039999999999999036  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.455]
 [0.455]
 [0.464]
 [0.482]
 [0.455]
 [0.455]] [[2.002]
 [2.356]
 [2.356]
 [1.513]
 [1.331]
 [2.356]
 [2.356]] [[0.431]
 [0.572]
 [0.572]
 [0.223]
 [0.158]
 [0.572]
 [0.572]]
maxi score, test score, baseline:  -0.19155333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  43 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7449836
actor:  1 policy actor:  1  step number:  47 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  53 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.5887070699789455
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.309031151225123
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
line 256 mcts: sample exp_bonus 0.5044323768089057
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.4 ]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]] [[4.011]
 [4.058]
 [4.058]
 [4.058]
 [4.058]
 [4.058]
 [4.058]] [[0.486]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]]
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.853]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]] [[2.122]
 [1.641]
 [2.122]
 [2.122]
 [2.122]
 [2.122]
 [2.122]] [[0.815]
 [0.853]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]]
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18878000000000014 0.6860000000000002 0.6860000000000002
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.18878000000000011 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.18878000000000011 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7366345
actor:  0 policy actor:  0  step number:  51 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.18611333333333346 0.6860000000000002 0.6860000000000002
line 256 mcts: sample exp_bonus 1.1845028743821315
maxi score, test score, baseline:  -0.18611333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7373114
maxi score, test score, baseline:  -0.18611333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18611333333333346 0.6860000000000002 0.6860000000000002
Printing some Q and Qe and total Qs values:  [[-0.016]
 [ 0.186]
 [ 0.186]
 [ 0.186]
 [ 0.186]
 [ 0.186]
 [ 0.186]] [[4.668]
 [1.24 ]
 [1.24 ]
 [1.24 ]
 [1.24 ]
 [1.24 ]
 [1.24 ]] [[ 0.344]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]]
maxi score, test score, baseline:  -0.18611333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.18611333333333346 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.18611333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[1.827]
 [1.827]
 [1.827]
 [1.827]
 [1.827]
 [1.827]
 [1.827]] [[0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  11091
line 256 mcts: sample exp_bonus -0.3401549947547783
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.18611333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
line 256 mcts: sample exp_bonus -0.28361404986940364
maxi score, test score, baseline:  -0.18611333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18611333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.739]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[-0.837]
 [-0.904]
 [-0.721]
 [-0.721]
 [-0.721]
 [-0.721]
 [-0.721]] [[0.696]
 [0.739]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]]
maxi score, test score, baseline:  -0.18611333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  0.17333333333333234  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.18611333333333346 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  54 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.531]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]] [[3.711]
 [4.549]
 [4.247]
 [4.247]
 [4.247]
 [4.247]
 [4.247]] [[0.22 ]
 [0.645]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.18359333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.515]
 [0.471]
 [0.455]
 [0.471]
 [0.471]
 [0.471]] [[4.178]
 [3.217]
 [3.615]
 [3.503]
 [3.615]
 [3.615]
 [3.615]] [[0.538]
 [0.316]
 [0.404]
 [0.351]
 [0.404]
 [0.404]
 [0.404]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.18359333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  70 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  11091
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.778]
 [0.6  ]
 [0.606]
 [0.621]
 [0.621]
 [0.621]] [[1.871]
 [1.908]
 [1.416]
 [1.087]
 [1.871]
 [1.871]
 [1.871]] [[0.621]
 [0.778]
 [0.6  ]
 [0.606]
 [0.621]
 [0.621]
 [0.621]]
maxi score, test score, baseline:  -0.18359333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7501576
maxi score, test score, baseline:  -0.18359333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18060666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18060666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.18060666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.17764666666666679 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17764666666666679 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  51 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.444]
 [0.404]
 [0.39 ]
 [0.429]
 [0.397]
 [0.424]] [[3.427]
 [3.716]
 [3.776]
 [3.427]
 [3.584]
 [3.811]
 [3.574]] [[0.107]
 [0.245]
 [0.227]
 [0.107]
 [0.191]
 [0.231]
 [0.183]]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[4.744]
 [4.744]
 [4.744]
 [4.744]
 [4.744]
 [4.744]
 [4.744]] [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.471]
 [0.463]
 [0.434]
 [0.437]
 [0.434]
 [0.432]] [[4.208]
 [4.363]
 [4.235]
 [4.028]
 [4.22 ]
 [4.107]
 [4.287]] [[0.365]
 [0.435]
 [0.387]
 [0.294]
 [0.358]
 [0.319]
 [0.374]]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.505]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]] [[3.216]
 [2.102]
 [3.685]
 [3.685]
 [3.685]
 [3.685]
 [3.685]] [[0.286]
 [0.037]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.757]
 [0.614]
 [0.607]
 [0.494]
 [0.664]
 [0.605]] [[ 0.612]
 [ 1.185]
 [ 0.528]
 [-1.678]
 [-1.675]
 [-0.082]
 [-0.11 ]] [[0.476]
 [0.757]
 [0.614]
 [0.607]
 [0.494]
 [0.664]
 [0.605]]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.858]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]] [[4.572]
 [4.208]
 [4.572]
 [4.572]
 [4.572]
 [4.572]
 [4.572]] [[0.794]
 [0.858]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17762000000000014 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  78 total reward:  0.09999999999999964  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1754200000000001 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.1754200000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9917995261248094
maxi score, test score, baseline:  -0.1754200000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1754200000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1754200000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1754200000000001 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.1754200000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1754200000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74468386
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.06 ]
 [-0.241]
 [-0.043]
 [-0.094]
 [-0.246]
 [-0.06 ]] [[-0.193]
 [-0.13 ]
 [-0.668]
 [-0.304]
 [-1.871]
 [-0.409]
 [ 0.024]] [[0.298]
 [0.316]
 [0.104]
 [0.307]
 [0.061]
 [0.134]
 [0.337]]
maxi score, test score, baseline:  -0.1754200000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1754200000000001 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.1754200000000001 0.6860000000000002 0.6860000000000002
actor:  1 policy actor:  1  step number:  58 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.1754200000000001 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  56 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.008]
 [ 0.045]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[2.074]
 [1.504]
 [2.074]
 [2.074]
 [2.074]
 [2.074]
 [2.074]] [[ 0.225]
 [-0.084]
 [ 0.225]
 [ 0.225]
 [ 0.225]
 [ 0.225]
 [ 0.225]]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7486088
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.433]
 [0.349]
 [0.441]
 [0.348]
 [0.446]
 [0.453]] [[0.938]
 [0.773]
 [0.941]
 [0.866]
 [1.025]
 [0.916]
 [0.87 ]] [[-0.146]
 [-0.178]
 [-0.234]
 [-0.155]
 [-0.222]
 [-0.141]
 [-0.142]]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  0.167
siam score:  -0.74741167
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  61 total reward:  0.2266666666666659  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2658139447030845
actor:  1 policy actor:  1  step number:  51 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 3.836099483948357
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.072]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]] [[1.328]
 [1.562]
 [1.328]
 [1.328]
 [1.328]
 [1.328]
 [1.328]] [[-0.003]
 [ 0.087]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.384]
 [0.331]
 [0.239]
 [0.312]
 [0.239]
 [0.239]] [[0.814]
 [1.403]
 [1.416]
 [0.814]
 [1.357]
 [0.814]
 [0.814]] [[-0.299]
 [ 0.158]
 [ 0.12 ]
 [-0.299]
 [ 0.07 ]
 [-0.299]
 [-0.299]]
Printing some Q and Qe and total Qs values:  [[ 1.5  ]
 [-0.048]
 [ 1.5  ]
 [-0.054]
 [ 1.5  ]
 [-0.061]
 [ 1.5  ]] [[ 0.   ]
 [-1.063]
 [ 0.   ]
 [-2.64 ]
 [ 0.   ]
 [-2.805]
 [ 0.   ]] [[ 2.375]
 [ 0.673]
 [ 2.375]
 [ 0.023]
 [ 2.375]
 [-0.05 ]
 [ 2.375]]
start point for exploration sampling:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.143 0.041 0.061 0.02  0.143 0.327 0.265]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.74421847
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]] [[2.761]
 [2.761]
 [2.761]
 [2.761]
 [2.761]
 [2.761]
 [2.761]] [[0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]]
using explorer policy with actor:  1
siam score:  -0.7487305
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  0.01999999999999935  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.018]
 [-0.026]
 [-0.043]
 [-0.043]
 [-0.045]
 [-0.027]] [[ 0.245]
 [ 0.56 ]
 [ 0.264]
 [-0.018]
 [ 0.027]
 [ 0.089]
 [ 0.245]] [[-0.429]
 [-0.303]
 [-0.416]
 [-0.531]
 [-0.514]
 [-0.493]
 [-0.424]]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.8106871409883487
maxi score, test score, baseline:  -0.17279333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  61 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.17071333333333347 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17071333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17071333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  11091
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.553]
 [0.363]
 [0.286]
 [0.369]
 [0.295]
 [0.341]] [[1.435]
 [0.708]
 [1.345]
 [1.292]
 [1.356]
 [1.319]
 [1.282]] [[ 0.044]
 [-0.045]
 [ 0.084]
 [-0.021]
 [ 0.095]
 [ 0.003]
 [ 0.03 ]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.0533333333333329  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.17071333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.17071333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17071333333333347 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17071333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17071333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17071333333333347 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17071333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17071333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.769]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]] [[1.691]
 [1.721]
 [1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]] [[0.731]
 [0.769]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]]
maxi score, test score, baseline:  -0.17071333333333347 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.17307333333333344 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  65 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.1036876492222959
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[2.193]
 [2.728]
 [2.728]
 [2.728]
 [2.728]
 [2.728]
 [2.728]] [[0.658]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]]
maxi score, test score, baseline:  -0.17580666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]] [[3.342]
 [3.342]
 [3.342]
 [3.342]
 [3.342]
 [3.342]
 [3.342]] [[0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]]
maxi score, test score, baseline:  -0.17580666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  62 total reward:  0.07333333333333247  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.17580666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17580666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.17580666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.019]
 [-0.023]
 [-0.026]
 [-0.029]
 [-0.022]
 [-0.029]] [[ 0.088]
 [ 0.305]
 [ 0.337]
 [ 0.016]
 [-0.063]
 [-0.048]
 [ 0.093]] [[-0.396]
 [-0.274]
 [-0.261]
 [-0.432]
 [-0.476]
 [-0.463]
 [-0.394]]
maxi score, test score, baseline:  -0.17580666666666678 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17580666666666678 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17580666666666678 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.17294000000000012 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]] [[1.517]
 [1.517]
 [1.517]
 [1.517]
 [1.517]
 [1.517]
 [1.517]] [[0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]]
maxi score, test score, baseline:  -0.17294000000000012 0.6860000000000002 0.6860000000000002
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.463]
 [0.395]
 [0.318]
 [0.337]
 [0.354]
 [0.355]] [[1.958]
 [1.807]
 [2.014]
 [1.907]
 [1.682]
 [2.03 ]
 [1.819]] [[-0.128]
 [-0.09 ]
 [-0.124]
 [-0.219]
 [-0.238]
 [-0.162]
 [-0.196]]
Starting evaluation
first move QE:  -0.6001328130270651
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.458]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[1.685]
 [1.791]
 [1.685]
 [1.685]
 [1.685]
 [1.685]
 [1.685]] [[0.5  ]
 [0.458]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.17294000000000012 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  -0.17294000000000012 0.6860000000000002 0.6860000000000002
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.77 ]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]] [[2.101]
 [1.776]
 [2.195]
 [2.195]
 [2.195]
 [2.195]
 [2.195]] [[0.681]
 [0.77 ]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.17294000000000012 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.17294000000000012 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.889]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]] [[2.684]
 [2.087]
 [2.375]
 [2.375]
 [2.375]
 [2.375]
 [2.375]] [[0.811]
 [0.889]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]]
maxi score, test score, baseline:  -0.17294000000000012 0.6860000000000002 0.6860000000000002
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.733]
 [0.683]
 [0.644]
 [0.632]
 [0.638]
 [0.695]] [[2.619]
 [1.113]
 [1.885]
 [2.662]
 [2.177]
 [2.015]
 [1.717]] [[0.71 ]
 [0.733]
 [0.683]
 [0.644]
 [0.632]
 [0.638]
 [0.695]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
using explorer policy with actor:  1
first move QE:  -0.600266323559483
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  65 total reward:  0.07999999999999896  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  50 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  77 total reward:  0.15999999999999814  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7408241
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.107]
 [-0.113]
 [-0.13 ]
 [-0.113]
 [-0.114]
 [-0.125]
 [-0.119]] [[2.211]
 [0.318]
 [0.729]
 [0.793]
 [0.459]
 [1.281]
 [0.673]] [[ 0.442]
 [-0.195]
 [-0.075]
 [-0.037]
 [-0.149]
 [ 0.114]
 [-0.082]]
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  11091
siam score:  -0.74006486
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.74080294
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  71 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.13807333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  46 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.13863333333333347 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  50 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.13863333333333347 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.13891333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13891333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13891333333333344 0.6880000000000001 0.6880000000000001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.4733161930017173
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]] [[3.381]
 [3.381]
 [3.381]
 [3.381]
 [3.381]
 [3.381]
 [3.381]] [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.17333333333333245  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.13891333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.13891333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.058]
 [-0.2  ]
 [-0.058]
 [-0.058]
 [-0.058]
 [-0.058]] [[ 0.195]
 [-1.255]
 [-0.441]
 [-1.255]
 [-1.255]
 [-1.255]
 [-1.255]] [[0.796]
 [0.382]
 [0.556]
 [0.382]
 [0.382]
 [0.382]
 [0.382]]
first move QE:  -0.6036039004648693
siam score:  -0.7467812
Printing some Q and Qe and total Qs values:  [[ 0.026]
 [ 0.113]
 [-0.   ]
 [ 0.001]
 [ 0.008]
 [-0.001]
 [-0.004]] [[0.991]
 [1.945]
 [1.288]
 [0.915]
 [0.882]
 [0.818]
 [0.808]] [[-0.255]
 [ 0.308]
 [-0.121]
 [-0.314]
 [-0.326]
 [-0.366]
 [-0.374]]
maxi score, test score, baseline:  -0.13891333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13891333333333344 0.6880000000000001 0.6880000000000001
line 256 mcts: sample exp_bonus 1.3716791931677181
maxi score, test score, baseline:  -0.13891333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13891333333333344 0.6880000000000001 0.6880000000000001
actor:  1 policy actor:  1  step number:  68 total reward:  0.04666666666666586  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.13891333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13891333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.86 ]
 [0.912]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]] [[2.868]
 [2.218]
 [2.868]
 [2.868]
 [2.868]
 [2.868]
 [2.868]] [[0.86 ]
 [0.912]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]]
Printing some Q and Qe and total Qs values:  [[0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.778]
 [0.802]] [[0.831]
 [0.144]
 [0.245]
 [0.195]
 [0.04 ]
 [0.856]
 [0.014]] [[0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.778]
 [0.802]]
actor:  0 policy actor:  0  step number:  29 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  63 total reward:  0.06666666666666599  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.078]
 [-0.14 ]
 [-0.071]
 [-0.071]
 [-0.075]
 [-0.074]
 [-0.069]] [[-3.871]
 [-1.332]
 [-3.891]
 [-4.009]
 [-4.231]
 [-4.212]
 [-4.231]] [[-0.172]
 [ 0.341]
 [-0.174]
 [-0.198]
 [-0.246]
 [-0.242]
 [-0.244]]
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  64 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.005792139739448643
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73570067
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.311]
 [0.243]
 [0.262]
 [0.262]
 [0.262]
 [0.262]] [[2.028]
 [1.941]
 [1.581]
 [2.072]
 [2.072]
 [2.072]
 [2.072]] [[-0.299]
 [-0.252]
 [-0.381]
 [-0.279]
 [-0.279]
 [-0.279]
 [-0.279]]
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.6066789813348289
actor:  1 policy actor:  1  step number:  68 total reward:  0.15333333333333232  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.183]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]] [[1.856]
 [1.831]
 [1.856]
 [1.856]
 [1.856]
 [1.856]
 [1.856]] [[-0.521]
 [-0.471]
 [-0.521]
 [-0.521]
 [-0.521]
 [-0.521]
 [-0.521]]
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  59 total reward:  0.05333333333333257  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  56 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.479]
 [0.375]
 [0.37 ]
 [0.361]
 [0.39 ]
 [0.369]] [[1.986]
 [1.173]
 [1.56 ]
 [1.137]
 [1.244]
 [1.691]
 [1.389]] [[0.362]
 [0.479]
 [0.375]
 [0.37 ]
 [0.361]
 [0.39 ]
 [0.369]]
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14003333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.74181384
maxi score, test score, baseline:  -0.1432066666666668 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.1432066666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1432066666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.081]
 [-0.076]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.087]
 [-0.081]] [[0.167]
 [0.621]
 [0.167]
 [0.167]
 [0.167]
 [0.03 ]
 [0.167]] [[0.439]
 [0.526]
 [0.439]
 [0.439]
 [0.439]
 [0.411]
 [0.439]]
maxi score, test score, baseline:  -0.1432066666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1432066666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1432066666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1432066666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1432066666666668 0.6880000000000001 0.6880000000000001
actor:  0 policy actor:  0  step number:  42 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.1435666666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1435666666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.14375333333333346 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.14375333333333346 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.543]
 [0.502]
 [0.486]
 [0.477]
 [0.474]
 [0.447]] [[3.993]
 [3.282]
 [3.502]
 [3.684]
 [3.703]
 [3.658]
 [3.151]] [[0.728]
 [0.577]
 [0.607]
 [0.639]
 [0.639]
 [0.628]
 [0.506]]
siam score:  -0.746022
maxi score, test score, baseline:  -0.14375333333333346 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74432814
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.492]
 [0.474]
 [0.452]
 [0.469]
 [0.45 ]
 [0.477]] [[-1.181]
 [-0.752]
 [-1.151]
 [-1.177]
 [-1.076]
 [-1.019]
 [-1.245]] [[0.477]
 [0.492]
 [0.474]
 [0.452]
 [0.469]
 [0.45 ]
 [0.477]]
maxi score, test score, baseline:  -0.14375333333333346 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.1437400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  45 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.14422000000000013 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.14422000000000013 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14422000000000013 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.14422000000000013 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14422000000000013 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[1.889]
 [1.889]
 [1.889]
 [1.889]
 [1.889]
 [1.889]
 [1.889]] [[-0.15]
 [-0.15]
 [-0.15]
 [-0.15]
 [-0.15]
 [-0.15]
 [-0.15]]
maxi score, test score, baseline:  -0.14422000000000013 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14422000000000013 0.6880000000000001 0.6880000000000001
actor:  1 policy actor:  1  step number:  61 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.14422000000000013 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14422000000000013 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14422000000000013 0.6880000000000001 0.6880000000000001
first move QE:  -0.6115266547988111
maxi score, test score, baseline:  -0.14422000000000013 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.636]
 [0.491]
 [0.557]
 [0.622]
 [0.622]
 [0.596]] [[1.63 ]
 [0.677]
 [1.039]
 [1.506]
 [0.153]
 [0.153]
 [1.726]] [[0.564]
 [0.636]
 [0.491]
 [0.557]
 [0.622]
 [0.622]
 [0.596]]
actor:  1 policy actor:  1  step number:  81 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.294]
 [-0.295]
 [-0.294]
 [-0.294]
 [-0.294]
 [-0.294]
 [-0.294]] [[-0.078]
 [-0.148]
 [-0.338]
 [-0.342]
 [-0.363]
 [-0.384]
 [-0.482]] [[0.446]
 [0.429]
 [0.384]
 [0.383]
 [0.378]
 [0.373]
 [0.349]]
actor:  0 policy actor:  0  step number:  36 total reward:  0.54  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]] [[2.13]
 [2.13]
 [2.13]
 [2.13]
 [2.13]
 [2.13]
 [2.13]] [[0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]]
maxi score, test score, baseline:  -0.14427333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.3638844875996265
maxi score, test score, baseline:  -0.14743333333333347 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14743333333333347 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.64 ]
 [0.615]
 [0.656]
 [0.635]
 [0.635]
 [0.635]] [[2.238]
 [1.221]
 [2.911]
 [3.096]
 [3.767]
 [3.767]
 [3.767]] [[0.629]
 [0.64 ]
 [0.615]
 [0.656]
 [0.635]
 [0.635]
 [0.635]]
actor:  0 policy actor:  0  step number:  60 total reward:  0.08666666666666578  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1483800000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1483800000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  64 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1483400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.1483400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1483400000000001 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.1483400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  11091
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  44 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  64 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 4.711933630818032
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7527348
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74979013
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.6143074949563172
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.057]
 [-0.089]
 [-0.108]
 [-0.108]
 [-0.124]
 [-0.052]] [[3.111]
 [1.981]
 [1.974]
 [1.655]
 [1.655]
 [0.883]
 [0.747]] [[ 0.318]
 [ 0.006]
 [-0.02 ]
 [-0.114]
 [-0.114]
 [-0.319]
 [-0.298]]
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14879333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14879333333333347 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.6147794463796419
actor:  1 policy actor:  1  step number:  57 total reward:  0.2133333333333326  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.14879333333333347 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14879333333333347 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7531085
maxi score, test score, baseline:  -0.14879333333333347 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]] [[1.981]
 [1.657]
 [1.981]
 [1.981]
 [1.981]
 [1.981]
 [1.981]] [[0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]]
maxi score, test score, baseline:  -0.14879333333333347 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14879333333333347 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  51 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.00655397838057455
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.168]
 [0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.053]] [[-0.382]
 [-0.177]
 [-0.382]
 [-0.382]
 [-0.382]
 [-0.382]
 [-0.018]] [[0.042]
 [0.168]
 [0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.053]]
maxi score, test score, baseline:  -0.14650000000000016 0.6880000000000001 0.6880000000000001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7933727214673767
actor:  0 policy actor:  0  step number:  45 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.5
siam score:  -0.75279295
Printing some Q and Qe and total Qs values:  [[-0.095]
 [-0.095]
 [-0.005]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]] [[1.784]
 [1.784]
 [2.507]
 [1.784]
 [1.784]
 [1.784]
 [1.784]] [[0.377]
 [0.377]
 [0.598]
 [0.377]
 [0.377]
 [0.377]
 [0.377]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.35214651456733703
actor:  1 policy actor:  1  step number:  53 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.098]
 [-0.033]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]] [[0.755]
 [1.915]
 [1.066]
 [1.066]
 [1.066]
 [1.066]
 [1.066]] [[-0.17 ]
 [ 0.339]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]]
siam score:  -0.74570036
maxi score, test score, baseline:  -0.14624666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  69 total reward:  0.21333333333333215  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.14624666666666683 0.6880000000000001 0.6880000000000001
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.415]] [[3.205]
 [3.205]
 [3.205]
 [3.205]
 [3.205]
 [3.205]
 [3.533]] [[0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.616]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  101 total reward:  0.02666666666666473  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.14624666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14624666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7393538
maxi score, test score, baseline:  -0.14624666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  44 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.1434866666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.783]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]] [[2.474]
 [2.363]
 [2.613]
 [2.613]
 [2.613]
 [2.613]
 [2.613]] [[0.711]
 [0.783]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]]
maxi score, test score, baseline:  -0.1434866666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.1404333333333335 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.1404333333333335 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1404333333333335 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1404333333333335 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  42 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.14054000000000014 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14054000000000014 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14054000000000014 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14054000000000014 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.14054000000000014 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.14054000000000014 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.018892761582593
siam score:  -0.7545115
siam score:  -0.75837034
actor:  0 policy actor:  0  step number:  44 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.13780666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.013]
 [-0.004]
 [-0.022]
 [-0.064]
 [-0.011]
 [-0.013]] [[-0.032]
 [-0.323]
 [-0.431]
 [-0.324]
 [-0.836]
 [-0.542]
 [-0.323]] [[-0.088]
 [-0.105]
 [-0.112]
 [-0.112]
 [-0.21 ]
 [-0.132]
 [-0.105]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.13780666666666683 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.13780666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13780666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13780666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13780666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13780666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13780666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13780666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13780666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13780666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.13780666666666683 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.13780666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13780666666666683 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.13723333333333343 0.6880000000000001 0.6880000000000001
Printing some Q and Qe and total Qs values:  [[-0.094]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.094]
 [-0.098]
 [-0.097]] [[ 0.43 ]
 [ 0.521]
 [ 0.418]
 [-0.282]
 [-0.258]
 [ 0.229]
 [-0.259]] [[-0.118]
 [-0.086]
 [-0.121]
 [-0.353]
 [-0.347]
 [-0.189]
 [-0.351]]
maxi score, test score, baseline:  -0.13723333333333343 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13723333333333343 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.084]
 [0.092]
 [0.097]
 [0.105]
 [0.1  ]
 [0.103]] [[3.562]
 [3.31 ]
 [3.33 ]
 [4.349]
 [2.575]
 [2.626]
 [2.724]] [[ 0.319]
 [ 0.226]
 [ 0.238]
 [ 0.586]
 [-0.008]
 [ 0.006]
 [ 0.04 ]]
maxi score, test score, baseline:  -0.13723333333333343 0.6880000000000001 0.6880000000000001
actor:  0 policy actor:  0  step number:  37 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]] [[5.349]
 [3.335]
 [3.335]
 [3.335]
 [3.335]
 [3.335]
 [3.335]] [[0.414]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]]
maxi score, test score, baseline:  -0.1341400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1341400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1341400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1341400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1341400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.672]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[0.345]
 [0.849]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[0.626]
 [0.672]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]]
maxi score, test score, baseline:  -0.1341400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1341400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7660375
maxi score, test score, baseline:  -0.1341400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]] [[2.125]
 [2.125]
 [2.125]
 [2.125]
 [2.125]
 [2.125]
 [2.125]] [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]]
maxi score, test score, baseline:  -0.1341400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1341400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1341400000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76400954
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  50 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1342600000000001 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.1342600000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1342600000000001 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.1342600000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.1342600000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.1342600000000001 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.852]
 [0.835]
 [0.731]
 [0.675]
 [0.731]
 [0.731]] [[1.108]
 [0.074]
 [0.571]
 [0.16 ]
 [0.317]
 [0.16 ]
 [0.16 ]] [[0.769]
 [0.852]
 [0.835]
 [0.731]
 [0.675]
 [0.731]
 [0.731]]
actor:  0 policy actor:  0  step number:  54 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.13168666666666679 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.13168666666666679 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13168666666666679 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  55 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  52 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.13234000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.941]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[ 1.167]
 [-0.029]
 [ 1.167]
 [ 1.167]
 [ 1.167]
 [ 1.167]
 [ 1.167]] [[0.504]
 [0.941]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]]
maxi score, test score, baseline:  -0.13234000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[1.91]
 [1.91]
 [1.91]
 [1.91]
 [1.91]
 [1.91]
 [1.91]] [[0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]]
maxi score, test score, baseline:  -0.13234000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.762]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[1.948]
 [2.105]
 [1.948]
 [1.948]
 [1.948]
 [1.948]
 [1.948]] [[0.675]
 [0.762]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]]
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.818]
 [0.513]
 [0.756]
 [0.718]
 [0.717]
 [0.797]] [[1.661]
 [1.006]
 [1.42 ]
 [1.718]
 [1.683]
 [1.902]
 [1.358]] [[0.774]
 [0.818]
 [0.513]
 [0.756]
 [0.718]
 [0.717]
 [0.797]]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.12950000000000014 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  45 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.12688666666666681 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12688666666666681 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76550263
maxi score, test score, baseline:  -0.12688666666666681 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.12688666666666681 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12688666666666681 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12688666666666681 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.12688666666666681 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12688666666666681 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  0.0599999999999995  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 1.4003056945557253
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.474]
 [0.359]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[2.316]
 [2.006]
 [2.101]
 [2.041]
 [2.041]
 [2.041]
 [2.041]] [[-0.111]
 [-0.052]
 [-0.151]
 [-0.176]
 [-0.176]
 [-0.176]
 [-0.176]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.522]
 [0.413]
 [0.425]
 [0.384]
 [0.447]
 [0.451]] [[2.883]
 [2.262]
 [2.632]
 [2.134]
 [2.392]
 [2.938]
 [2.417]] [[-0.093]
 [ 0.03 ]
 [-0.017]
 [-0.088]
 [-0.086]
 [ 0.068]
 [-0.015]]
first move QE:  -0.6226279474480142
maxi score, test score, baseline:  -0.12688666666666681 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  55 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1266466666666668 0.6880000000000001 0.6880000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1266466666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1266466666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1266466666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1266466666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  40 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.698]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]] [[1.157]
 [0.224]
 [1.157]
 [1.157]
 [1.157]
 [1.157]
 [1.157]] [[0.666]
 [0.698]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]]
actor:  0 policy actor:  0  step number:  43 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.12398000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.135]
 [0.075]
 [0.135]
 [0.135]
 [0.135]
 [0.135]] [[2.277]
 [2.277]
 [2.548]
 [2.277]
 [2.277]
 [2.277]
 [2.277]] [[0.319]
 [0.319]
 [0.374]
 [0.319]
 [0.319]
 [0.319]
 [0.319]]
maxi score, test score, baseline:  -0.12398000000000012 0.6880000000000001 0.6880000000000001
actor:  1 policy actor:  1  step number:  51 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.12398000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12398000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12398000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12398000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12398000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12398000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12398000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12398000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.12398000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12398000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.12398000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.12398000000000013 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12398000000000013 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.12398000000000013 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.12092666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.015508988033618
maxi score, test score, baseline:  -0.12092666666666678 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12092666666666678 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.12036666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.12036666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12036666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12036666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12036666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12036666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.12036666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.381]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.594]] [[3.032]
 [1.429]
 [2.081]
 [2.081]
 [2.081]
 [2.081]
 [1.034]] [[0.787]
 [0.381]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.594]]
maxi score, test score, baseline:  -0.12036666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.11730000000000011 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8899929251673439
actor:  0 policy actor:  0  step number:  44 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.11456666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11456666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.5001839070673375
using explorer policy with actor:  1
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.11456666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.11456666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11456666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11456666666666677 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.11456666666666677 0.6880000000000001 0.6880000000000001
line 256 mcts: sample exp_bonus 4.232632092715882
maxi score, test score, baseline:  -0.11456666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.923]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]] [[1.296]
 [1.492]
 [1.296]
 [1.296]
 [1.296]
 [1.296]
 [1.296]] [[0.827]
 [0.923]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]]
first move QE:  -0.6298487815987784
maxi score, test score, baseline:  -0.11456666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11456666666666677 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.11456666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  0.1466666666666665  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.11456666666666679 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.239]
 [0.106]
 [0.129]
 [0.133]
 [0.163]
 [0.143]] [[0.941]
 [1.09 ]
 [1.092]
 [1.147]
 [1.062]
 [1.048]
 [1.149]] [[-0.131]
 [ 0.032]
 [-0.064]
 [-0.02 ]
 [-0.059]
 [-0.044]
 [-0.009]]
maxi score, test score, baseline:  -0.11456666666666679 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11456666666666679 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.11456666666666679 0.6880000000000001 0.6880000000000001
actor:  1 policy actor:  1  step number:  42 total reward:  0.5  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]
 [0.4]] [[2.589]
 [2.589]
 [2.589]
 [2.589]
 [2.589]
 [2.589]
 [2.589]] [[0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.11456666666666679 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11456666666666679 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.78 ]
 [0.706]
 [0.649]
 [0.649]
 [0.649]
 [0.649]] [[3.827]
 [4.284]
 [4.568]
 [3.827]
 [3.827]
 [3.827]
 [3.827]] [[0.649]
 [0.78 ]
 [0.706]
 [0.649]
 [0.649]
 [0.649]
 [0.649]]
maxi score, test score, baseline:  -0.11456666666666679 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11456666666666679 0.6880000000000001 0.6880000000000001
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.504]
 [0.459]
 [0.548]] [[1.289]
 [1.289]
 [1.289]
 [1.289]
 [1.299]
 [1.289]
 [1.499]] [[0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.211]
 [0.16 ]
 [0.389]]
maxi score, test score, baseline:  -0.11456666666666679 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.11456666666666679 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.769504
maxi score, test score, baseline:  -0.11667333333333343 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  51 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.11403333333333343 0.6880000000000001 0.6880000000000001
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]] [[3.268]
 [3.268]
 [3.268]
 [3.268]
 [3.268]
 [3.268]
 [3.268]] [[0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]]
maxi score, test score, baseline:  -0.11403333333333343 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11403333333333343 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.11403333333333343 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11403333333333343 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11403333333333343 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  0.013333333333332864  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 2.2105507166247467
actor:  1 policy actor:  1  step number:  54 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  48 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.18 ]
 [0.195]
 [0.195]
 [0.178]
 [0.161]
 [0.203]] [[3.791]
 [3.189]
 [4.321]
 [3.812]
 [3.791]
 [3.124]
 [3.356]] [[0.358]
 [0.154]
 [0.55 ]
 [0.377]
 [0.358]
 [0.12 ]
 [0.227]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.11356666666666677 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.11356666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.11356666666666677 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  61 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.711]
 [0.653]
 [0.63 ]
 [0.65 ]
 [0.593]
 [0.593]] [[2.105]
 [1.597]
 [1.676]
 [1.598]
 [1.531]
 [2.066]
 [2.066]] [[0.388]
 [0.273]
 [0.241]
 [0.191]
 [0.189]
 [0.31 ]
 [0.31 ]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  69 total reward:  0.09333333333333216  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.11062000000000011 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.11062000000000011 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]] [[0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]] [[0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.013]
 [-0.015]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[0.934]
 [1.178]
 [0.818]
 [1.938]
 [1.938]
 [1.938]
 [1.938]] [[-0.825]
 [-0.712]
 [-0.895]
 [-0.334]
 [-0.334]
 [-0.334]
 [-0.334]]
maxi score, test score, baseline:  -0.11062000000000011 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11062000000000011 0.6880000000000001 0.6880000000000001
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.4  ]
 [0.432]
 [0.426]
 [0.435]
 [0.443]
 [0.444]] [[3.307]
 [3.257]
 [3.572]
 [3.705]
 [3.629]
 [3.437]
 [3.643]] [[0.323]
 [0.287]
 [0.421]
 [0.463]
 [0.443]
 [0.381]
 [0.454]]
maxi score, test score, baseline:  -0.11062000000000011 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.428]
 [0.404]
 [0.365]
 [0.365]
 [0.342]
 [0.385]] [[4.346]
 [4.38 ]
 [4.772]
 [3.945]
 [3.945]
 [4.845]
 [4.274]] [[0.609]
 [0.652]
 [0.74 ]
 [0.51 ]
 [0.51 ]
 [0.727]
 [0.603]]
maxi score, test score, baseline:  -0.11062000000000011 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.564]
 [0.437]
 [0.544]
 [0.437]
 [0.302]
 [0.437]] [[3.403]
 [2.955]
 [2.963]
 [2.953]
 [2.963]
 [3.423]
 [2.963]] [[0.554]
 [0.468]
 [0.356]
 [0.449]
 [0.356]
 [0.442]
 [0.356]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.11062000000000011 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11062000000000011 0.6880000000000001 0.6880000000000001
siam score:  -0.77763194
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.501]
 [0.501]
 [0.62 ]
 [0.501]
 [0.501]
 [0.617]] [[2.821]
 [3.178]
 [3.178]
 [2.428]
 [3.178]
 [3.178]
 [2.548]] [[0.584]
 [0.647]
 [0.647]
 [0.43 ]
 [0.647]
 [0.647]
 [0.479]]
maxi score, test score, baseline:  -0.11062000000000011 0.6880000000000001 0.6880000000000001
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.471]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]] [[3.635]
 [3.906]
 [3.635]
 [3.635]
 [3.635]
 [3.635]
 [3.635]] [[0.395]
 [0.521]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.11062000000000011 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.11062000000000011 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11062000000000011 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.5264393683964605
maxi score, test score, baseline:  -0.11062000000000011 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.11062000000000011 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  66 total reward:  0.16666666666666596  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.11062000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[ 0.073]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]] [[2.251]
 [1.528]
 [1.528]
 [1.528]
 [1.528]
 [1.528]
 [1.528]] [[0.432]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.11062000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.11062000000000012 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.11062000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.449]
 [0.364]
 [0.368]
 [0.376]
 [0.371]
 [0.401]] [[1.615]
 [1.593]
 [1.605]
 [1.485]
 [1.437]
 [1.475]
 [1.517]] [[-0.073]
 [-0.056]
 [-0.138]
 [-0.155]
 [-0.155]
 [-0.153]
 [-0.116]]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]] [[2.333]
 [1.908]
 [1.908]
 [1.908]
 [1.908]
 [1.908]
 [1.908]] [[0.273]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]] [[1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]] [[-0.296]
 [-0.296]
 [-0.296]
 [-0.296]
 [-0.296]
 [-0.296]
 [-0.296]]
start point for exploration sampling:  11091
siam score:  -0.78310454
maxi score, test score, baseline:  -0.11062000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11062000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11062000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.11062000000000012 0.6880000000000001 0.6880000000000001
actor:  1 policy actor:  1  step number:  58 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.11062000000000012 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.11062000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7784186
maxi score, test score, baseline:  -0.11062000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.058]
 [-0.036]
 [-0.058]
 [-0.058]
 [-0.058]
 [-0.092]
 [-0.058]] [[0.487]
 [0.505]
 [0.487]
 [0.487]
 [0.487]
 [0.167]
 [0.487]] [[0.519]
 [0.528]
 [0.519]
 [0.519]
 [0.519]
 [0.447]
 [0.519]]
maxi score, test score, baseline:  -0.11062000000000012 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  67 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  47 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.10779333333333348 0.6880000000000001 0.6880000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10779333333333348 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.10779333333333348 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10779333333333348 0.6880000000000001 0.6880000000000001
actor:  1 policy actor:  1  step number:  59 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.10779333333333348 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10779333333333348 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10779333333333348 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.10779333333333348 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  66 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.126]
 [0.06 ]
 [0.091]
 [0.071]
 [0.07 ]
 [0.103]] [[-2.359]
 [-1.995]
 [-2.144]
 [-2.421]
 [-2.398]
 [-2.489]
 [-2.535]] [[0.053]
 [0.126]
 [0.06 ]
 [0.091]
 [0.071]
 [0.07 ]
 [0.103]]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  36 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.667
UNIT TEST: sample policy line 217 mcts : [0.102 0.347 0.184 0.143 0.061 0.061 0.102]
maxi score, test score, baseline:  -0.1048466666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1048466666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1048466666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.632]
 [0.532]
 [0.632]
 [0.646]
 [0.632]
 [0.632]] [[1.02 ]
 [0.787]
 [2.35 ]
 [0.787]
 [0.802]
 [0.787]
 [0.787]] [[0.672]
 [0.632]
 [0.532]
 [0.632]
 [0.646]
 [0.632]
 [0.632]]
maxi score, test score, baseline:  -0.1048466666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.891]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]] [[2.398]
 [1.415]
 [2.243]
 [2.243]
 [2.243]
 [2.243]
 [2.243]] [[0.751]
 [0.891]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]]
maxi score, test score, baseline:  -0.1048466666666668 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.1048466666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.1048466666666668 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  46 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.10227333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10227333333333345 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.10483333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10483333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.2  ]
 [0.369]
 [0.415]
 [0.411]
 [0.412]
 [0.421]] [[1.5  ]
 [0.501]
 [2.082]
 [2.083]
 [2.028]
 [1.546]
 [1.003]] [[ 0.1  ]
 [-0.255]
 [ 0.519]
 [ 0.556]
 [ 0.531]
 [ 0.338]
 [ 0.126]]
line 256 mcts: sample exp_bonus 0.5339347324607101
actor:  1 policy actor:  1  step number:  61 total reward:  0.1599999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.10483333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10483333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
siam score:  -0.7712625
maxi score, test score, baseline:  -0.10483333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10483333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10483333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  64 total reward:  0.20666666666666567  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]
 [0.51]] [[1.391]
 [1.391]
 [1.391]
 [1.391]
 [1.391]
 [1.391]
 [1.391]] [[0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.10483333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10483333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10483333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  56 total reward:  0.31333333333333324  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[1.728]
 [1.728]
 [1.728]
 [1.728]
 [1.728]
 [1.728]
 [1.728]] [[-0.387]
 [-0.387]
 [-0.387]
 [-0.387]
 [-0.387]
 [-0.387]
 [-0.387]]
line 256 mcts: sample exp_bonus -1.0508540452932231
maxi score, test score, baseline:  -0.10483333333333344 0.6880000000000001 0.6880000000000001
maxi score, test score, baseline:  -0.10483333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10483333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10483333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  46 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.692]
 [0.621]
 [0.574]
 [0.55 ]
 [0.556]
 [0.59 ]] [[3.351]
 [3.141]
 [2.956]
 [2.788]
 [2.982]
 [3.224]
 [2.566]] [[0.636]
 [0.632]
 [0.503]
 [0.4  ]
 [0.458]
 [0.558]
 [0.326]]
maxi score, test score, baseline:  -0.10220666666666675 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10220666666666675 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10220666666666675 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 2.5423130997902437
maxi score, test score, baseline:  -0.10220666666666675 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10220666666666675 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  92 total reward:  0.006666666666664933  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.10220666666666675 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.10220666666666675 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.167]
 [0.129]
 [0.071]
 [0.073]
 [0.089]
 [0.108]] [[5.009]
 [3.725]
 [4.826]
 [2.914]
 [2.914]
 [3.535]
 [4.282]] [[0.688]
 [0.468]
 [0.673]
 [0.266]
 [0.267]
 [0.398]
 [0.556]]
maxi score, test score, baseline:  -0.10220666666666675 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.183]
 [0.11 ]
 [0.072]
 [0.114]
 [0.089]
 [0.112]] [[4.544]
 [2.483]
 [3.251]
 [2.809]
 [3.042]
 [3.032]
 [2.965]] [[ 0.019]
 [-0.242]
 [-0.187]
 [-0.298]
 [-0.218]
 [-0.244]
 [-0.232]]
Printing some Q and Qe and total Qs values:  [[0.882]
 [0.961]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]] [[4.794]
 [4.587]
 [4.389]
 [4.389]
 [4.389]
 [4.389]
 [4.389]] [[0.882]
 [0.961]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]]
maxi score, test score, baseline:  -0.10220666666666675 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.696]
 [0.393]
 [0.569]
 [0.576]
 [0.533]
 [0.522]] [[4.237]
 [3.911]
 [3.419]
 [3.319]
 [2.973]
 [3.249]
 [3.088]] [[0.343]
 [0.451]
 [0.066]
 [0.225]
 [0.174]
 [0.177]
 [0.139]]
actor:  0 policy actor:  0  step number:  57 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  60 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.167
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.09951333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.09951333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.803]
 [0.906]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]] [[2.181]
 [2.336]
 [2.181]
 [2.181]
 [2.181]
 [2.181]
 [2.181]] [[0.803]
 [0.906]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.8  ]
 [0.857]
 [0.83 ]
 [0.771]
 [0.833]
 [0.757]
 [0.764]] [[3.02 ]
 [2.413]
 [3.369]
 [2.798]
 [2.77 ]
 [2.705]
 [2.427]] [[0.8  ]
 [0.857]
 [0.83 ]
 [0.771]
 [0.833]
 [0.757]
 [0.764]]
line 256 mcts: sample exp_bonus 1.1167912195345782
Printing some Q and Qe and total Qs values:  [[1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]] [[1.973]
 [1.973]
 [1.973]
 [1.973]
 [1.973]
 [1.973]
 [1.973]] [[1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]]
maxi score, test score, baseline:  -0.09951333333333345 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.819]
 [0.939]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.866]] [[3.037]
 [1.314]
 [2.417]
 [2.417]
 [2.417]
 [2.417]
 [2.409]] [[0.819]
 [0.939]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.866]]
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.937]
 [0.688]
 [0.821]
 [0.788]
 [0.875]
 [0.904]] [[3.021]
 [1.262]
 [1.743]
 [2.048]
 [2.496]
 [1.412]
 [2.312]] [[0.794]
 [0.937]
 [0.688]
 [0.821]
 [0.788]
 [0.875]
 [0.904]]
maxi score, test score, baseline:  -0.09951333333333344 0.6880000000000001 0.6880000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]] [[2.228]
 [2.228]
 [2.228]
 [2.228]
 [2.228]
 [2.228]
 [2.228]] [[0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]]
line 256 mcts: sample exp_bonus 1.8459563501297445
actor:  1 policy actor:  1  step number:  42 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.04159333333333344 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  41 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.03879333333333344 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  58 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  46 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
first move QE:  -0.6376881625176538
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.03364666666666678 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.03364666666666678 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.1599999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.03364666666666678 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.03364666666666678 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.03652666666666675 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.03652666666666675 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]] [[1.691]
 [1.691]
 [1.691]
 [1.691]
 [1.691]
 [1.691]
 [1.691]] [[-0.19]
 [-0.19]
 [-0.19]
 [-0.19]
 [-0.19]
 [-0.19]
 [-0.19]]
maxi score, test score, baseline:  -0.03652666666666675 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.03652666666666675 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.471]
 [0.439]
 [0.438]
 [0.439]
 [0.439]
 [0.439]] [[1.308]
 [2.091]
 [1.308]
 [1.792]
 [1.308]
 [1.308]
 [1.308]] [[-0.191]
 [-0.029]
 [-0.191]
 [-0.112]
 [-0.191]
 [-0.191]
 [-0.191]]
maxi score, test score, baseline:  -0.03652666666666675 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.03652666666666675 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  62 total reward:  0.07333333333333258  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  64 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.03652666666666675 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.03652666666666675 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  0.48  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.036433333333333415 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  48 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.03675333333333342 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.03675333333333342 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03675333333333345 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  43 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.03338000000000012 0.6920000000000001 0.6920000000000001
line 256 mcts: sample exp_bonus 0.33083880924426157
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.203]
 [-0.027]
 [-0.027]
 [-0.026]
 [-0.022]
 [-0.023]] [[-1.366]
 [ 0.292]
 [-1.505]
 [-1.506]
 [-1.507]
 [-0.938]
 [-1.366]] [[-0.569]
 [ 0.253]
 [-0.651]
 [-0.651]
 [-0.651]
 [-0.312]
 [-0.565]]
maxi score, test score, baseline:  -0.03338000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  -0.03338000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.033380000000000104 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.030606666666666765 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.030606666666666765 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.030606666666666765 0.6920000000000001 0.6920000000000001
actor:  0 policy actor:  0  step number:  45 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.027966666666666765 0.6920000000000001 0.6920000000000001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  69 total reward:  0.013333333333332309  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  40 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.027966666666666765 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  65 total reward:  0.09333333333333238  reward:  1.0 rdn_beta:  0.333
siam score:  -0.78403765
maxi score, test score, baseline:  -0.027966666666666765 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.575]
 [0.546]
 [0.546]
 [0.561]
 [0.534]
 [0.541]] [[-0.207]
 [ 0.346]
 [-0.101]
 [ 0.004]
 [-0.153]
 [-0.186]
 [-0.209]] [[0.561]
 [0.575]
 [0.546]
 [0.546]
 [0.561]
 [0.534]
 [0.541]]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.552]
 [0.531]
 [0.507]
 [0.505]
 [0.493]
 [0.525]] [[3.117]
 [3.377]
 [3.247]
 [3.195]
 [3.269]
 [3.38 ]
 [3.29 ]] [[0.38 ]
 [0.503]
 [0.439]
 [0.398]
 [0.42 ]
 [0.445]
 [0.447]]
maxi score, test score, baseline:  -0.027966666666666765 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.027966666666666765 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
first move QE:  -0.6399954234512469
actor:  1 policy actor:  1  step number:  54 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.030180000000000103 0.6920000000000001 0.6920000000000001
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.774]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]] [[0.811]
 [1.399]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]] [[0.756]
 [0.774]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.030180000000000103 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  0 policy actor:  0  step number:  35 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
from probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
from probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.027113333333333423 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.027113333333333423 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.876]
 [0.878]
 [0.88 ]
 [0.848]
 [0.875]
 [0.861]
 [0.859]] [[0.562]
 [0.541]
 [0.519]
 [0.404]
 [0.456]
 [0.633]
 [0.451]] [[0.876]
 [0.878]
 [0.88 ]
 [0.848]
 [0.875]
 [0.861]
 [0.859]]
line 256 mcts: sample exp_bonus -0.3415981814950177
actor:  0 policy actor:  0  step number:  32 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.3746305208924852
siam score:  -0.7868536
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  69 total reward:  0.18666666666666543  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.026220000000000108 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.026220000000000115 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.835]
 [0.666]
 [0.66 ]
 [0.738]
 [0.659]
 [0.679]] [[ 0.005]
 [-0.362]
 [-0.281]
 [-0.286]
 [-0.222]
 [-0.136]
 [ 0.132]] [[0.666]
 [0.835]
 [0.666]
 [0.66 ]
 [0.738]
 [0.659]
 [0.679]]
maxi score, test score, baseline:  -0.026220000000000115 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.026220000000000115 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[1.013]
 [0.867]
 [0.899]
 [0.867]
 [0.674]
 [0.674]
 [0.793]] [[1.42 ]
 [0.592]
 [0.612]
 [0.795]
 [1.368]
 [1.368]
 [0.616]] [[1.013]
 [0.867]
 [0.899]
 [0.867]
 [0.674]
 [0.674]
 [0.793]]
Printing some Q and Qe and total Qs values:  [[-0.024]
 [ 0.099]
 [ 0.099]
 [ 0.099]
 [ 0.099]
 [ 0.099]
 [ 0.099]] [[8.939]
 [2.762]
 [2.762]
 [2.762]
 [2.762]
 [2.762]
 [2.762]] [[0.861]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]]
actor:  0 policy actor:  0  step number:  43 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.036]
 [ 0.101]
 [ 0.101]
 [ 0.101]
 [ 0.101]
 [ 0.101]
 [ 0.101]] [[8.582]
 [3.954]
 [3.954]
 [3.954]
 [3.954]
 [3.954]
 [3.954]] [[0.845]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]]
maxi score, test score, baseline:  -0.023473333333333436 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.745]
 [0.646]
 [0.645]
 [0.63 ]
 [0.52 ]
 [0.683]] [[1.39 ]
 [1.018]
 [1.668]
 [1.446]
 [1.566]
 [1.37 ]
 [1.482]] [[ 0.051]
 [ 0.063]
 [ 0.072]
 [ 0.035]
 [ 0.039]
 [-0.104]
 [ 0.078]]
actor:  1 policy actor:  1  step number:  86 total reward:  0.11333333333333329  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.673 0.082 0.041 0.102 0.02  0.02  0.061]
maxi score, test score, baseline:  -0.023473333333333436 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.023473333333333436 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
line 256 mcts: sample exp_bonus 3.234770839341083
maxi score, test score, baseline:  -0.023473333333333436 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.023473333333333436 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.5  ]
 [0.451]
 [0.451]
 [0.451]
 [0.408]
 [0.451]] [[2.452]
 [1.879]
 [2.093]
 [2.093]
 [2.093]
 [2.809]
 [2.093]] [[-0.11 ]
 [-0.088]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.024]
 [-0.101]]
maxi score, test score, baseline:  -0.023473333333333436 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  60 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.023473333333333443 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.759]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]] [[1.095]
 [0.945]
 [1.095]
 [1.095]
 [1.095]
 [1.095]
 [1.095]] [[0.687]
 [0.759]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]]
maxi score, test score, baseline:  -0.023473333333333443 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  0 policy actor:  0  step number:  49 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.021046666666666783 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.021046666666666783 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.021046666666666783 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.021046666666666783 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
using explorer policy with actor:  0
actor:  0 policy actor:  0  step number:  48 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  77 total reward:  0.05333333333333201  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.02128666666666677 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.02128666666666677 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
siam score:  -0.79025954
actor:  0 policy actor:  1  step number:  69 total reward:  0.06666666666666576  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.019153333333333453 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.019153333333333453 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.019153333333333453 0.6920000000000001 0.6920000000000001
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  68 total reward:  0.046666666666665635  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  49 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.017060000000000117 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.017060000000000117 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]] [[0.998]
 [0.998]
 [0.998]
 [0.998]
 [0.998]
 [0.998]
 [0.998]] [[0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.017060000000000117 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.568]
 [0.547]
 [0.544]
 [0.335]
 [0.545]
 [0.556]] [[3.563]
 [2.905]
 [3.626]
 [3.094]
 [3.378]
 [2.989]
 [3.003]] [[0.534]
 [0.395]
 [0.619]
 [0.442]
 [0.398]
 [0.408]
 [0.42 ]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  39 total reward:  0.46666666666666634  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.01130000000000012 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.01130000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.01130000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.01130000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.01130000000000012 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.01130000000000012 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.01130000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.024]
 [ 0.239]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]] [[2.103]
 [2.103]
 [3.067]
 [2.103]
 [2.103]
 [2.103]
 [2.103]] [[-0.641]
 [-0.641]
 [-0.057]
 [-0.641]
 [-0.641]
 [-0.641]
 [-0.641]]
using explorer policy with actor:  0
first move QE:  -0.6469862075632502
maxi score, test score, baseline:  -0.011300000000000112 0.6920000000000001 0.6920000000000001
actor:  1 policy actor:  1  step number:  47 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.011300000000000112 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.011300000000000112 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.011300000000000112 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.011300000000000112 0.6920000000000001 0.6920000000000001
line 256 mcts: sample exp_bonus 1.8708171022916884
line 256 mcts: sample exp_bonus 1.7417658285408861
maxi score, test score, baseline:  -0.011300000000000112 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.011300000000000112 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.011300000000000112 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.011300000000000112 0.6920000000000001 0.6920000000000001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.011300000000000112 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  57 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.011300000000000112 0.6920000000000001 0.6920000000000001
from probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
siam score:  -0.7982072
siam score:  -0.7972374
maxi score, test score, baseline:  -0.011300000000000112 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.011300000000000112 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  0 policy actor:  1  step number:  52 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.008860000000000114 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[1.007]
 [1.022]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]] [[1.315]
 [0.775]
 [1.315]
 [1.315]
 [1.315]
 [1.315]
 [1.315]] [[1.007]
 [1.022]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]]
maxi score, test score, baseline:  -0.008860000000000114 0.6920000000000001 0.6920000000000001
actor:  1 policy actor:  1  step number:  63 total reward:  0.30666666666666575  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.6487950983645195
maxi score, test score, baseline:  -0.008860000000000114 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  0 policy actor:  0  step number:  55 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.409]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]] [[2.417]
 [2.564]
 [2.417]
 [2.417]
 [2.417]
 [2.417]
 [2.417]] [[0.432]
 [0.521]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.451]
 [0.49 ]
 [0.484]
 [0.42 ]
 [0.416]
 [0.482]] [[2.162]
 [2.819]
 [2.677]
 [2.723]
 [2.694]
 [2.832]
 [2.619]] [[-0.06 ]
 [ 0.219]
 [ 0.211]
 [ 0.219]
 [ 0.146]
 [ 0.188]
 [ 0.184]]
maxi score, test score, baseline:  -0.009073333333333447 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.266]
 [0.196]
 [0.207]
 [0.218]
 [0.207]
 [0.205]] [[2.222]
 [2.096]
 [1.548]
 [1.766]
 [1.744]
 [2.596]
 [2.077]] [[-0.242]
 [-0.257]
 [-0.418]
 [-0.371]
 [-0.364]
 [-0.232]
 [-0.321]]
from probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.655]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[2.078]
 [2.364]
 [2.078]
 [2.078]
 [2.078]
 [2.078]
 [2.078]] [[0.217]
 [0.415]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]]
maxi score, test score, baseline:  -0.009073333333333447 0.6920000000000001 0.6920000000000001
siam score:  -0.79634595
maxi score, test score, baseline:  -0.011526666666666779 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.011526666666666779 0.6920000000000001 0.6920000000000001
line 256 mcts: sample exp_bonus 0.8848562258100354
actor:  1 policy actor:  1  step number:  68 total reward:  0.1533333333333322  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.014380000000000125 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.014380000000000125 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.014380000000000125 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  48 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.09 ]
 [0.147]
 [0.063]
 [0.063]
 [0.066]
 [0.123]
 [0.065]] [[-0.757]
 [-0.3  ]
 [-0.961]
 [-0.989]
 [-0.946]
 [-0.636]
 [-0.908]] [[-0.548]
 [-0.415]
 [-0.609]
 [-0.614]
 [-0.604]
 [-0.495]
 [-0.599]]
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.003]
 [-0.031]
 [-0.031]
 [-0.045]
 [-0.005]
 [-0.031]] [[ 0.   ]
 [ 0.343]
 [ 0.   ]
 [ 0.   ]
 [-0.921]
 [ 0.708]
 [ 0.   ]] [[-0.542]
 [-0.456]
 [-0.542]
 [-0.542]
 [-0.709]
 [-0.398]
 [-0.542]]
actor:  0 policy actor:  0  step number:  66 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.012260000000000115 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  53 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  59 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.012260000000000115 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.012260000000000115 0.6920000000000001 0.6920000000000001
actor:  1 policy actor:  1  step number:  44 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.809]
 [0.913]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]] [[1.819]
 [1.701]
 [1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]] [[0.809]
 [0.913]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]]
maxi score, test score, baseline:  -0.012260000000000115 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.012260000000000115 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
line 256 mcts: sample exp_bonus 5.57080072078795
actor:  0 policy actor:  0  step number:  38 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]] [[1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]] [[0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]] [[5.594]
 [1.361]
 [1.361]
 [1.361]
 [1.361]
 [1.361]
 [1.361]] [[ 0.652]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]]
actor:  1 policy actor:  1  step number:  77 total reward:  0.013333333333332198  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.011446666666666773 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
UNIT TEST: sample policy line 217 mcts : [0.327 0.449 0.02  0.02  0.02  0.143 0.02 ]
siam score:  -0.8046237
maxi score, test score, baseline:  -0.011446666666666773 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]] [[2.882]
 [1.806]
 [1.806]
 [1.806]
 [1.806]
 [1.806]
 [1.806]] [[0.45 ]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.011446666666666773 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.864]
 [0.923]
 [0.822]
 [0.85 ]
 [0.863]
 [0.843]
 [0.852]] [[2.038]
 [1.965]
 [1.075]
 [1.253]
 [1.563]
 [1.19 ]
 [1.091]] [[0.864]
 [0.923]
 [0.822]
 [0.85 ]
 [0.863]
 [0.843]
 [0.852]]
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.011446666666666773 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.011446666666666773 0.6920000000000001 0.6920000000000001
using another actor
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]] [[5.958]
 [1.119]
 [1.119]
 [1.119]
 [1.119]
 [1.119]
 [1.119]] [[ 0.792]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]]
maxi score, test score, baseline:  -0.014540000000000122 0.6920000000000001 0.6920000000000001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  72 total reward:  0.3399999999999992  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.014540000000000122 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.459]
 [0.423]
 [0.394]
 [0.402]
 [0.441]
 [0.419]] [[0.685]
 [0.359]
 [0.606]
 [0.499]
 [0.506]
 [0.741]
 [0.517]] [[-0.05 ]
 [ 0.025]
 [ 0.029]
 [-0.017]
 [-0.009]
 [ 0.07 ]
 [ 0.011]]
using another actor
actor:  1 policy actor:  1  step number:  74 total reward:  0.16666666666666574  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.014540000000000122 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  0 policy actor:  1  step number:  79 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  52 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.009993333333333448 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.882]
 [0.934]
 [0.823]
 [0.798]
 [0.823]
 [0.823]
 [0.821]] [[1.293]
 [1.344]
 [2.236]
 [1.935]
 [2.236]
 [2.236]
 [1.96 ]] [[0.882]
 [0.934]
 [0.823]
 [0.798]
 [0.823]
 [0.823]
 [0.821]]
UNIT TEST: sample policy line 217 mcts : [0.245 0.327 0.143 0.041 0.041 0.163 0.041]
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.735 0.02  0.02  0.184 0.   ]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.4647273935408778
siam score:  -0.8103986
maxi score, test score, baseline:  -0.009993333333333448 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.237]
 [0.238]
 [0.311]
 [0.315]
 [0.294]
 [0.237]] [[1.963]
 [2.371]
 [2.347]
 [2.007]
 [1.978]
 [1.912]
 [2.371]] [[0.297]
 [0.436]
 [0.421]
 [0.268]
 [0.252]
 [0.186]
 [0.436]]
maxi score, test score, baseline:  -0.009993333333333448 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.592]
 [0.589]
 [0.572]
 [0.572]
 [0.572]
 [0.597]] [[ 0.   ]
 [-0.286]
 [-0.9  ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-1.564]] [[0.572]
 [0.592]
 [0.589]
 [0.572]
 [0.572]
 [0.572]
 [0.597]]
siam score:  -0.8115754
maxi score, test score, baseline:  -0.009993333333333448 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  0 policy actor:  1  step number:  50 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  64 total reward:  0.11333333333333262  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.010953333333333449 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
using another actor
using explorer policy with actor:  0
actor:  0 policy actor:  0  step number:  33 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  55 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5133333333333336  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.01866000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.01866000000000012 0.6920000000000001 0.6920000000000001
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.18 ]
 [0.234]
 [0.171]
 [0.171]
 [0.178]
 [0.171]] [[2.234]
 [2.335]
 [2.248]
 [2.234]
 [2.234]
 [2.176]
 [2.234]] [[0.16 ]
 [0.212]
 [0.22 ]
 [0.16 ]
 [0.16 ]
 [0.141]
 [0.16 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.01866000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.01866000000000012 0.6920000000000001 0.6920000000000001
actor:  1 policy actor:  1  step number:  57 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.134]
 [-0.011]
 [ 0.184]
 [-0.011]
 [-0.12 ]
 [ 0.016]
 [-0.011]] [[1.848]
 [1.342]
 [3.421]
 [1.342]
 [0.573]
 [2.497]
 [1.342]] [[ 0.082]
 [ 0.108]
 [ 0.48 ]
 [ 0.108]
 [-0.055]
 [ 0.259]
 [ 0.108]]
maxi score, test score, baseline:  -0.022060000000000125 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.022060000000000125 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
using explorer policy with actor:  1
siam score:  -0.7838279
maxi score, test score, baseline:  -0.022060000000000125 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.022060000000000125 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  58 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.022060000000000125 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  58 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  60 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.023286666666666785 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.75 ]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[2.157]
 [2.544]
 [2.251]
 [2.251]
 [2.251]
 [2.251]
 [2.251]] [[0.749]
 [0.75 ]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]]
maxi score, test score, baseline:  -0.023286666666666785 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.023286666666666785 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  0 policy actor:  1  step number:  46 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  43 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.333
from probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.024580000000000123 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.024580000000000123 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.024580000000000123 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.027980000000000112 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
using another actor
siam score:  -0.78816587
maxi score, test score, baseline:  -0.031340000000000125 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.031340000000000125 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  0 policy actor:  1  step number:  64 total reward:  0.15333333333333243  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.03239333333333346 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.03239333333333346 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.03239333333333346 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  48 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.03239333333333346 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.03239333333333346 0.6920000000000001 0.6920000000000001
actor:  0 policy actor:  1  step number:  55 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.033300000000000114 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.033300000000000114 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  -0.033300000000000114 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.033300000000000114 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
siam score:  -0.79290664
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.319116323228646
Printing some Q and Qe and total Qs values:  [[-0.12 ]
 [-0.124]
 [-0.116]
 [-0.117]
 [-0.12 ]
 [-0.12 ]
 [-0.121]] [[0.631]
 [0.682]
 [0.759]
 [0.742]
 [0.236]
 [0.645]
 [0.087]] [[-0.194]
 [-0.19 ]
 [-0.17 ]
 [-0.173]
 [-0.261]
 [-0.192]
 [-0.287]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3533333333333326  reward:  1.0 rdn_beta:  0.5
from probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
siam score:  -0.79276335
maxi score, test score, baseline:  -0.033300000000000114 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.033300000000000114 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
line 256 mcts: sample exp_bonus 0.4925977386292031
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  0 policy actor:  1  step number:  61 total reward:  0.2799999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.03410000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.03410000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  51 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  59 total reward:  0.31999999999999995  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.03410000000000012 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.03410000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
using explorer policy with actor:  1
siam score:  -0.7889566
maxi score, test score, baseline:  -0.037460000000000125 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.896]
 [0.827]
 [0.764]
 [0.789]
 [0.814]
 [0.827]] [[1.352]
 [1.439]
 [1.397]
 [1.567]
 [2.731]
 [1.706]
 [1.486]] [[0.836]
 [0.896]
 [0.827]
 [0.764]
 [0.789]
 [0.814]
 [0.827]]
using another actor
from probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
line 256 mcts: sample exp_bonus -0.24588495002613042
actor:  1 policy actor:  1  step number:  43 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  53 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.038340000000000124 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.038340000000000124 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.038340000000000124 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.038340000000000124 0.6920000000000001 0.6920000000000001
actor:  1 policy actor:  1  step number:  58 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.021]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.584]
 [1.274]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[0.145]
 [0.376]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]]
Printing some Q and Qe and total Qs values:  [[0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]] [[1.368]
 [1.368]
 [1.368]
 [1.368]
 [1.368]
 [1.368]
 [1.368]] [[0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]]
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.955]
 [0.829]
 [0.869]
 [0.866]
 [0.89 ]
 [0.816]] [[3.209]
 [1.551]
 [2.484]
 [2.838]
 [2.003]
 [2.123]
 [2.483]] [[0.871]
 [0.955]
 [0.829]
 [0.869]
 [0.866]
 [0.89 ]
 [0.816]]
maxi score, test score, baseline:  -0.04504666666666678 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1983663985821082
actor:  1 policy actor:  1  step number:  43 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.04842000000000011 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.04842000000000011 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.04842000000000011 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
siam score:  -0.7998597
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[1.115]
 [1.211]
 [1.211]
 [1.211]
 [1.211]
 [1.211]
 [1.211]] [[0.05 ]
 [0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]]
maxi score, test score, baseline:  -0.04842000000000011 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.04842000000000011 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.04842000000000011 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  65 total reward:  0.24  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.04842000000000011 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.04842000000000011 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  -0.04842000000000011 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.04842000000000011 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.04842000000000011 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.04842000000000011 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[2.174]
 [2.705]
 [2.705]
 [2.705]
 [2.705]
 [2.705]
 [2.705]] [[0.188]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.6000000000000003  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.05179333333333344 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.191]
 [0.125]
 [0.009]
 [0.009]
 [0.015]
 [0.027]] [[ 0.932]
 [ 1.888]
 [ 1.309]
 [ 1.423]
 [ 1.423]
 [-0.217]
 [ 0.199]] [[-0.001]
 [ 0.325]
 [ 0.134]
 [ 0.076]
 [ 0.076]
 [-0.324]
 [-0.213]]
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.238]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[1.646]
 [1.664]
 [1.498]
 [1.498]
 [1.498]
 [1.498]
 [1.498]] [[0.279]
 [0.301]
 [0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.265]]
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.161]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]] [[3.236]
 [4.034]
 [2.693]
 [2.693]
 [2.693]
 [2.693]
 [2.693]] [[-0.174]
 [ 0.003]
 [-0.242]
 [-0.242]
 [-0.242]
 [-0.242]
 [-0.242]]
siam score:  -0.79922956
line 256 mcts: sample exp_bonus 3.130440696758748
maxi score, test score, baseline:  -0.05179333333333344 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.448]
 [0.411]
 [0.411]
 [0.412]
 [0.411]
 [0.408]] [[3.176]
 [3.292]
 [3.094]
 [3.091]
 [3.019]
 [3.119]
 [3.137]] [[0.162]
 [0.242]
 [0.139]
 [0.139]
 [0.115]
 [0.148]
 [0.15 ]]
maxi score, test score, baseline:  -0.05179333333333344 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  46 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7899031
maxi score, test score, baseline:  -0.05179333333333344 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.05179333333333344 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.05179333333333346 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  0 policy actor:  0  step number:  55 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  51 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.04936666666666678 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.214]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]] [[1.593]
 [2.247]
 [1.593]
 [1.593]
 [1.593]
 [1.593]
 [1.593]] [[-0.386]
 [-0.223]
 [-0.386]
 [-0.386]
 [-0.386]
 [-0.386]
 [-0.386]]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.419]
 [0.344]
 [0.386]
 [0.356]
 [0.343]
 [0.364]] [[0.607]
 [0.882]
 [0.729]
 [0.607]
 [0.805]
 [0.784]
 [0.988]] [[-0.32 ]
 [-0.104]
 [-0.281]
 [-0.32 ]
 [-0.218]
 [-0.245]
 [-0.088]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.04936666666666678 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  62 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.04936666666666678 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.04936666666666678 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
siam score:  -0.7749751
maxi score, test score, baseline:  -0.04936666666666678 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.04936666666666678 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.04936666666666678 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.04936666666666678 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  108 total reward:  0.006666666666665488  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.013]
 [ 0.65 ]
 [ 0.616]
 [ 0.65 ]
 [ 0.65 ]
 [ 0.461]
 [ 0.65 ]] [[1.433]
 [0.666]
 [0.994]
 [0.666]
 [0.666]
 [0.737]
 [0.666]] [[0.491]
 [0.615]
 [0.765]
 [0.615]
 [0.615]
 [0.499]
 [0.615]]
actor:  1 policy actor:  1  step number:  102 total reward:  0.046666666666663636  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.04936666666666678 0.6920000000000001 0.6920000000000001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.401]
 [-0.401]
 [-0.401]
 [-0.401]
 [-0.401]
 [-0.401]
 [-0.401]]
maxi score, test score, baseline:  -0.04936666666666678 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.04936666666666678 0.6920000000000001 0.6920000000000001
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.293]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]] [[2.284]
 [2.404]
 [2.446]
 [2.446]
 [2.446]
 [2.446]
 [2.446]] [[-0.211]
 [-0.104]
 [-0.178]
 [-0.178]
 [-0.178]
 [-0.178]
 [-0.178]]
first move QE:  -0.6704516148383702
actor:  0 policy actor:  0  step number:  42 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.667
using another actor
start point for exploration sampling:  11091
maxi score, test score, baseline:  -0.04655333333333346 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
siam score:  -0.76989526
maxi score, test score, baseline:  -0.04655333333333346 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.04655333333333346 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.04655333333333346 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
using explorer policy with actor:  0
actor:  0 policy actor:  0  step number:  43 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.463]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[ 1.142]
 [-0.191]
 [ 1.142]
 [ 1.142]
 [ 1.142]
 [ 1.142]
 [ 1.142]] [[0.366]
 [0.463]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.569]
 [0.495]
 [0.521]
 [0.493]
 [0.495]
 [0.495]] [[2.132]
 [1.035]
 [1.587]
 [2.397]
 [2.546]
 [1.587]
 [1.587]] [[0.519]
 [0.569]
 [0.495]
 [0.521]
 [0.493]
 [0.495]
 [0.495]]
actor:  1 policy actor:  1  step number:  73 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.369]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]] [[1.781]
 [1.831]
 [1.781]
 [1.781]
 [1.781]
 [1.781]
 [1.781]] [[-0.169]
 [-0.007]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
siam score:  -0.7692377
maxi score, test score, baseline:  -0.04364666666666679 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  0 policy actor:  1  step number:  61 total reward:  0.19999999999999918  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  0 policy actor:  0  step number:  52 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.03846000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.03846000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.825]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]] [[-1.184]
 [-0.643]
 [-1.184]
 [-1.184]
 [-1.184]
 [-1.184]
 [-1.184]] [[0.775]
 [0.825]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.436]
 [0.451]
 [0.431]
 [0.43 ]
 [0.462]
 [0.44 ]] [[0.812]
 [1.237]
 [0.878]
 [1.128]
 [1.046]
 [1.001]
 [0.963]] [[-0.347]
 [-0.192]
 [-0.237]
 [-0.216]
 [-0.23 ]
 [-0.205]
 [-0.234]]
maxi score, test score, baseline:  -0.03846000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.03846000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[-0.047]
 [-0.047]
 [-0.05 ]
 [-0.058]
 [-0.058]
 [-0.058]
 [-0.041]] [[-3.151]
 [-0.614]
 [-2.873]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-2.74 ]] [[-0.555]
 [ 0.081]
 [-0.487]
 [ 0.226]
 [ 0.226]
 [ 0.226]
 [-0.448]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.03846000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.03846000000000012 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  0 policy actor:  1  step number:  72 total reward:  0.033333333333332216  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.03639333333333348 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  42 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 3.3622162954491914
maxi score, test score, baseline:  -0.03355333333333348 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [ 0.001]
 [ 0.001]
 [ 0.001]
 [ 0.001]
 [ 0.001]
 [ 0.001]] [[ 5.747]
 [-0.512]
 [-0.512]
 [-0.512]
 [-0.512]
 [-0.512]
 [-0.512]] [[ 0.644]
 [-0.173]
 [-0.173]
 [-0.173]
 [-0.173]
 [-0.173]
 [-0.173]]
maxi score, test score, baseline:  -0.03355333333333348 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  64 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.03355333333333348 0.6920000000000001 0.6920000000000001
siam score:  -0.7811623
maxi score, test score, baseline:  -0.03355333333333348 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.03355333333333348 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.03355333333333348 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  1 policy actor:  1  step number:  47 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.56 ]
 [0.516]
 [0.513]
 [0.47 ]
 [0.189]
 [0.386]] [[2.743]
 [1.923]
 [2.137]
 [2.156]
 [2.033]
 [1.828]
 [2.285]] [[0.58 ]
 [0.442]
 [0.484]
 [0.489]
 [0.419]
 [0.163]
 [0.447]]
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.189]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]] [[-0.182]
 [ 0.605]
 [-0.182]
 [-0.182]
 [-0.182]
 [-0.182]
 [-0.182]] [[-0.007]
 [ 0.246]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]]
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]] [[1.748]
 [1.748]
 [1.748]
 [1.748]
 [1.748]
 [1.748]
 [1.748]] [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]]
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.748]
 [0.795]
 [0.813]
 [0.809]
 [0.776]
 [0.817]] [[2.408]
 [2.663]
 [2.564]
 [2.681]
 [2.606]
 [2.366]
 [2.435]] [[0.786]
 [0.748]
 [0.795]
 [0.813]
 [0.809]
 [0.776]
 [0.817]]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  59 total reward:  0.30666666666666653  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  -0.03355333333333348 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.03355333333333348 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  0 policy actor:  1  step number:  62 total reward:  0.13999999999999901  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  -0.03127333333333347 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
line 256 mcts: sample exp_bonus 0.9727001992100472
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  43 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  -0.03127333333333347 0.6920000000000001 0.6920000000000001
maxi score, test score, baseline:  -0.03127333333333347 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
maxi score, test score, baseline:  -0.03127333333333347 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Starting evaluation
using explorer policy with actor:  1
siam score:  -0.7849357
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.785]
 [0.424]
 [0.684]
 [0.595]
 [0.676]
 [0.764]] [[ 0.223]
 [ 0.08 ]
 [ 1.269]
 [-0.819]
 [-1.457]
 [ 0.   ]
 [-0.146]] [[0.704]
 [0.785]
 [0.424]
 [0.684]
 [0.595]
 [0.676]
 [0.764]]
siam score:  -0.78598905
maxi score, test score, baseline:  -0.03127333333333347 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.03127333333333347 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.621]
 [0.538]
 [0.496]
 [0.502]
 [0.538]
 [0.514]] [[1.513]
 [0.564]
 [0.521]
 [0.895]
 [0.84 ]
 [0.521]
 [1.181]] [[0.506]
 [0.621]
 [0.538]
 [0.496]
 [0.502]
 [0.538]
 [0.514]]
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.727]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[1.33]
 [1.33]
 [1.33]
 [1.33]
 [1.33]
 [1.33]
 [1.33]] [[0.713]
 [0.727]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.417]] [[0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.25999999999999923  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus -1.3582548871468805
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8248551161352429
maxi score, test score, baseline:  -0.03127333333333347 0.6920000000000001 0.6920000000000001
probs:  [0.5812323083993661, 0.13958923053354463, 0.13958923053354463, 0.13958923053354463]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.01575333333333322 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.01575333333333322 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
siam score:  -0.7863733
maxi score, test score, baseline:  0.01575333333333322 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.01575333333333322 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  0 policy actor:  0  step number:  43 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  81 total reward:  0.1999999999999983  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[-0.047]
 [-0.017]
 [-0.11 ]
 [-0.034]
 [-0.039]
 [-0.1  ]
 [-0.025]] [[-0.392]
 [-1.291]
 [-0.774]
 [-0.979]
 [-1.95 ]
 [-0.827]
 [-0.978]] [[-0.194]
 [-0.314]
 [-0.32 ]
 [-0.279]
 [-0.446]
 [-0.32 ]
 [-0.27 ]]
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
line 256 mcts: sample exp_bonus 1.6141025770968724
actor:  1 policy actor:  1  step number:  66 total reward:  0.0066666666666661545  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  35 total reward:  0.64  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.516]
 [0.497]
 [0.495]
 [0.489]
 [0.49 ]
 [0.415]] [[1.162]
 [0.92 ]
 [1.148]
 [1.217]
 [0.924]
 [0.597]
 [1.063]] [[-0.118]
 [-0.133]
 [-0.114]
 [-0.105]
 [-0.159]
 [-0.213]
 [-0.21 ]]
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  43 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7744672
actor:  1 policy actor:  1  step number:  57 total reward:  0.38666666666666605  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]] [[3.584]
 [2.384]
 [2.384]
 [2.384]
 [2.384]
 [2.384]
 [2.384]] [[0.219]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [ 0.007]
 [ 0.007]
 [ 0.007]
 [ 0.007]
 [ 0.007]
 [ 0.007]] [[8.756]
 [3.272]
 [3.272]
 [3.272]
 [3.272]
 [3.272]
 [3.272]] [[0.836]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  48 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.028]
 [ 0.028]
 [ 0.028]
 [ 0.028]
 [ 0.028]
 [ 0.028]] [[6.915]
 [2.703]
 [2.703]
 [2.703]
 [2.703]
 [2.703]
 [2.703]] [[0.666]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[7.54 ]
 [2.753]
 [2.753]
 [2.753]
 [2.753]
 [2.753]
 [2.753]] [[0.742]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]]
siam score:  -0.78005576
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  63 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.057]
 [-0.03 ]
 [-0.03 ]
 [-0.031]
 [-0.029]
 [-0.028]] [[-3.703]
 [-0.33 ]
 [-3.574]
 [-3.683]
 [-3.708]
 [-3.723]
 [-3.893]] [[ 0.028]
 [ 0.812]
 [ 0.059]
 [ 0.033]
 [ 0.027]
 [ 0.024]
 [-0.015]]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.019]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.013]
 [-0.028]] [[ 0.338]
 [ 0.534]
 [ 0.338]
 [ 0.338]
 [ 0.338]
 [-0.058]
 [ 0.888]] [[0.347]
 [0.388]
 [0.347]
 [0.347]
 [0.347]
 [0.298]
 [0.44 ]]
using another actor
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
line 256 mcts: sample exp_bonus 0.7111884761184634
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
line 256 mcts: sample exp_bonus 1.7444002030355503
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.791]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[-0.56 ]
 [ 1.549]
 [-0.56 ]
 [-0.56 ]
 [-0.56 ]
 [-0.56 ]
 [-0.56 ]] [[0.652]
 [0.791]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.548]
 [0.455]
 [0.497]
 [0.354]
 [0.481]
 [0.516]] [[1.897]
 [1.043]
 [1.825]
 [1.204]
 [1.32 ]
 [1.346]
 [1.412]] [[ 0.021]
 [ 0.019]
 [ 0.056]
 [-0.005]
 [-0.13 ]
 [ 0.002]
 [ 0.048]]
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.648]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]] [[2.623]
 [1.51 ]
 [1.964]
 [1.964]
 [1.964]
 [1.964]
 [1.964]] [[0.215]
 [0.024]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]]
maxi score, test score, baseline:  0.016393333333333208 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  47 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  55 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.454]
 [0.413]
 [0.409]
 [0.414]
 [0.43 ]
 [0.428]] [[2.586]
 [2.832]
 [2.529]
 [2.317]
 [2.511]
 [2.591]
 [2.568]] [[0.331]
 [0.458]
 [0.287]
 [0.19 ]
 [0.28 ]
 [0.33 ]
 [0.319]]
maxi score, test score, baseline:  0.019099999999999884 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.019099999999999884 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.019099999999999884 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  43 total reward:  0.6000000000000002  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.019099999999999884 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.019099999999999884 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.019099999999999884 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  85 total reward:  0.15999999999999792  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.019099999999999884 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.019099999999999884 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.019099999999999884 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.272]
 [0.207]
 [0.244]
 [0.213]
 [0.208]
 [0.192]] [[1.522]
 [1.25 ]
 [1.48 ]
 [1.262]
 [1.401]
 [1.465]
 [1.224]] [[-0.38 ]
 [-0.395]
 [-0.421]
 [-0.421]
 [-0.429]
 [-0.423]
 [-0.479]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.019099999999999884 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.019099999999999884 0.6980000000000001 0.6980000000000001
actor:  1 policy actor:  1  step number:  40 total reward:  0.54  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.019099999999999884 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  62 total reward:  0.08666666666666623  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  54 total reward:  0.3666666666666658  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.143 0.408 0.204 0.082 0.082 0.041 0.041]
Printing some Q and Qe and total Qs values:  [[0.795]
 [0.877]
 [0.791]
 [0.763]
 [0.795]
 [0.795]
 [0.795]] [[3.134]
 [2.507]
 [2.364]
 [2.065]
 [3.134]
 [3.134]
 [3.134]] [[0.795]
 [0.877]
 [0.791]
 [0.763]
 [0.795]
 [0.795]
 [0.795]]
maxi score, test score, baseline:  0.016179999999999885 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  0 policy actor:  0  step number:  46 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.016526666666666547 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.804]
 [0.837]
 [0.804]
 [0.801]
 [0.676]
 [0.804]
 [0.804]] [[0.704]
 [0.237]
 [0.704]
 [0.949]
 [0.222]
 [0.704]
 [0.704]] [[0.804]
 [0.837]
 [0.804]
 [0.801]
 [0.676]
 [0.804]
 [0.804]]
actor:  0 policy actor:  0  step number:  53 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  61 total reward:  0.17333333333333312  reward:  1.0 rdn_beta:  0.167
siam score:  -0.78357726
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.639]
 [0.617]
 [0.686]
 [0.639]
 [0.615]
 [0.639]] [[2.415]
 [2.317]
 [2.491]
 [2.531]
 [2.317]
 [2.757]
 [2.317]] [[0.504]
 [0.391]
 [0.449]
 [0.529]
 [0.391]
 [0.568]
 [0.391]]
maxi score, test score, baseline:  0.016686666666666544 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
line 256 mcts: sample exp_bonus 1.7579547131881579
using explorer policy with actor:  1
maxi score, test score, baseline:  0.016686666666666544 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.016686666666666544 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.016686666666666544 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.016686666666666544 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
siam score:  -0.7881718
Printing some Q and Qe and total Qs values:  [[0.924]
 [0.961]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]] [[1.505]
 [1.214]
 [1.505]
 [1.505]
 [1.505]
 [1.505]
 [1.505]] [[0.924]
 [0.961]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]]
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.016686666666666544 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.016686666666666544 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  48 total reward:  0.326666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.01933999999999989 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.01933999999999989 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  66 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
siam score:  -0.78903913
maxi score, test score, baseline:  0.01933999999999989 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.323]
 [0.282]
 [0.261]
 [0.282]
 [0.282]
 [0.075]] [[3.429]
 [4.027]
 [3.243]
 [2.427]
 [3.243]
 [3.243]
 [2.988]] [[0.397]
 [0.487]
 [0.301]
 [0.12 ]
 [0.301]
 [0.301]
 [0.121]]
maxi score, test score, baseline:  0.01933999999999989 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.01933999999999989 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.016459999999999884 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  71 total reward:  0.13333333333333242  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.016459999999999884 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  0 policy actor:  0  step number:  36 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.016659999999999887 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.278]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]] [[-1.037]
 [-0.082]
 [-1.037]
 [-1.037]
 [-1.037]
 [-1.037]
 [-1.037]] [[-0.672]
 [-0.363]
 [-0.672]
 [-0.672]
 [-0.672]
 [-0.672]
 [-0.672]]
maxi score, test score, baseline:  0.016659999999999887 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.016659999999999887 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.016659999999999887 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.016659999999999887 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.016659999999999887 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  0 policy actor:  1  step number:  55 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.233]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[1.666]
 [1.666]
 [1.868]
 [1.666]
 [1.666]
 [1.666]
 [1.666]] [[-0.367]
 [-0.367]
 [-0.1  ]
 [-0.367]
 [-0.367]
 [-0.367]
 [-0.367]]
line 256 mcts: sample exp_bonus 1.6127368238715674
actor:  1 policy actor:  1  step number:  59 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.8598367489228336
maxi score, test score, baseline:  0.01640666666666656 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  56 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  0.167
from probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.016406666666666528 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.604]
 [0.552]
 [0.535]
 [0.558]
 [0.523]
 [0.538]] [[ 0.215]
 [ 0.153]
 [ 0.064]
 [-0.249]
 [ 0.296]
 [-0.446]
 [-0.339]] [[0.566]
 [0.604]
 [0.552]
 [0.535]
 [0.558]
 [0.523]
 [0.538]]
maxi score, test score, baseline:  0.016406666666666528 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  0 policy actor:  0  step number:  52 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.018846666666666533 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.018846666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  43 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.239]
 [0.27 ]
 [0.276]
 [0.289]
 [0.289]
 [0.289]] [[3.651]
 [3.421]
 [3.641]
 [3.596]
 [3.553]
 [3.621]
 [3.709]] [[0.233]
 [0.112]
 [0.216]
 [0.207]
 [0.206]
 [0.228]
 [0.258]]
actor:  0 policy actor:  0  step number:  49 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.024366666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.024366666666666537 0.6980000000000001 0.6980000000000001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]] [[2.765]
 [2.765]
 [2.765]
 [2.765]
 [2.765]
 [2.765]
 [2.765]] [[0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]]
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.396]
 [0.277]
 [0.277]
 [0.277]
 [0.28 ]
 [0.277]] [[1.379]
 [1.489]
 [1.379]
 [1.379]
 [1.379]
 [1.093]
 [1.379]] [[-0.275]
 [-0.138]
 [-0.275]
 [-0.275]
 [-0.275]
 [-0.321]
 [-0.275]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.024366666666666537 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.024366666666666537 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.024366666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.024366666666666537 0.6980000000000001 0.6980000000000001
actor:  1 policy actor:  1  step number:  47 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.024366666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]] [[3.288]
 [3.288]
 [3.288]
 [3.288]
 [3.288]
 [3.288]
 [3.288]] [[0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]]
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.165]
 [0.13 ]
 [0.13 ]
 [0.13 ]
 [0.13 ]
 [0.13 ]] [[1.396]
 [2.244]
 [1.396]
 [1.396]
 [1.396]
 [1.396]
 [1.396]] [[-0.267]
 [ 0.178]
 [-0.267]
 [-0.267]
 [-0.267]
 [-0.267]
 [-0.267]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.024366666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  0 policy actor:  0  step number:  36 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.024806666666666543 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  62 total reward:  0.19333333333333258  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 2.52389975451415
maxi score, test score, baseline:  0.024806666666666543 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.024806666666666543 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.024806666666666543 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.024806666666666543 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  0 policy actor:  0  step number:  60 total reward:  0.16666666666666596  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  56 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.011]
 [ 0.068]
 [-0.021]
 [-0.011]
 [-0.011]
 [-0.013]
 [-0.011]] [[1.146]
 [0.672]
 [1.917]
 [1.146]
 [1.146]
 [0.097]
 [1.146]] [[ 0.035]
 [-0.134]
 [ 0.394]
 [ 0.035]
 [ 0.035]
 [-0.467]
 [ 0.035]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.02713999999999987 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.02713999999999987 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
from probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.02713999999999987 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.448]
 [0.409]
 [0.408]
 [0.415]
 [0.407]
 [0.406]] [[1.363]
 [1.29 ]
 [1.286]
 [1.233]
 [2.009]
 [1.285]
 [1.223]] [[0.391]
 [0.384]
 [0.366]
 [0.35 ]
 [0.579]
 [0.365]
 [0.346]]
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.165]] [[8.041]
 [3.064]
 [3.064]
 [3.064]
 [3.064]
 [3.064]
 [3.064]] [[0.655]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]]
maxi score, test score, baseline:  0.02713999999999987 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
siam score:  -0.7778917
maxi score, test score, baseline:  0.02713999999999987 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
line 256 mcts: sample exp_bonus 1.5165962037845537
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.476]
 [0.43 ]
 [0.264]
 [0.352]
 [0.376]
 [0.425]] [[1.38 ]
 [0.478]
 [0.911]
 [1.928]
 [0.807]
 [0.897]
 [0.81 ]] [[0.367]
 [0.022]
 [0.221]
 [0.639]
 [0.1  ]
 [0.169]
 [0.162]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.2599999999999989  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.056]
 [ 0.339]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]] [[2.453]
 [6.428]
 [2.453]
 [2.453]
 [2.453]
 [2.453]
 [2.453]] [[0.39 ]
 [0.873]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.383604789690883
maxi score, test score, baseline:  0.02713999999999987 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  74 total reward:  0.059999999999999165  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  68 total reward:  0.059999999999999165  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
from probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.029259999999999883 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  101 total reward:  0.0799999999999973  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.022]
 [-0.023]
 [-0.022]
 [-0.016]
 [-0.023]
 [-0.023]] [[-2.16 ]
 [-0.539]
 [-2.328]
 [-2.47 ]
 [-1.146]
 [-2.538]
 [-2.444]] [[0.162]
 [0.43 ]
 [0.131]
 [0.108]
 [0.335]
 [0.096]
 [0.112]]
maxi score, test score, baseline:  0.029259999999999883 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.029259999999999883 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.449 0.143 0.061 0.082 0.082 0.082 0.102]
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.435]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[2.799]
 [2.799]
 [3.425]
 [2.799]
 [2.799]
 [2.799]
 [2.799]] [[0.202]
 [0.202]
 [0.561]
 [0.202]
 [0.202]
 [0.202]
 [0.202]]
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.705]
 [0.296]
 [0.37 ]
 [0.298]
 [0.429]
 [0.344]] [[2.167]
 [0.267]
 [1.341]
 [1.211]
 [1.544]
 [1.48 ]
 [1.142]] [[0.402]
 [0.242]
 [0.194]
 [0.222]
 [0.259]
 [0.362]
 [0.176]]
using explorer policy with actor:  1
siam score:  -0.7915483
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.243]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.162]] [[1.466]
 [1.636]
 [1.182]
 [1.182]
 [1.182]
 [1.182]
 [1.879]] [[-0.104]
 [-0.043]
 [-0.22 ]
 [-0.22 ]
 [-0.22 ]
 [-0.22 ]
 [-0.084]]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.46 ]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[0.889]
 [0.782]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]] [[-0.229]
 [-0.131]
 [-0.229]
 [-0.229]
 [-0.229]
 [-0.229]
 [-0.229]]
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.676]
 [0.647]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[1.137]
 [1.477]
 [0.968]
 [1.024]
 [1.477]
 [1.477]
 [1.477]] [[ 0.018]
 [ 0.083]
 [-0.032]
 [ 0.007]
 [ 0.083]
 [ 0.083]
 [ 0.083]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.13700795517759395
actor:  1 policy actor:  1  step number:  48 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using another actor
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
siam score:  -0.79027086
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
siam score:  -0.7904516
siam score:  -0.79118025
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.023766666666666537 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.02376666666666654 0.6980000000000001 0.6980000000000001
actor:  1 policy actor:  1  step number:  37 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7926502
maxi score, test score, baseline:  0.02376666666666654 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.02376666666666654 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.414]
 [0.365]
 [0.349]
 [0.362]
 [0.359]
 [0.368]] [[1.322]
 [1.277]
 [0.932]
 [0.886]
 [1.322]
 [1.011]
 [0.991]] [[-0.202]
 [-0.158]
 [-0.264]
 [-0.289]
 [-0.202]
 [-0.258]
 [-0.252]]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.625]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[1.495]
 [1.227]
 [1.539]
 [1.539]
 [1.539]
 [1.539]
 [1.539]] [[-0.064]
 [ 0.072]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]]
maxi score, test score, baseline:  0.02376666666666654 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.02376666666666654 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.02376666666666654 0.6980000000000001 0.6980000000000001
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  60 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.02376666666666654 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  0 policy actor:  0  step number:  43 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.023419999999999875 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.023419999999999875 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  53 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.023419999999999875 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.293]
 [0.299]
 [0.258]
 [0.218]
 [0.276]
 [0.277]] [[1.378]
 [1.576]
 [2.083]
 [1.549]
 [1.378]
 [2.283]
 [2.586]] [[-0.494]
 [-0.387]
 [-0.296]
 [-0.426]
 [-0.494]
 [-0.286]
 [-0.233]]
maxi score, test score, baseline:  0.023419999999999875 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.023419999999999875 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.023419999999999875 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.023419999999999875 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.023419999999999875 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]] [[1.497]
 [1.497]
 [1.497]
 [1.497]
 [1.497]
 [1.497]
 [1.497]] [[0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.023419999999999875 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2933333333333332  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
siam score:  -0.7837276
actor:  1 policy actor:  1  step number:  57 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  67 total reward:  0.466666666666667  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.057]
 [ 0.145]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]] [[1.105]
 [2.042]
 [1.105]
 [1.105]
 [1.105]
 [1.105]
 [1.105]] [[-0.118]
 [ 0.295]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]]
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
actor:  1 policy actor:  1  step number:  64 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.461]
 [0.339]
 [0.278]
 [0.339]
 [0.339]
 [0.244]] [[1.807]
 [1.35 ]
 [1.867]
 [1.694]
 [1.867]
 [1.867]
 [1.123]] [[-0.257]
 [-0.158]
 [-0.193]
 [-0.283]
 [-0.193]
 [-0.193]
 [-0.413]]
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]] [[3.906]
 [3.032]
 [3.032]
 [3.032]
 [3.032]
 [3.032]
 [3.032]] [[0.568]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]]
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  57 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  57 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  0.167
siam score:  -0.77691877
using another actor
from probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
line 256 mcts: sample exp_bonus 1.8993096756855468
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
actor:  1 policy actor:  1  step number:  43 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]] [[2.026]
 [1.798]
 [1.798]
 [1.798]
 [1.798]
 [1.798]
 [1.798]] [[0.175]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]]
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.872]
 [0.837]
 [0.86 ]
 [0.853]
 [0.851]
 [0.857]] [[0.832]
 [1.022]
 [0.868]
 [0.718]
 [0.681]
 [0.491]
 [0.503]] [[0.831]
 [0.872]
 [0.837]
 [0.86 ]
 [0.853]
 [0.851]
 [0.857]]
actor:  1 policy actor:  1  step number:  67 total reward:  0.41333333333333355  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.020193333333333213 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  54 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.487]
 [0.408]
 [0.433]
 [0.421]
 [0.416]
 [0.423]] [[0.984]
 [1.102]
 [1.132]
 [1.189]
 [1.182]
 [1.097]
 [1.105]] [[-0.086]
 [ 0.021]
 [-0.028]
 [ 0.024]
 [ 0.01 ]
 [-0.041]
 [-0.03 ]]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.264]
 [0.2  ]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[1.807]
 [2.383]
 [1.46 ]
 [1.807]
 [1.807]
 [1.807]
 [1.807]] [[ 0.042]
 [ 0.197]
 [-0.021]
 [ 0.042]
 [ 0.042]
 [ 0.042]
 [ 0.042]]
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[2.28 ]
 [2.131]
 [2.131]
 [2.131]
 [2.131]
 [2.131]
 [2.131]] [[0.629]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]]
maxi score, test score, baseline:  0.022713333333333204 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.467]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]] [[1.156]
 [1.399]
 [1.156]
 [1.156]
 [1.156]
 [1.156]
 [1.156]] [[0.109]
 [0.248]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.40666666666666595  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.022713333333333214 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.022713333333333214 0.6980000000000001 0.6980000000000001
actor:  1 policy actor:  1  step number:  58 total reward:  0.42000000000000015  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.022713333333333214 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]] [[1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]] [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]] [[1.361]
 [1.799]
 [1.799]
 [1.799]
 [1.799]
 [1.799]
 [1.799]] [[0.692]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]]
maxi score, test score, baseline:  0.022713333333333214 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  42 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.55 ]
 [0.514]
 [0.492]
 [0.492]
 [0.39 ]
 [0.492]] [[1.296]
 [0.829]
 [1.113]
 [1.296]
 [1.296]
 [1.698]
 [1.296]] [[-0.103]
 [-0.123]
 [-0.112]
 [-0.103]
 [-0.103]
 [-0.139]
 [-0.103]]
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  55 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
siam score:  -0.77955115
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.683]
 [0.584]
 [0.588]
 [0.598]
 [0.608]
 [0.615]] [[2.188]
 [1.766]
 [2.583]
 [2.744]
 [2.288]
 [2.253]
 [2.224]] [[0.239]
 [0.185]
 [0.359]
 [0.416]
 [0.274]
 [0.273]
 [0.269]]
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  62 total reward:  0.13999999999999913  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  91 total reward:  0.06666666666666565  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.207]
 [0.142]
 [0.188]
 [0.138]
 [0.138]
 [0.135]] [[-0.68 ]
 [-0.406]
 [-0.474]
 [-0.678]
 [-0.615]
 [-0.851]
 [-0.821]] [[-0.648]
 [-0.531]
 [-0.608]
 [-0.597]
 [-0.636]
 [-0.675]
 [-0.673]]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.347]
 [0.413]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[1.319]
 [1.319]
 [0.959]
 [1.319]
 [1.319]
 [1.319]
 [1.319]] [[-0.276]
 [-0.276]
 [-0.271]
 [-0.276]
 [-0.276]
 [-0.276]
 [-0.276]]
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  55 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  46 total reward:  0.5  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.164]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[1.25 ]
 [1.276]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]] [[-0.409]
 [-0.238]
 [-0.409]
 [-0.409]
 [-0.409]
 [-0.409]
 [-0.409]]
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.269]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[1.081]
 [1.262]
 [1.081]
 [1.081]
 [1.081]
 [1.081]
 [1.081]] [[-0.506]
 [-0.447]
 [-0.506]
 [-0.506]
 [-0.506]
 [-0.506]
 [-0.506]]
line 256 mcts: sample exp_bonus 1.1009504232341147
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
first move QE:  -0.6937132467003594
actor:  1 policy actor:  1  step number:  47 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  54 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.057]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]] [[1.202]
 [2.253]
 [1.202]
 [1.202]
 [1.202]
 [1.202]
 [1.202]] [[0.108]
 [0.242]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]]
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.509]
 [0.482]
 [0.437]
 [0.396]
 [0.407]
 [0.437]] [[1.097]
 [1.685]
 [2.176]
 [1.148]
 [0.964]
 [1.428]
 [1.148]] [[-0.241]
 [-0.033]
 [ 0.021]
 [-0.195]
 [-0.267]
 [-0.178]
 [-0.195]]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.523]
 [0.303]
 [0.346]
 [0.369]
 [0.369]
 [0.369]] [[2.258]
 [1.292]
 [2.596]
 [2.747]
 [2.321]
 [2.321]
 [2.321]] [[0.425]
 [0.067]
 [0.496]
 [0.613]
 [0.425]
 [0.425]
 [0.425]]
Printing some Q and Qe and total Qs values:  [[-0.152]
 [ 0.076]
 [-0.008]
 [-0.017]
 [-0.016]
 [-0.007]
 [-0.028]] [[ 0.36 ]
 [ 0.471]
 [-0.743]
 [-2.188]
 [ 0.   ]
 [-0.319]
 [ 1.039]] [[-0.084]
 [ 0.136]
 [-0.121]
 [-0.344]
 [-0.016]
 [-0.056]
 [ 0.128]]
start point for exploration sampling:  11091
actor:  1 policy actor:  1  step number:  44 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  79 total reward:  0.02666666666666595  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  82 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.461]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[0.29 ]
 [0.956]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]] [[0.456]
 [0.461]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]]
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  55 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.023006666666666533 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  0 policy actor:  0  step number:  45 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  47 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.5
siam score:  -0.77658975
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.514]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]] [[1.604]
 [1.349]
 [1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]] [[0.177]
 [0.222]
 [0.177]
 [0.177]
 [0.177]
 [0.177]
 [0.177]]
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[1.76 ]
 [1.725]
 [1.725]
 [1.725]
 [1.725]
 [1.725]
 [1.725]] [[-0.055]
 [-0.236]
 [-0.236]
 [-0.236]
 [-0.236]
 [-0.236]
 [-0.236]]
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5200000000000001  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
line 256 mcts: sample exp_bonus -1.2462130157913898
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.566666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  66 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5399999999999999  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
siam score:  -0.7806781
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  49 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.025446666666666534 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  0 policy actor:  1  step number:  54 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.02804666666666654 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.02804666666666654 0.6980000000000001 0.6980000000000001
line 256 mcts: sample exp_bonus 2.1815778067877636
start point for exploration sampling:  11091
actor:  1 policy actor:  1  step number:  107 total reward:  0.0799999999999974  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.02804666666666653 0.6980000000000001 0.6980000000000001
actor:  1 policy actor:  1  step number:  66 total reward:  0.29999999999999927  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.02804666666666653 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.02804666666666653 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
using another actor
maxi score, test score, baseline:  0.02804666666666653 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
siam score:  -0.7615149
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  46 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  58 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.02804666666666653 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.02804666666666653 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]] [[8.777]
 [2.1  ]
 [2.1  ]
 [2.1  ]
 [2.1  ]
 [2.1  ]
 [2.1  ]] [[0.873]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  56 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.03053999999999987 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
siam score:  -0.7654659
maxi score, test score, baseline:  0.03053999999999987 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.167
Starting evaluation
maxi score, test score, baseline:  0.03053999999999987 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.554]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]] [[1.443]
 [0.745]
 [1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.443]] [[0.687]
 [0.554]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]]
maxi score, test score, baseline:  0.03053999999999987 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.87 ]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.847]] [[1.744]
 [0.684]
 [1.489]
 [1.489]
 [1.489]
 [1.489]
 [1.305]] [[0.776]
 [0.87 ]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.847]]
maxi score, test score, baseline:  0.03053999999999987 0.6980000000000001 0.6980000000000001
Printing some Q and Qe and total Qs values:  [[0.803]
 [0.734]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]] [[1.911]
 [2.008]
 [1.731]
 [1.731]
 [1.731]
 [1.731]
 [1.731]] [[0.803]
 [0.734]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.638]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[1.802]
 [1.632]
 [1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.802]] [[0.617]
 [0.638]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]]
maxi score, test score, baseline:  0.03053999999999987 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.707]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[1.865]
 [1.53 ]
 [1.972]
 [1.972]
 [1.972]
 [1.972]
 [1.972]] [[0.596]
 [0.707]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]]
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.926]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[1.877]
 [1.744]
 [1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.877]] [[0.796]
 [0.926]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]]
Printing some Q and Qe and total Qs values:  [[1.015]
 [1.037]
 [1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]] [[0.558]
 [0.542]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[1.015]
 [1.037]
 [1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]]
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.902]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]] [[1.89 ]
 [1.179]
 [1.89 ]
 [1.89 ]
 [1.89 ]
 [1.89 ]
 [1.89 ]] [[0.791]
 [0.902]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]]
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.67 ]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]] [[1.83 ]
 [1.833]
 [1.83 ]
 [1.83 ]
 [1.83 ]
 [1.83 ]
 [1.83 ]] [[0.594]
 [0.67 ]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.923]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[1.943]
 [0.91 ]
 [1.943]
 [1.943]
 [1.943]
 [1.943]
 [1.943]] [[0.729]
 [0.923]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
maxi score, test score, baseline:  0.03053999999999987 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
maxi score, test score, baseline:  0.03053999999999987 0.6980000000000001 0.6980000000000001
probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  1 policy actor:  1  step number:  43 total reward:  0.48  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
from probs:  [0.5486861423331592, 0.15043795255561365, 0.15043795255561365, 0.15043795255561365]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.08085999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08085999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]] [[2.783]
 [1.551]
 [1.551]
 [1.551]
 [1.551]
 [1.551]
 [1.551]] [[ 0.232]
 [-0.201]
 [-0.201]
 [-0.201]
 [-0.201]
 [-0.201]
 [-0.201]]
maxi score, test score, baseline:  0.08085999999999988 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08085999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08085999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08085999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  0 policy actor:  0  step number:  35 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08072666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 1.8088747212363832
maxi score, test score, baseline:  0.08072666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08072666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  0 policy actor:  1  step number:  48 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9133781369425358
maxi score, test score, baseline:  0.0836199999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0836199999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  0 policy actor:  0  step number:  57 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  11091
from probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08108666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08108666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08108666666666657 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08108666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08108666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.08108666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  0 policy actor:  0  step number:  41 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  37 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  60 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08708666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
line 256 mcts: sample exp_bonus 0.08593101106224595
actor:  0 policy actor:  0  step number:  51 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08733999999999988 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[-0.024]
 [ 0.113]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]] [[1.948]
 [2.091]
 [1.948]
 [1.948]
 [1.948]
 [1.948]
 [1.948]] [[-0.405]
 [-0.244]
 [-0.405]
 [-0.405]
 [-0.405]
 [-0.405]
 [-0.405]]
maxi score, test score, baseline:  0.08733999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
start point for exploration sampling:  11091
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.494]
 [0.441]
 [0.403]
 [0.403]
 [0.45 ]
 [0.471]] [[2.735]
 [2.882]
 [2.739]
 [1.938]
 [1.938]
 [2.968]
 [2.755]] [[ 0.318]
 [ 0.42 ]
 [ 0.321]
 [-0.027]
 [-0.027]
 [ 0.419]
 [ 0.351]]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.088]
 [0.266]
 [0.225]
 [0.349]
 [0.347]
 [0.352]] [[ 0.539]
 [-0.064]
 [ 0.421]
 [ 0.386]
 [ 0.729]
 [ 0.631]
 [ 0.266]] [[ 0.051]
 [-0.275]
 [-0.016]
 [-0.063]
 [ 0.119]
 [ 0.1  ]
 [ 0.044]]
maxi score, test score, baseline:  0.08733999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  81 total reward:  0.06666666666666587  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08733999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  51 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.08733999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08733999999999988 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08733999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.86 ]
 [0.808]
 [0.77 ]
 [0.815]
 [0.823]
 [0.789]] [[2.388]
 [2.678]
 [2.621]
 [2.203]
 [2.511]
 [2.451]
 [2.556]] [[0.807]
 [0.86 ]
 [0.808]
 [0.77 ]
 [0.815]
 [0.823]
 [0.789]]
actor:  0 policy actor:  1  step number:  59 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  52 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  55 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.012]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[-1.843]
 [-0.295]
 [-1.843]
 [-1.843]
 [-1.843]
 [-1.843]
 [-1.843]] [[-0.768]
 [-0.501]
 [-0.768]
 [-0.768]
 [-0.768]
 [-0.768]
 [-0.768]]
line 256 mcts: sample exp_bonus 10.0
deleting a thread, now have 3 threads
Frames:  141358 train batches done:  16559 episodes:  4229
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.08864666666666654 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08864666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08864666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  0 policy actor:  0  step number:  58 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  0.167
deleting a thread, now have 2 threads
Frames:  141534 train batches done:  16576 episodes:  4233
maxi score, test score, baseline:  0.08749999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[ 0.381]
 [ 0.401]
 [-0.026]
 [ 0.298]
 [ 0.267]
 [-0.003]
 [ 0.344]] [[1.522]
 [1.657]
 [1.45 ]
 [1.521]
 [1.469]
 [0.75 ]
 [1.656]] [[ 0.375]
 [ 0.441]
 [ 0.104]
 [ 0.326]
 [ 0.286]
 [-0.16 ]
 [ 0.407]]
maxi score, test score, baseline:  0.08749999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08749999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08749999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
deleting a thread, now have 1 threads
Frames:  141594 train batches done:  16591 episodes:  4235
maxi score, test score, baseline:  0.08749999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08749999999999988 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08749999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08749999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08749999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
from probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
siam score:  -0.7615164
maxi score, test score, baseline:  0.08749999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
siam score:  -0.76269877
maxi score, test score, baseline:  0.08749999999999988 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08749999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08749999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08749999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  0 policy actor:  0  step number:  54 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.08688666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08688666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08688666666666656 0.6990000000000001 0.6990000000000001
siam score:  -0.75710386
maxi score, test score, baseline:  0.08688666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08688666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08688666666666656 0.6990000000000001 0.6990000000000001
first move QE:  -0.6967898090293752
maxi score, test score, baseline:  0.08688666666666656 0.6990000000000001 0.6990000000000001
actor:  0 policy actor:  0  step number:  44 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.61 ]
 [0.419]
 [0.554]
 [0.542]
 [0.563]
 [0.581]] [[3.435]
 [2.576]
 [2.106]
 [2.094]
 [2.119]
 [2.634]
 [2.896]] [[0.778]
 [0.51 ]
 [0.205]
 [0.297]
 [0.297]
 [0.497]
 [0.604]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.7403],
        [-0.0000],
        [0.5160],
        [0.2028],
        [-0.0000],
        [-0.0000],
        [0.4169],
        [0.1696],
        [-0.0000],
        [0.1772]], dtype=torch.float64)
-0.07090238119799999 0.6694215785795868
-0.9702 -0.9702
-0.071551887066 0.444495923625273
-0.032346567066 0.1704039941477412
-0.7580100000000001 -0.7580100000000001
-0.534109592016 -0.534109592016
-0.09703970119800001 0.31982066252179575
-0.071031754398 0.09857625334300082
0.957165 0.957165
-0.045026434398 0.13220632834374432
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.184]
 [0.103]
 [0.118]
 [0.118]
 [0.078]
 [0.117]] [[1.73 ]
 [2.975]
 [2.106]
 [2.113]
 [2.113]
 [1.272]
 [2.618]] [[-0.277]
 [-0.029]
 [-0.255]
 [-0.239]
 [-0.239]
 [-0.419]
 [-0.156]]
line 256 mcts: sample exp_bonus 1.9881261939463446
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
first move QE:  -0.6970678515829839
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
using explorer policy with actor:  1
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
first move QE:  -0.6973063311190572
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.253]
 [0.253]
 [0.22 ]
 [0.253]
 [0.253]
 [0.247]] [[1.217]
 [0.715]
 [0.846]
 [1.251]
 [0.846]
 [0.846]
 [1.104]] [[-0.384]
 [-0.424]
 [-0.402]
 [-0.367]
 [-0.402]
 [-0.402]
 [-0.365]]
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.225]
 [0.203]
 [0.203]
 [0.203]
 [0.209]
 [0.203]] [[1.207]
 [1.213]
 [0.853]
 [0.853]
 [0.853]
 [0.874]
 [0.853]] [[-0.435]
 [-0.429]
 [-0.511]
 [-0.511]
 [-0.511]
 [-0.501]
 [-0.511]]
Printing some Q and Qe and total Qs values:  [[0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]] [[0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]] [[-0.279]
 [-0.279]
 [-0.279]
 [-0.279]
 [-0.279]
 [-0.279]
 [-0.279]]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  57 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.438]
 [0.421]
 [0.367]
 [0.367]
 [0.367]
 [0.367]] [[0.611]
 [0.879]
 [0.546]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[-0.254]
 [-0.138]
 [-0.211]
 [-0.254]
 [-0.254]
 [-0.254]
 [-0.254]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[-0.021]
 [ 0.02 ]
 [-0.026]
 [-0.028]
 [-0.028]
 [-0.205]
 [-0.014]] [[-0.071]
 [-0.757]
 [ 0.   ]
 [-1.695]
 [-1.673]
 [-0.428]
 [-0.784]] [[ 0.132]
 [-0.119]
 [ 0.156]
 [-0.547]
 [-0.538]
 [-0.17 ]
 [-0.159]]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
using explorer policy with actor:  0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.4420],
        [0.0000],
        [0.3674],
        [0.6029],
        [0.0042],
        [0.8421],
        [0.0729],
        [0.6185],
        [0.1652],
        [0.1231]], dtype=torch.float64)
-0.09703970119800001 0.344914736328623
-0.8448 -0.8448
-0.045414567066 0.3220051751301911
-0.032346567066 0.5705674543510714
-0.032346567066 -0.028141171998732423
-0.07129183386599999 0.7708520020517889
-0.09703970119800001 -0.024160086964179248
-0.09703970119800001 0.5214507170133692
-0.032346567066 0.1328633313761306
-0.070771701198 0.05230123884586982
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
siam score:  -0.75080216
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08619333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  56 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  44 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.333
from probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08279333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  0 policy actor:  0  step number:  39 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.08227333333333323 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08227333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
first move QE:  -0.698786761423328
from probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08227333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08227333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08227333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08227333333333323 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08227333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
siam score:  -0.7567564
actor:  0 policy actor:  0  step number:  50 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08131333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08131333333333321 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08131333333333321 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08131333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08131333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08131333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  0 policy actor:  0  step number:  49 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08052666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08052666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08052666666666657 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08052666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.434]
 [0.241]
 [0.23 ]
 [0.227]
 [0.391]
 [0.233]] [[1.978]
 [2.146]
 [1.311]
 [1.081]
 [1.006]
 [1.437]
 [0.864]] [[-0.175]
 [ 0.058]
 [-0.274]
 [-0.323]
 [-0.339]
 [-0.103]
 [-0.357]]
maxi score, test score, baseline:  0.08052666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.08052666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.08052666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  56 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08052666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.786]
 [0.647]
 [0.694]
 [0.694]
 [0.694]
 [0.623]] [[1.142]
 [0.756]
 [0.943]
 [1.297]
 [1.297]
 [1.297]
 [1.276]] [[0.64 ]
 [0.786]
 [0.647]
 [0.694]
 [0.694]
 [0.694]
 [0.623]]
maxi score, test score, baseline:  0.08052666666666657 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08052666666666657 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08052666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  48 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  43 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.07997999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]] [[2.263]
 [2.556]
 [2.556]
 [2.556]
 [2.556]
 [2.556]
 [2.556]] [[0.215]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]]
maxi score, test score, baseline:  0.0765799999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0765799999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0765799999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  0 policy actor:  0  step number:  39 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.07597999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07597999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07597999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.07597999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07597999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  65 total reward:  0.15999999999999925  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07597999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07597999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.07597999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07597999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.493]
 [0.44 ]
 [0.442]
 [0.414]
 [0.441]
 [0.43 ]] [[1.026]
 [1.001]
 [1.052]
 [1.085]
 [1.054]
 [1.215]
 [0.757]] [[-0.182]
 [-0.116]
 [-0.16 ]
 [-0.152]
 [-0.186]
 [-0.132]
 [-0.22 ]]
maxi score, test score, baseline:  0.07597999999999988 0.6990000000000001 0.6990000000000001
siam score:  -0.7564728
actor:  1 policy actor:  1  step number:  60 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07597999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
siam score:  -0.7555444
start point for exploration sampling:  11091
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1058668766659685
actor:  0 policy actor:  0  step number:  55 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.0714999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0714999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0714999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0714999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0714999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.0714999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0714999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
siam score:  -0.75999403
maxi score, test score, baseline:  0.0714999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0714999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0714999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.837]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]] [[1.026]
 [1.087]
 [1.026]
 [1.026]
 [1.026]
 [1.026]
 [1.026]] [[0.811]
 [0.837]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]]
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.883]
 [0.857]
 [0.858]
 [0.798]
 [0.823]
 [0.863]] [[1.853]
 [0.666]
 [1.334]
 [1.204]
 [1.632]
 [1.719]
 [1.041]] [[0.761]
 [0.883]
 [0.857]
 [0.858]
 [0.798]
 [0.823]
 [0.863]]
maxi score, test score, baseline:  0.0714999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0714999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0714999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  0 policy actor:  1  step number:  59 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
siam score:  -0.76252073
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.6840],
        [0.3382],
        [0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [0.5939],
        [0.3275],
        [0.4296],
        [0.8331]], dtype=torch.float64)
-0.071422513866 0.6125365328028493
-0.032346567066 0.3058294505496829
-0.03286799999999987 -0.03286799999999987
-0.23759999999999956 -0.23759999999999956
-0.95755836 -0.95755836
-0.5207789120160001 -0.5207789120160001
-0.032346567066 0.5615449226676903
-0.032346567066 0.29519107642171605
-0.08423175439800001 0.34541745078755615
-0.05809183386599999 0.7750375349391917
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]] [[3.039]
 [3.039]
 [3.039]
 [3.039]
 [3.039]
 [3.039]
 [3.039]] [[0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  41 total reward:  0.52  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
first move QE:  -0.7020637265503417
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[0.785]
 [0.837]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]] [[2.382]
 [2.406]
 [2.382]
 [2.382]
 [2.382]
 [2.382]
 [2.382]] [[0.785]
 [0.837]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]]
siam score:  -0.76076186
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.07041999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
line 256 mcts: sample exp_bonus 1.7224375592452064
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.173]
 [0.147]
 [0.164]
 [0.158]
 [0.127]
 [0.149]] [[0.882]
 [1.045]
 [0.948]
 [0.954]
 [1.275]
 [0.697]
 [1.074]] [[-0.459]
 [-0.394]
 [-0.437]
 [-0.418]
 [-0.37 ]
 [-0.498]
 [-0.413]]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  57 total reward:  0.26666666666666583  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.7020626025003716
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.296]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]] [[6.11 ]
 [2.333]
 [2.709]
 [2.709]
 [2.709]
 [2.709]
 [2.709]] [[0.786]
 [0.242]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.442]
 [0.4  ]
 [0.411]
 [0.403]
 [0.388]
 [0.403]] [[2.157]
 [1.963]
 [1.859]
 [2.03 ]
 [1.729]
 [2.021]
 [1.729]] [[0.217]
 [0.18 ]
 [0.086]
 [0.182]
 [0.024]
 [0.155]
 [0.024]]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  0.31999999999999995  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.7133972677691834
start point for exploration sampling:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  61 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06701999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.06363333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06363333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06363333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06363333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06363333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  52 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7611626
maxi score, test score, baseline:  0.06363333333333321 0.6990000000000001 0.6990000000000001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  0 policy actor:  0  step number:  42 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  0.06313999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.537]
 [0.787]
 [0.537]
 [0.537]
 [0.537]
 [0.537]] [[1.71 ]
 [1.71 ]
 [2.359]
 [1.71 ]
 [1.71 ]
 [1.71 ]
 [1.71 ]] [[0.537]
 [0.537]
 [0.787]
 [0.537]
 [0.537]
 [0.537]
 [0.537]]
maxi score, test score, baseline:  0.06313999999999986 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06313999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06313999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.06313999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.809]
 [0.766]
 [0.765]
 [0.782]
 [0.767]
 [0.758]] [[1.511]
 [2.06 ]
 [1.398]
 [1.363]
 [1.511]
 [1.166]
 [1.208]] [[0.769]
 [0.809]
 [0.766]
 [0.765]
 [0.782]
 [0.767]
 [0.758]]
maxi score, test score, baseline:  0.06313999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.06313999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  0 policy actor:  1  step number:  70 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
siam score:  -0.7555185
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.6365248466884275
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
siam score:  -0.7555367
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
siam score:  -0.75482345
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
line 256 mcts: sample exp_bonus 1.3923772170633806
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  50 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[ 0.004]
 [ 0.065]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[0.804]
 [0.601]
 [1.247]
 [1.247]
 [1.247]
 [1.247]
 [1.247]] [[0.099]
 [0.126]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
siam score:  -0.7471904
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
siam score:  -0.74797624
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  0.167
using another actor
first move QE:  -0.7024891828791245
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.0621533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  0 policy actor:  1  step number:  63 total reward:  0.02666666666666606  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.05761999999999988 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.05761999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
from probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.05228666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.05228666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.05228666666666655 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.05228666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.540169126490398, 0.15327695783653397, 0.15327695783653397, 0.15327695783653397]
maxi score, test score, baseline:  0.05228666666666655 0.6990000000000001 0.6990000000000001
start point for exploration sampling:  11091
actor:  1 policy actor:  1  step number:  34 total reward:  0.7133333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.05228666666666655 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.05228666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05228666666666655 0.6990000000000001 0.6990000000000001
siam score:  -0.7429447
maxi score, test score, baseline:  0.05228666666666655 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
siam score:  -0.74241334
maxi score, test score, baseline:  0.05228666666666655 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.05228666666666655 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  38 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.167
from probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05228666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05228666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05228666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
siam score:  -0.75135213
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.378033661623811
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
siam score:  -0.752488
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[1.41 ]
 [1.853]
 [1.853]
 [1.853]
 [1.853]
 [1.853]
 [1.853]] [[-0.038]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5933333333333334  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
siam score:  -0.7457796
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[ 0.148]
 [-0.03 ]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]] [[1.003]
 [0.744]
 [1.787]
 [1.787]
 [1.787]
 [1.787]
 [1.787]] [[0.334]
 [0.143]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  86 total reward:  0.01999999999999824  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  35 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  53 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.052286666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  0 policy actor:  0  step number:  38 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.05236666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  0 policy actor:  0  step number:  38 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.559]
 [0.536]
 [0.548]
 [0.533]
 [0.535]
 [0.553]] [[0.894]
 [1.434]
 [0.894]
 [0.975]
 [1.208]
 [1.105]
 [1.066]] [[-0.109]
 [ 0.005]
 [-0.109]
 [-0.083]
 [-0.06 ]
 [-0.074]
 [-0.063]]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.291]
 [0.162]
 [0.344]
 [0.37 ]
 [0.144]
 [0.183]] [[3.779]
 [4.095]
 [5.521]
 [4.933]
 [2.783]
 [3.366]
 [5.226]] [[0.516]
 [0.62 ]
 [0.822]
 [0.77 ]
 [0.424]
 [0.464]
 [0.779]]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
using another actor
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[-0.03 ]
 [ 0.027]
 [-0.186]
 [-0.022]
 [-0.028]
 [-0.157]
 [-0.013]] [[ 0.027]
 [-1.083]
 [-0.071]
 [-2.058]
 [-1.98 ]
 [-0.405]
 [-0.772]] [[-0.023]
 [-0.151]
 [-0.195]
 [-0.362]
 [-0.356]
 [-0.221]
 [-0.139]]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.105]
 [-0.091]
 [-0.104]
 [-0.091]
 [-0.093]
 [-0.091]
 [-0.091]] [[2.121]
 [2.042]
 [1.605]
 [2.042]
 [0.859]
 [2.042]
 [2.042]] [[ 0.425]
 [ 0.404]
 [ 0.24 ]
 [ 0.404]
 [-0.023]
 [ 0.404]
 [ 0.404]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  58 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
siam score:  -0.75427
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  44 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[-0.116]
 [ 0.134]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]] [[2.144]
 [2.872]
 [2.144]
 [2.144]
 [2.144]
 [2.144]
 [2.144]] [[0.573]
 [0.707]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
siam score:  -0.74759895
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  79 total reward:  0.11999999999999833  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]] [[0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]] [[-0.543]
 [-0.543]
 [-0.543]
 [-0.543]
 [-0.543]
 [-0.543]
 [-0.543]]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
using explorer policy with actor:  1
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.055313333333333214 0.6990000000000001 0.6990000000000001
actor:  0 policy actor:  0  step number:  32 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  0.333
from probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.058419999999999875 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.058419999999999875 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.058419999999999875 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.058419999999999875 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  0 policy actor:  0  step number:  54 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.059046666666666546 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.059046666666666546 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.059046666666666546 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  0 policy actor:  0  step number:  39 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.05925999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05925999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05925999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05925999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05925999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05925999999999989 0.6990000000000001 0.6990000000000001
first move QE:  -0.7046773090717379
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.05925999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
line 256 mcts: sample exp_bonus 1.423249827163005
maxi score, test score, baseline:  0.05925999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05925999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05925999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05925999999999987 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.05925999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.05925999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05925999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05925999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05925999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
siam score:  -0.74094826
actor:  1 policy actor:  1  step number:  67 total reward:  0.3333333333333336  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.05925999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05925999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05925999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  64 total reward:  0.17999999999999927  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.05925999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7487191646576394
maxi score, test score, baseline:  0.05925999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.854]
 [0.811]
 [0.81 ]
 [0.808]
 [0.815]
 [0.819]] [[1.998]
 [1.777]
 [1.982]
 [1.579]
 [1.672]
 [1.938]
 [1.697]] [[0.818]
 [0.854]
 [0.811]
 [0.81 ]
 [0.808]
 [0.815]
 [0.819]]
maxi score, test score, baseline:  0.05925999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  0 policy actor:  1  step number:  76 total reward:  0.01999999999999913  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 1.642196706173291
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
siam score:  -0.75048864
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06129999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.061299999999999875 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.061299999999999875 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.061299999999999875 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.061299999999999875 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.061299999999999875 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  60 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.060753333333333194 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[ 0.164]
 [ 0.213]
 [-0.034]
 [ 0.156]
 [ 0.155]
 [ 0.103]
 [ 0.103]] [[1.296]
 [1.202]
 [0.677]
 [1.524]
 [1.455]
 [1.077]
 [1.077]] [[-0.029]
 [ 0.004]
 [-0.331]
 [ 0.001]
 [-0.012]
 [-0.127]
 [-0.127]]
maxi score, test score, baseline:  0.060753333333333194 0.6990000000000001 0.6990000000000001
line 256 mcts: sample exp_bonus 0.4378068853618448
actor:  1 policy actor:  1  step number:  32 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.167
siam score:  -0.74007857
maxi score, test score, baseline:  0.060753333333333194 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060753333333333194 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.060753333333333194 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.060753333333333194 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060753333333333194 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.181]
 [0.306]
 [0.306]
 [0.306]
 [0.171]
 [0.306]] [[0.376]
 [1.049]
 [0.642]
 [0.642]
 [0.642]
 [0.336]
 [0.642]] [[-0.467]
 [-0.348]
 [-0.292]
 [-0.292]
 [-0.292]
 [-0.478]
 [-0.292]]
maxi score, test score, baseline:  0.060753333333333194 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  64 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.060753333333333194 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
line 256 mcts: sample exp_bonus 0.7350527616829883
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  0 policy actor:  1  step number:  42 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.162]
 [0.177]
 [0.154]
 [0.169]
 [0.145]
 [0.138]] [[0.675]
 [0.775]
 [0.82 ]
 [0.795]
 [0.493]
 [0.424]
 [0.54 ]] [[-0.448]
 [-0.458]
 [-0.435]
 [-0.463]
 [-0.498]
 [-0.534]
 [-0.521]]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  60 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  56 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.381]
 [-0.045]
 [-0.013]
 [ 0.285]
 [ 0.331]
 [-0.011]
 [ 0.285]] [[ 1.135]
 [ 0.83 ]
 [ 1.868]
 [ 0.967]
 [ 1.48 ]
 [-0.057]
 [ 0.967]] [[ 0.371]
 [-0.042]
 [ 0.136]
 [ 0.263]
 [ 0.378]
 [-0.141]
 [ 0.263]]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  59 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.256]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]] [[1.676]
 [1.232]
 [1.676]
 [1.676]
 [1.676]
 [1.676]
 [1.676]] [[-0.362]
 [-0.413]
 [-0.362]
 [-0.362]
 [-0.362]
 [-0.362]
 [-0.362]]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.481]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]] [[0.719]
 [0.769]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]] [[-0.222]
 [-0.104]
 [-0.222]
 [-0.222]
 [-0.222]
 [-0.222]
 [-0.222]]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  60 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.060966666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  0 policy actor:  0  step number:  40 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.0638333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  0 policy actor:  0  step number:  51 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06365999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06365999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06365999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06365999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06365999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  54 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  0.167
from probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06365999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06365999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06365999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06365999999999988 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06365999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06365999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06365999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  0 policy actor:  1  step number:  83 total reward:  0.1066666666666648  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06264666666666653 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06264666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.359]
 [0.409]
 [0.409]
 [0.381]
 [0.394]
 [0.411]] [[-1.444]
 [-1.054]
 [-2.081]
 [-1.802]
 [-1.263]
 [-2.397]
 [-2.087]] [[0.382]
 [0.359]
 [0.409]
 [0.409]
 [0.381]
 [0.394]
 [0.411]]
maxi score, test score, baseline:  0.06264666666666653 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06264666666666653 0.6990000000000001 0.6990000000000001
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.06264666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.06264666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06264666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
siam score:  -0.74923104
maxi score, test score, baseline:  0.06264666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.827]
 [0.692]
 [0.738]
 [0.766]
 [0.744]
 [0.754]] [[1.378]
 [1.307]
 [1.787]
 [1.655]
 [1.888]
 [1.268]
 [1.723]] [[0.674]
 [0.827]
 [0.692]
 [0.738]
 [0.766]
 [0.744]
 [0.754]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.06264666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06264666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06264666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
actor:  0 policy actor:  1  step number:  85 total reward:  0.13333333333333253  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.057]
 [ 0.09 ]
 [ 0.059]
 [-0.005]
 [ 0.057]
 [ 0.057]
 [-0.005]] [[ 0.117]
 [ 0.536]
 [-0.046]
 [-1.004]
 [ 0.117]
 [ 0.117]
 [-0.912]] [[-0.524]
 [-0.421]
 [-0.55 ]
 [-0.773]
 [-0.524]
 [-0.524]
 [-0.758]]
maxi score, test score, baseline:  0.06229999999999987 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
using another actor
maxi score, test score, baseline:  0.06229999999999987 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.005]
 [0.005]
 [0.676]
 [0.676]
 [0.004]
 [0.676]] [[ 0.274]
 [-0.   ]
 [-0.001]
 [ 0.274]
 [ 0.274]
 [ 0.006]
 [ 0.274]] [[ 0.233]
 [-0.483]
 [-0.483]
 [ 0.233]
 [ 0.233]
 [-0.483]
 [ 0.233]]
maxi score, test score, baseline:  0.06229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  70 total reward:  0.12666666666666548  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.893]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]] [[0.43 ]
 [0.567]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]] [[0.826]
 [0.893]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]] [[2.2  ]
 [1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]] [[ 0.067]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]]
maxi score, test score, baseline:  0.06229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  0 policy actor:  1  step number:  56 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  0.167
siam score:  -0.74281996
maxi score, test score, baseline:  0.06484666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06484666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06484666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06484666666666652 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  44 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06484666666666652 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  42 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[ 0.076]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.028]
 [-0.017]
 [-0.008]] [[ 1.793]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-1.24 ]
 [ 0.   ]
 [-0.03 ]] [[ 0.315]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.253]
 [-0.05 ]
 [-0.046]]
maxi score, test score, baseline:  0.06484666666666652 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06484666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[ 0.181]
 [ 0.259]
 [ 0.129]
 [ 0.181]
 [-0.049]
 [ 0.187]
 [-0.068]] [[2.773]
 [3.959]
 [3.703]
 [2.773]
 [1.917]
 [3.882]
 [2.662]] [[0.375]
 [0.609]
 [0.463]
 [0.375]
 [0.06 ]
 [0.537]
 [0.15 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.319]
 [0.354]
 [0.35 ]
 [0.319]
 [0.319]
 [0.405]] [[1.667]
 [2.01 ]
 [2.534]
 [2.019]
 [2.01 ]
 [2.01 ]
 [1.576]] [[0.133]
 [0.218]
 [0.377]
 [0.233]
 [0.218]
 [0.218]
 [0.133]]
maxi score, test score, baseline:  0.06484666666666652 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06484666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06484666666666652 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06484666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.168]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[1.673]
 [1.203]
 [1.673]
 [1.673]
 [1.673]
 [1.673]
 [1.673]] [[0.154]
 [0.227]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06484666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06484666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06484666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]] [[0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]] [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6500530036374927
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.7482079
siam score:  -0.7496756
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
UNIT TEST: sample policy line 217 mcts : [0.143 0.163 0.163 0.245 0.143 0.061 0.082]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.159]
 [0.143]
 [0.123]
 [0.109]
 [0.144]
 [0.113]] [[1.988]
 [2.133]
 [2.16 ]
 [2.13 ]
 [1.753]
 [2.074]
 [2.27 ]] [[-0.29 ]
 [-0.188]
 [-0.195]
 [-0.224]
 [-0.365]
 [-0.223]
 [-0.188]]
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  75 total reward:  0.026666666666665506  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  49 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.049]
 [-0.041]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]] [[-0.763]
 [-0.763]
 [ 1.087]
 [-0.763]
 [-0.763]
 [-0.763]
 [-0.763]] [[0.144]
 [0.144]
 [0.543]
 [0.144]
 [0.144]
 [0.144]
 [0.144]]
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
siam score:  -0.73896456
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  67 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.403]
 [0.315]
 [0.282]
 [0.26 ]
 [0.315]
 [0.338]] [[1.814]
 [1.426]
 [1.986]
 [1.498]
 [1.565]
 [1.986]
 [1.882]] [[ 0.108]
 [ 0.075]
 [ 0.081]
 [-0.034]
 [-0.045]
 [ 0.081]
 [ 0.086]]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.531]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[1.511]
 [1.999]
 [1.511]
 [1.511]
 [1.511]
 [1.511]
 [1.511]] [[0.056]
 [0.202]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]]
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  65 total reward:  0.1466666666666665  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
line 256 mcts: sample exp_bonus 1.6487578540145613
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  50 total reward:  0.6466666666666671  reward:  1.0 rdn_beta:  0.5
from probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0625133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06008666666666654 0.6990000000000001 0.6990000000000001
line 256 mcts: sample exp_bonus 1.135684020273147
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]] [[1.74 ]
 [1.571]
 [1.571]
 [1.571]
 [1.571]
 [1.571]
 [1.571]] [[ 0.17 ]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]]
maxi score, test score, baseline:  0.06008666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06008666666666654 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06008666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
line 256 mcts: sample exp_bonus 1.3066738774851447
actor:  1 policy actor:  1  step number:  47 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06008666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.06008666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[-0.037]
 [ 0.013]
 [-0.183]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]] [[-0.25 ]
 [-1.156]
 [-0.333]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.023]
 [-0.124]
 [-0.183]
 [ 0.031]
 [ 0.031]
 [ 0.031]
 [ 0.031]]
maxi score, test score, baseline:  0.06008666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  70 total reward:  0.4733333333333337  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  61 total reward:  0.02666666666666606  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0596733333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  33 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0596733333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.0596733333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0596733333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0596733333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0596733333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.0596733333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0596733333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5200000000000001  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0596733333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0596733333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  67 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.022]
 [ 0.17 ]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[0.452]
 [1.069]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]] [[0.298]
 [0.497]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
maxi score, test score, baseline:  0.0596733333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.0596733333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]] [[-0.225]
 [-0.225]
 [-0.225]
 [-0.225]
 [-0.225]
 [-0.225]
 [-0.225]]
maxi score, test score, baseline:  0.0596733333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
line 256 mcts: sample exp_bonus 0.2525970487656287
actor:  1 policy actor:  1  step number:  56 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0596733333333332 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  0 policy actor:  1  step number:  54 total reward:  0.3666666666666658  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.062406666666666534 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.062406666666666534 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.062406666666666534 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.062406666666666534 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  43 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.062406666666666534 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
from probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05949999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05949999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.05949999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.197]
 [0.211]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[1.399]
 [1.399]
 [1.606]
 [1.399]
 [1.399]
 [1.399]
 [1.399]] [[-0.361]
 [-0.361]
 [-0.313]
 [-0.361]
 [-0.361]
 [-0.361]
 [-0.361]]
maxi score, test score, baseline:  0.05949999999999988 0.6990000000000001 0.6990000000000001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.05949999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
maxi score, test score, baseline:  0.059499999999999865 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  47 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.059499999999999865 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[ 0.066]
 [ 0.25 ]
 [ 0.054]
 [ 0.073]
 [ 0.039]
 [-0.002]
 [ 0.103]] [[-3.375]
 [-0.765]
 [-3.453]
 [-3.48 ]
 [-3.515]
 [-3.254]
 [-3.401]] [[ 0.066]
 [ 0.25 ]
 [ 0.054]
 [ 0.073]
 [ 0.039]
 [-0.002]
 [ 0.103]]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.588]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.453]] [[-0.194]
 [ 0.403]
 [-0.194]
 [-0.194]
 [-0.194]
 [-0.194]
 [ 0.091]] [[0.462]
 [0.588]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.453]]
maxi score, test score, baseline:  0.059499999999999865 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.059499999999999865 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.828]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[1.513]
 [1.473]
 [1.513]
 [1.513]
 [1.513]
 [1.513]
 [1.513]] [[0.735]
 [0.828]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]]
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.943]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]] [[1.154]
 [0.834]
 [1.599]
 [1.599]
 [1.599]
 [1.599]
 [1.599]] [[0.871]
 [0.943]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]]
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.942]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]] [[1.861]
 [1.584]
 [1.861]
 [1.861]
 [1.861]
 [1.861]
 [1.861]] [[0.869]
 [0.942]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]]
maxi score, test score, baseline:  0.059499999999999865 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.601]
 [0.507]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[2.004]
 [1.949]
 [2.084]
 [1.995]
 [2.046]
 [2.046]
 [2.046]] [[0.28 ]
 [0.327]
 [0.305]
 [0.284]
 [0.307]
 [0.307]
 [0.307]]
start point for exploration sampling:  11091
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10051333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.5771323924784814, 0.1409558691738395, 0.1409558691738395, 0.1409558691738395]
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
from probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.1087133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.1087133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.1087133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
actor:  0 policy actor:  0  step number:  63 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.7138753839782155
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  0 policy actor:  0  step number:  45 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.013]
 [ 0.316]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]] [[1.4  ]
 [1.256]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.4  ]] [[0.3  ]
 [0.561]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]]
maxi score, test score, baseline:  0.10841999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10841999999999988 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[2.108]
 [2.108]
 [2.108]
 [2.108]
 [2.108]
 [2.108]
 [2.108]] [[0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]]
maxi score, test score, baseline:  0.10841999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
actor:  0 policy actor:  0  step number:  52 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10893999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.731]
 [0.669]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[-0.176]
 [-0.186]
 [-0.699]
 [-0.176]
 [-0.176]
 [-0.176]
 [-0.176]] [[0.646]
 [0.731]
 [0.669]
 [0.646]
 [0.646]
 [0.646]
 [0.646]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10893999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
actor:  1 policy actor:  1  step number:  63 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10893999999999988 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.10893999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
Printing some Q and Qe and total Qs values:  [[ 0.134]
 [ 0.031]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.004]] [[1.068]
 [0.49 ]
 [1.612]
 [0.606]
 [0.606]
 [0.606]
 [0.023]] [[ 0.164]
 [ 0.021]
 [ 0.186]
 [ 0.024]
 [ 0.024]
 [ 0.024]
 [-0.072]]
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[-0.007]
 [ 0.11 ]
 [-0.008]
 [-0.016]
 [-0.027]
 [-0.008]
 [-0.017]] [[ 1.864]
 [ 1.886]
 [-0.116]
 [-0.959]
 [-0.929]
 [-0.181]
 [-0.043]] [[ 0.322]
 [ 0.371]
 [-0.059]
 [-0.224]
 [-0.222]
 [-0.071]
 [-0.048]]
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
Printing some Q and Qe and total Qs values:  [[-0.1  ]
 [ 0.165]
 [-0.007]
 [-0.021]
 [-0.021]
 [-0.074]
 [-0.158]] [[0.627]
 [2.552]
 [0.855]
 [2.667]
 [2.667]
 [0.81 ]
 [0.31 ]] [[ 0.002]
 [ 0.413]
 [ 0.093]
 [ 0.296]
 [ 0.296]
 [ 0.041]
 [-0.077]]
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
siam score:  -0.74134463
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
line 256 mcts: sample exp_bonus -0.34289379920825486
siam score:  -0.7435583
actor:  1 policy actor:  1  step number:  47 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
Printing some Q and Qe and total Qs values:  [[ 0.04 ]
 [ 0.014]
 [ 0.04 ]
 [ 0.04 ]
 [-0.025]
 [ 0.04 ]
 [ 0.111]] [[ 0.302]
 [ 0.767]
 [ 0.302]
 [ 0.302]
 [-0.548]
 [ 0.302]
 [ 1.235]] [[-0.491]
 [-0.439]
 [-0.491]
 [-0.491]
 [-0.698]
 [-0.491]
 [-0.264]]
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.1067133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10425999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
line 256 mcts: sample exp_bonus 2.4488387721908627
actor:  1 policy actor:  1  step number:  72 total reward:  0.2733333333333323  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[1.79 ]
 [1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]] [[0.44 ]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]]
siam score:  -0.7442578
maxi score, test score, baseline:  0.10425999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10425999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.465]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.309]] [[2.138]
 [1.839]
 [1.307]
 [1.307]
 [1.307]
 [1.307]
 [1.876]] [[ 0.23 ]
 [ 0.242]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [ 0.098]]
maxi score, test score, baseline:  0.10425999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10425999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10425999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10425999999999987 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.10425999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10425999999999987 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.10425999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
from probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]] [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]]
maxi score, test score, baseline:  0.10425999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10425999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10425999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10425999999999987 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.021]
 [ 0.027]
 [ 0.027]
 [ 0.027]
 [ 0.027]
 [ 0.027]] [[ 0.621]
 [ 0.598]
 [-0.176]
 [-0.176]
 [-0.176]
 [-0.176]
 [-0.176]] [[0.16 ]
 [0.147]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.008]
 [-0.036]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[1.083]
 [1.083]
 [1.546]
 [1.083]
 [1.083]
 [1.083]
 [1.083]] [[-0.611]
 [-0.611]
 [-0.485]
 [-0.611]
 [-0.611]
 [-0.611]
 [-0.611]]
siam score:  -0.7386759
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
actor:  1 policy actor:  1  step number:  56 total reward:  0.4199999999999995  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[ 0.25 ]
 [-0.028]
 [-0.028]
 [-0.05 ]
 [-0.028]
 [-0.028]
 [-0.028]] [[1.03 ]
 [0.291]
 [0.291]
 [0.155]
 [0.291]
 [0.291]
 [0.291]] [[0.467]
 [0.176]
 [0.176]
 [0.143]
 [0.176]
 [0.176]
 [0.176]]
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
UNIT TEST: sample policy line 217 mcts : [0.02  0.898 0.02  0.    0.    0.02  0.041]
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
actor:  1 policy actor:  1  step number:  64 total reward:  0.20666666666666555  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  34 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.514]
 [0.919]
 [0.78 ]
 [0.703]
 [0.514]
 [0.977]] [[1.394]
 [1.068]
 [0.275]
 [0.209]
 [0.44 ]
 [1.068]
 [0.13 ]] [[0.8  ]
 [0.531]
 [0.537]
 [0.396]
 [0.428]
 [0.531]
 [0.526]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.404]
 [0.405]
 [0.396]
 [0.328]
 [0.328]
 [0.37 ]] [[-0.058]
 [-0.111]
 [-0.294]
 [-0.26 ]
 [ 0.003]
 [ 0.04 ]
 [-0.106]] [[-0.388]
 [-0.295]
 [-0.325]
 [-0.329]
 [-0.352]
 [-0.346]
 [-0.328]]
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10136666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
first move QE:  -0.7162503844839181
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[1.351]
 [1.351]
 [1.351]
 [1.351]
 [1.351]
 [1.351]
 [1.351]] [[0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]]
actor:  0 policy actor:  0  step number:  40 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10205999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
actor:  0 policy actor:  0  step number:  40 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.106]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[1.468]
 [0.936]
 [1.468]
 [1.468]
 [1.468]
 [1.468]
 [1.468]] [[-0.187]
 [-0.185]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.187]]
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.168]
 [ 0.053]
 [-0.035]
 [-0.091]
 [-0.028]
 [ 0.095]] [[0.94 ]
 [1.387]
 [1.434]
 [0.084]
 [0.172]
 [0.321]
 [0.975]] [[ 0.071]
 [ 0.267]
 [ 0.182]
 [-0.068]
 [-0.1  ]
 [-0.03 ]
 [ 0.154]]
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.176]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]] [[1.978]
 [1.979]
 [1.978]
 [1.978]
 [1.978]
 [1.978]
 [1.978]] [[0.164]
 [0.188]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]]
maxi score, test score, baseline:  0.10492666666666654 0.6900000000000002 0.6900000000000002
start point for exploration sampling:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10492666666666654 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.10492666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10492666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10492666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.728]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]] [[0.839]
 [0.455]
 [0.839]
 [0.839]
 [0.839]
 [0.839]
 [0.839]] [[0.089]
 [0.039]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]]
siam score:  -0.7364452
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]] [[0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]] [[0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]]
maxi score, test score, baseline:  0.10492666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
actor:  1 policy actor:  1  step number:  58 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10492666666666654 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.10492666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
first move QE:  -0.7175239220807847
maxi score, test score, baseline:  0.10492666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10492666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
actor:  1 policy actor:  1  step number:  62 total reward:  0.5800000000000005  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  39 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  0.167
siam score:  -0.74160856
line 256 mcts: sample exp_bonus 0.4916149965492925
actor:  1 policy actor:  1  step number:  52 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.149]
 [0.231]
 [0.149]
 [0.149]
 [0.253]
 [0.046]] [[1.94 ]
 [1.94 ]
 [2.06 ]
 [1.94 ]
 [1.94 ]
 [1.729]
 [1.79 ]] [[0.251]
 [0.251]
 [0.411]
 [0.251]
 [0.251]
 [0.215]
 [0.051]]
maxi score, test score, baseline:  0.10501999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]] [[-0.46]
 [-0.46]
 [-0.46]
 [-0.46]
 [-0.46]
 [-0.46]
 [-0.46]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.22 ]
 [ 0.317]
 [ 0.223]
 [ 0.218]
 [ 0.207]
 [-0.004]
 [ 0.255]] [[ 1.571]
 [ 0.453]
 [ 0.744]
 [ 1.271]
 [ 1.337]
 [-0.371]
 [ 1.258]] [[-0.15 ]
 [-0.241]
 [-0.286]
 [-0.203]
 [-0.203]
 [-0.699]
 [-0.168]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10501999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10501999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]] [[0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]] [[0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]]
maxi score, test score, baseline:  0.10501999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
start point for exploration sampling:  11091
line 256 mcts: sample exp_bonus -1.2685714505078673
maxi score, test score, baseline:  0.10501999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10501999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
actor:  1 policy actor:  1  step number:  84 total reward:  0.3399999999999993  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  58 total reward:  0.39333333333333353  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10215333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10215333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10215333333333321 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.10215333333333321 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.10215333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10215333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.305]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]] [[-0.566]
 [ 0.383]
 [-0.566]
 [-0.566]
 [-0.566]
 [-0.566]
 [-0.566]] [[-0.455]
 [-0.1  ]
 [-0.455]
 [-0.455]
 [-0.455]
 [-0.455]
 [-0.455]]
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.10215333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10215333333333321 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.10215333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10215333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10215333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
actor:  1 policy actor:  1  step number:  54 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  0
maxi score, test score, baseline:  0.10215333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.727]
 [0.817]
 [0.794]
 [0.833]
 [0.729]
 [0.727]] [[0.953]
 [1.397]
 [3.136]
 [1.699]
 [2.337]
 [1.922]
 [2.636]] [[0.436]
 [0.727]
 [0.817]
 [0.794]
 [0.833]
 [0.729]
 [0.727]]
maxi score, test score, baseline:  0.10215333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
UNIT TEST: sample policy line 217 mcts : [0.204 0.694 0.02  0.02  0.02  0.02  0.02 ]
actor:  0 policy actor:  1  step number:  48 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.10224666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10224666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10224666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.155]
 [0.202]
 [0.155]
 [0.155]
 [0.155]
 [0.155]] [[2.616]
 [2.616]
 [1.847]
 [2.616]
 [2.616]
 [2.616]
 [2.616]] [[-0.31 ]
 [-0.31 ]
 [-0.391]
 [-0.31 ]
 [-0.31 ]
 [-0.31 ]
 [-0.31 ]]
maxi score, test score, baseline:  0.10224666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10224666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10224666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
actor:  0 policy actor:  0  step number:  40 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10529999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10529999999999988 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.303]
 [0.225]
 [0.228]
 [0.185]
 [0.225]
 [0.185]] [[2.945]
 [2.258]
 [1.598]
 [1.336]
 [2.945]
 [1.826]
 [2.945]] [[ 0.282]
 [ 0.057]
 [-0.351]
 [-0.479]
 [ 0.282]
 [-0.238]
 [ 0.282]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7437233
maxi score, test score, baseline:  0.10529999999999988 0.6900000000000002 0.6900000000000002
actor:  1 policy actor:  1  step number:  62 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10529999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10529999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10529999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10529999999999988 0.6900000000000002 0.6900000000000002
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]] [[4.636]
 [1.709]
 [1.709]
 [1.709]
 [1.709]
 [1.709]
 [1.709]] [[ 0.326]
 [-0.154]
 [-0.154]
 [-0.154]
 [-0.154]
 [-0.154]
 [-0.154]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  49 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10780666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
Printing some Q and Qe and total Qs values:  [[-0.025]
 [ 0.126]
 [ 0.126]
 [ 0.126]
 [ 0.126]
 [ 0.126]
 [ 0.126]] [[7.521]
 [2.936]
 [2.936]
 [2.936]
 [2.936]
 [2.936]
 [2.936]] [[0.841]
 [0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.165]]
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.241]
 [0.212]
 [0.207]
 [0.152]
 [0.206]
 [0.209]] [[1.913]
 [2.046]
 [2.193]
 [1.824]
 [3.614]
 [1.729]
 [1.69 ]] [[ 0.037]
 [ 0.098]
 [ 0.122]
 [ 0.023]
 [ 0.462]
 [-0.003]
 [-0.012]]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.257]
 [0.088]
 [0.208]
 [0.208]
 [0.206]
 [0.202]] [[1.834]
 [2.07 ]
 [5.303]
 [1.885]
 [1.692]
 [1.814]
 [1.986]] [[0.029]
 [0.085]
 [0.56 ]
 [0.038]
 [0.007]
 [0.026]
 [0.053]]
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.035]
 [0.029]
 [0.032]
 [0.032]
 [0.032]
 [0.033]] [[2.171]
 [2.169]
 [2.868]
 [1.952]
 [1.92 ]
 [2.042]
 [1.96 ]] [[-0.206]
 [-0.202]
 [ 0.092]
 [-0.297]
 [-0.311]
 [-0.259]
 [-0.293]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.10780666666666655 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]] [[2.006]
 [2.006]
 [2.006]
 [2.006]
 [2.006]
 [2.006]
 [2.006]] [[1.044]
 [1.044]
 [1.044]
 [1.044]
 [1.044]
 [1.044]
 [1.044]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.37999999999999945  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10803333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
maxi score, test score, baseline:  0.10803333333333322 0.6900000000000002 0.6900000000000002
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.237]
 [0.206]
 [0.207]
 [0.205]
 [0.205]
 [0.209]] [[0.076]
 [0.194]
 [0.21 ]
 [0.139]
 [0.035]
 [0.167]
 [0.217]] [[-0.522]
 [-0.46 ]
 [-0.488]
 [-0.498]
 [-0.518]
 [-0.496]
 [-0.484]]
maxi score, test score, baseline:  0.10803333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6024016715144972, 0.13092912805482873, 0.13092912805482873, 0.13574007237584537]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.492]
 [0.418]
 [0.375]
 [0.377]
 [0.377]
 [0.399]] [[1.656]
 [1.805]
 [1.913]
 [2.097]
 [2.088]
 [1.954]
 [1.92 ]] [[-0.01 ]
 [ 0.134]
 [ 0.114]
 [ 0.163]
 [ 0.16 ]
 [ 0.093]
 [ 0.099]]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10803333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.10803333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.57 ]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[1.489]
 [1.994]
 [1.489]
 [1.489]
 [1.489]
 [1.489]
 [1.489]] [[0.165]
 [0.516]
 [0.165]
 [0.165]
 [0.165]
 [0.165]
 [0.165]]
UNIT TEST: sample policy line 217 mcts : [0.163 0.449 0.    0.265 0.02  0.    0.102]
maxi score, test score, baseline:  0.10803333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.10803333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  74 total reward:  0.03333333333333233  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  56 total reward:  0.4333333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10803333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.10803333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  0.167
using another actor
siam score:  -0.74320054
maxi score, test score, baseline:  0.10803333333333322 0.6900000000000002 0.6900000000000002
actor:  1 policy actor:  1  step number:  40 total reward:  0.54  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7444976
maxi score, test score, baseline:  0.10803333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  45 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10803333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.217]
 [0.226]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[1.004]
 [0.907]
 [1.239]
 [1.004]
 [1.004]
 [1.004]
 [1.004]] [[-0.228]
 [-0.294]
 [-0.228]
 [-0.228]
 [-0.228]
 [-0.228]
 [-0.228]]
maxi score, test score, baseline:  0.10803333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  0 policy actor:  1  step number:  45 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  73 total reward:  0.11999999999999955  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.10780666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
siam score:  -0.74459183
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10780666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1097991269585543
maxi score, test score, baseline:  0.10780666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.10780666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  56 total reward:  0.40666666666666607  reward:  1.0 rdn_beta:  0.333
from probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.10780666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.798]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]] [[ 0.155]
 [-0.036]
 [ 0.155]
 [ 0.155]
 [ 0.155]
 [ 0.155]
 [ 0.155]] [[0.747]
 [0.798]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]]
maxi score, test score, baseline:  0.10780666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  54 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  11091
actor:  0 policy actor:  1  step number:  52 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10793999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.10793999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  0 policy actor:  0  step number:  58 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.873]
 [0.784]
 [0.777]
 [0.777]
 [0.777]
 [0.777]] [[-1.022]
 [ 0.435]
 [-0.386]
 [-0.645]
 [-0.645]
 [-0.645]
 [-0.645]] [[0.768]
 [0.873]
 [0.784]
 [0.777]
 [0.777]
 [0.777]
 [0.777]]
maxi score, test score, baseline:  0.10781999999999989 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.10781999999999989 0.6900000000000002 0.6900000000000002
actor:  0 policy actor:  0  step number:  43 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.126]
 [0.141]
 [0.131]
 [0.125]
 [0.106]
 [0.119]] [[1.159]
 [1.191]
 [0.93 ]
 [1.048]
 [1.13 ]
 [0.996]
 [1.023]] [[-0.57 ]
 [-0.555]
 [-0.583]
 [-0.574]
 [-0.566]
 [-0.607]
 [-0.589]]
maxi score, test score, baseline:  0.11075333333333323 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  0 policy actor:  1  step number:  59 total reward:  0.2533333333333325  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11084666666666657 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11084666666666657 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11084666666666657 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  65 total reward:  0.23999999999999921  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11084666666666657 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.757]
 [0.684]
 [0.678]
 [0.726]
 [0.726]
 [0.701]] [[0.92 ]
 [1.785]
 [1.807]
 [2.196]
 [2.158]
 [2.158]
 [0.495]] [[0.073]
 [0.276]
 [0.207]
 [0.266]
 [0.308]
 [0.308]
 [0.005]]
siam score:  -0.72215706
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.487501579878747
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.416]
 [0.414]
 [0.402]
 [0.402]
 [0.397]
 [0.399]] [[1.968]
 [1.8  ]
 [2.083]
 [1.982]
 [2.052]
 [1.984]
 [1.769]] [[-0.098]
 [-0.118]
 [-0.073]
 [-0.102]
 [-0.09 ]
 [-0.107]
 [-0.141]]
from probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  63 total reward:  0.4666666666666671  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  64 total reward:  0.5133333333333339  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.3190674356075602
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.18 ]
 [0.183]
 [0.139]
 [0.115]
 [0.183]
 [0.166]] [[2.174]
 [1.819]
 [1.913]
 [1.36 ]
 [1.252]
 [1.913]
 [1.64 ]] [[ 0.071]
 [-0.026]
 [ 0.009]
 [-0.22 ]
 [-0.28 ]
 [ 0.009]
 [-0.099]]
actor:  0 policy actor:  1  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11117999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11117999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.10797999999999988 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.10797999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.10797999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.553]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[1.464]
 [1.425]
 [1.464]
 [1.464]
 [1.464]
 [1.464]
 [1.464]] [[-0.002]
 [ 0.028]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]]
maxi score, test score, baseline:  0.10797999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.10797999999999988 0.6900000000000002 0.6900000000000002
actor:  1 policy actor:  1  step number:  61 total reward:  0.13333333333333297  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  76 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10797999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
siam score:  -0.7237587
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.698]
 [0.671]
 [0.663]
 [0.671]
 [0.392]
 [0.691]] [[0.833]
 [0.242]
 [1.223]
 [0.727]
 [0.85 ]
 [0.248]
 [0.461]] [[0.653]
 [0.698]
 [0.671]
 [0.663]
 [0.671]
 [0.392]
 [0.691]]
maxi score, test score, baseline:  0.10797999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  0 policy actor:  0  step number:  44 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10809999999999989 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]] [[0.925]
 [1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.256]] [[0.839]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  43 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]] [[-0.263]
 [-0.263]
 [-0.263]
 [-0.263]
 [-0.263]
 [-0.263]
 [-0.263]] [[-0.458]
 [-0.458]
 [-0.458]
 [-0.458]
 [-0.458]
 [-0.458]
 [-0.458]]
maxi score, test score, baseline:  0.10804666666666654 0.6900000000000002 0.6900000000000002
using another actor
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5883452741369328
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10804666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  0 policy actor:  1  step number:  47 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11092666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.012]
 [0.283]
 [0.345]
 [0.322]
 [0.336]
 [0.32 ]] [[1.596]
 [1.302]
 [1.743]
 [1.812]
 [1.755]
 [1.52 ]
 [1.343]] [[0.35 ]
 [0.004]
 [0.393]
 [0.466]
 [0.424]
 [0.326]
 [0.234]]
maxi score, test score, baseline:  0.11092666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11092666666666656 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.865]
 [0.768]
 [0.847]
 [0.768]
 [0.758]
 [0.828]] [[1.854]
 [1.063]
 [0.804]
 [1.133]
 [0.804]
 [1.413]
 [1.151]] [[0.75 ]
 [0.865]
 [0.768]
 [0.847]
 [0.768]
 [0.758]
 [0.828]]
maxi score, test score, baseline:  0.11092666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  0 policy actor:  0  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11372666666666656 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.486]
 [0.476]
 [0.443]
 [0.344]
 [0.413]
 [0.486]] [[2.191]
 [2.836]
 [2.9  ]
 [2.347]
 [2.448]
 [2.679]
 [2.836]] [[-0.166]
 [ 0.275]
 [ 0.286]
 [ 0.068]
 [ 0.003]
 [ 0.149]
 [ 0.275]]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.047]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[4.278]
 [3.557]
 [5.118]
 [5.118]
 [5.118]
 [5.118]
 [5.118]] [[0.558]
 [0.354]
 [0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]]
maxi score, test score, baseline:  0.11372666666666656 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.11372666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11372666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.582]
 [0.73 ]
 [0.66 ]
 [0.478]
 [0.643]
 [0.582]] [[1.498]
 [1.498]
 [0.923]
 [0.566]
 [0.38 ]
 [1.891]
 [1.498]] [[ 0.166]
 [ 0.166]
 [ 0.122]
 [-0.067]
 [-0.311]
 [ 0.358]
 [ 0.166]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]] [[5.706]
 [2.102]
 [2.102]
 [2.102]
 [2.102]
 [2.102]
 [2.102]] [[0.751]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]]
actor:  0 policy actor:  1  step number:  64 total reward:  0.0066666666666660435  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11339333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]] [[3.983]
 [2.404]
 [2.404]
 [2.404]
 [2.404]
 [2.404]
 [2.404]] [[0.528]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]]
maxi score, test score, baseline:  0.11339333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11339333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11339333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Printing some Q and Qe and total Qs values:  [[ 0.091]
 [-0.009]
 [-0.083]
 [-0.019]
 [-0.022]
 [-0.136]
 [-0.009]] [[ 0.369]
 [-0.113]
 [-0.167]
 [-2.442]
 [-3.882]
 [-0.404]
 [-0.274]] [[ 0.345]
 [ 0.226]
 [ 0.172]
 [-0.035]
 [-0.195]
 [ 0.111]
 [ 0.208]]
maxi score, test score, baseline:  0.11339333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  102 total reward:  0.046666666666664525  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.11339333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
siam score:  -0.7405333
actor:  1 policy actor:  1  step number:  52 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
siam score:  -0.73733765
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.453]
 [0.453]
 [0.515]
 [0.453]
 [0.453]
 [0.487]] [[2.819]
 [2.819]
 [2.819]
 [3.563]
 [2.819]
 [2.819]
 [2.161]] [[0.445]
 [0.445]
 [0.445]
 [0.802]
 [0.445]
 [0.445]
 [0.186]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.49333333333333396  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[1.776]
 [1.776]
 [1.776]
 [1.776]
 [1.776]
 [1.776]
 [1.776]] [[0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]]
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.078]
 [0.153]
 [0.078]
 [0.078]
 [0.078]
 [0.078]] [[1.92 ]
 [1.92 ]
 [1.631]
 [1.92 ]
 [1.92 ]
 [1.92 ]
 [1.92 ]] [[0.252]
 [0.252]
 [0.278]
 [0.252]
 [0.252]
 [0.252]
 [0.252]]
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
siam score:  -0.7354776
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.363]
 [0.279]
 [0.294]
 [0.288]
 [0.246]
 [0.297]] [[1.134]
 [0.76 ]
 [1.222]
 [1.212]
 [0.914]
 [1.218]
 [0.821]] [[-0.346]
 [-0.332]
 [-0.338]
 [-0.326]
 [-0.381]
 [-0.373]
 [-0.388]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
siam score:  -0.73723674
actor:  1 policy actor:  1  step number:  46 total reward:  0.54  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]] [[1.713]
 [1.713]
 [1.713]
 [1.713]
 [1.713]
 [1.713]
 [1.713]] [[-0.225]
 [-0.225]
 [-0.225]
 [-0.225]
 [-0.225]
 [-0.225]
 [-0.225]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.352204201297252
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.416]
 [0.363]
 [0.36 ]
 [0.359]
 [0.36 ]
 [0.343]] [[2.077]
 [1.731]
 [1.992]
 [1.973]
 [1.978]
 [1.955]
 [1.547]] [[ 0.038]
 [ 0.003]
 [ 0.036]
 [ 0.027]
 [ 0.028]
 [ 0.021]
 [-0.131]]
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  51 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  0 policy actor:  1  step number:  62 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11251333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.528]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]] [[2.539]
 [3.189]
 [2.539]
 [2.539]
 [2.539]
 [2.539]
 [2.539]] [[0.222]
 [0.496]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]]
maxi score, test score, baseline:  0.11251333333333322 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.11251333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
siam score:  -0.7292496
maxi score, test score, baseline:  0.11251333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11251333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11251333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11251333333333322 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.11251333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  0 policy actor:  0  step number:  41 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.11201999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.11201999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  0 policy actor:  1  step number:  43 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11136666666666654 0.6900000000000002 0.6900000000000002
actor:  0 policy actor:  0  step number:  56 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  45 total reward:  0.6400000000000003  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1067933333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.1067933333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.8970129717610478
line 256 mcts: sample exp_bonus 1.5985538528911685
actor:  1 policy actor:  1  step number:  39 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.10339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  86 total reward:  0.17999999999999805  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[2.024]
 [2.024]
 [2.024]
 [2.024]
 [2.024]
 [2.024]
 [2.024]] [[-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]]
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.438]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[2.132]
 [2.171]
 [2.368]
 [2.368]
 [2.368]
 [2.368]
 [2.368]] [[-0.072]
 [ 0.058]
 [ 0.104]
 [ 0.104]
 [ 0.104]
 [ 0.104]
 [ 0.104]]
UNIT TEST: sample policy line 217 mcts : [0.122 0.306 0.082 0.163 0.02  0.061 0.245]
maxi score, test score, baseline:  0.10339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.10339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
first move QE:  -0.735791474937836
maxi score, test score, baseline:  0.10339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.10339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]] [[8.255]
 [3.43 ]
 [3.43 ]
 [3.43 ]
 [3.43 ]
 [3.43 ]
 [3.43 ]] [[0.865]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]] [[8.45 ]
 [3.306]
 [3.306]
 [3.306]
 [3.306]
 [3.306]
 [3.306]] [[0.88 ]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
siam score:  -0.73113656
maxi score, test score, baseline:  0.10339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.10339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  115 total reward:  0.18666666666666532  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.281]
 [0.235]
 [0.201]
 [0.201]
 [0.201]
 [0.269]] [[1.22 ]
 [1.026]
 [0.737]
 [1.226]
 [1.226]
 [1.226]
 [1.19 ]] [[0.328]
 [0.29 ]
 [0.201]
 [0.246]
 [0.246]
 [0.246]
 [0.303]]
maxi score, test score, baseline:  0.10339333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  0 policy actor:  1  step number:  61 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10225999999999989 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
from probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
using another actor
maxi score, test score, baseline:  0.10225999999999989 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.843]
 [0.909]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]] [[-0.163]
 [ 0.556]
 [-0.163]
 [-0.163]
 [-0.163]
 [-0.163]
 [-0.163]] [[0.843]
 [0.909]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]]
actor:  0 policy actor:  1  step number:  49 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10157999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  0 policy actor:  0  step number:  44 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.245]
 [ 0.   ]
 [-0.245]
 [-0.245]
 [-0.245]
 [-0.245]
 [-0.245]] [[-0.063]
 [ 0.   ]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]] [[-0.246]
 [ 0.041]
 [-0.246]
 [-0.246]
 [-0.246]
 [-0.246]
 [-0.246]]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.012]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[-3.493]
 [-0.768]
 [-3.493]
 [-3.493]
 [-3.493]
 [-3.493]
 [-3.493]] [[0.06 ]
 [0.454]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.10107333333333321 0.6900000000000002 0.6900000000000002
using explorer policy with actor:  0
maxi score, test score, baseline:  0.10107333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.384]
 [0.346]
 [0.29 ]
 [0.233]
 [0.342]
 [0.324]] [[ 1.334]
 [-0.059]
 [ 1.027]
 [ 0.763]
 [ 0.838]
 [ 1.085]
 [ 0.36 ]] [[-0.274]
 [-0.404]
 [-0.26 ]
 [-0.361]
 [-0.405]
 [-0.255]
 [-0.394]]
maxi score, test score, baseline:  0.09767333333333321 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.09767333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  63 total reward:  0.1466666666666655  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.263]
 [0.242]
 [0.07 ]
 [0.051]
 [0.242]
 [0.101]] [[-0.047]
 [-0.987]
 [ 0.   ]
 [-1.3  ]
 [-3.462]
 [ 0.   ]
 [-0.421]] [[0.058]
 [0.263]
 [0.242]
 [0.07 ]
 [0.051]
 [0.242]
 [0.101]]
Printing some Q and Qe and total Qs values:  [[0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]] [[0.73]
 [0.73]
 [0.73]
 [0.73]
 [0.73]
 [0.73]
 [0.73]] [[0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]]
maxi score, test score, baseline:  0.09767333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
start point for exploration sampling:  11091
actor:  1 policy actor:  1  step number:  59 total reward:  0.3733333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09427333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  0 policy actor:  0  step number:  53 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09324666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
maxi score, test score, baseline:  0.09324666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  0 policy actor:  0  step number:  69 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
siam score:  -0.73680776
maxi score, test score, baseline:  0.0920333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.229]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[1.372]
 [1.501]
 [1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]] [[-0.083]
 [-0.074]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]]
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.207]
 [0.212]
 [0.14 ]
 [0.14 ]
 [0.145]
 [0.185]] [[1.662]
 [1.553]
 [2.097]
 [1.804]
 [1.804]
 [0.059]
 [1.655]] [[-0.033]
 [-0.087]
 [ 0.008]
 [-0.112]
 [-0.112]
 [-0.399]
 [-0.093]]
maxi score, test score, baseline:  0.0920333333333332 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.0920333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0920333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6026160297341105, 0.13087296292766534, 0.13087296292766534, 0.13563804441055877]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0920333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
maxi score, test score, baseline:  0.0920333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
maxi score, test score, baseline:  0.0920333333333332 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[ 0.017]
 [-0.022]
 [ 0.022]
 [-0.   ]
 [-0.027]
 [ 0.004]
 [ 0.05 ]] [[ 0.262]
 [ 0.727]
 [-0.866]
 [-1.61 ]
 [ 0.64 ]
 [ 0.014]
 [ 1.595]] [[ 0.199]
 [ 0.318]
 [-0.128]
 [-0.354]
 [ 0.29 ]
 [ 0.121]
 [ 0.603]]
maxi score, test score, baseline:  0.0920333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
Printing some Q and Qe and total Qs values:  [[-0.   ]
 [ 0.369]
 [ 0.306]
 [ 0.276]
 [ 0.175]
 [ 0.336]
 [ 0.333]] [[1.186]
 [2.405]
 [2.714]
 [1.455]
 [1.529]
 [2.569]
 [2.144]] [[-0.138]
 [ 0.434]
 [ 0.423]
 [ 0.183]
 [ 0.094]
 [ 0.429]
 [ 0.355]]
maxi score, test score, baseline:  0.0920333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
maxi score, test score, baseline:  0.0920333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
maxi score, test score, baseline:  0.0920333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
maxi score, test score, baseline:  0.0920333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6000000000000003  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0886333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.171]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]] [[0.227]
 [0.816]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]] [[-0.606]
 [-0.412]
 [-0.606]
 [-0.606]
 [-0.606]
 [-0.606]
 [-0.606]]
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.355]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.276]] [[1.932]
 [2.026]
 [1.771]
 [1.771]
 [1.771]
 [1.771]
 [2.242]] [[-0.225]
 [-0.214]
 [-0.403]
 [-0.403]
 [-0.403]
 [-0.403]
 [-0.257]]
maxi score, test score, baseline:  0.0886333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
line 256 mcts: sample exp_bonus 0.5344467295233553
actor:  1 policy actor:  1  step number:  51 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0886333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
maxi score, test score, baseline:  0.0886333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
actor:  0 policy actor:  0  step number:  58 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  0.5
from probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
maxi score, test score, baseline:  0.08752666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.08752666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.535]
 [0.488]
 [0.527]
 [0.482]
 [0.514]
 [0.465]] [[0.426]
 [0.603]
 [0.297]
 [0.47 ]
 [0.147]
 [0.335]
 [0.208]] [[0.494]
 [0.535]
 [0.488]
 [0.527]
 [0.482]
 [0.514]
 [0.465]]
maxi score, test score, baseline:  0.08752666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
from probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
line 256 mcts: sample exp_bonus 0.8911368436492765
maxi score, test score, baseline:  0.08413999999999987 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.08413999999999987 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.08077999999999987 0.6900000000000002 0.6900000000000002
actor:  0 policy actor:  1  step number:  59 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0797133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
maxi score, test score, baseline:  0.0797133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6069023569023576, 0.12974987974987892, 0.12974987974987892, 0.1335978835978847]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 2.3416472061263796
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.648]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.586]] [[1.374]
 [0.204]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.773]] [[0.58 ]
 [0.648]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.586]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0797133333333332 0.6900000000000002 0.6900000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  67 total reward:  0.0533333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.07635333333333319 0.6900000000000002 0.6900000000000002
probs:  [0.5960087886232319, 0.13260416100283115, 0.13260416100283115, 0.1387828893711057]
maxi score, test score, baseline:  0.07635333333333319 0.6900000000000002 0.6900000000000002
probs:  [0.5960087886232319, 0.13260416100283115, 0.13260416100283115, 0.1387828893711057]
maxi score, test score, baseline:  0.07635333333333319 0.6900000000000002 0.6900000000000002
probs:  [0.5960087886232319, 0.13260416100283115, 0.13260416100283115, 0.1387828893711057]
maxi score, test score, baseline:  0.07635333333333319 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.07635333333333319 0.6900000000000002 0.6900000000000002
probs:  [0.5960087886232319, 0.13260416100283115, 0.13260416100283115, 0.1387828893711057]
maxi score, test score, baseline:  0.07635333333333319 0.6900000000000002 0.6900000000000002
probs:  [0.5960087886232319, 0.13260416100283115, 0.13260416100283115, 0.1387828893711057]
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.5  ]
 [0.307]
 [0.335]
 [0.333]
 [0.335]
 [0.367]] [[ 0.03 ]
 [ 0.078]
 [ 0.137]
 [-0.033]
 [-0.058]
 [ 0.125]
 [-0.008]] [[-0.253]
 [-0.101]
 [-0.284]
 [-0.285]
 [-0.291]
 [-0.259]
 [-0.248]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.333
using another actor
first move QE:  -0.7390940122893065
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.538]
 [0.415]
 [0.421]
 [0.421]
 [0.438]
 [0.441]] [[0.529]
 [0.547]
 [0.442]
 [0.072]
 [0.072]
 [0.378]
 [0.208]] [[-0.223]
 [-0.113]
 [-0.253]
 [-0.309]
 [-0.309]
 [-0.241]
 [-0.266]]
maxi score, test score, baseline:  0.07635333333333319 0.6900000000000002 0.6900000000000002
actor:  1 policy actor:  1  step number:  51 total reward:  0.5200000000000005  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[-0.965]
 [-2.218]
 [-2.218]
 [-2.218]
 [-2.218]
 [-2.218]
 [-2.218]] [[0.325]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3533333333333327  reward:  1.0 rdn_beta:  0.167
siam score:  -0.72328305
first move QE:  -0.7399067984508799
siam score:  -0.7234935
actor:  1 policy actor:  1  step number:  30 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07300666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]] [[0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]]
maxi score, test score, baseline:  0.0732733333333332 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.0732733333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
siam score:  -0.72246414
maxi score, test score, baseline:  0.0732733333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.724771920417808
maxi score, test score, baseline:  0.0732733333333332 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
maxi score, test score, baseline:  0.0732733333333332 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.0732733333333332 0.6900000000000002 0.6900000000000002
actor:  0 policy actor:  1  step number:  58 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  0.07345999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
Printing some Q and Qe and total Qs values:  [[0.925]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]] [[1.61 ]
 [2.105]
 [2.105]
 [2.105]
 [2.105]
 [2.105]
 [2.105]] [[0.925]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]]
maxi score, test score, baseline:  0.07345999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
maxi score, test score, baseline:  0.07345999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07345999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
maxi score, test score, baseline:  0.07345999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.504]
 [0.504]
 [0.472]
 [0.504]
 [0.504]
 [0.504]] [[1.71 ]
 [1.519]
 [1.519]
 [1.237]
 [1.519]
 [1.519]
 [1.519]] [[0.261]
 [0.195]
 [0.195]
 [0.034]
 [0.195]
 [0.195]
 [0.195]]
maxi score, test score, baseline:  0.07345999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
maxi score, test score, baseline:  0.07345999999999987 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.07345999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
maxi score, test score, baseline:  0.07345999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
actor:  0 policy actor:  1  step number:  59 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.07609999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
maxi score, test score, baseline:  0.07609999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
maxi score, test score, baseline:  0.07609999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
Starting evaluation
line 256 mcts: sample exp_bonus 0.5699081768922455
maxi score, test score, baseline:  0.07609999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.887]
 [0.736]
 [0.76 ]
 [0.78 ]
 [0.722]
 [0.776]] [[1.263]
 [0.533]
 [1.299]
 [0.68 ]
 [0.553]
 [0.931]
 [0.754]] [[0.496]
 [0.887]
 [0.736]
 [0.76 ]
 [0.78 ]
 [0.722]
 [0.776]]
Printing some Q and Qe and total Qs values:  [[0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]] [[0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]] [[0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10509999999999989 0.6900000000000002 0.6900000000000002
probs:  [0.6220029155400633, 0.1257933027748855, 0.1257933027748855, 0.12641047891016557]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  38 total reward:  0.4999999999999999  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.10835333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10835333333333322 0.6996666666666668 0.6996666666666668
using another actor
maxi score, test score, baseline:  0.10835333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10835333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]] [[1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]] [[-0.295]
 [-0.295]
 [-0.295]
 [-0.295]
 [-0.295]
 [-0.295]
 [-0.295]]
maxi score, test score, baseline:  0.10835333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10835333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10835333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10835333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  0 policy actor:  1  step number:  44 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.333]
 [0.309]
 [0.243]
 [0.243]
 [0.179]
 [0.319]] [[0.983]
 [1.698]
 [1.033]
 [0.714]
 [0.714]
 [0.608]
 [1.436]] [[ 0.171]
 [ 0.278]
 [ 0.143]
 [ 0.025]
 [ 0.025]
 [-0.058]
 [ 0.221]]
maxi score, test score, baseline:  0.10832666666666654 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10832666666666654 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10832666666666654 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10832666666666654 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10832666666666654 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
from probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10832666666666654 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.10832666666666654 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10521999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  74 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10521999999999988 0.6996666666666668 0.6996666666666668
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10521999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10521999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10521999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10521999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.581]
 [0.517]
 [0.538]
 [0.523]
 [0.533]
 [0.547]] [[1.805]
 [1.59 ]
 [1.709]
 [1.279]
 [1.316]
 [1.514]
 [1.589]] [[ 0.093]
 [ 0.06 ]
 [ 0.035]
 [-0.087]
 [-0.09 ]
 [-0.013]
 [ 0.026]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  46 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10525999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
Printing some Q and Qe and total Qs values:  [[0.946]
 [0.945]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]] [[1.257]
 [0.087]
 [0.289]
 [0.715]
 [0.011]
 [0.244]
 [0.028]] [[0.946]
 [0.945]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
using another actor
from probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  59 total reward:  0.21333333333333315  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]] [[4.334]
 [1.918]
 [1.918]
 [1.918]
 [1.918]
 [1.918]
 [1.918]] [[ 0.334]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.562]
 [0.491]
 [0.516]
 [0.452]
 [0.513]
 [0.516]] [[0.304]
 [0.053]
 [0.162]
 [0.28 ]
 [0.203]
 [0.165]
 [0.28 ]] [[0.181]
 [0.184]
 [0.131]
 [0.175]
 [0.099]
 [0.153]
 [0.175]]
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  57 total reward:  0.4800000000000002  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  54 total reward:  0.5000000000000003  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]] [[0.23]
 [0.23]
 [0.23]
 [0.23]
 [0.23]
 [0.23]
 [0.23]] [[-0.251]
 [-0.251]
 [-0.251]
 [-0.251]
 [-0.251]
 [-0.251]
 [-0.251]]
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
siam score:  -0.725559
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  64 total reward:  0.05999999999999961  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.797]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]] [[ 0.067]
 [-0.244]
 [ 0.067]
 [ 0.067]
 [ 0.067]
 [ 0.067]
 [ 0.067]] [[0.74 ]
 [0.797]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.10016666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  60 total reward:  0.17999999999999972  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09984666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  60 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
siam score:  -0.7270821
maxi score, test score, baseline:  0.10271333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10271333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10271333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10271333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  0.167
siam score:  -0.72859323
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  0 policy actor:  0  step number:  55 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10508666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10508666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.718]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[2.007]
 [1.245]
 [2.007]
 [2.007]
 [2.007]
 [2.007]
 [2.007]] [[0.047]
 [0.108]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]]
start point for exploration sampling:  11091
actor:  0 policy actor:  0  step number:  48 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  11091
actor:  1 policy actor:  1  step number:  82 total reward:  0.3400000000000004  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  82 total reward:  0.21999999999999809  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.682]
 [0.704]
 [0.682]
 [0.682]
 [0.682]
 [0.682]] [[1.098]
 [1.098]
 [1.35 ]
 [1.098]
 [1.098]
 [1.098]
 [1.098]] [[0.682]
 [0.682]
 [0.704]
 [0.682]
 [0.682]
 [0.682]
 [0.682]]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]] [[2.083]
 [2.083]
 [2.083]
 [2.083]
 [2.083]
 [2.083]
 [2.083]] [[-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]]
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.327]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]] [[2.497]
 [1.665]
 [2.497]
 [2.497]
 [2.497]
 [2.497]
 [2.497]] [[ 0.03 ]
 [-0.052]
 [ 0.03 ]
 [ 0.03 ]
 [ 0.03 ]
 [ 0.03 ]
 [ 0.03 ]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.6466666666666668  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1076599999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
UNIT TEST: sample policy line 217 mcts : [0.02  0.388 0.143 0.122 0.02  0.102 0.204]
maxi score, test score, baseline:  0.1076599999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.1076599999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
Printing some Q and Qe and total Qs values:  [[-0.017]
 [ 0.112]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]] [[1.814]
 [1.195]
 [1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]] [[0.081]
 [0.004]
 [0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]]
maxi score, test score, baseline:  0.1076599999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  0 policy actor:  0  step number:  50 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.204]
 [ 0.116]
 [ 0.138]
 [ 0.138]
 [ 0.138]
 [ 0.138]
 [ 0.138]] [[1.161]
 [1.056]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[-0.204]
 [ 0.099]
 [ 0.038]
 [ 0.038]
 [ 0.038]
 [ 0.038]
 [ 0.038]]
maxi score, test score, baseline:  0.10756666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10756666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10756666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10756666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10756666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10756666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10756666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  0 policy actor:  0  step number:  52 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.5
from probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.002]
 [-0.008]
 [-0.016]
 [-0.027]
 [-0.008]
 [-0.014]] [[-0.639]
 [-0.438]
 [-1.556]
 [-1.95 ]
 [-1.398]
 [-1.556]
 [-0.879]] [[0.351]
 [0.387]
 [0.207]
 [0.138]
 [0.214]
 [0.207]
 [0.307]]
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [ 0.149]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[1.243]
 [0.314]
 [1.243]
 [1.243]
 [1.243]
 [1.243]
 [1.243]] [[0.256]
 [0.265]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]]
from probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  43 total reward:  0.52  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.7507911645076577
actor:  1 policy actor:  1  step number:  53 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10151333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
using another actor
maxi score, test score, baseline:  0.10151333333333322 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10151333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  55 total reward:  0.26666666666666594  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10151333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10151333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -2.8957630823445446
maxi score, test score, baseline:  0.09907333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
Printing some Q and Qe and total Qs values:  [[-0.138]
 [ 0.096]
 [ 0.151]
 [ 0.151]
 [ 0.151]
 [-0.022]
 [ 0.151]] [[1.514]
 [1.477]
 [1.94 ]
 [1.94 ]
 [1.94 ]
 [1.021]
 [1.94 ]] [[-0.403]
 [-0.175]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.369]
 [-0.042]]
maxi score, test score, baseline:  0.09623333333333321 0.6996666666666668 0.6996666666666668
actor:  1 policy actor:  1  step number:  51 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09623333333333321 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.09623333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.09623333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.78 ]
 [0.754]
 [0.757]
 [0.757]
 [0.772]
 [0.772]] [[1.425]
 [1.058]
 [1.342]
 [1.425]
 [1.425]
 [1.793]
 [1.394]] [[0.757]
 [0.78 ]
 [0.754]
 [0.757]
 [0.757]
 [0.772]
 [0.772]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.7599],
        [ 0.3942],
        [-0.0000],
        [ 0.4732],
        [ 0.0000],
        [ 0.5500],
        [-0.0000],
        [-0.0000],
        [ 0.0000],
        [-0.0345]], dtype=torch.float64)
-0.032346567066 -0.7922271565634298
-0.045026434398 0.34921486904647214
0.957165 0.957165
-0.070771701198 0.40238132400700444
-0.9108 -0.9108
-0.084359833866 0.4656548109541989
-0.9516369486359999 -0.9516369486359999
-0.95403 -0.95403
0.99 0.99
-0.09703970119800001 -0.13157073259960053
from probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  0 policy actor:  1  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]] [[0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]] [[0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]]
actor:  0 policy actor:  0  step number:  66 total reward:  0.01999999999999924  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.368899022985348
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.3  ]
 [0.218]
 [0.3  ]
 [0.3  ]
 [0.299]
 [0.3  ]] [[1.37 ]
 [2.661]
 [2.462]
 [2.661]
 [2.661]
 [1.58 ]
 [2.661]] [[0.382]
 [0.669]
 [0.585]
 [0.669]
 [0.669]
 [0.386]
 [0.669]]
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.982]
 [0.979]
 [0.95 ]
 [0.956]
 [0.959]
 [0.979]] [[0.999]
 [0.182]
 [0.788]
 [0.757]
 [1.034]
 [1.081]
 [0.281]] [[0.913]
 [0.982]
 [0.979]
 [0.95 ]
 [0.956]
 [0.959]
 [0.979]]
actor:  0 policy actor:  1  step number:  53 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.639475001159869
maxi score, test score, baseline:  0.09775333333333322 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.09775333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5933333333333333  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.603]
 [0.525]
 [0.551]
 [0.515]
 [0.573]
 [0.521]] [[6.246]
 [1.784]
 [1.518]
 [1.533]
 [1.289]
 [1.508]
 [1.37 ]] [[0.852]
 [0.206]
 [0.14 ]
 [0.15 ]
 [0.098]
 [0.151]
 [0.114]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.203]
 [0.225]
 [0.222]
 [0.236]
 [0.237]
 [0.266]] [[1.62 ]
 [1.884]
 [1.933]
 [1.8  ]
 [1.702]
 [1.645]
 [1.385]] [[-0.147]
 [-0.124]
 [-0.093]
 [-0.119]
 [-0.121]
 [-0.13 ]
 [-0.144]]
maxi score, test score, baseline:  0.09775333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.09775333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.09775333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.09775333333333322 0.6996666666666668 0.6996666666666668
using explorer policy with actor:  1
siam score:  -0.72099215
actor:  1 policy actor:  1  step number:  60 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09775333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.09775333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09775333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  60 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.09775333333333322 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.09775333333333322 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.09775333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09775333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  56 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[-0.101]
 [-0.133]
 [-0.129]
 [-0.127]
 [-0.138]
 [-0.138]
 [-0.138]] [[0.948]
 [0.575]
 [0.825]
 [0.377]
 [0.965]
 [0.965]
 [0.965]] [[-0.092]
 [-0.372]
 [-0.202]
 [-0.498]
 [-0.117]
 [-0.117]
 [-0.117]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  38 total reward:  0.54  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09825999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.09825999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
Printing some Q and Qe and total Qs values:  [[ 0.131]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]] [[1.676]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[ 0.   ]
 [-0.347]
 [-0.347]
 [-0.347]
 [-0.347]
 [-0.347]
 [-0.347]]
maxi score, test score, baseline:  0.09825999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  0 policy actor:  1  step number:  57 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.7531200461951657
first move QE:  -0.7531457393367219
actor:  1 policy actor:  1  step number:  46 total reward:  0.54  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.202]
 [0.202]
 [0.273]
 [0.202]
 [0.202]
 [0.202]] [[1.866]
 [2.317]
 [2.317]
 [1.753]
 [2.317]
 [2.317]
 [2.317]] [[0.062]
 [0.113]
 [0.113]
 [0.09 ]
 [0.113]
 [0.113]
 [0.113]]
maxi score, test score, baseline:  0.10076666666666655 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10076666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10076666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10076666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
line 256 mcts: sample exp_bonus 1.9966157115358527
maxi score, test score, baseline:  0.10076666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
maxi score, test score, baseline:  0.10076666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  44 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10076666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
siam score:  -0.72228074
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10076666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
actor:  1 policy actor:  1  step number:  75 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.10076666666666655 0.6996666666666668 0.6996666666666668
actor:  1 policy actor:  1  step number:  41 total reward:  0.56  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10076666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6215558067222625, 0.1261480644259125, 0.1261480644259125, 0.1261480644259125]
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]] [[3.619]
 [3.619]
 [3.619]
 [3.619]
 [3.619]
 [3.619]
 [3.619]] [[0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10076666666666655 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10076666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.335]
 [0.328]
 [0.338]
 [0.338]
 [0.312]
 [0.338]] [[2.539]
 [1.878]
 [2.12 ]
 [2.539]
 [2.539]
 [3.014]
 [2.539]] [[0.361]
 [0.108]
 [0.195]
 [0.361]
 [0.361]
 [0.521]
 [0.361]]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.037]
 [0.03 ]
 [0.631]
 [0.029]
 [0.029]
 [0.029]] [[-0.008]
 [ 0.001]
 [-0.002]
 [-0.031]
 [-0.001]
 [ 0.003]
 [-0.023]] [[-0.349]
 [-0.346]
 [-0.354]
 [ 0.243]
 [-0.355]
 [-0.354]
 [-0.359]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10076666666666655 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10076666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  56 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7246205
Printing some Q and Qe and total Qs values:  [[ 0.069]
 [ 0.117]
 [-0.009]
 [-0.011]
 [-0.011]
 [-0.008]
 [-0.004]] [[ 1.527]
 [ 1.408]
 [ 0.046]
 [-1.992]
 [-1.022]
 [ 1.061]
 [ 0.7  ]] [[ 0.46 ]
 [ 0.455]
 [ 0.111]
 [-0.325]
 [-0.118]
 [ 0.328]
 [ 0.253]]
first move QE:  -0.753390066201881
maxi score, test score, baseline:  0.10339333333333321 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10339333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10339333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10339333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  0 policy actor:  0  step number:  54 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  51 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10319333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
from probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
using another actor
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  32 total reward:  0.7400000000000001  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10667333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.015]
 [-0.203]
 [-0.016]
 [-0.023]
 [-0.248]
 [-0.015]] [[ 0.075]
 [-0.613]
 [-0.582]
 [-0.159]
 [-0.001]
 [-0.62 ]
 [-0.094]] [[ 0.114]
 [ 0.043]
 [-0.109]
 [ 0.075]
 [ 0.089]
 [-0.146]
 [ 0.083]]
actor:  0 policy actor:  1  step number:  59 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.1037799999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.1037799999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.1037799999999999 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.1037799999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.1037799999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.1037799999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.1037799999999999 0.6996666666666668 0.6996666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1037799999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  0 policy actor:  1  step number:  69 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10331333333333323 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10331333333333323 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10331333333333323 0.6996666666666668 0.6996666666666668
actor:  1 policy actor:  1  step number:  58 total reward:  0.4866666666666669  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10331333333333323 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.128]
 [ 0.128]
 [ 0.128]
 [ 0.128]
 [ 0.128]
 [ 0.128]] [[7.15 ]
 [1.912]
 [1.912]
 [1.912]
 [1.912]
 [1.912]
 [1.912]] [[0.766]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]]
maxi score, test score, baseline:  0.10331333333333323 0.6996666666666668 0.6996666666666668
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[5.848]
 [2.628]
 [2.628]
 [2.628]
 [2.628]
 [2.628]
 [2.628]] [[0.819]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]
 [0.205]]
maxi score, test score, baseline:  0.10331333333333323 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
from probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10331333333333323 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.10331333333333323 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10331333333333323 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10331333333333323 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10331333333333323 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10331333333333323 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[0.916]
 [0.915]
 [0.916]
 [0.916]
 [0.915]
 [0.915]
 [0.915]] [[ 0.024]
 [ 0.002]
 [-0.003]
 [ 0.375]
 [ 0.041]
 [ 0.227]
 [-0.025]] [[0.916]
 [0.915]
 [0.916]
 [0.916]
 [0.915]
 [0.915]
 [0.915]]
actor:  0 policy actor:  0  step number:  64 total reward:  0.0733333333333328  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]] [[0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]] [[-0.459]
 [-0.459]
 [-0.459]
 [-0.459]
 [-0.459]
 [-0.459]
 [-0.459]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3382948785766378
maxi score, test score, baseline:  0.10255333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
siam score:  -0.7224092
maxi score, test score, baseline:  0.10255333333333322 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  0 policy actor:  1  step number:  40 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  54 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10544666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.011]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[-0.713]
 [ 0.431]
 [-0.713]
 [-0.713]
 [-0.713]
 [-0.713]
 [-0.713]] [[-0.762]
 [-0.567]
 [-0.762]
 [-0.762]
 [-0.762]
 [-0.762]
 [-0.762]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10544666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10544666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10544666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10544666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10544666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  62 total reward:  0.17999999999999994  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  0.06666666666666599  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10544666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.10544666666666655 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10544666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  0 policy actor:  0  step number:  54 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10537999999999989 0.6996666666666668 0.6996666666666668
actor:  1 policy actor:  1  step number:  66 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  86 total reward:  0.11333333333333151  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10537999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]] [[2.146]
 [2.146]
 [2.146]
 [2.146]
 [2.146]
 [2.146]
 [2.146]] [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10285999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10285999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10285999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10285999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  37 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  69 total reward:  0.14666666666666528  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  59 total reward:  0.346666666666666  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  65 total reward:  0.11999999999999922  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.166]
 [0.101]
 [0.101]
 [0.075]
 [0.098]
 [0.147]] [[0.973]
 [1.28 ]
 [0.649]
 [0.631]
 [0.641]
 [0.435]
 [1.179]] [[-0.544]
 [-0.395]
 [-0.566]
 [-0.568]
 [-0.593]
 [-0.604]
 [-0.431]]
siam score:  -0.71689653
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
siam score:  -0.7167893
line 256 mcts: sample exp_bonus 1.4525652379599778
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  43 total reward:  0.56  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
from probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.518]
 [0.483]
 [0.477]
 [0.495]
 [0.496]
 [0.466]] [[1.587]
 [1.278]
 [1.571]
 [1.581]
 [1.554]
 [1.599]
 [1.474]] [[0.341]
 [0.155]
 [0.314]
 [0.315]
 [0.315]
 [0.346]
 [0.233]]
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  58 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10256666666666656 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
siam score:  -0.7172393
maxi score, test score, baseline:  0.09808666666666654 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.09808666666666654 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4533333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1009933333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.1009933333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.1009933333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  88 total reward:  0.03333333333333144  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1009933333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[0.936]
 [0.997]
 [0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]] [[1.25 ]
 [0.553]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]] [[0.936]
 [0.997]
 [0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.3105508948946754
maxi score, test score, baseline:  0.10385999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  46 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10669999999999986 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  57 total reward:  0.14666666666666628  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.115]
 [0.2  ]
 [0.115]
 [0.115]
 [0.127]
 [0.115]] [[1.838]
 [1.838]
 [1.694]
 [1.838]
 [1.838]
 [1.618]
 [1.838]] [[0.081]
 [0.081]
 [0.115]
 [0.081]
 [0.081]
 [0.022]
 [0.081]]
maxi score, test score, baseline:  0.10669999999999986 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10669999999999986 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10669999999999986 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]] [[1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]] [[0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]]
actor:  1 policy actor:  1  step number:  78 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.163]
 [0.185]
 [0.17 ]
 [0.168]
 [0.199]
 [0.158]
 [0.176]] [[-0.569]
 [-0.334]
 [-0.599]
 [-0.64 ]
 [-0.787]
 [-1.033]
 [-0.601]] [[-0.581]
 [-0.52 ]
 [-0.58 ]
 [-0.589]
 [-0.582]
 [-0.664]
 [-0.574]]
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.76 ]
 [0.697]
 [0.691]
 [0.7  ]
 [0.699]
 [0.699]] [[1.172]
 [0.307]
 [1.311]
 [1.274]
 [0.893]
 [0.8  ]
 [0.675]] [[0.377]
 [0.315]
 [0.42 ]
 [0.408]
 [0.353]
 [0.337]
 [0.316]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
siam score:  -0.7201865
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5466666666666671  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
actor:  0 policy actor:  0  step number:  45 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  77 total reward:  0.119999999999999  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  43 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.068]
 [ 0.071]
 [-0.032]
 [-0.026]
 [ 0.147]
 [-0.018]
 [ 0.042]] [[ 0.974]
 [ 1.209]
 [ 0.606]
 [-2.045]
 [ 1.237]
 [ 0.396]
 [ 1.949]] [[ 0.454]
 [ 0.557]
 [ 0.394]
 [-0.137]
 [ 0.593]
 [ 0.357]
 [ 0.694]]
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [-0.155]
 [-0.004]
 [-0.097]
 [-0.006]
 [-0.001]
 [-0.061]] [[ 0.002]
 [ 0.154]
 [-0.403]
 [-0.038]
 [-0.508]
 [-0.428]
 [-0.211]] [[-0.066]
 [-0.095]
 [-0.223]
 [-0.133]
 [-0.277]
 [-0.232]
 [-0.183]]
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
siam score:  -0.72870463
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.619]
 [0.514]
 [0.531]
 [0.447]
 [0.518]
 [0.453]] [[1.424]
 [0.063]
 [0.348]
 [0.431]
 [0.359]
 [0.256]
 [0.27 ]] [[ 0.165]
 [ 0.032]
 [-0.025]
 [ 0.007]
 [-0.09 ]
 [-0.036]
 [-0.098]]
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  69 total reward:  0.2799999999999986  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  71 total reward:  0.48000000000000054  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
first move QE:  -0.756090733376201
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  45 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  76 total reward:  0.19333333333333225  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10723333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.008]
 [0.005]
 [0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[-0.381]
 [-0.381]
 [-0.381]
 [-0.381]
 [-0.381]
 [-0.381]
 [-0.381]]
actor:  1 policy actor:  1  step number:  74 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[0.66]
 [0.66]
 [0.66]
 [0.66]
 [0.66]
 [0.66]
 [0.66]] [[2.095]
 [2.095]
 [2.095]
 [2.095]
 [2.095]
 [2.095]
 [2.095]] [[0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]]
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  37 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.5
from probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
first move QE:  -0.7573204836248094
maxi score, test score, baseline:  0.10437999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10181999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  0 policy actor:  1  step number:  50 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.002]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[-0.312]
 [ 1.147]
 [ 0.188]
 [ 0.188]
 [ 0.188]
 [ 0.188]
 [ 0.188]] [[-0.523]
 [-0.277]
 [-0.438]
 [-0.438]
 [-0.438]
 [-0.438]
 [-0.438]]
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.34 ]
 [0.308]
 [0.328]
 [0.308]
 [0.308]
 [0.308]] [[2.013]
 [3.286]
 [2.57 ]
 [1.864]
 [2.57 ]
 [2.57 ]
 [2.57 ]] [[-0.149]
 [ 0.1  ]
 [-0.052]
 [-0.149]
 [-0.052]
 [-0.052]
 [-0.052]]
maxi score, test score, baseline:  0.10439333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10439333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.719]
 [0.573]
 [0.115]
 [0.27 ]
 [0.277]
 [0.623]] [[0.651]
 [0.459]
 [0.532]
 [0.406]
 [0.064]
 [0.099]
 [0.012]] [[ 0.245]
 [ 0.315]
 [ 0.182]
 [-0.297]
 [-0.199]
 [-0.187]
 [ 0.145]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  47 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]] [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]]
maxi score, test score, baseline:  0.10479333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10479333333333321 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  51 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  0.167
siam score:  -0.72122467
maxi score, test score, baseline:  0.10389999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[0.286]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]] [[-0.354]
 [-0.047]
 [-0.047]
 [-0.047]
 [-0.047]
 [-0.047]
 [-0.047]]
maxi score, test score, baseline:  0.10389999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.294]
 [0.028]
 [0.066]
 [0.077]
 [0.095]
 [0.062]] [[0.831]
 [2.911]
 [0.758]
 [0.707]
 [0.628]
 [0.634]
 [0.72 ]] [[-0.229]
 [ 0.516]
 [-0.256]
 [-0.254]
 [-0.274]
 [-0.263]
 [-0.252]]
maxi score, test score, baseline:  0.10389999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  54 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10389999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10389999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.10389999999999988 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.10389999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[2.589]
 [2.672]
 [2.672]
 [2.672]
 [2.672]
 [2.672]
 [2.672]] [[0.496]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1004999999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[-2.492]
 [-2.492]
 [-2.492]
 [-2.492]
 [-2.492]
 [-2.492]
 [-2.492]] [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]]
maxi score, test score, baseline:  0.1004999999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.574]
 [0.573]
 [0.617]
 [0.574]
 [0.574]
 [0.499]] [[2.519]
 [1.971]
 [2.131]
 [2.552]
 [1.971]
 [1.971]
 [1.448]] [[0.577]
 [0.425]
 [0.472]
 [0.625]
 [0.425]
 [0.425]
 [0.223]]
maxi score, test score, baseline:  0.1004999999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  0 policy actor:  0  step number:  49 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  0.5
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.822171002188127
maxi score, test score, baseline:  0.09984666666666654 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  40 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  84 total reward:  0.206666666666665  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.09928666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.09588666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
maxi score, test score, baseline:  0.09588666666666655 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  0 policy actor:  0  step number:  55 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7540424652814864
maxi score, test score, baseline:  0.09477999999999988 0.6996666666666668 0.6996666666666668
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.064]
 [0.399]
 [0.388]
 [0.317]
 [0.393]
 [0.317]] [[1.592]
 [0.912]
 [1.839]
 [1.867]
 [1.411]
 [1.667]
 [1.411]] [[ 0.415]
 [-0.043]
 [ 0.529]
 [ 0.527]
 [ 0.329]
 [ 0.473]
 [ 0.329]]
maxi score, test score, baseline:  0.09477999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.09477999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
line 256 mcts: sample exp_bonus 2.1860330772326697
maxi score, test score, baseline:  0.09477999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
siam score:  -0.7181767
maxi score, test score, baseline:  0.09477999999999988 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.09477999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6217481216170598, 0.12608395946098006, 0.12608395946098006, 0.12608395946098006]
actor:  1 policy actor:  1  step number:  33 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7150885
siam score:  -0.71512365
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  73 total reward:  0.42666666666666697  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.09477999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
actor:  1 policy actor:  1  step number:  78 total reward:  0.19333333333333336  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.944]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]] [[1.556]
 [0.736]
 [2.771]
 [2.771]
 [2.771]
 [2.771]
 [2.771]] [[0.728]
 [0.944]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09137999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
maxi score, test score, baseline:  0.09137999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.608]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]] [[1.188]
 [1.776]
 [1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.188]] [[0.013]
 [0.18 ]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]]
maxi score, test score, baseline:  0.09137999999999988 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.09137999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
actor:  1 policy actor:  1  step number:  61 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09137999999999988 0.6996666666666668 0.6996666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09137999999999988 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
actor:  1 policy actor:  1  step number:  67 total reward:  0.4400000000000004  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  67 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.09137999999999988 0.6996666666666668 0.6996666666666668
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.692929050706981
line 256 mcts: sample exp_bonus -1.4140152595824804
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08797999999999985 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
maxi score, test score, baseline:  0.08457999999999986 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
maxi score, test score, baseline:  0.08457999999999986 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
Printing some Q and Qe and total Qs values:  [[0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]] [[1.605]
 [1.605]
 [1.605]
 [1.605]
 [1.605]
 [1.605]
 [1.605]] [[0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]]
from probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.881296550950497
actor:  0 policy actor:  0  step number:  39 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0841933333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
maxi score, test score, baseline:  0.0841933333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
actor:  1 policy actor:  1  step number:  68 total reward:  0.20666666666666655  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.588]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[1.339]
 [1.779]
 [1.339]
 [1.339]
 [1.339]
 [1.339]
 [1.339]] [[-0.055]
 [ 0.1  ]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]]
maxi score, test score, baseline:  0.0841933333333332 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.0841933333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
maxi score, test score, baseline:  0.0841933333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.0841933333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
maxi score, test score, baseline:  0.0841933333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
actor:  1 policy actor:  1  step number:  66 total reward:  0.15333333333333232  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0841933333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
maxi score, test score, baseline:  0.0841933333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
actor:  0 policy actor:  0  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0838733333333332 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.0838733333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
maxi score, test score, baseline:  0.0805133333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
using explorer policy with actor:  1
Starting evaluation
maxi score, test score, baseline:  0.0805133333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
maxi score, test score, baseline:  0.0805133333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
maxi score, test score, baseline:  0.0805133333333332 0.6996666666666668 0.6996666666666668
probs:  [0.6219194463989811, 0.12602685120033966, 0.12602685120033966, 0.12602685120033966]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.747]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.653]] [[1.444]
 [1.144]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.414]] [[0.676]
 [0.747]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.653]]
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]] [[1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]] [[0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]]
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.941]
 [0.837]
 [0.828]
 [0.837]
 [0.813]
 [0.837]] [[1.128]
 [1.013]
 [1.665]
 [0.973]
 [1.665]
 [1.083]
 [1.665]] [[0.822]
 [0.941]
 [0.837]
 [0.828]
 [0.837]
 [0.813]
 [0.837]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.31333333333333346  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.006]
 [1.005]
 [1.006]
 [1.005]
 [1.006]
 [1.005]
 [1.006]] [[0.935]
 [0.301]
 [0.494]
 [0.279]
 [0.68 ]
 [0.852]
 [0.063]] [[1.006]
 [1.005]
 [1.006]
 [1.005]
 [1.006]
 [1.005]
 [1.006]]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10689999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  0.5533333333333332  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.10387333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]] [[5.863]
 [1.858]
 [1.858]
 [1.858]
 [1.858]
 [1.858]
 [1.858]] [[ 0.605]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]]
siam score:  -0.71864563
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10387333333333322 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.201]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]] [[2.801]
 [2.169]
 [1.948]
 [1.948]
 [1.948]
 [1.948]
 [1.948]] [[0.181]
 [0.052]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  0 policy actor:  0  step number:  51 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10344666666666655 0.6990000000000001 0.6990000000000001
from probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
siam score:  -0.7091125
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.366]
 [0.245]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[0.673]
 [2.09 ]
 [3.622]
 [2.09 ]
 [2.09 ]
 [2.09 ]
 [2.09 ]] [[0.069]
 [0.38 ]
 [0.687]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]]
maxi score, test score, baseline:  0.10344666666666655 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.10344666666666655 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.10344666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.10344666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  0 policy actor:  0  step number:  53 total reward:  0.3599999999999992  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 2.4342244804506614
Printing some Q and Qe and total Qs values:  [[ 0.032]
 [-0.   ]
 [ 0.008]
 [ 0.031]
 [-0.   ]
 [ 0.021]
 [-0.   ]] [[1.945]
 [1.595]
 [1.941]
 [1.868]
 [1.595]
 [1.912]
 [1.595]] [[-0.171]
 [-0.32 ]
 [-0.196]
 [-0.198]
 [-0.32 ]
 [-0.193]
 [-0.32 ]]
maxi score, test score, baseline:  0.10333999999999989 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.10333999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  0 policy actor:  1  step number:  54 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.167
siam score:  -0.717205
maxi score, test score, baseline:  0.10313999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  39 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10313999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.85 ]
 [0.772]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]] [[0.799]
 [1.094]
 [1.165]
 [0.799]
 [0.799]
 [0.799]
 [0.799]] [[0.83 ]
 [0.85 ]
 [0.772]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9675799292865762
maxi score, test score, baseline:  0.10313999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.10313999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.10313999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.10313999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [1.012]
 [0.815]
 [0.798]
 [0.798]
 [0.798]
 [0.809]] [[ 0.148]
 [-0.18 ]
 [ 0.014]
 [ 0.31 ]
 [ 0.31 ]
 [ 0.31 ]
 [-0.001]] [[0.81 ]
 [1.012]
 [0.815]
 [0.798]
 [0.798]
 [0.798]
 [0.809]]
actor:  0 policy actor:  1  step number:  43 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10352666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10352666666666654 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.10352666666666654 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.10352666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  0 policy actor:  0  step number:  46 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10639333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[-0.168]
 [-0.23 ]
 [-0.03 ]
 [-0.032]
 [-0.136]
 [-0.165]
 [-0.032]] [[-0.085]
 [-0.025]
 [-0.981]
 [-1.074]
 [-0.309]
 [-0.45 ]
 [-0.996]] [[-0.153]
 [-0.192]
 [-0.303]
 [-0.334]
 [-0.193]
 [-0.263]
 [-0.309]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.10639333333333321 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.413]
 [0.35 ]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[3.027]
 [3.027]
 [2.725]
 [3.027]
 [3.027]
 [3.027]
 [3.027]] [[0.671]
 [0.671]
 [0.533]
 [0.671]
 [0.671]
 [0.671]
 [0.671]]
maxi score, test score, baseline:  0.10639333333333321 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.584]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[1.528]
 [0.764]
 [1.528]
 [1.528]
 [1.528]
 [1.528]
 [1.528]] [[-0.161]
 [-0.135]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]]
maxi score, test score, baseline:  0.10639333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
first move QE:  -0.7596124078570543
maxi score, test score, baseline:  0.10639333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  45 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10639333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.22 ]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]] [[1.352]
 [1.268]
 [1.352]
 [1.352]
 [1.352]
 [1.352]
 [1.352]] [[-0.2 ]
 [-0.14]
 [-0.2 ]
 [-0.2 ]
 [-0.2 ]
 [-0.2 ]
 [-0.2 ]]
maxi score, test score, baseline:  0.10639333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]] [[1.171]
 [1.39 ]
 [1.39 ]
 [1.39 ]
 [1.39 ]
 [1.39 ]
 [1.39 ]] [[0.026]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]]
maxi score, test score, baseline:  0.10639333333333321 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.10639333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
siam score:  -0.7146317
maxi score, test score, baseline:  0.10639333333333321 0.6990000000000001 0.6990000000000001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.10347333333333322 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  41 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10347333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.10347333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.477]
 [0.355]
 [0.416]
 [0.416]
 [0.381]
 [0.328]] [[1.896]
 [1.372]
 [1.913]
 [1.896]
 [1.896]
 [2.694]
 [2.661]] [[ 0.044]
 [ 0.018]
 [-0.014]
 [ 0.044]
 [ 0.044]
 [ 0.143]
 [ 0.084]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.465]
 [0.565]
 [0.465]
 [0.483]
 [0.465]
 [0.465]] [[1.478]
 [0.663]
 [1.667]
 [0.663]
 [1.197]
 [0.663]
 [0.663]] [[ 0.022]
 [-0.188]
 [ 0.08 ]
 [-0.188]
 [-0.08 ]
 [-0.188]
 [-0.188]]
maxi score, test score, baseline:  0.10347333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  0 policy actor:  1  step number:  43 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.167
siam score:  -0.7188189
actor:  1 policy actor:  1  step number:  64 total reward:  0.25999999999999934  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10404666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.10404666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.10404666666666654 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.10404666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6066666666666669  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.10404666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.10404666666666654 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  47 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.2640698330016156
maxi score, test score, baseline:  0.10404666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  0 policy actor:  1  step number:  61 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  58 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  35 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1045533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.1045533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.1045533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10197999999999989 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.10197999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  65 total reward:  0.24  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10197999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.10197999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
siam score:  -0.72460985
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  88 total reward:  0.04666666666666619  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]] [[0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]] [[0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]]
using explorer policy with actor:  1
siam score:  -0.7240889
maxi score, test score, baseline:  0.10197999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.10197999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  51 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.167
using another actor
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  66 total reward:  0.21999999999999909  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.3533333333333334  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09895333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09895333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
from probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.468]
 [0.438]
 [0.405]
 [0.413]
 [0.418]
 [0.414]] [[2.269]
 [2.406]
 [2.522]
 [2.586]
 [2.423]
 [2.526]
 [2.266]] [[0.034]
 [0.13 ]
 [0.138]
 [0.127]
 [0.08 ]
 [0.119]
 [0.029]]
maxi score, test score, baseline:  0.09620666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09620666666666655 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  47 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09620666666666655 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.09620666666666655 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.09620666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.558]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[2.702]
 [2.257]
 [2.702]
 [2.702]
 [2.702]
 [2.702]
 [2.702]] [[0.367]
 [0.351]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]]
maxi score, test score, baseline:  0.09620666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09620666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09412666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09412666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09412666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
using another actor
actor:  1 policy actor:  1  step number:  77 total reward:  0.15999999999999903  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]] [[1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]] [[0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09412666666666654 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.197]
 [ 0.   ]
 [-0.197]
 [ 0.   ]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.447]
 [0.   ]
 [0.447]
 [0.   ]] [[-0.086]
 [-0.086]
 [-0.086]
 [-0.208]
 [-0.086]
 [-0.208]
 [-0.086]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09412666666666654 0.6990000000000001 0.6990000000000001
siam score:  -0.7122128
actor:  1 policy actor:  1  step number:  35 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.167
Sims:  50 1 epoch:  172533 pick best:  False frame count:  172533
Printing some Q and Qe and total Qs values:  [[ 0.003]
 [-0.001]
 [ 0.024]
 [ 0.024]
 [ 0.024]
 [ 0.024]
 [ 0.024]] [[1.342]
 [2.198]
 [1.32 ]
 [1.32 ]
 [1.32 ]
 [1.32 ]
 [1.32 ]] [[-0.268]
 [ 0.039]
 [-0.26 ]
 [-0.26 ]
 [-0.26 ]
 [-0.26 ]
 [-0.26 ]]
maxi score, test score, baseline:  0.09412666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09412666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09412666666666654 0.6990000000000001 0.6990000000000001
start point for exploration sampling:  11091
actor:  0 policy actor:  1  step number:  53 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0968999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.0968999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.508]
 [0.409]
 [0.339]
 [0.508]
 [0.491]
 [0.417]] [[1.101]
 [1.491]
 [1.5  ]
 [0.982]
 [1.491]
 [1.572]
 [1.321]] [[ 0.032]
 [ 0.484]
 [ 0.391]
 [-0.025]
 [ 0.484]
 [ 0.52 ]
 [ 0.279]]
maxi score, test score, baseline:  0.0968999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
using explorer policy with actor:  1
from probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09689999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
first move QE:  -0.7625937647264694
Printing some Q and Qe and total Qs values:  [[0.15]
 [0.15]
 [0.15]
 [0.15]
 [0.15]
 [0.15]
 [0.15]] [[2.782]
 [2.782]
 [2.782]
 [2.782]
 [2.782]
 [2.782]
 [2.782]] [[0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]]
maxi score, test score, baseline:  0.09689999999999989 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
siam score:  -0.7164038
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.261]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[2.299]
 [2.729]
 [2.299]
 [2.299]
 [2.299]
 [2.299]
 [2.299]] [[-0.208]
 [ 0.005]
 [-0.208]
 [-0.208]
 [-0.208]
 [-0.208]
 [-0.208]]
start point for exploration sampling:  11091
actor:  1 policy actor:  1  step number:  67 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.006]
 [ 0.126]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[1.966]
 [2.103]
 [1.966]
 [1.966]
 [1.966]
 [1.966]
 [1.966]] [[0.024]
 [0.151]
 [0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]]
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]] [[3.082]
 [3.082]
 [3.082]
 [3.082]
 [3.082]
 [3.082]
 [3.082]] [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]]
Printing some Q and Qe and total Qs values:  [[ 0.108]
 [-0.006]
 [-0.018]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.941]
 [0.775]
 [1.65 ]
 [0.775]
 [0.775]
 [0.775]
 [0.775]] [[-0.138]
 [-0.287]
 [ 0.033]
 [-0.287]
 [-0.287]
 [-0.287]
 [-0.287]]
siam score:  -0.7230781
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.215]
 [0.146]
 [0.149]
 [0.141]
 [0.141]
 [0.141]] [[1.465]
 [1.868]
 [1.453]
 [1.665]
 [0.951]
 [0.951]
 [0.951]] [[-0.114]
 [ 0.096]
 [-0.122]
 [-0.036]
 [-0.323]
 [-0.323]
 [-0.323]]
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  52 total reward:  0.6466666666666671  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[1.777]
 [1.777]
 [1.777]
 [1.777]
 [1.777]
 [1.777]
 [1.777]] [[0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.02 ]
 [ 0.069]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[1.739]
 [1.739]
 [1.616]
 [1.739]
 [1.739]
 [1.739]
 [1.739]] [[0.722]
 [0.722]
 [0.791]
 [0.722]
 [0.722]
 [0.722]
 [0.722]]
Printing some Q and Qe and total Qs values:  [[-0.027]
 [ 0.073]
 [-0.01 ]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.02 ]] [[ 0.744]
 [ 1.16 ]
 [ 0.075]
 [ 0.153]
 [ 0.153]
 [ 0.153]
 [-0.021]] [[0.43 ]
 [0.514]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.342]]
siam score:  -0.7250906
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.064]] [[3.547]
 [3.547]
 [3.547]
 [3.547]
 [3.547]
 [3.547]
 [3.547]] [[-0.152]
 [-0.152]
 [-0.152]
 [-0.152]
 [-0.152]
 [-0.152]
 [-0.152]]
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]] [[1.904]
 [2.766]
 [2.766]
 [2.766]
 [2.766]
 [2.766]
 [2.766]] [[0.333]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]] [[0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.5333333333333339  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.582]
 [0.543]
 [0.539]
 [0.546]
 [0.545]
 [0.546]] [[1.911]
 [2.094]
 [1.928]
 [2.004]
 [1.688]
 [1.501]
 [0.916]] [[ 0.042]
 [ 0.113]
 [ 0.046]
 [ 0.055]
 [ 0.009]
 [-0.023]
 [-0.12 ]]
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  44 total reward:  0.6466666666666668  reward:  1.0 rdn_beta:  0.167
siam score:  -0.72786105
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09433999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.006]
 [-0.011]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.092]] [[1.988]
 [1.391]
 [1.072]
 [1.391]
 [1.391]
 [1.391]
 [1.086]] [[0.557]
 [0.41 ]
 [0.329]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.303]]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  63 total reward:  0.2933333333333332  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.266]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]] [[1.567]
 [1.525]
 [2.095]
 [2.095]
 [2.095]
 [2.095]
 [2.095]] [[0.377]
 [0.432]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9447997577495704
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[ 0.082]
 [-0.029]
 [-0.029]
 [-0.026]
 [-0.029]
 [-0.029]
 [-0.05 ]] [[1.92 ]
 [0.84 ]
 [0.84 ]
 [1.061]
 [0.84 ]
 [0.84 ]
 [0.821]] [[-0.367]
 [-0.658]
 [-0.658]
 [-0.618]
 [-0.658]
 [-0.658]
 [-0.682]]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  52 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  62 total reward:  0.1666666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
line 256 mcts: sample exp_bonus 1.0063180821033446
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.606]
 [0.132]
 [0.578]
 [0.525]
 [0.596]
 [0.575]] [[2.209]
 [1.251]
 [1.951]
 [1.511]
 [1.974]
 [1.736]
 [1.334]] [[0.574]
 [0.381]
 [0.023]
 [0.396]
 [0.42 ]
 [0.452]
 [0.364]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  60 total reward:  0.40666666666666607  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  11091
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.26 ]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]] [[2.511]
 [3.086]
 [2.511]
 [2.511]
 [2.511]
 [2.511]
 [2.511]] [[-0.266]
 [-0.063]
 [-0.266]
 [-0.266]
 [-0.266]
 [-0.266]
 [-0.266]]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  79 total reward:  0.42666666666666686  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.069]
 [0.246]
 [0.069]
 [0.069]
 [0.069]
 [0.069]] [[1.87 ]
 [1.87 ]
 [1.525]
 [1.87 ]
 [1.87 ]
 [1.87 ]
 [1.87 ]] [[-0.104]
 [-0.104]
 [ 0.015]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[ 0.05 ]
 [ 0.195]
 [ 0.051]
 [-0.001]
 [ 0.007]
 [-0.001]
 [-0.001]] [[1.786]
 [1.915]
 [1.194]
 [1.094]
 [0.147]
 [1.094]
 [1.094]] [[-0.445]
 [-0.278]
 [-0.542]
 [-0.611]
 [-0.761]
 [-0.611]
 [-0.611]]
siam score:  -0.7202832
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[-0.005]
 [-0.04 ]
 [ 0.002]
 [-0.004]
 [-0.01 ]
 [ 0.057]
 [-0.241]] [[0.159]
 [0.153]
 [0.16 ]
 [0.159]
 [0.158]
 [0.169]
 [0.12 ]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.43333333333333346  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  42 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.131]
 [0.103]
 [0.131]
 [0.131]
 [0.131]
 [0.28 ]] [[2.081]
 [2.081]
 [2.061]
 [2.081]
 [2.081]
 [2.081]
 [1.937]] [[0.509]
 [0.509]
 [0.49 ]
 [0.509]
 [0.509]
 [0.509]
 [0.534]]
line 256 mcts: sample exp_bonus 1.929037477560447
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
siam score:  -0.7238892
maxi score, test score, baseline:  0.09183333333333324 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08927333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.08927333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.723]
 [0.731]
 [0.747]
 [0.723]
 [0.723]
 [0.723]] [[0.699]
 [0.853]
 [1.035]
 [0.481]
 [0.853]
 [0.853]
 [0.853]] [[0.051]
 [0.05 ]
 [0.089]
 [0.012]
 [0.05 ]
 [0.05 ]
 [0.05 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.649884451005641
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  64 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08927333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.464]
 [0.42 ]
 [0.424]
 [0.432]
 [0.428]
 [0.424]] [[3.612]
 [2.964]
 [2.89 ]
 [2.682]
 [2.641]
 [2.586]
 [2.679]] [[0.67 ]
 [0.431]
 [0.37 ]
 [0.283]
 [0.271]
 [0.244]
 [0.281]]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.025]
 [-0.025]
 [-0.005]
 [-0.003]
 [-0.007]
 [-0.025]] [[0.856]
 [0.752]
 [0.752]
 [0.789]
 [0.788]
 [0.792]
 [0.783]] [[-0.471]
 [-0.486]
 [-0.486]
 [-0.46 ]
 [-0.458]
 [-0.461]
 [-0.481]]
line 256 mcts: sample exp_bonus 3.1620362485873468
siam score:  -0.7245555
from probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  63 total reward:  0.26666666666666583  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08927333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]] [[ 0.414]
 [-0.474]
 [-0.474]
 [-0.474]
 [-0.474]
 [-0.474]
 [-0.474]] [[0.581]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]]
Printing some Q and Qe and total Qs values:  [[ 0.025]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]] [[ 0.521]
 [-0.683]
 [-0.683]
 [-0.683]
 [-0.683]
 [-0.683]
 [-0.683]] [[0.624]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]]
Printing some Q and Qe and total Qs values:  [[-0.058]
 [-0.034]
 [-0.058]
 [-0.058]
 [-0.058]
 [-0.058]
 [-0.058]] [[0.242]
 [1.467]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[0.4  ]
 [0.652]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]]
maxi score, test score, baseline:  0.08927333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.08927333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
UNIT TEST: sample policy line 217 mcts : [0.388 0.102 0.    0.204 0.245 0.02  0.041]
actor:  0 policy actor:  0  step number:  48 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0917399999999999 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
line 256 mcts: sample exp_bonus 1.0085920617216693
maxi score, test score, baseline:  0.0917399999999999 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0917399999999999 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  55 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.438]
 [0.675]
 [0.675]
 [0.666]
 [0.657]
 [0.644]] [[1.492]
 [0.534]
 [1.103]
 [1.499]
 [1.384]
 [1.348]
 [1.32 ]] [[0.625]
 [0.438]
 [0.675]
 [0.675]
 [0.666]
 [0.657]
 [0.644]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.912]
 [0.904]
 [0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.915]] [[2.239]
 [2.279]
 [2.077]
 [2.077]
 [2.077]
 [2.077]
 [2.077]] [[0.912]
 [0.904]
 [0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.915]]
maxi score, test score, baseline:  0.0917399999999999 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.08896666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
Printing some Q and Qe and total Qs values:  [[-0.046]
 [ 0.147]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]] [[1.358]
 [1.184]
 [1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.358]] [[-0.665]
 [-0.502]
 [-0.665]
 [-0.665]
 [-0.665]
 [-0.665]
 [-0.665]]
actor:  0 policy actor:  1  step number:  40 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09207333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
maxi score, test score, baseline:  0.09207333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.7066666666666669  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6219924758253964, 0.1260025080582012, 0.1260025080582012, 0.1260025080582012]
start point for exploration sampling:  11091
actor:  1 policy actor:  1  step number:  42 total reward:  0.47333333333333305  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.1965217048228598
maxi score, test score, baseline:  0.08940666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.08940666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
line 256 mcts: sample exp_bonus 0.13082089609500383
actor:  0 policy actor:  1  step number:  62 total reward:  0.33999999999999997  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09208666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.08956666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.08956666666666656 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.78 ]
 [0.786]
 [0.769]
 [0.72 ]
 [0.754]
 [0.778]] [[0.308]
 [0.153]
 [0.703]
 [0.504]
 [0.49 ]
 [0.893]
 [0.277]] [[0.45 ]
 [0.78 ]
 [0.786]
 [0.769]
 [0.72 ]
 [0.754]
 [0.778]]
actor:  0 policy actor:  0  step number:  57 total reward:  0.3333333333333326  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.08924666666666657 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08924666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.08924666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[2.684]
 [2.733]
 [2.733]
 [2.733]
 [2.733]
 [2.733]
 [2.733]] [[0.698]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]]
using another actor
maxi score, test score, baseline:  0.08924666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
line 256 mcts: sample exp_bonus 2.4186353416410196
first move QE:  -0.761121190281234
maxi score, test score, baseline:  0.08924666666666657 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[0.828]
 [0.828]
 [0.832]
 [0.828]
 [0.828]
 [0.828]
 [0.828]] [[3.715]
 [3.715]
 [2.224]
 [3.715]
 [3.715]
 [3.715]
 [3.715]] [[0.828]
 [0.828]
 [0.832]
 [0.828]
 [0.828]
 [0.828]
 [0.828]]
actor:  0 policy actor:  0  step number:  40 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.08933999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.08933999999999989 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
actor:  0 policy actor:  1  step number:  47 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0896599999999999 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5200000000000001  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
from probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.939]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]] [[0.456]
 [1.486]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[0.866]
 [0.939]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]]
actor:  0 policy actor:  1  step number:  73 total reward:  0.29333333333333333  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09004666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09004666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09004666666666657 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
actor:  0 policy actor:  0  step number:  51 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.031]
 [-0.008]
 [-0.031]
 [-0.031]
 [-0.031]
 [ 0.007]] [[0.774]
 [0.774]
 [2.044]
 [0.774]
 [0.774]
 [0.774]
 [0.985]] [[-0.693]
 [-0.693]
 [-0.247]
 [-0.693]
 [-0.693]
 [-0.693]
 [-0.585]]
first move QE:  -0.7610540462585405
maxi score, test score, baseline:  0.0900599999999999 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
using explorer policy with actor:  0
from probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.0900599999999999 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
using another actor
from probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.0900599999999999 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.0900599999999999 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.0900599999999999 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
line 256 mcts: sample exp_bonus 0.7700209264106087
Printing some Q and Qe and total Qs values:  [[0.802]
 [0.837]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]] [[0.871]
 [1.063]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]] [[0.802]
 [0.837]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]]
maxi score, test score, baseline:  0.0900599999999999 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
actor:  0 policy actor:  1  step number:  40 total reward:  0.6066666666666668  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09119333333333324 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
actor:  0 policy actor:  0  step number:  52 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]]
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
siam score:  -0.7197198
actor:  1 policy actor:  1  step number:  56 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  44 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.3262515792598215
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
first move QE:  -0.7599213113656578
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.221]
 [0.343]
 [0.262]
 [0.221]
 [0.221]
 [0.29 ]] [[2.248]
 [1.473]
 [1.849]
 [1.173]
 [1.473]
 [1.473]
 [1.232]] [[-0.294]
 [-0.425]
 [-0.241]
 [-0.435]
 [-0.425]
 [-0.425]
 [-0.396]]
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -1.2605273723602295e-05
actor:  1 policy actor:  1  step number:  40 total reward:  0.5933333333333333  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.564]
 [0.533]
 [0.522]
 [0.512]
 [0.487]
 [0.461]] [[ 0.215]
 [ 0.06 ]
 [-0.15 ]
 [ 0.237]
 [ 0.044]
 [ 0.115]
 [ 0.133]] [[-0.183]
 [-0.136]
 [-0.203]
 [-0.148]
 [-0.19 ]
 [-0.204]
 [-0.227]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  55 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6866666666666669  reward:  1.0 rdn_beta:  0.333
from probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
line 256 mcts: sample exp_bonus 1.6565997564802797
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.09379333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
siam score:  -0.720226
actor:  0 policy actor:  0  step number:  48 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.298]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.248]] [[2.18 ]
 [1.971]
 [1.685]
 [1.985]
 [1.685]
 [1.685]
 [2.222]] [[ 0.   ]
 [-0.003]
 [-0.138]
 [-0.037]
 [-0.138]
 [-0.138]
 [ 0.031]]
maxi score, test score, baseline:  0.09392666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
siam score:  -0.72155833
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]] [[1.953]
 [1.953]
 [1.953]
 [1.953]
 [1.953]
 [1.953]
 [1.953]] [[0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]]
maxi score, test score, baseline:  0.09052666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09052666666666656 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09052666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09052666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09052666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
actor:  1 policy actor:  1  step number:  60 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  42 total reward:  0.686666666666667  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09052666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.09052666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.19782802998952392
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[ 0.295]
 [ 0.256]
 [ 0.256]
 [ 0.256]
 [ 0.256]
 [-0.055]
 [ 0.305]] [[1.551]
 [0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.556]
 [0.837]] [[-0.056]
 [-0.209]
 [-0.209]
 [-0.209]
 [-0.209]
 [-0.572]
 [-0.165]]
siam score:  -0.7223282
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
actor:  1 policy actor:  1  step number:  84 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
actor:  1 policy actor:  1  step number:  51 total reward:  0.6000000000000005  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08712666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
actor:  1 policy actor:  1  step number:  86 total reward:  0.07333333333333203  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08032666666666655 0.6990000000000001 0.6990000000000001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.08032666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
actor:  1 policy actor:  1  step number:  59 total reward:  0.4400000000000003  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08032666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.11 ]
 [0.1  ]
 [0.096]
 [0.096]
 [0.096]
 [0.124]] [[1.89 ]
 [2.088]
 [1.164]
 [1.922]
 [1.922]
 [1.922]
 [1.724]] [[-0.329]
 [-0.289]
 [-0.452]
 [-0.33 ]
 [-0.33 ]
 [-0.33 ]
 [-0.335]]
line 256 mcts: sample exp_bonus 0.14859214832230327
maxi score, test score, baseline:  0.07692666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
from probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.07692666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.774]
 [0.727]
 [0.697]
 [0.709]
 [0.697]
 [0.709]] [[-0.053]
 [ 0.487]
 [ 0.148]
 [-0.434]
 [-0.095]
 [-0.546]
 [-0.2  ]] [[0.712]
 [0.774]
 [0.727]
 [0.697]
 [0.709]
 [0.697]
 [0.709]]
maxi score, test score, baseline:  0.07352666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.07352666666666656 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7359452538490295
actor:  0 policy actor:  0  step number:  50 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  53 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07259333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.288]
 [0.17 ]
 [0.172]
 [0.17 ]
 [0.17 ]
 [0.17 ]] [[0.963]
 [1.344]
 [0.404]
 [1.237]
 [0.404]
 [0.404]
 [0.404]] [[-0.567]
 [-0.306]
 [-0.582]
 [-0.44 ]
 [-0.582]
 [-0.582]
 [-0.582]]
maxi score, test score, baseline:  0.07259333333333323 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.07259333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.601]
 [0.436]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[0.002]
 [0.455]
 [0.109]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[-0.116]
 [ 0.076]
 [-0.262]
 [ 0.076]
 [ 0.076]
 [ 0.076]
 [ 0.076]]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.037]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]] [[1.15 ]
 [2.228]
 [1.15 ]
 [1.15 ]
 [1.15 ]
 [1.15 ]
 [1.15 ]] [[-0.579]
 [-0.352]
 [-0.579]
 [-0.579]
 [-0.579]
 [-0.579]
 [-0.579]]
first move QE:  -0.7639926758814132
maxi score, test score, baseline:  0.07259333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
siam score:  -0.70838517
maxi score, test score, baseline:  0.07259333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.07259333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.07259333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.07259333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.571]
 [0.481]
 [0.447]
 [0.464]
 [0.49 ]
 [0.447]] [[0.175]
 [0.342]
 [0.284]
 [0.18 ]
 [0.2  ]
 [0.322]
 [0.178]] [[-0.139]
 [ 0.037]
 [-0.062]
 [-0.114]
 [-0.093]
 [-0.047]
 [-0.114]]
actor:  1 policy actor:  1  step number:  67 total reward:  0.2266666666666659  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  51 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07259333333333323 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.741]
 [0.713]
 [0.71 ]
 [0.706]
 [0.703]
 [0.707]] [[1.989]
 [2.251]
 [2.205]
 [2.205]
 [2.136]
 [2.131]
 [2.133]] [[0.704]
 [0.741]
 [0.713]
 [0.71 ]
 [0.706]
 [0.703]
 [0.707]]
maxi score, test score, baseline:  0.07259333333333323 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
line 256 mcts: sample exp_bonus 1.9827499122836214
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]] [[-0.444]
 [-0.444]
 [-0.444]
 [-0.444]
 [-0.444]
 [-0.444]
 [-0.444]]
maxi score, test score, baseline:  0.06919333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.06919333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[1.688]
 [1.742]
 [1.742]
 [1.742]
 [1.742]
 [1.742]
 [1.742]] [[-0.149]
 [-0.159]
 [-0.159]
 [-0.159]
 [-0.159]
 [-0.159]
 [-0.159]]
maxi score, test score, baseline:  0.06919333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
actor:  1 policy actor:  1  step number:  63 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]] [[1.644]
 [1.644]
 [1.644]
 [1.644]
 [1.644]
 [1.644]
 [1.644]] [[0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  42 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06873999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.06873999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.06873999999999988 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06873999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[-0.055]
 [-0.085]
 [-0.058]
 [-0.06 ]
 [-0.064]
 [-0.056]
 [-0.058]] [[0.741]
 [0.749]
 [0.606]
 [0.69 ]
 [0.554]
 [0.676]
 [0.655]] [[-0.287]
 [-0.312]
 [-0.379]
 [-0.326]
 [-0.421]
 [-0.331]
 [-0.347]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.996482372283935e-05
actor:  1 policy actor:  1  step number:  50 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.06873999999999988 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[0.872]
 [0.983]
 [0.954]
 [0.906]
 [0.906]
 [0.947]
 [0.967]] [[0.607]
 [0.637]
 [0.823]
 [0.616]
 [0.616]
 [0.676]
 [0.407]] [[0.872]
 [0.983]
 [0.954]
 [0.906]
 [0.906]
 [0.947]
 [0.967]]
siam score:  -0.7207336
actor:  0 policy actor:  1  step number:  87 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.133889242761735
actor:  0 policy actor:  1  step number:  43 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  42 total reward:  0.5800000000000002  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06723333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.018]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.014]] [[ 0.054]
 [-0.492]
 [ 0.054]
 [ 0.054]
 [ 0.054]
 [ 0.054]
 [-1.404]] [[-0.488]
 [-0.595]
 [-0.488]
 [-0.488]
 [-0.488]
 [-0.488]
 [-0.743]]
maxi score, test score, baseline:  0.06723333333333321 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06723333333333321 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.06723333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.06723333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.801]
 [0.717]
 [0.713]
 [0.719]
 [0.777]
 [0.718]] [[-0.094]
 [-0.537]
 [-0.52 ]
 [-0.314]
 [-0.466]
 [-0.951]
 [-0.568]] [[0.786]
 [0.801]
 [0.717]
 [0.713]
 [0.719]
 [0.777]
 [0.718]]
maxi score, test score, baseline:  0.06723333333333321 0.6990000000000001 0.6990000000000001
siam score:  -0.7180528
maxi score, test score, baseline:  0.06723333333333321 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
siam score:  -0.71916044
actor:  0 policy actor:  0  step number:  46 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06684666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
first move QE:  -0.7620441800860916
maxi score, test score, baseline:  0.06684666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.06684666666666655 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.6333333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06684666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
maxi score, test score, baseline:  0.06684666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06684666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.168]
 [0.132]
 [0.149]
 [0.156]
 [0.156]
 [0.161]] [[2.745]
 [1.738]
 [1.374]
 [1.421]
 [2.745]
 [2.745]
 [1.764]] [[0.272]
 [0.116]
 [0.02 ]
 [0.045]
 [0.272]
 [0.272]
 [0.114]]
maxi score, test score, baseline:  0.06684666666666655 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  50 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[-0.032]
 [ 0.04 ]
 [ 0.04 ]
 [ 0.04 ]
 [ 0.04 ]
 [ 0.04 ]
 [ 0.04 ]] [[0.963]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]] [[0.214]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.01 ]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[-3.925]
 [ 0.313]
 [-3.925]
 [-3.925]
 [-3.925]
 [-3.925]
 [-3.925]] [[-0.376]
 [ 0.249]
 [-0.376]
 [-0.376]
 [-0.376]
 [-0.376]
 [-0.376]]
maxi score, test score, baseline:  0.06684666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6220142194315832, 0.1259952601894722, 0.1259952601894722, 0.1259952601894722]
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.018]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]] [[-2.907]
 [-0.906]
 [-2.907]
 [-2.907]
 [-2.907]
 [-2.907]
 [-2.907]] [[0.053]
 [0.4  ]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[1.713]
 [1.713]
 [1.713]
 [1.713]
 [1.713]
 [1.713]
 [1.713]] [[-0.419]
 [-0.419]
 [-0.419]
 [-0.419]
 [-0.419]
 [-0.419]
 [-0.419]]
maxi score, test score, baseline:  0.06348666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
maxi score, test score, baseline:  0.060126666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
maxi score, test score, baseline:  0.060126666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
maxi score, test score, baseline:  0.060126666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
actor:  1 policy actor:  1  step number:  60 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.060126666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
from probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.244]
 [0.243]
 [0.265]
 [0.243]
 [0.243]
 [0.306]] [[2.279]
 [2.664]
 [2.557]
 [0.953]
 [2.557]
 [2.557]
 [1.996]] [[ 0.366]
 [ 0.462]
 [ 0.422]
 [-0.162]
 [ 0.422]
 [ 0.422]
 [ 0.248]]
maxi score, test score, baseline:  0.060126666666666544 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.060126666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.060126666666666544 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.758]
 [0.757]
 [0.758]
 [0.758]
 [0.61 ]
 [0.61 ]] [[0.461]
 [0.01 ]
 [0.15 ]
 [0.026]
 [0.027]
 [0.   ]
 [0.   ]] [[0.346]
 [0.271]
 [0.294]
 [0.273]
 [0.273]
 [0.121]
 [0.121]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
using another actor
maxi score, test score, baseline:  0.05676666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
maxi score, test score, baseline:  0.05676666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
Printing some Q and Qe and total Qs values:  [[ 0.183]
 [ 0.296]
 [-0.101]
 [ 0.093]
 [ 0.117]
 [-0.114]
 [ 0.057]] [[1.394]
 [1.086]
 [0.498]
 [1.262]
 [1.367]
 [0.49 ]
 [1.16 ]] [[ 0.037]
 [ 0.099]
 [-0.396]
 [-0.075]
 [-0.033]
 [-0.41 ]
 [-0.128]]
maxi score, test score, baseline:  0.05676666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
maxi score, test score, baseline:  0.05676666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
maxi score, test score, baseline:  0.05676666666666655 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.05676666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
maxi score, test score, baseline:  0.05340666666666655 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.855]
 [0.948]
 [0.929]
 [0.855]
 [0.855]
 [0.872]
 [0.855]] [[0.87 ]
 [1.304]
 [1.175]
 [0.87 ]
 [0.87 ]
 [1.282]
 [0.87 ]] [[0.855]
 [0.948]
 [0.929]
 [0.855]
 [0.855]
 [0.872]
 [0.855]]
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]] [[1.559]
 [1.077]
 [1.077]
 [1.077]
 [1.077]
 [1.077]
 [1.077]] [[-0.208]
 [-0.321]
 [-0.321]
 [-0.321]
 [-0.321]
 [-0.321]
 [-0.321]]
maxi score, test score, baseline:  0.05004666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
actor:  0 policy actor:  1  step number:  62 total reward:  0.2999999999999994  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.04928666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  0.167
using another actor
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.822]
 [0.687]
 [0.684]
 [0.684]
 [0.692]
 [0.696]] [[0.738]
 [0.075]
 [0.586]
 [0.569]
 [0.59 ]
 [0.755]
 [0.446]] [[0.692]
 [0.822]
 [0.687]
 [0.684]
 [0.684]
 [0.692]
 [0.696]]
maxi score, test score, baseline:  0.04928666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.464]
 [0.577]] [[2.456]
 [2.456]
 [2.456]
 [2.456]
 [2.456]
 [5.312]
 [2.456]] [[0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.723]
 [0.262]]
actor:  0 policy actor:  0  step number:  52 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  0.667
from probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
maxi score, test score, baseline:  0.04503333333333321 0.6990000000000001 0.6990000000000001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Starting evaluation
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
maxi score, test score, baseline:  0.04235333333333322 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.886]
 [0.826]
 [0.84 ]
 [0.837]
 [0.835]
 [0.859]] [[ 0.569]
 [-0.119]
 [ 0.015]
 [ 0.16 ]
 [-0.284]
 [ 0.484]
 [ 0.107]] [[0.867]
 [0.886]
 [0.826]
 [0.84 ]
 [0.837]
 [0.835]
 [0.859]]
maxi score, test score, baseline:  0.04235333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]] [[0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]]
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.828]
 [0.772]
 [0.75 ]
 [0.758]
 [0.752]
 [0.771]] [[1.104]
 [0.211]
 [0.766]
 [0.401]
 [0.229]
 [0.455]
 [0.653]] [[0.757]
 [0.828]
 [0.772]
 [0.75 ]
 [0.758]
 [0.752]
 [0.771]]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.872]
 [0.752]
 [0.826]
 [0.748]
 [0.817]
 [0.91 ]] [[0.583]
 [0.371]
 [1.299]
 [0.682]
 [1.532]
 [1.448]
 [0.448]] [[0.566]
 [0.872]
 [0.752]
 [0.826]
 [0.748]
 [0.817]
 [0.91 ]]
maxi score, test score, baseline:  0.039419999999999886 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.039619999999999884 0.6990000000000001 0.6990000000000001
probs:  [0.6219367568566585, 0.12602108104778045, 0.12602108104778045, 0.12602108104778045]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7133333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.059926666666666566 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.059926666666666566 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.059926666666666566 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  65 total reward:  0.19999999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  79 total reward:  0.053333333333333344  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.931]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]] [[1.322]
 [1.192]
 [1.192]
 [1.192]
 [1.192]
 [1.192]
 [1.192]] [[0.931]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]]
Printing some Q and Qe and total Qs values:  [[-0.011]
 [ 0.123]
 [-0.062]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[1.457]
 [1.523]
 [1.107]
 [1.457]
 [1.457]
 [1.457]
 [1.457]] [[ 0.128]
 [ 0.223]
 [-0.001]
 [ 0.128]
 [ 0.128]
 [ 0.128]
 [ 0.128]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.674]
 [0.495]
 [0.639]
 [0.615]
 [0.63 ]
 [0.655]] [[2.215]
 [1.649]
 [2.19 ]
 [1.874]
 [1.653]
 [1.917]
 [1.891]] [[-0.026]
 [ 0.283]
 [ 0.194]
 [ 0.285]
 [ 0.225]
 [ 0.283]
 [ 0.304]]
actor:  1 policy actor:  1  step number:  70 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.059926666666666566 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6600000000000003  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[-0.088]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.025]
 [-0.037]
 [-0.037]] [[1.694]
 [1.008]
 [2.395]
 [1.008]
 [0.695]
 [1.008]
 [1.008]] [[-0.355]
 [-0.419]
 [-0.187]
 [-0.419]
 [-0.459]
 [-0.419]
 [-0.419]]
maxi score, test score, baseline:  0.059926666666666566 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.059926666666666566 0.6996666666666668 0.6996666666666668
actor:  0 policy actor:  1  step number:  75 total reward:  0.2666666666666653  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.059366666666666554 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.231]
 [0.142]
 [0.142]
 [0.201]
 [0.203]
 [0.142]] [[1.468]
 [1.475]
 [1.468]
 [1.468]
 [0.986]
 [0.96 ]
 [1.468]] [[-0.254]
 [-0.164]
 [-0.254]
 [-0.254]
 [-0.276]
 [-0.278]
 [-0.254]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11091
actor:  1 policy actor:  1  step number:  96 total reward:  0.08666666666666523  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.059366666666666554 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.059366666666666554 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.059366666666666554 0.6996666666666668 0.6996666666666668
actor:  1 policy actor:  1  step number:  58 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  11091
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.059366666666666554 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.059366666666666554 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.449]
 [0.344]
 [0.345]
 [0.344]
 [0.34 ]
 [0.348]] [[-0.468]
 [-0.558]
 [-0.515]
 [-0.508]
 [-0.505]
 [-0.89 ]
 [-0.376]] [[-0.323]
 [-0.234]
 [-0.332]
 [-0.33 ]
 [-0.33 ]
 [-0.399]
 [-0.305]]
maxi score, test score, baseline:  0.059366666666666554 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
actor:  1 policy actor:  1  step number:  43 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[ 0.06 ]
 [ 0.166]
 [ 0.06 ]
 [-0.009]
 [-0.196]
 [ 0.06 ]
 [-0.03 ]] [[1.044]
 [1.442]
 [1.044]
 [1.387]
 [0.16 ]
 [1.044]
 [1.357]] [[ 0.102]
 [ 0.255]
 [ 0.102]
 [ 0.092]
 [-0.254]
 [ 0.102]
 [ 0.069]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  76 total reward:  0.28666666666666685  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.059366666666666554 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
Printing some Q and Qe and total Qs values:  [[-0.017]
 [ 0.14 ]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]] [[0.114]
 [0.801]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[-0.559]
 [-0.287]
 [-0.559]
 [-0.559]
 [-0.559]
 [-0.559]
 [-0.559]]
maxi score, test score, baseline:  0.059366666666666554 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.059366666666666554 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.059366666666666554 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.059366666666666554 0.6996666666666668 0.6996666666666668
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  50 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9013932202082012
using another actor
maxi score, test score, baseline:  0.059366666666666554 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.059366666666666554 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
Printing some Q and Qe and total Qs values:  [[ 1.5  ]
 [ 0.058]
 [-0.035]
 [ 1.5  ]
 [-0.035]
 [ 1.5  ]
 [-0.033]] [[ 0.   ]
 [-1.073]
 [-3.256]
 [ 0.   ]
 [-3.2  ]
 [ 0.   ]
 [-3.127]] [[ 2.044]
 [ 0.423]
 [-0.035]
 [ 2.044]
 [-0.026]
 [ 2.044]
 [-0.011]]
actor:  0 policy actor:  0  step number:  42 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.203]
 [0.125]
 [0.141]
 [0.141]
 [0.137]
 [0.19 ]] [[2.419]
 [2.613]
 [3.955]
 [2.419]
 [2.419]
 [4.155]
 [2.497]] [[0.124]
 [0.211]
 [0.346]
 [0.124]
 [0.124]
 [0.387]
 [0.182]]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.157]
 [0.065]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[2.014]
 [4.642]
 [1.423]
 [2.014]
 [2.014]
 [2.014]
 [2.014]] [[-0.192]
 [ 0.225]
 [-0.292]
 [-0.192]
 [-0.192]
 [-0.192]
 [-0.192]]
maxi score, test score, baseline:  0.05729999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
actor:  1 policy actor:  1  step number:  45 total reward:  0.56  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.05729999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.05729999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.05729999999999989 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.05729999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.05729999999999989 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.05729999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.05729999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
from probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.05729999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
actor:  1 policy actor:  1  step number:  66 total reward:  0.3400000000000001  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.05729999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.05729999999999989 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.756]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]] [[1.149]
 [1.081]
 [1.149]
 [1.149]
 [1.149]
 [1.149]
 [1.149]] [[0.728]
 [0.756]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.05729999999999989 0.6996666666666668 0.6996666666666668
actor:  0 policy actor:  1  step number:  56 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.154]
 [0.152]
 [0.165]
 [0.167]
 [0.152]
 [0.165]] [[2.148]
 [1.958]
 [1.903]
 [2.03 ]
 [2.092]
 [1.903]
 [2.037]] [[-0.271]
 [-0.331]
 [-0.352]
 [-0.297]
 [-0.274]
 [-0.352]
 [-0.294]]
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
actor:  1 policy actor:  1  step number:  54 total reward:  0.5133333333333339  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
Printing some Q and Qe and total Qs values:  [[-0.014]
 [ 0.011]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.034]] [[ 0.085]
 [-0.423]
 [ 0.085]
 [ 0.085]
 [ 0.085]
 [ 0.085]
 [ 0.499]] [[-0.278]
 [-0.338]
 [-0.278]
 [-0.278]
 [-0.278]
 [-0.278]
 [-0.229]]
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
from probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
maxi score, test score, baseline:  0.0579399999999999 0.6996666666666668 0.6996666666666668
probs:  [0.6218644693699824, 0.12604517687667255, 0.12604517687667255, 0.12604517687667255]
actor:  1 policy actor:  1  step number:  30 total reward:  0.7800000000000001  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  42 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.5
append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:191000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:190000
load_in_model:True
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
loaded in model from saved file
append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:191000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:190000
load_in_model:True
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
loaded in model from saved file
append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:191000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:190000
load_in_model:True
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
loaded in model from saved file
main train batch thing paused
add a thread
Adding thread: now have 3 threads
append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:191000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:190000
load_in_model:True
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
loaded in model from saved file
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[ 0.03 ]
 [-0.005]
 [ 0.03 ]
 [ 0.03 ]
 [ 0.03 ]
 [ 0.03 ]
 [ 0.03 ]] [[1.075]
 [1.309]
 [1.075]
 [1.075]
 [1.075]
 [1.075]
 [1.075]] [[-0.48 ]
 [-0.475]
 [-0.48 ]
 [-0.48 ]
 [-0.48 ]
 [-0.48 ]
 [-0.48 ]]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 5 threads
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
main train batch thing paused
add a thread
Adding thread: now have 6 threads
Printing some Q and Qe and total Qs values:  [[0.965]
 [0.981]
 [0.965]
 [0.965]
 [0.965]
 [0.965]
 [0.965]] [[1.499]
 [1.354]
 [1.499]
 [1.499]
 [1.499]
 [1.499]
 [1.499]] [[0.965]
 [0.981]
 [0.965]
 [0.965]
 [0.965]
 [0.965]
 [0.965]]
rdn beta is 0 so we're just using the maxi policy
main train batch thing paused
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.926]
 [0.923]
 [0.859]
 [0.926]
 [0.926]
 [0.926]
 [0.926]] [[1.709]
 [1.045]
 [1.415]
 [1.709]
 [1.709]
 [1.709]
 [1.709]] [[0.926]
 [0.923]
 [0.859]
 [0.926]
 [0.926]
 [0.926]
 [0.926]]
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.823]
 [0.729]
 [0.737]
 [0.748]
 [0.743]
 [0.743]] [[2.477]
 [0.905]
 [1.061]
 [1.049]
 [0.898]
 [1.389]
 [2.109]] [[0.717]
 [0.823]
 [0.729]
 [0.737]
 [0.748]
 [0.743]
 [0.743]]
rdn beta is 0 so we're just using the maxi policy
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.797]
 [0.698]
 [0.708]
 [0.691]
 [0.684]
 [0.737]] [[1.796]
 [0.607]
 [1.16 ]
 [1.475]
 [1.671]
 [1.437]
 [0.509]] [[0.711]
 [0.797]
 [0.698]
 [0.708]
 [0.691]
 [0.684]
 [0.737]]
main train batch thing paused
actor:  1 policy actor:  1  step number:  54 total reward:  0.5133333333333336  reward:  1.0 rdn_beta:  0.167
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.707]
 [0.707]
 [0.698]
 [0.668]
 [0.671]
 [0.705]] [[-0.059]
 [-0.343]
 [-0.361]
 [ 0.351]
 [ 0.   ]
 [-0.302]
 [-0.287]] [[0.38 ]
 [0.707]
 [0.707]
 [0.698]
 [0.668]
 [0.671]
 [0.705]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
main train batch thing paused
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7133333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.427057229594683
main train batch thing paused
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6807349206349207 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 5 threads
Frames:  190281 train batches done:  12670 episodes:  3811
actor:  0 policy actor:  0  step number:  39 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  56 total reward:  0.33999999999999897  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6728272727272727 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  0.31333333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  55 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.064]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[4.143]
 [1.446]
 [1.446]
 [1.446]
 [1.446]
 [1.446]
 [1.446]] [[ 0.141]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]]
deleting a thread, now have 4 threads
Frames:  190491 train batches done:  12691 episodes:  3815
Printing some Q and Qe and total Qs values:  [[-0.006]
 [ 0.274]
 [ 0.274]
 [ 0.274]
 [ 0.274]
 [ 0.274]
 [ 0.274]] [[6.941]
 [2.171]
 [2.171]
 [2.171]
 [2.171]
 [2.171]
 [2.171]] [[0.545]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 3 threads
Frames:  190599 train batches done:  12716 episodes:  3816
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  190599 train batches done:  12743 episodes:  3816
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 1 threads
Frames:  190599 train batches done:  12768 episodes:  3816
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8876816
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89157414
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.9270],
        [0.4925],
        [0.4840],
        [0.0000],
        [0.0000],
        [0.4873],
        [0.4873],
        [0.6792],
        [0.0000]], dtype=torch.float64)
0.9119154164999999 0.9119154164999999
-0.058483887065999995 0.8684841207105178
-0.04528388706599999 0.44725891184539124
-0.045546567066 0.4384341041074511
-0.03286799999999954 -0.03286799999999954
0.943965 0.943965
-0.071422513866 0.41585255921683567
-0.071422513866 0.41585255921683567
-0.09703970119800001 0.5821350678932006
-0.7722 -0.7722
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
siam score:  -0.8992612
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9020326
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90234345
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9121015
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89943343
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90164655
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9113076
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
siam score:  -0.90368867
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9036106
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.91209626
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90384245
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90533197
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9000351
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8983171
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89905745
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89972585
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9012179
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9099195
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9143044
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89956665
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89992595
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90879464
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90635186
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90857106
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90529096
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90781295
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.91000766
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9118794
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9110379
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9105382
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0508],
        [0.0000],
        [0.2915],
        [0.5193],
        [0.4592],
        [0.7694],
        [0.4093],
        [0.4592],
        [0.3616]], dtype=torch.float64)
0.99 0.99
-0.032346567066 0.0184555392384144
-0.8864031 -0.8864031
-0.04528388706599999 0.2462507285872192
-0.045026434398 0.4742776307251335
-0.045414567066 0.41380689859666087
-0.032346567066 0.737079593988345
-0.070771701198 0.3385235486011967
-0.045414567066 0.41380689859666087
-0.032346567066 0.3292718174805076
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89891744
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89917326
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
siam score:  -0.9001461
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9067803
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9082851
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9050726
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9098999
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90926516
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9051988
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90424085
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
siam score:  -0.90511096
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90881073
siam score:  -0.9101904
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.91514283
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9165029
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
siam score:  -0.9009095
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
siam score:  -0.9090355
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9051375
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9122298
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
siam score:  -0.9063266
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9030517
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90544814
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9111892
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
siam score:  -0.9084769
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9102476
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90717757
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9093497
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90663165
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9009294
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90104175
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90810114
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9096604
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9027292
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90080583
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90286183
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9002223
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9021501
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90359926
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90387446
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9081851
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.907932
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.91709685
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9192243
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9150583
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24404988765777577
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90844244
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.91115427
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90595645
maxi score, test score, baseline:  0.6580710144927536 0.7146666666666668 0.7146666666666668
probs:  [0.25, 0.25, 0.25, 0.25]
append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:211000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:210000
load_in_model:True
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
loaded in model from saved file
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Starting evaluation
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]]
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.416]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[-0.237]
 [-0.235]
 [-0.237]
 [-0.237]
 [-0.237]
 [-0.237]
 [-0.237]] [[0.397]
 [0.416]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.524]
 [0.43 ]
 [0.451]
 [0.43 ]
 [0.43 ]
 [0.43 ]] [[ 0.   ]
 [-0.24 ]
 [ 0.   ]
 [-0.357]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.319]
 [-0.305]
 [-0.319]
 [-0.417]
 [-0.319]
 [-0.319]
 [-0.319]]
line 256 mcts: sample exp_bonus -0.23620384087739857
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]] [[-0.235]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]] [[0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.005199996791712852
Printing some Q and Qe and total Qs values:  [[0.926]
 [0.916]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]] [[-0.241]
 [-0.786]
 [-0.241]
 [-0.241]
 [-0.241]
 [-0.241]
 [-0.241]] [[0.926]
 [0.916]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]]
main train batch thing paused
add a thread
Adding thread: now have 5 threads
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 6 threads
Printing some Q and Qe and total Qs values:  [[0.927]
 [0.927]
 [0.927]
 [0.927]
 [0.927]
 [0.927]
 [0.927]] [[-0.223]
 [-0.223]
 [-0.223]
 [-0.223]
 [-0.223]
 [-0.223]
 [-0.223]] [[0.927]
 [0.927]
 [0.927]
 [0.927]
 [0.927]
 [0.927]
 [0.927]]
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.989]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]] [[-0.241]
 [-0.351]
 [-0.241]
 [-0.241]
 [-0.241]
 [-0.241]
 [-0.241]] [[0.826]
 [0.989]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]]
main train batch thing paused
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.714]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[-0.232]
 [-0.235]
 [-0.232]
 [-0.232]
 [-0.232]
 [-0.232]
 [-0.232]] [[0.699]
 [0.714]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]]
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[-0.241]
 [-0.241]
 [-0.241]
 [-0.241]
 [-0.241]
 [-0.241]
 [-0.241]] [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
line 256 mcts: sample exp_bonus -0.4636299571658404
main train batch thing paused
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.002899918981403243
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.843]
 [0.746]
 [0.746]
 [0.746]
 [0.722]
 [0.707]] [[-0.345]
 [-0.659]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.235]
 [-0.238]] [[0.654]
 [0.843]
 [0.746]
 [0.746]
 [0.746]
 [0.722]
 [0.707]]
Printing some Q and Qe and total Qs values:  [[0.964]
 [0.982]
 [0.969]
 [0.96 ]
 [0.964]
 [0.965]
 [0.973]] [[-0.461]
 [-0.606]
 [-0.508]
 [-0.236]
 [-0.459]
 [-0.456]
 [-0.356]] [[0.964]
 [0.982]
 [0.969]
 [0.96 ]
 [0.964]
 [0.965]
 [0.973]]
main train batch thing paused
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.076]
 [0.072]
 [0.076]
 [0.073]
 [0.071]
 [0.07 ]] [[-0.009]
 [-0.012]
 [-0.009]
 [-0.009]
 [-0.01 ]
 [-0.009]
 [-0.01 ]] [[0.069]
 [0.076]
 [0.072]
 [0.076]
 [0.073]
 [0.071]
 [0.07 ]]
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  0.7000000000000001  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
main train batch thing paused
main train batch thing paused
line 256 mcts: sample exp_bonus -0.44299562965793854
Printing some Q and Qe and total Qs values:  [[0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]] [[-0.229]
 [-0.229]
 [-0.229]
 [-0.229]
 [-0.229]
 [-0.229]
 [-0.229]] [[-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]]
main train batch thing paused
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.766]
 [0.689]
 [0.686]
 [0.694]
 [0.69 ]
 [0.696]] [[-0.381]
 [-0.41 ]
 [-0.49 ]
 [-0.349]
 [-0.413]
 [-0.399]
 [-0.404]] [[-0.108]
 [ 0.094]
 [-0.037]
 [ 0.054]
 [ 0.02 ]
 [ 0.025]
 [ 0.027]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3733333333333334  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.334]
 [0.241]
 [0.266]
 [0.266]
 [0.266]
 [0.333]] [[-0.24 ]
 [-0.394]
 [-0.238]
 [-0.243]
 [-0.243]
 [-0.243]
 [-0.24 ]] [[0.25 ]
 [0.334]
 [0.241]
 [0.266]
 [0.266]
 [0.266]
 [0.333]]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.401]
 [0.321]
 [0.348]
 [0.361]
 [0.38 ]
 [0.392]] [[-0.731]
 [-0.456]
 [-0.227]
 [-0.346]
 [-0.544]
 [-0.459]
 [-0.593]] [[-0.506]
 [-0.379]
 [-0.344]
 [-0.376]
 [-0.462]
 [-0.401]
 [-0.456]]
main train batch thing paused
from probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 9.999605276498187e-05
main train batch thing paused
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.831]
 [0.775]
 [0.775]
 [0.775]
 [0.769]
 [0.823]] [[-0.46 ]
 [-0.93 ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.23 ]
 [-0.236]] [[0.78 ]
 [0.831]
 [0.775]
 [0.775]
 [0.775]
 [0.769]
 [0.823]]
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]] [[-0.233]
 [-0.233]
 [-0.233]
 [-0.233]
 [-0.233]
 [-0.233]
 [-0.233]] [[0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]]
Printing some Q and Qe and total Qs values:  [[ 0.362]
 [-0.189]
 [ 0.362]
 [ 0.362]
 [ 0.362]
 [-0.19 ]
 [-0.191]] [[ 0.   ]
 [-0.234]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.233]
 [-0.234]] [[ 0.47 ]
 [-0.198]
 [ 0.47 ]
 [ 0.47 ]
 [ 0.47 ]
 [-0.198]
 [-0.2  ]]
main train batch thing paused
actor:  0 policy actor:  0  step number:  40 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]] [[-0.232]
 [-0.232]
 [-0.232]
 [-0.232]
 [-0.232]
 [-0.232]
 [-0.232]] [[-0.255]
 [-0.255]
 [-0.255]
 [-0.255]
 [-0.255]
 [-0.255]
 [-0.255]]
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.488]
 [0.446]
 [0.603]
 [0.603]
 [0.603]
 [0.603]] [[-0.345]
 [-0.348]
 [-0.232]
 [-0.233]
 [-0.233]
 [-0.233]
 [-0.233]] [[0.693]
 [0.488]
 [0.446]
 [0.603]
 [0.603]
 [0.603]
 [0.603]]
main train batch thing paused
using explorer policy with actor:  1
main train batch thing paused
using explorer policy with actor:  1
main train batch thing paused
main train batch thing paused
append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:211000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:210000
load_in_model:True
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
loaded in model from saved file
main train batch thing paused
add a thread
from probs:  [0.25, 0.25, 0.25, 0.25]
Adding thread: now have 3 threads
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]]
line 256 mcts: sample exp_bonus -1.5842600863953562
main train batch thing paused
add a thread
Adding thread: now have 5 threads
main train batch thing paused
add a thread
Adding thread: now have 6 threads
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.717]
 [0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]] [[-0.23 ]
 [-0.503]
 [-0.236]
 [-0.236]
 [-0.236]
 [-0.236]
 [-0.236]] [[0.646]
 [0.717]
 [0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]]
main train batch thing paused
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.821]
 [0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.814]] [[-0.48 ]
 [-0.349]
 [-0.245]
 [-0.245]
 [-0.245]
 [-0.245]
 [-0.347]] [[0.79 ]
 [0.821]
 [0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.814]]
main train batch thing paused
rdn beta is 0 so we're just using the maxi policy
main train batch thing paused
line 256 mcts: sample exp_bonus -0.23758881121268938
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7466666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7133333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7133333333333334  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
actor:  0 policy actor:  0  step number:  56 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
main train batch thing paused
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5951724637681158 0.728 0.728
maxi score, test score, baseline:  0.5951724637681158 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.2782364273336766
deleting a thread, now have 5 threads
Frames:  210310 train batches done:  14005 episodes:  4217
maxi score, test score, baseline:  0.5951724637681158 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.583]
 [0.408]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[0.176]
 [0.022]
 [0.742]
 [0.065]
 [0.065]
 [0.065]
 [0.065]] [[-0.13 ]
 [ 0.006]
 [ 0.31 ]
 [ 0.017]
 [ 0.017]
 [ 0.017]
 [ 0.017]]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.392]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]] [[0.735]
 [0.911]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[0.542]
 [0.667]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5933333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5951000000000001 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5951000000000001 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  46 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 4 threads
Frames:  210527 train batches done:  14028 episodes:  4222
siam score:  -0.6063637134547417
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 3 threads
Frames:  210527 train batches done:  14054 episodes:  4222
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.2791],
        [0.0000],
        [0.0000],
        [0.0892],
        [0.5311],
        [0.0000],
        [0.0000],
        [0.3600],
        [0.0000],
        [0.0000]], dtype=torch.float64)
-0.032346567066 0.24671131818984401
-0.924 -0.924
-0.96402702 -0.96402702
-0.057834381198 0.031359915021453875
-0.09703970119800001 0.4341013292380118
-0.9289668828 -0.9289668828
-0.9702 -0.9702
-0.032346567066 0.3276290123200875
-0.9503999999999999 -0.9503999999999999
-0.16728610139999947 -0.16728610139999947
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  210527 train batches done:  14084 episodes:  4222
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 1 threads
Frames:  210527 train batches done:  14118 episodes:  4222
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6507094
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.66536486
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6691567
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6741041
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.67112046
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6725134
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.67004067
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.66945547
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6695562
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.68006116
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.67956513
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6811576
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.67146826
siam score:  -0.67223525
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.67404956
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6735405
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6717641
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
siam score:  -0.67542356
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.6806403109912618
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6818783
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.68045855
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6837213
first move QE:  -0.6806403109912618
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
siam score:  -0.6887999
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.68644786
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6855904
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6744847
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6853559
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6902131
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6908747
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.68688494
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
maxi score, test score, baseline:  0.5875666666666667 0.728 0.728
append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:211000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:210000
load_in_model:True
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
loaded in model from saved file
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.418]
 [0.388]
 [0.363]
 [0.321]
 [0.343]
 [0.459]] [[-0.509]
 [-0.567]
 [-0.469]
 [-0.474]
 [-0.474]
 [-0.537]
 [-0.563]] [[-0.495]
 [-0.463]
 [-0.428]
 [-0.456]
 [-0.498]
 [-0.518]
 [-0.42 ]]
from probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.494]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.548]] [[-0.241]
 [-0.358]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.399]] [[0.406]
 [0.494]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.548]]
Training Flag: False
Self play flag: True
add more workers flag:  True
expV_train_flag:  False
expV_train_start_flag:  255000
main train batch thing paused
add a thread
Adding thread: now have 5 threads
Printing some Q and Qe and total Qs values:  [[0.992]
 [0.995]
 [0.999]
 [0.999]
 [0.999]
 [0.999]
 [0.999]] [[-0.342]
 [-0.358]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]
 [-0.244]] [[0.992]
 [0.995]
 [0.999]
 [0.999]
 [0.999]
 [0.999]
 [0.999]]
main train batch thing paused
add a thread
Adding thread: now have 6 threads
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.95 ]
 [0.944]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]] [[-0.233]
 [-0.233]
 [-0.233]
 [-0.233]
 [-0.233]
 [-0.233]
 [-0.233]] [[0.95 ]
 [0.944]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]]
rdn beta is 0 so we're just using the maxi policy
main train batch thing paused
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.963]
 [0.973]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]] [[-0.239]
 [-0.473]
 [-0.239]
 [-0.239]
 [-0.239]
 [-0.239]
 [-0.239]] [[0.963]
 [0.973]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]]
actor:  0 policy actor:  0  step number:  39 total reward:  0.52  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
Printing some Q and Qe and total Qs values:  [[0.956]
 [0.996]
 [0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]] [[-0.235]
 [-0.347]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]
 [-0.235]] [[0.956]
 [0.996]
 [0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7666666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7533333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7666666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7666666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.7666666666666667  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7600000000000001  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
actor:  0 policy actor:  0  step number:  26 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7133333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7133333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  0.667
main train batch thing paused
Training Flag: False
Self play flag: True
add more workers flag:  True
expV_train_flag:  False
expV_train_start_flag:  255000
main train batch thing paused
maxi score, test score, baseline:  0.48398888888888886 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.0008319799062864687
rdn beta is 0 so we're just using the maxi policy
deleting a thread, now have 5 threads
Frames:  210355 train batches done:  24574.0 episodes:  4212
maxi score, test score, baseline:  0.48398888888888886 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.48398888888888886 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.393]
 [0.387]
 [0.401]
 [0.376]
 [0.375]
 [0.375]] [[1.497]
 [1.5  ]
 [5.132]
 [1.493]
 [1.492]
 [1.494]
 [1.497]] [[-0.043]
 [-0.03 ]
 [ 0.728]
 [-0.027]
 [-0.042]
 [-0.043]
 [-0.042]]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.372]
 [0.409]
 [0.372]
 [0.372]
 [0.372]
 [0.378]] [[5.099]
 [5.099]
 [1.468]
 [5.099]
 [5.099]
 [5.099]
 [1.495]] [[ 1.134]
 [ 1.134]
 [-0.037]
 [ 1.134]
 [ 1.134]
 [ 1.134]
 [-0.06 ]]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.0000],
        [ 0.4575],
        [-0.1594],
        [ 0.6502],
        [ 0.2661],
        [ 0.4734],
        [ 0.2928],
        [ 0.3125],
        [ 0.0000],
        [ 0.2178]], dtype=torch.float64)
0.92779335 0.92779335
-0.09703970119800001 0.3604843250314296
-0.032346567066 -0.19179521539923428
-0.032346567066 0.6178947538206597
-0.032346567066 0.23378710302059377
-0.09703970119800001 0.3763753520492039
-0.032346567066 0.2604714230950912
-0.09703970119800001 0.21545743656735253
-0.30496371470399913 -0.30496371470399913
-0.032346567066 0.18543251017445436
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
deleting a thread, now have 4 threads
Frames:  210606 train batches done:  24596.0 episodes:  4217
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 3 threads
Frames:  210606 train batches done:  24624.0 episodes:  4217
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  210606 train batches done:  24652.0 episodes:  4217
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
deleting a thread, now have 1 threads
Frames:  210606 train batches done:  24682.0 episodes:  4217
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7164873
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.395]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]] [[0.072]
 [0.004]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]] [[-0.308]
 [-0.276]
 [-0.308]
 [-0.308]
 [-0.308]
 [-0.308]
 [-0.308]]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73713654
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4246333333333333 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.05151999411898219
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.14733838857674117
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7399118
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
siam score:  -0.7406819
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.1096622759699448
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.003699997660462708
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.155]
 [0.254]
 [0.266]
 [0.313]
 [0.294]
 [0.327]] [[-0.232]
 [-0.337]
 [-0.252]
 [-0.275]
 [-0.227]
 [-0.247]
 [-0.326]] [[-0.297]
 [-0.403]
 [-0.275]
 [-0.271]
 [-0.209]
 [-0.234]
 [-0.227]]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.3191123456790123 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.33200476190476186 0.6546666666666667 0.6546666666666667
siam score:  -0.7539968
maxi score, test score, baseline:  0.33200476190476186 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33200476190476186 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33200476190476186 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.33200476190476186 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.577]
 [0.56 ]
 [0.517]
 [0.531]
 [0.517]
 [0.585]] [[ 0.065]
 [-0.09 ]
 [-0.006]
 [ 0.173]
 [-0.002]
 [ 0.173]
 [-0.   ]] [[-0.095]
 [-0.191]
 [-0.152]
 [-0.076]
 [-0.178]
 [-0.076]
 [-0.123]]
maxi score, test score, baseline:  0.33200476190476186 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33200476190476186 0.6546666666666667 0.6546666666666667
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.3  ]
 [0.3  ]
 [0.35 ]
 [0.363]
 [0.3  ]
 [0.323]] [[-0.023]
 [-0.023]
 [-0.023]
 [-0.338]
 [-0.355]
 [-0.023]
 [-0.255]] [[-0.308]
 [-0.308]
 [-0.308]
 [-0.362]
 [-0.355]
 [-0.308]
 [-0.362]]
maxi score, test score, baseline:  0.33200476190476186 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33200476190476186 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.33200476190476186 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33200476190476186 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.33200476190476186 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.95 ]
 [0.968]
 [0.887]
 [0.852]
 [0.921]
 [0.942]] [[-0.001]
 [-0.038]
 [ 0.096]
 [-0.002]
 [-0.011]
 [-0.   ]
 [ 0.706]] [[0.402]
 [0.95 ]
 [0.968]
 [0.887]
 [0.852]
 [0.921]
 [0.942]]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.777763
first move QE:  -0.3504697295660304
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]] [[-0.164]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.289]
 [0.272]
 [0.376]
 [0.374]
 [0.373]
 [0.356]] [[-0.019]
 [ 0.023]
 [ 0.101]
 [ 0.005]
 [ 0.013]
 [-0.01 ]
 [-0.057]] [[-0.015]
 [-0.006]
 [-0.01 ]
 [ 0.078]
 [ 0.077]
 [ 0.072]
 [ 0.047]]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]] [[0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.28607701149425285 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.114]
 [0.567]
 [0.586]
 [0.588]
 [0.569]
 [0.584]] [[-0.237]
 [-0.074]
 [-0.144]
 [-0.213]
 [-0.323]
 [-0.422]
 [-0.252]] [[-0.166]
 [-0.538]
 [-0.121]
 [-0.136]
 [-0.189]
 [-0.258]
 [-0.158]]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78874034
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]] [[-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]] [[-0.275]
 [-0.275]
 [-0.275]
 [-0.275]
 [-0.275]
 [-0.275]
 [-0.275]]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.378]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]] [[-0.065]
 [ 0.001]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]] [[-0.33 ]
 [-0.346]
 [-0.33 ]
 [-0.33 ]
 [-0.33 ]
 [-0.33 ]
 [-0.33 ]]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.167]
 [0.662]
 [0.677]
 [0.675]
 [0.684]
 [0.681]] [[-0.097]
 [-0.074]
 [-0.209]
 [-0.24 ]
 [-0.304]
 [-0.32 ]
 [-0.268]] [[-0.04 ]
 [-0.468]
 [-0.019]
 [-0.013]
 [-0.037]
 [-0.033]
 [-0.019]]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
siam score:  -0.79787064
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.3204306444636963
line 256 mcts: sample exp_bonus 0.1666571794974363
maxi score, test score, baseline:  0.2432111111111111 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.39208334822492596
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.16376538630002566
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.538]
 [0.538]
 [0.572]
 [0.538]
 [0.538]
 [0.415]] [[-0.072]
 [-0.131]
 [-0.131]
 [-0.231]
 [-0.131]
 [-0.131]
 [-0.109]] [[-0.001]
 [-0.038]
 [-0.038]
 [-0.071]
 [-0.038]
 [-0.038]
 [-0.146]]
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]] [[0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[-0.287]
 [-0.287]
 [-0.287]
 [-0.287]
 [-0.287]
 [-0.287]
 [-0.287]]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8096855
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81269336
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.594]
 [0.59 ]
 [0.596]] [[-0.34 ]
 [-0.34 ]
 [-0.34 ]
 [-0.34 ]
 [-0.162]
 [-0.145]
 [-0.34 ]] [[-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.022]
 [-0.023]
 [-0.049]]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
line 256 mcts: sample exp_bonus -0.4500178444151337
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.86 ]
 [0.123]
 [0.86 ]
 [0.881]
 [0.881]
 [0.89 ]
 [0.871]] [[-0.39 ]
 [-0.095]
 [-0.548]
 [-0.216]
 [-0.269]
 [-0.297]
 [-0.372]] [[ 0.37 ]
 [-0.318]
 [ 0.344]
 [ 0.421]
 [ 0.411]
 [ 0.415]
 [ 0.384]]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.39371230739187885
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
siam score:  -0.84063506
first move QE:  -0.39918182648379696
maxi score, test score, baseline:  0.20311075268817202 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  51 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2067666666666667 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.83898133
maxi score, test score, baseline:  0.2067666666666667 0.6546666666666667 0.6546666666666667
first move QE:  -0.4113125299838801
siam score:  -0.8360173
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.06 ]
 [-0.02 ]
 [-0.06 ]
 [ 0.087]
 [-0.06 ]
 [-0.06 ]
 [ 0.068]] [[0.051]
 [0.199]
 [0.051]
 [0.073]
 [0.051]
 [0.051]
 [0.336]] [[0.004]
 [0.142]
 [0.004]
 [0.165]
 [0.004]
 [0.004]
 [0.322]]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0133],
        [ 0.0727],
        [-0.0822],
        [-0.0824],
        [ 0.0000],
        [ 0.1724],
        [ 0.1972],
        [-0.7083],
        [ 0.0000],
        [ 0.0000]], dtype=torch.float64)
-0.032346567066 -0.04566708061203329
-0.09703970119800001 -0.024348764261105507
-0.032346567066 -0.11453605238588918
-0.032346567066 -0.11475437569383909
-0.50900731002 -0.50900731002
-0.032346567066 0.14003682630285724
-0.032346567066 0.16485570158675683
-0.032346567066 -0.7406508361525368
-0.08500799999999865 -0.08500799999999865
-0.869217810978 -0.869217810978
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]] [[-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84220594
Printing some Q and Qe and total Qs values:  [[0.857]
 [0.921]
 [0.881]
 [0.782]
 [0.906]
 [0.827]
 [0.841]] [[ 0.126]
 [-0.537]
 [ 0.01 ]
 [ 0.045]
 [-0.382]
 [ 0.147]
 [ 0.17 ]] [[0.857]
 [0.921]
 [0.881]
 [0.782]
 [0.906]
 [0.827]
 [0.841]]
maxi score, test score, baseline:  0.17020101010101013 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  56 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
Printing some Q and Qe and total Qs values:  [[0.92 ]
 [0.923]
 [0.93 ]
 [0.906]
 [0.929]
 [0.938]
 [0.902]] [[-0.246]
 [-0.303]
 [-0.341]
 [-0.541]
 [-0.433]
 [-0.489]
 [-0.381]] [[0.319]
 [0.313]
 [0.314]
 [0.257]
 [0.298]
 [0.297]
 [0.28 ]]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.4186641105377282
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
siam score:  -0.84514165
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
Printing some Q and Qe and total Qs values:  [[0.893]
 [0.927]
 [0.893]
 [0.893]
 [0.893]
 [0.893]
 [0.893]] [[ 0.201]
 [-0.124]
 [ 0.201]
 [ 0.201]
 [ 0.201]
 [ 0.201]
 [ 0.201]] [[0.893]
 [0.927]
 [0.893]
 [0.893]
 [0.893]
 [0.893]
 [0.893]]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.1759823529411765 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  53 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1823857142857143 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1823857142857143 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1823857142857143 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.1823857142857143 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1823857142857143 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1823857142857143 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1823857142857143 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.1823857142857143 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1823857142857143 0.6546666666666667 0.6546666666666667
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1823857142857143 0.6546666666666667 0.6546666666666667
maxi score, test score, baseline:  0.1823857142857143 0.6546666666666667 0.6546666666666667
actor:  0 policy actor:  0  step number:  69 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  0.5
Sims:  50 1 epoch:  214547 pick best:  False frame count:  214547
line 256 mcts: sample exp_bonus -0.2912562747338663
