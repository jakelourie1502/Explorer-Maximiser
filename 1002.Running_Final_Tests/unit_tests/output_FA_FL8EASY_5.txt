append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'H' 'F' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'F' 'H' 'F' 'H' 'H' 'F']
 ['H' 'F' 'F' 'H' 'F' 'F' 'F' 'H']
 ['H' 'F' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'H' 'H' 'F' 'H' 'F']
 ['H' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:205000
exp_gamma:0.95
atari_env:False
memory_size:60
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'H' 'F' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'F' 'H' 'F' 'H' 'H' 'F']
 ['H' 'F' 'F' 'H' 'F' 'F' 'F' 'H']
 ['H' 'F' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'H' 'H' 'F' 'H' 'F']
 ['H' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]]
from probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.  0.2 0.4]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
siam score:  0.006460243963043798
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  1254 train batches done:  35 episodes:  199
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 1 threads
Frames:  1254 train batches done:  80 episodes:  199
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.08222738368809435
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.37949675
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [ 0.003]] [[-1.5]
 [-1.5]
 [-1.5]
 [ 0. ]
 [-1.5]] [[-0.015]
 [-0.014]
 [-0.014]
 [ 0.237]
 [-0.011]]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 1 threads
Frames:  1654 train batches done:  157 episodes:  263
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.39867124
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.24181724563775703
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.3163004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.32453853
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.37045926
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.38654605
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.22109145499201377
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.378461
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.049]
 [-0.047]
 [-0.046]
 [-0.026]
 [-0.053]] [[0.021]
 [0.022]
 [0.022]
 [0.029]
 [0.02 ]]
siam score:  -0.4046057
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.4404197
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.4999857
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.423202
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.096]
 [-0.186]
 [-0.143]
 [-0.101]
 [-0.075]] [[0.06 ]
 [0.   ]
 [0.029]
 [0.057]
 [0.074]]
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.46495414
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [-0.287]
 [ 0.   ]
 [-0.41 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.49194327
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.49211562
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.10836355218310345
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.4514768
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.105]
 [ 0.096]
 [ 0.103]
 [ 0.03 ]
 [ 0.019]] [[0.082]
 [0.149]
 [0.151]
 [0.127]
 [0.123]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.3845254
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.093]
 [-0.132]
 [-0.001]
 [-0.005]
 [-0.138]] [[0.337]
 [0.312]
 [0.399]
 [0.396]
 [0.308]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.722]
 [-0.214]
 [-0.022]
 [-0.234]
 [-0.286]] [[0.   ]
 [0.169]
 [0.233]
 [0.162]
 [0.145]]
siam score:  -0.41246742
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.43126845
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.38898677
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.09845865542585866
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.013]
 [-0.007]] [[0.001]
 [0.001]
 [0.001]
 [0.004]
 [0.   ]]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.077]] [[0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.44314095
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
siam score:  -0.3950985
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.38408113
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.39959484
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.085010956785875
deleting a thread, now have 2 threads
Frames:  10973 train batches done:  1281 episodes:  1876
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.431118
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.44139045
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 1 threads
Frames:  11349 train batches done:  1329 episodes:  1930
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.46455738
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.47071686
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.4772917
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.009]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.0007065119230747223
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.4824221
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.49260142
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.   ]
 [-0.005]
 [-0.004]
 [-0.005]
 [-0.005]] [[0.002]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.49996993
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.007]
 [0.004]
 [0.002]
 [0.002]] [[0.006]
 [0.005]
 [0.003]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.4997702
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.014]
 [0.009]
 [0.014]
 [0.014]] [[0.004]
 [0.005]
 [0.003]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.004]
 [0.006]
 [0.006]
 [0.006]] [[0.003]
 [0.002]
 [0.003]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [-0.003]
 [ 0.   ]
 [ 0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.48710892
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.43019775
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[0.001]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  -0.07149609266529623
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.4262891
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.333 0.167 0.167 0.125 0.208]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
first move QE:  -0.07025312261263432
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 1.8597051637992043e-06
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.47142494
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.011]
 [0.008]
 [0.008]
 [0.011]] [[0.001]
 [0.002]
 [0.001]
 [0.001]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.0008805540335224941
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.42825326
first move QE:  -0.06283165283465048
first move QE:  -0.06250471612463915
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.46200264
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06139481480251695
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.001]
 [-0.007]
 [-0.007]
 [-0.007]] [[0.   ]
 [0.003]
 [0.   ]
 [0.   ]
 [0.   ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.002]
 [-0.007]
 [-0.007]
 [-0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.46415535
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.05893761158998744
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.05819234707458428
siam score:  -0.4983036
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.005]
 [-0.005]
 [-0.006]
 [-0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.006]
 [0.006]
 [0.007]
 [0.007]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
using explorer policy with actor:  1
first move QE:  -0.05691288720388516
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.05628955773618213
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.51388764
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5199665
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.01 ]
 [0.01 ]
 [0.004]
 [0.01 ]] [[0.005]
 [0.005]
 [0.005]
 [0.002]
 [0.005]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.125 0.167 0.083 0.458 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 1 threads
Frames:  17508 train batches done:  2052 episodes:  2938
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.45849892
siam score:  -0.4606906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.4728986
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.052647067373550474
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.002]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]]
siam score:  -0.544736
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.49137124
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.44563812
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.39773017
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.00562538428902626
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.014]
 [-0.013]
 [-0.013]
 [-0.014]
 [-0.013]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.04961966651396518
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5319668
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.04945324124601797
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.48193178
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5316412
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
STARTED EXPV TRAINING ON FRAME NO.  20031
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.011]
 [ 0.001]
 [-0.011]
 [ 0.016]
 [ 0.199]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5042799
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.41215634
siam score:  -0.4062262
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.3736621
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]] [[0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.4340119
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.47970092
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5227711
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.035]
 [-0.29 ]
 [-0.035]
 [-0.035]
 [-0.135]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.348]
 [ 0.007]
 [-0.003]
 [ 0.444]
 [ 0.908]] [[0.122]
 [0.009]
 [0.005]
 [0.154]
 [0.309]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]]
siam score:  -0.5395944
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.131]
 [1.107]
 [1.107]
 [1.107]
 [1.107]] [[0.034]
 [0.523]
 [0.523]
 [0.523]
 [0.523]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.002]
 [ 0.002]
 [ 0.002]
 [-0.108]
 [-0.064]] [[0.112]
 [0.112]
 [0.112]
 [0.057]
 [0.079]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.197]
 [-0.253]
 [-0.206]
 [-0.04 ]
 [-0.134]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5241916
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5101318
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.189]
 [3.189]
 [3.189]
 [3.189]
 [3.189]] [[0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.05639291421044999
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.05019240763783117
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.047495212685208635
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.36625457
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.667]
 [2.667]
 [2.667]
 [2.667]
 [2.295]] [[0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.659]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.36587203
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.114719436994172
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.843]] [[0.245]
 [0.245]
 [0.245]
 [0.245]
 [0.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.118]
 [ 0.166]
 [ 0.084]
 [ 0.084]
 [ 0.248]] [[0.   ]
 [0.095]
 [0.067]
 [0.067]
 [0.122]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.4969387
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.232]
 [2.125]
 [1.7  ]
 [1.943]
 [2.977]] [[0.848]
 [0.295]
 [0.083]
 [0.204]
 [0.721]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.32831953689677457
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.029306817732572064
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.542883
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.954]
 [1.954]
 [1.954]
 [1.954]
 [2.373]] [[0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.964]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.597]
 [2.597]
 [2.597]
 [2.597]
 [2.597]] [[1.732]
 [1.732]
 [1.732]
 [1.732]
 [1.732]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.392]
 [1.392]
 [1.392]
 [1.392]
 [1.392]] [[0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.649]
 [2.205]
 [2.649]
 [2.649]
 [2.535]] [[0.435]
 [0.361]
 [0.435]
 [0.435]
 [0.416]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.347]
 [5.117]
 [4.347]
 [4.347]
 [4.871]] [[0.537]
 [0.761]
 [0.537]
 [0.537]
 [0.689]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.749733059092425
siam score:  -0.5751959
siam score:  -0.5764905
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5653842
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.741]
 [3.193]
 [3.556]
 [2.53 ]
 [3.382]] [[0.149]
 [0.224]
 [0.285]
 [0.113]
 [0.256]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.5928491734948382
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.936]
 [0.936]
 [0.936]
 [0.936]
 [3.072]] [[0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.875]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.0038720895234886977
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.034959026360984
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.56375515
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 1 threads
Frames:  27687 train batches done:  3242 episodes:  4099
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]] [[0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5769733
siam score:  -0.5762315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.56017286
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.276]
 [8.044]
 [8.278]
 [7.795]
 [8.379]] [[0.566]
 [0.721]
 [0.769]
 [0.671]
 [0.789]]
siam score:  -0.5644339
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.092]
 [-0.267]
 [ 0.405]
 [ 0.092]
 [ 0.092]] [[0.18 ]
 [0.   ]
 [0.336]
 [0.18 ]
 [0.18 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.335]
 [1.335]
 [1.335]
 [1.335]
 [2.074]] [[0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.48 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5863872
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9002085728385718
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.868]
 [3.868]
 [3.868]
 [3.868]
 [5.888]] [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.813]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.064]
 [0.526]
 [0.064]
 [0.064]
 [0.064]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.51081455
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.59007967
siam score:  -0.588379
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
using explorer policy with actor:  1
siam score:  -0.5779741
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.262]
 [-0.201]
 [-0.327]
 [-0.677]
 [-0.328]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5679005
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.39 ]
 [0.033]
 [0.471]
 [0.298]
 [0.557]] [[0.238]
 [0.   ]
 [0.292]
 [0.177]
 [0.349]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.659]
 [5.929]
 [4.095]
 [2.01 ]
 [3.923]] [[0.   ]
 [0.766]
 [0.437]
 [0.063]
 [0.406]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.016212058730575116
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[8.05]
 [8.05]
 [8.05]
 [8.05]
 [8.05]] [[0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.048]
 [5.001]
 [5.001]
 [5.001]
 [6.277]] [[0.63 ]
 [0.618]
 [0.618]
 [0.618]
 [0.921]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 3.6271302388861503e-10
0.0 0.0
0.0 2.0854960973828612e-10
0.0 1.5942213030744042e-10
0.0 1.1735240147747385e-10
0.0 2.2674753516682214e-10
0.0 0.0
0.0 0.0
0.0 1.8101054385819713e-10
first move QE:  0.019689866906428165
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.01964467176215855
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.577107
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]] [[0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.5848443
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.002]] [[1.106]
 [1.325]
 [1.137]
 [0.935]
 [1.446]] [[0.474]
 [0.62 ]
 [0.495]
 [0.361]
 [0.703]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.020577348441456297
using explorer policy with actor:  1
siam score:  -0.5745626
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.75 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [1.044]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.44 ]
 [0.364]
 [0.187]
 [0.735]
 [0.736]] [[0.284]
 [0.233]
 [0.114]
 [0.48 ]
 [0.481]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
siam score:  -0.59208536
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.03 ]] [[2.53]
 [2.53]
 [2.53]
 [2.53]
 [2.79]] [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.812]]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.034]
 [0.028]
 [0.028]
 [0.034]] [[1.79 ]
 [2.045]
 [2.303]
 [2.303]
 [2.437]] [[0.594]
 [0.767]
 [0.922]
 [0.922]
 [1.011]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
siam score:  -0.6404389
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using another actor
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
first move QE:  0.01836600143806523
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
UNIT TEST: sample policy line 217 mcts : [0.042 0.333 0.    0.25  0.375]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]] [[0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.0010608399271965026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[ 0.158]
 [ 0.152]
 [-0.134]
 [-0.032]
 [-0.005]] [[0.236]
 [0.232]
 [0.041]
 [0.109]
 [0.128]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
siam score:  -0.8221819
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.009]
 [0.006]
 [0.007]
 [0.013]] [[-0.242]
 [ 0.026]
 [-0.428]
 [-0.276]
 [ 0.988]] [[0.146]
 [0.265]
 [0.063]
 [0.13 ]
 [0.69 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using another actor
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.002]
 [0.001]
 [0.001]] [[-0.828]
 [-0.085]
 [-0.696]
 [-0.817]
 [-0.803]] [[0.013]
 [0.136]
 [0.035]
 [0.014]
 [0.017]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.603619095445759
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.292 0.208 0.25  0.125 0.125]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.375 0.042 0.5  ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.024]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.668]] [[0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.637]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.033]] [[0.058]
 [0.058]
 [0.058]
 [0.058]
 [1.444]] [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.033]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
using explorer policy with actor:  1
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
siam score:  -0.8574764
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using another actor
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
siam score:  -0.8635251
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.018]
 [0.038]
 [0.01 ]
 [0.017]] [[ 0.282]
 [ 0.151]
 [ 0.28 ]
 [-0.227]
 [ 0.298]] [[0.01 ]
 [0.018]
 [0.038]
 [0.01 ]
 [0.017]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.07 ]
 [0.073]
 [0.07 ]
 [0.066]] [[1.007]
 [1.007]
 [0.076]
 [1.007]
 [0.363]] [[0.771]
 [0.771]
 [0.154]
 [0.771]
 [0.338]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.025]
 [0.013]
 [0.017]
 [0.021]] [[-0.915]
 [-0.228]
 [-0.538]
 [-0.606]
 [-0.375]] [[0.129]
 [0.533]
 [0.348]
 [0.312]
 [0.446]]
using explorer policy with actor:  1
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using another actor
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using another actor
using explorer policy with actor:  1
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
first move QE:  -0.015818486355516595
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using another actor
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.012]] [[-0.874]
 [-0.874]
 [-0.874]
 [-0.874]
 [-0.532]] [[0.027]
 [0.027]
 [0.027]
 [0.027]
 [0.084]]
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]] [[0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]] [[0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using explorer policy with actor:  0
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
siam score:  -0.8716987
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using explorer policy with actor:  1
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.   ]
 [0.004]
 [0.002]
 [0.003]] [[-0.729]
 [-0.056]
 [-0.894]
 [-0.721]
 [-0.504]] [[0.003]
 [0.   ]
 [0.004]
 [0.002]
 [0.003]]
siam score:  -0.87569827
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
from probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
siam score:  -0.87602973
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504123714160834, 0.12504123714160834, 0.12504123714160834, 0.624876288575175]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.002]
 [0.01 ]
 [0.007]
 [0.011]] [[-0.284]
 [-0.105]
 [-0.616]
 [-0.564]
 [-0.599]] [[0.306]
 [0.422]
 [0.09 ]
 [0.121]
 [0.103]]
siam score:  -0.8707051
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [ 0.035]] [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.409]]
from probs:  [0.12504125218131776, 0.12504125218131776, 0.12504125218131776, 0.6248762434560468]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.12504125218131776, 0.12504125218131776, 0.12504125218131776, 0.6248762434560468]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.12504125218131776, 0.12504125218131776, 0.12504125218131776, 0.6248762434560468]
using explorer policy with actor:  1
from probs:  [0.12504125218131776, 0.12504125218131776, 0.12504125218131776, 0.6248762434560468]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.12504125218131776, 0.12504125218131776, 0.12504125218131776, 0.6248762434560468]
first move QE:  -0.027794320174630086
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.12504125218131776, 0.12504125218131776, 0.12504125218131776, 0.6248762434560468]
first move QE:  -0.027787076666794124
using another actor
from probs:  [0.12504125218131776, 0.12504125218131776, 0.12504125218131776, 0.6248762434560468]
siam score:  -0.8571318
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.12504125218131776, 0.12504125218131776, 0.12504125218131776, 0.6248762434560468]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.004]
 [0.005]
 [0.013]] [[-0.796]
 [-0.021]
 [-0.428]
 [-0.344]
 [-0.11 ]] [[0.006]
 [0.006]
 [0.004]
 [0.005]
 [0.013]]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.1250450153825861, 0.1250450153825861, 0.1250450153825861, 0.6248649538522416]
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.094]] [[0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.231]]
line 256 mcts: sample exp_bonus 0.07845445941979971
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.7919209
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.003]
 [0.067]
 [0.01 ]] [[ 0.482]
 [ 0.014]
 [-0.278]
 [ 0.011]
 [ 1.911]] [[0.252]
 [0.097]
 [0.001]
 [0.162]
 [0.737]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8018539
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.001]
 [0.   ]] [[ 0.008]
 [ 0.02 ]
 [-0.007]
 [-0.012]
 [ 0.127]] [[0.057]
 [0.06 ]
 [0.055]
 [0.054]
 [0.078]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.375 0.167 0.167 0.125 0.167]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8319669
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.375]] [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.004]
 [0.024]
 [0.011]
 [0.013]] [[ 0.071]
 [ 0.271]
 [-0.134]
 [-0.027]
 [ 0.109]] [[0.278]
 [0.372]
 [0.189]
 [0.23 ]
 [0.3  ]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.354]
 [-0.354]
 [-0.354]
 [-0.354]
 [-0.354]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.15707527563286555
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.003]
 [0.002]
 [0.001]] [[ 0.136]
 [-0.073]
 [-0.408]
 [-0.194]
 [ 0.628]] [[0.181]
 [0.112]
 [0.001]
 [0.072]
 [0.344]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.01877188543660456
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.   ]
 [0.008]
 [0.001]
 [0.001]] [[-0.046]
 [-0.036]
 [-0.18 ]
 [-0.185]
 [-0.047]] [[0.163]
 [0.165]
 [0.101]
 [0.091]
 [0.16 ]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.659]
 [ 0.03 ]
 [-0.094]
 [-0.001]
 [ 0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8121782
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]] [[0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.   ]] [[-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [ 0.322]] [[0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.122]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]] [[0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.013]] [[0.249]
 [0.249]
 [0.249]
 [0.249]
 [1.73 ]] [[0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.945]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8270874
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.006]
 [0.004]
 [0.003]] [[-0.211]
 [-0.184]
 [-0.776]
 [-0.45 ]
 [-0.217]] [[0.093]
 [0.095]
 [0.002]
 [0.054]
 [0.093]]
first move QE:  -0.02086028801865723
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.013]
 [0.009]
 [0.005]] [[0.282]
 [0.417]
 [0.28 ]
 [0.064]
 [0.528]] [[0.13 ]
 [0.196]
 [0.138]
 [0.027]
 [0.254]]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8391275
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.147]
 [0.153]
 [0.133]
 [0.142]] [[1.598]
 [2.638]
 [2.004]
 [2.042]
 [2.21 ]] [[0.349]
 [0.974]
 [0.662]
 [0.662]
 [0.755]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.021623225170373916
using explorer policy with actor:  1
siam score:  -0.84213835
first move QE:  -0.01975813663260911
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.012]
 [0.031]
 [0.011]
 [0.022]] [[0.841]
 [0.738]
 [0.313]
 [0.31 ]
 [1.025]] [[0.386]
 [0.333]
 [0.14 ]
 [0.118]
 [0.487]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[-0.078]
 [-0.078]
 [-0.078]
 [-0.078]
 [-0.078]] [[0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.01990654186941217
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.02007882389906433
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.189]
 [0.223]
 [0.207]
 [0.116]] [[2.864]
 [2.049]
 [2.939]
 [2.481]
 [4.979]] [[0.351]
 [0.205]
 [0.454]
 [0.327]
 [0.953]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.054]
 [0.054]
 [0.043]
 [0.043]] [[0.76 ]
 [0.76 ]
 [0.76 ]
 [0.153]
 [0.537]] [[0.49 ]
 [0.49 ]
 [0.49 ]
 [0.215]
 [0.384]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86380464
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.009]
 [0.002]
 [0.002]] [[ 0.103]
 [-0.024]
 [-0.267]
 [-0.225]
 [ 0.042]] [[0.227]
 [0.161]
 [0.049]
 [0.062]
 [0.196]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.6010451378734246
first move QE:  -0.024262105133125433
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.19698463497944177
siam score:  -0.86167526
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86174846
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 1 threads
Frames:  43584 train batches done:  5107 episodes:  6363
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8658095
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.04678389222547412
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8568573
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.8539137
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81625766
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.001]
 [0.002]
 [0.002]] [[-0.061]
 [ 0.211]
 [-0.183]
 [ 0.048]
 [ 0.194]] [[0.062]
 [0.2  ]
 [0.   ]
 [0.117]
 [0.191]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.004]
 [0.004]
 [0.002]
 [0.003]] [[0.29 ]
 [0.35 ]
 [0.192]
 [0.29 ]
 [0.583]] [[0.002]
 [0.004]
 [0.004]
 [0.002]
 [0.003]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.79623824
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.800502
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.026382103619221577
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7986538
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.01 ]
 [0.006]
 [0.004]
 [0.007]] [[ 0.678]
 [ 0.293]
 [ 0.678]
 [-0.043]
 [ 0.361]] [[0.368]
 [0.179]
 [0.368]
 [0.006]
 [0.21 ]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.   ]
 [0.029]
 [0.015]
 [0.016]] [[ 0.129]
 [ 0.136]
 [ 0.05 ]
 [-0.168]
 [ 0.153]] [[0.051]
 [0.041]
 [0.055]
 [0.005]
 [0.059]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.024]
 [ 0.261]
 [ 0.261]
 [-0.003]] [[0.116]
 [0.112]
 [0.16 ]
 [0.16 ]
 [0.115]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.026651952321684373
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.004]
 [0.002]
 [0.002]] [[-0.059]
 [ 0.023]
 [-0.369]
 [-0.271]
 [-0.164]] [[0.153]
 [0.177]
 [0.05 ]
 [0.082]
 [0.118]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.01 ]
 [0.009]
 [0.006]
 [0.009]] [[ 0.373]
 [ 0.23 ]
 [-0.321]
 [-0.107]
 [-0.019]] [[0.233]
 [0.187]
 [0.003]
 [0.071]
 [0.104]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84178627
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.02622223721002005
siam score:  -0.8309016
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.003]
 [0.002]
 [0.002]] [[-0.022]
 [ 0.076]
 [-0.477]
 [-0.157]
 [-0.027]] [[0.152]
 [0.182]
 [0.001]
 [0.107]
 [0.15 ]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8365454
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.3325184906208462
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.845]
 [0.845]
 [0.845]
 [0.845]
 [0.845]] [[0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]]
first move QE:  -0.02683098264876029
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8764755
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.   ]
 [0.007]
 [0.006]
 [0.005]] [[-0.529]
 [-0.195]
 [-0.742]
 [-0.564]
 [-0.297]] [[0.037]
 [0.088]
 [0.003]
 [0.033]
 [0.076]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.006]
 [0.001]
 [0.003]] [[ 0.   ]
 [ 0.013]
 [-0.241]
 [-0.166]
 [ 0.   ]] [[0.003]
 [0.002]
 [0.006]
 [0.001]
 [0.003]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
siam score:  -0.8915716
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8901368
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.6180746630706144
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
UNIT TEST: sample policy line 217 mcts : [0.208 0.375 0.042 0.167 0.208]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8770032
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.02265278858663412
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8717572
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8713908
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87525344
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87375975
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.259]
 [0.223]
 [0.223]
 [0.211]] [[2.836]
 [1.634]
 [2.836]
 [2.836]
 [2.737]] [[0.961]
 [0.495]
 [0.961]
 [0.961]
 [0.913]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86554617
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90738493
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.885626
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88717973
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86928594
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.110584951644781
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.005]] [[1.932]
 [1.932]
 [1.932]
 [1.932]
 [2.111]] [[0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.809]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[-0.27 ]
 [ 0.144]
 [-0.27 ]
 [-0.27 ]
 [ 0.169]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.8664225
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86197925
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.2532097912237538
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
siam score:  -0.8719113
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.006]
 [0.   ]
 [0.004]
 [0.004]] [[ 0.14 ]
 [-0.105]
 [-0.079]
 [-0.226]
 [-0.027]] [[0.002]
 [0.006]
 [0.   ]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86676896
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8697347
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.02304995766592952
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.004]
 [0.004]
 [0.004]
 [0.006]] [[3.132]
 [2.232]
 [2.232]
 [2.232]
 [2.567]] [[1.001]
 [0.634]
 [0.634]
 [0.634]
 [0.772]]
siam score:  -0.8728171
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86857146
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.181]
 [1.181]
 [1.181]
 [1.181]
 [1.181]] [[0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]] [[0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87252104
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87174624
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88217616
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.071]
 [-0.252]
 [-0.202]
 [-0.089]] [[0.166]
 [0.12 ]
 [0.   ]
 [0.033]
 [0.109]]
first move QE:  -0.016061245773005314
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88333774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.015174262617942661
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.12327923428097273
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9007132
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9034557
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[3.351]
 [3.532]
 [3.532]
 [3.532]
 [3.158]] [[0.934]
 [1.003]
 [1.003]
 [1.003]
 [0.861]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88916826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.014792217181698129
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.042 0.042 0.875]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8649298
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8689935
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.01544116333436801
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8615181
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.099]
 [0.09 ]
 [0.038]
 [0.074]] [[1.186]
 [1.38 ]
 [1.897]
 [1.379]
 [1.817]] [[0.343]
 [0.508]
 [0.844]
 [0.447]
 [0.775]]
siam score:  -0.86256224
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8640878
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.016011095699666383
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.005]] [[0.199]
 [0.199]
 [0.199]
 [0.199]
 [1.298]] [[0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.836]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8533562
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.02 ]] [[1.332]
 [1.332]
 [1.332]
 [1.332]
 [1.678]] [[0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.8  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.061]] [[3.27 ]
 [3.27 ]
 [3.27 ]
 [3.27 ]
 [3.657]] [[0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.365]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.1741523136163021
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.022]
 [0.001]
 [0.018]
 [0.018]] [[-0.113]
 [-0.142]
 [-0.332]
 [-0.31 ]
 [-0.031]] [[0.121]
 [0.116]
 [0.   ]
 [0.028]
 [0.168]]
siam score:  -0.8572757
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
using explorer policy with actor:  1
first move QE:  -0.01627981532224103
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8470398
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84525955
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84478426
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84799576
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8468375
UNIT TEST: sample policy line 217 mcts : [0.25  0.292 0.083 0.083 0.292]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.01648963077892187
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
using explorer policy with actor:  1
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.018]
 [0.012]
 [0.012]
 [0.013]] [[-0.272]
 [ 0.03 ]
 [-0.272]
 [-0.272]
 [ 0.041]] [[0.012]
 [0.018]
 [0.012]
 [0.012]
 [0.013]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0334],
        [0.0285],
        [0.0872],
        [0.0064],
        [0.1304],
        [0.2964],
        [0.2875],
        [0.0027],
        [0.1646],
        [0.0090]], dtype=torch.float64)
0.0 0.03343457359903021
0.0 0.028536642275654805
0.0 0.0872430254428167
0.0 0.006351732041074344
0.0 0.13038719546955493
0.0 0.2964088369318199
0.0 0.28753045438764335
0.0 0.0026683471251229256
0.0 0.16455009185054015
0.0 0.009008611043155004
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  0 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.002]
 [0.009]
 [0.012]
 [0.008]] [[1.681]
 [0.667]
 [1.071]
 [1.087]
 [1.48 ]] [[0.781]
 [0.165]
 [0.414]
 [0.427]
 [0.66 ]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[1.854]
 [1.854]
 [1.854]
 [1.854]
 [1.854]] [[0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
from probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.213598492911414
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.424699973140694
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
using another actor
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.009]
 [0.001]
 [0.002]
 [0.001]] [[-0.058]
 [-0.031]
 [ 0.031]
 [-0.035]
 [ 0.102]] [[0.036]
 [0.046]
 [0.048]
 [0.039]
 [0.06 ]]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
using explorer policy with actor:  1
using another actor
from probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
from probs:  [0.1250467417743733, 0.1250467417743733, 0.1250467417743733, 0.6248597746768801]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
from probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
from probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
from probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
siam score:  -0.8734108
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.072]
 [0.072]
 [0.072]
 [0.06 ]
 [0.387]] [[0.005]
 [0.005]
 [0.005]
 [0.001]
 [0.11 ]]
siam score:  -0.875787
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.026]
 [0.   ]
 [0.008]
 [0.011]] [[0.304]
 [0.566]
 [0.138]
 [0.265]
 [0.61 ]] [[0.188]
 [0.383]
 [0.072]
 [0.165]
 [0.398]]
Printing some Q and Qe and total Qs values:  [[1]
 [1]
 [1]
 [1]
 [1]] [[0]
 [0]
 [0]
 [0]
 [0]] [[1]
 [1]
 [1]
 [1]
 [1]]
maxi score, test score, baseline:  0.0041 0.1 0.1
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
siam score:  -0.8747327
first move QE:  -0.015051179205985614
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
from probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
siam score:  -0.88397783
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.175]] [[2.976]
 [2.976]
 [2.976]
 [2.976]
 [3.153]] [[0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.918]]
siam score:  -0.88450897
line 256 mcts: sample exp_bonus 5.479921480932871
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
from probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
from probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
from probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
UNIT TEST: sample policy line 217 mcts : [0.208 0.292 0.208 0.083 0.208]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.01 ]
 [0.012]
 [0.012]
 [0.011]] [[1.446]
 [1.301]
 [1.183]
 [1.183]
 [1.16 ]] [[0.844]
 [0.759]
 [0.691]
 [0.691]
 [0.677]]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0041 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
actor:  0 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
from probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.   ]
 [0.013]
 [0.007]
 [0.006]] [[-0.147]
 [ 0.226]
 [-0.131]
 [-0.189]
 [-0.239]] [[0.005]
 [0.   ]
 [0.013]
 [0.007]
 [0.006]]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0061 0.1 0.1
from probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504211543516816, 0.12504211543516816, 0.12504211543516816, 0.6248736536944957]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.9058387
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
UNIT TEST: sample policy line 217 mcts : [0.708 0.042 0.042 0.042 0.167]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.002]
 [0.01 ]
 [0.007]
 [0.009]] [[1.205]
 [0.894]
 [0.736]
 [1.056]
 [3.003]] [[0.23 ]
 [0.12 ]
 [0.07 ]
 [0.179]
 [0.854]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.721]
 [1.447]
 [1.447]
 [1.447]
 [1.447]] [[0.46 ]
 [0.871]
 [0.871]
 [0.871]
 [0.871]]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
start point for exploration sampling:  20031
first move QE:  -0.012812213805463825
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.196]
 [0.157]
 [0.157]
 [0.159]] [[2.675]
 [2.424]
 [2.675]
 [2.675]
 [3.593]] [[0.702]
 [0.652]
 [0.702]
 [0.702]
 [0.937]]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]] [[0.093]
 [0.093]
 [0.093]
 [0.093]
 [0.093]] [[0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]]
siam score:  -0.8960535
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
siam score:  -0.8934451
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
siam score:  -0.8910701
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]] [[1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]] [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]] [[2.861]
 [2.861]
 [2.861]
 [2.861]
 [2.861]] [[1.068]
 [1.068]
 [1.068]
 [1.068]
 [1.068]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
from probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
line 256 mcts: sample exp_bonus -0.3933770293356563
using explorer policy with actor:  1
from probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
siam score:  -0.896258
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.047]
 [0.072]
 [0.072]
 [0.03 ]] [[0.333]
 [0.188]
 [0.333]
 [0.333]
 [1.044]] [[0.275]
 [0.153]
 [0.275]
 [0.275]
 [0.707]]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
using explorer policy with actor:  1
from probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
from probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
from probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
using another actor
from probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
siam score:  -0.90124667
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
siam score:  -0.9054964
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
first move QE:  -0.014218852650668282
using explorer policy with actor:  1
start point for exploration sampling:  20031
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]] [[0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]] [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2592474868899677
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
from probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
from probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
siam score:  -0.90921134
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
siam score:  -0.9108255
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
from probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
siam score:  -0.9084894
using another actor
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
from probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.013]
 [0.02 ]
 [0.02 ]
 [0.024]] [[1.656]
 [1.113]
 [1.893]
 [1.893]
 [1.605]] [[0.459]
 [0.185]
 [0.582]
 [0.582]
 [0.442]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.042 0.833]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.365]
 [0.297]
 [0.297]
 [0.251]] [[3.481]
 [3.488]
 [3.179]
 [3.179]
 [2.745]] [[0.895]
 [0.942]
 [0.775]
 [0.775]
 [0.572]]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.375]] [[2.576]
 [2.576]
 [2.576]
 [2.576]
 [2.369]] [[0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.731]]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
using explorer policy with actor:  1
siam score:  -0.92367655
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
using another actor
from probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
from probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.423]] [[2.789]
 [2.789]
 [2.789]
 [2.789]
 [2.8  ]] [[0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.887]]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12504057324597076, 0.12504057324597076, 0.12504057324597076, 0.6248782802620878]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
using another actor
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
siam score:  -0.9269022
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.708 0.083 0.125]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.254]] [[3.052]
 [3.052]
 [3.052]
 [3.052]
 [2.698]] [[0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.729]]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.001]
 [0.057]
 [0.043]
 [0.021]] [[ 0.004]
 [ 0.029]
 [-0.059]
 [-0.157]
 [-0.041]] [[0.208]
 [0.189]
 [0.186]
 [0.106]
 [0.162]]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
siam score:  -0.9254311
from probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
UNIT TEST: sample policy line 217 mcts : [0.042 0.542 0.083 0.042 0.292]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
siam score:  -0.9186969
Printing some Q and Qe and total Qs values:  [[0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]] [[1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]] [[0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]]
first move QE:  -0.009584159049758123
from probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
from probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.033]
 [0.033]
 [0.033]
 [0.026]] [[-0.345]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.267]] [[0.004]
 [0.139]
 [0.139]
 [0.139]
 [0.043]]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
from probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
using another actor
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
from probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.001]
 [0.048]
 [0.023]
 [0.016]] [[-0.123]
 [ 0.092]
 [-0.184]
 [-0.312]
 [-0.168]] [[0.076]
 [0.13 ]
 [0.085]
 [0.017]
 [0.058]]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
from probs:  [0.12503980213709706, 0.12503980213709706, 0.12503980213709706, 0.6248805935887087]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.26279996694913393
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.22223193533126273, 0.11115967665631364, 0.11115967665631364, 0.55544871135611]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.066]
 [0.012]
 [0.012]
 [0.057]] [[-0.06 ]
 [ 0.112]
 [ 0.   ]
 [ 0.   ]
 [ 0.041]] [[0.056]
 [0.192]
 [0.082]
 [0.082]
 [0.147]]
from probs:  [0.22223193533126273, 0.11115967665631364, 0.11115967665631364, 0.55544871135611]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.22223193533126273, 0.11115967665631364, 0.11115967665631364, 0.55544871135611]
maxi score, test score, baseline:  0.0061 0.1 0.1
from probs:  [0.22223193533126273, 0.11115967665631364, 0.11115967665631364, 0.55544871135611]
maxi score, test score, baseline:  0.0061 0.1 0.1
siam score:  -0.90679115
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.22223193533126273, 0.11115967665631364, 0.11115967665631364, 0.55544871135611]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.22223193533126273, 0.11115967665631364, 0.11115967665631364, 0.55544871135611]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.22223193533126273, 0.11115967665631364, 0.11115967665631364, 0.55544871135611]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.22223193533126273, 0.11115967665631364, 0.11115967665631364, 0.55544871135611]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.22223193533126273, 0.11115967665631364, 0.11115967665631364, 0.55544871135611]
maxi score, test score, baseline:  0.0061 0.1 0.1
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.90810364
from probs:  [0.2000187337190653, 0.10005620115719589, 0.2000187337190653, 0.4999063314046735]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.2000187337190653, 0.10005620115719589, 0.2000187337190653, 0.4999063314046735]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.2000187337190653, 0.10005620115719589, 0.2000187337190653, 0.4999063314046735]
actor:  1 policy actor:  1  step number:  97 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.18752263840201114, 0.10421948960469263, 0.18752263840201114, 0.5207352335912852]
using another actor
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.18752263840201114, 0.10421948960469263, 0.18752263840201114, 0.5207352335912852]
siam score:  -0.90803176
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.019]
 [0.021]
 [0.021]
 [0.028]] [[0.728]
 [0.986]
 [0.881]
 [0.881]
 [0.628]] [[0.111]
 [0.148]
 [0.133]
 [0.133]
 [0.098]]
maxi score, test score, baseline:  0.0061 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.028]
 [0.021]
 [0.019]
 [0.022]] [[-0.421]
 [-0.047]
 [-0.421]
 [-0.128]
 [-0.117]] [[0.015]
 [0.084]
 [0.015]
 [0.061]
 [0.066]]
siam score:  -0.90509737
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.18752263840201114, 0.10421948960469263, 0.18752263840201114, 0.5207352335912852]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
using another actor
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.05 ]
 [0.05 ]
 [0.05 ]
 [0.064]] [[3.767]
 [3.767]
 [3.767]
 [3.767]
 [3.497]] [[0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.561]]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.063]] [[2.19 ]
 [2.19 ]
 [2.19 ]
 [2.19 ]
 [2.891]] [[0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.354]]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.18752263840201114, 0.10421948960469263, 0.18752263840201114, 0.5207352335912852]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.18752263840201114, 0.10421948960469263, 0.18752263840201114, 0.5207352335912852]
from probs:  [0.18752263840201114, 0.10421948960469263, 0.18752263840201114, 0.5207352335912852]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.18752263840201114, 0.10421948960469263, 0.18752263840201114, 0.5207352335912852]
from probs:  [0.18752263840201114, 0.10421948960469263, 0.18752263840201114, 0.5207352335912852]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.18752263840201114, 0.10421948960469263, 0.18752263840201114, 0.5207352335912852]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.18752263840201114, 0.10421948960469263, 0.18752263840201114, 0.5207352335912852]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
from probs:  [0.25, 0.09621238183984669, 0.17310619091992335, 0.4806814272402299]
maxi score, test score, baseline:  0.0061 0.1 0.1
using another actor
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.25, 0.09621238183984669, 0.17310619091992335, 0.4806814272402299]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.23333949566308557, 0.10005546096777042, 0.166697478315428, 0.49990756505371586]
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.464]
 [0.873]
 [0.564]
 [0.466]] [[0.888]
 [3.003]
 [3.618]
 [2.161]
 [2.07 ]] [[0.023]
 [0.656]
 [0.962]
 [0.483]
 [0.424]]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.23333949566308557, 0.10005546096777042, 0.166697478315428, 0.49990756505371586]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.23333949566308557, 0.10005546096777042, 0.166697478315428, 0.49990756505371586]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.220598868383112, 0.10299434191555994, 0.16179660514933597, 0.514610184551992]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.220598868383112, 0.10299434191555994, 0.16179660514933597, 0.514610184551992]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.220598868383112, 0.10299434191555994, 0.16179660514933597, 0.514610184551992]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.220598868383112, 0.10299434191555994, 0.16179660514933597, 0.514610184551992]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.220598868383112, 0.10299434191555994, 0.16179660514933597, 0.514610184551992]
siam score:  -0.91781515
maxi score, test score, baseline:  0.0061 0.1 0.1
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.220598868383112, 0.10299434191555994, 0.16179660514933597, 0.514610184551992]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.3026112318354851, 0.09216630449354478, 0.14477753632902984, 0.4604449273419402]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.3026112318354851, 0.09216630449354478, 0.14477753632902984, 0.4604449273419402]
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.3026112318354851, 0.09216630449354478, 0.14477753632902984, 0.4604449273419402]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.3026112318354851, 0.09216630449354478, 0.14477753632902984, 0.4604449273419402]
from probs:  [0.3026112318354851, 0.09216630449354478, 0.14477753632902984, 0.4604449273419402]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
deleting a thread, now have 1 threads
Frames:  69896 train batches done:  8188 episodes:  9324
maxi score, test score, baseline:  0.0061 0.1 0.1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.3374652406654689, 0.08756455304984351, 0.1375446905729686, 0.437425515711719]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.3374652406654689, 0.08756455304984351, 0.1375446905729686, 0.437425515711719]
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.3374652406654689, 0.08756455304984351, 0.1375446905729686, 0.437425515711719]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.1 0.1
probs:  [0.3374652406654689, 0.08756455304984351, 0.1375446905729686, 0.437425515711719]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.314]] [[2.378]
 [2.378]
 [2.378]
 [2.378]
 [2.53 ]] [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.314]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0141 0.1 0.1
probs:  [0.3374652406654689, 0.08756455304984351, 0.1375446905729686, 0.437425515711719]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.018099999999999998 0.1 0.1
probs:  [0.3374652406654689, 0.08756455304984351, 0.1375446905729686, 0.437425515711719]
siam score:  -0.9266565
actor:  0 policy actor:  0  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.715]] [[4.271]
 [4.271]
 [4.271]
 [4.271]
 [4.952]] [[0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.593]]
line 256 mcts: sample exp_bonus 2.514363931361824
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.33746464668630094, 0.08756565615401253, 0.1375454542604702, 0.4374242428992163]
maxi score, test score, baseline:  0.0201 0.35 0.35
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.33746464668630094, 0.08756565615401253, 0.1375454542604702, 0.4374242428992163]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.33746464668630094, 0.08756565615401253, 0.1375454542604702, 0.4374242428992163]
maxi score, test score, baseline:  0.0201 0.35 0.35
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.33746464668630094, 0.08756565615401253, 0.1375454542604702, 0.4374242428992163]
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.021]
 [0.042]
 [0.042]
 [0.041]] [[0.952]
 [0.871]
 [0.867]
 [0.867]
 [0.923]] [[0.668]
 [0.597]
 [0.615]
 [0.615]
 [0.652]]
maxi score, test score, baseline:  0.0201 0.35 0.35
from probs:  [0.33746464668630094, 0.08756565615401253, 0.1375454542604702, 0.4374242428992163]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.45669698264221686
from probs:  [0.33746464668630094, 0.08756565615401253, 0.1375454542604702, 0.4374242428992163]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.33746464668630094, 0.08756565615401253, 0.1375454542604702, 0.4374242428992163]
maxi score, test score, baseline:  0.0201 0.35 0.35
maxi score, test score, baseline:  0.0201 0.35 0.35
maxi score, test score, baseline:  0.0201 0.35 0.35
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.3689984102375698, 0.08340222566740223, 0.13100158976243015, 0.41659777433259776]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.3689984102375698, 0.08340222566740223, 0.13100158976243015, 0.41659777433259776]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[1.83]
 [1.83]
 [1.83]
 [1.83]
 [1.83]] [[1.006]
 [1.006]
 [1.006]
 [1.006]
 [1.006]]
maxi score, test score, baseline:  0.0201 0.35 0.35
from probs:  [0.3689984102375698, 0.08340222566740223, 0.13100158976243015, 0.41659777433259776]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.3689984102375698, 0.08340222566740223, 0.13100158976243015, 0.41659777433259776]
siam score:  -0.9340809
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.3689984102375698, 0.08340222566740223, 0.13100158976243015, 0.41659777433259776]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.3689984102375698, 0.08340222566740223, 0.13100158976243015, 0.41659777433259776]
using another actor
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.3689984102375698, 0.08340222566740223, 0.13100158976243015, 0.41659777433259776]
maxi score, test score, baseline:  0.0201 0.35 0.35
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.39766495795615026, 0.07961735620444198, 0.12505272788325744, 0.3976649579561503]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.39766495795615026, 0.07961735620444198, 0.12505272788325744, 0.3976649579561503]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.39766495795615026, 0.07961735620444198, 0.12505272788325744, 0.3976649579561503]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.39766495795615026, 0.07961735620444198, 0.12505272788325744, 0.3976649579561503]
siam score:  -0.9349179
siam score:  -0.93426806
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.39766495795615026, 0.07961735620444198, 0.12505272788325744, 0.3976649579561503]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.016]
 [0.016]
 [0.016]
 [0.017]] [[1.474]
 [1.525]
 [1.525]
 [1.525]
 [1.409]] [[0.945]
 [0.978]
 [0.978]
 [0.978]
 [0.904]]
maxi score, test score, baseline:  0.0201 0.35 0.35
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.39766495795615026, 0.07961735620444196, 0.12505272788325744, 0.39766495795615026]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.39766495795615026, 0.07961735620444196, 0.12505272788325744, 0.39766495795615026]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.057]
 [0.051]
 [0.125]
 [0.063]] [[ 0.5  ]
 [ 0.245]
 [-0.109]
 [-0.048]
 [ 0.202]] [[0.451]
 [0.285]
 [0.084]
 [0.179]
 [0.266]]
UNIT TEST: sample policy line 217 mcts : [0.083 0.292 0.042 0.083 0.5  ]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
from probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
siam score:  -0.93667054
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.3486671693862285
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.331]
 [0.341]
 [0.231]
 [0.28 ]] [[0.956]
 [1.437]
 [1.77 ]
 [0.956]
 [1.685]] [[0.231]
 [0.331]
 [0.341]
 [0.231]
 [0.28 ]]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
actor:  1 policy actor:  1  step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  20031
from probs:  [0.35572659845536114, 0.08660434784171461, 0.1250503836436641, 0.4326186700592601]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.35572659845536114, 0.08660434784171461, 0.1250503836436641, 0.4326186700592601]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.35572659845536114, 0.08660434784171461, 0.1250503836436641, 0.4326186700592601]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.35572659845536114, 0.08660434784171461, 0.1250503836436641, 0.4326186700592601]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.35572659845536114, 0.08660434784171461, 0.1250503836436641, 0.4326186700592601]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.35572659845536114, 0.08660434784171461, 0.1250503836436641, 0.4326186700592601]
first move QE:  -0.00917961485083157
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.35572659845536114, 0.08660434784171461, 0.1250503836436641, 0.4326186700592601]
from probs:  [0.35572659845536114, 0.08660434784171461, 0.1250503836436641, 0.4326186700592601]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.35572659845536114, 0.08660434784171461, 0.1250503836436641, 0.4326186700592601]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.35572659845536114, 0.08660434784171461, 0.1250503836436641, 0.4326186700592601]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.35572659845536114, 0.08660434784171461, 0.1250503836436641, 0.4326186700592601]
first move QE:  -0.009229564768487472
from probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
from probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
maxi score, test score, baseline:  0.0201 0.35 0.35
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.37494854190226984, 0.08340194413030688, 0.1250514580977302, 0.4165980558696931]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0201 0.35 0.35
probs:  [0.39993712222647076, 0.08007126147666642, 0.120054494070392, 0.3999371222264708]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.39993712222647076, 0.08007126147666642, 0.120054494070392, 0.3999371222264708]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.39993712222647076, 0.08007126147666642, 0.120054494070392, 0.3999371222264708]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.39993712222647076, 0.08007126147666642, 0.120054494070392, 0.3999371222264708]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
probs:  [0.39993712222647076, 0.08007126147666642, 0.120054494070392, 0.3999371222264708]
maxi score, test score, baseline:  0.022099999999999998 0.35 0.35
actor:  0 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.   ]
 [0.076]
 [0.076]
 [0.06 ]] [[-0.028]
 [ 0.241]
 [-0.063]
 [-0.063]
 [-0.309]] [[0.165]
 [0.236]
 [0.16 ]
 [0.16 ]
 [0.021]]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.39993712222647076, 0.08007126147666642, 0.120054494070392, 0.3999371222264708]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.39993712222647076, 0.0800712614766664, 0.12005449407039197, 0.39993712222647076]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.39993712222647076, 0.0800712614766664, 0.12005449407039197, 0.39993712222647076]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.39993712222647076, 0.0800712614766664, 0.12005449407039197, 0.39993712222647076]
from probs:  [0.39993712222647076, 0.0800712614766664, 0.12005449407039197, 0.39993712222647076]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.39993712222647076, 0.0800712614766664, 0.12005449407039197, 0.39993712222647076]
maxi score, test score, baseline:  0.0241 0.35 0.35
probs:  [0.39993712222647076, 0.0800712614766664, 0.12005449407039197, 0.39993712222647076]
actor:  0 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.39993712222647076, 0.0800712614766664, 0.12005449407039197, 0.39993712222647076]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.39993712222647076, 0.0800712614766664, 0.12005449407039197, 0.39993712222647076]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.39993712222647076, 0.0800712614766664, 0.12005449407039197, 0.39993712222647076]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.39993712222647076, 0.0800712614766664, 0.12005449407039197, 0.39993712222647076]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]] [[0.999]
 [0.999]
 [0.999]
 [0.999]
 [0.999]]
Printing some Q and Qe and total Qs values:  [[0.983]
 [1.203]
 [1.007]
 [0.72 ]
 [0.949]] [[2.064]
 [2.445]
 [2.236]
 [1.981]
 [2.038]] [[0.797]
 [1.061]
 [0.868]
 [0.601]
 [0.767]]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
UNIT TEST: sample policy line 217 mcts : [0.167 0.167 0.042 0.083 0.542]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.41659827484364736, 0.08340172515635265, 0.12042356401049649, 0.37957643598950347]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
maxi score, test score, baseline:  0.026099999999999998 0.35 0.35
probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0281 0.35 0.35
probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
maxi score, test score, baseline:  0.0281 0.35 0.35
probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
using another actor
from probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
maxi score, test score, baseline:  0.0281 0.35 0.35
maxi score, test score, baseline:  0.0281 0.35 0.35
probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
maxi score, test score, baseline:  0.0281 0.35 0.35
probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
maxi score, test score, baseline:  0.0281 0.35 0.35
probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
maxi score, test score, baseline:  0.0281 0.35 0.35
probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
maxi score, test score, baseline:  0.0281 0.35 0.35
probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
maxi score, test score, baseline:  0.0281 0.35 0.35
probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
maxi score, test score, baseline:  0.0281 0.35 0.35
probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
using another actor
from probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
siam score:  -0.9462105
maxi score, test score, baseline:  0.0281 0.35 0.35
probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
maxi score, test score, baseline:  0.0281 0.35 0.35
probs:  [0.4309615738168417, 0.08627286178476226, 0.12074173298797022, 0.3620238314104258]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0281 0.35 0.35
probs:  [0.42961518230437323, 0.08600352920035487, 0.11724095220981108, 0.36714033628546083]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.42961518230437323, 0.08600352920035487, 0.11724095220981108, 0.36714033628546083]
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.42961518230437323, 0.08600352920035487, 0.11724095220981108, 0.36714033628546083]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
siam score:  -0.93878263
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.4411006821728966, 0.08829942277677971, 0.11769952772645613, 0.35290036732386754]
maxi score, test score, baseline:  0.0301 0.35 0.35
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.4411006821728966, 0.08829942277677971, 0.11769952772645613, 0.35290036732386754]
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.4411006821728966, 0.08829942277677971, 0.11769952772645613, 0.35290036732386754]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.143]]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]]
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.4411006821728966, 0.08829942277677971, 0.11769952772645613, 0.35290036732386754]
using another actor
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.4411006821728966, 0.08829942277677971, 0.11769952772645613, 0.35290036732386754]
maxi score, test score, baseline:  0.0301 0.35 0.35
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.4411006821728966, 0.08829942277677971, 0.11769952772645613, 0.35290036732386754]
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.4411006821728966, 0.08829942277677971, 0.11769952772645613, 0.35290036732386754]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.   ]
 [0.034]
 [0.034]
 [0.037]] [[ 0.112]
 [-0.014]
 [-0.185]
 [-0.225]
 [-0.108]] [[0.423]
 [0.301]
 [0.221]
 [0.194]
 [0.275]]
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.365]
 [0.714]
 [0.365]
 [0.365]] [[1.069]
 [1.069]
 [1.775]
 [1.069]
 [1.069]] [[0.315]
 [0.315]
 [0.723]
 [0.315]
 [0.315]]
maxi score, test score, baseline:  0.0301 0.35 0.35
from probs:  [0.4513101317594323, 0.09034024032872608, 0.11810715505416504, 0.3402424728576766]
Printing some Q and Qe and total Qs values:  [[0.579]
 [1.264]
 [0.069]
 [0.579]
 [0.579]] [[0.   ]
 [4.752]
 [0.875]
 [0.   ]
 [0.   ]] [[0.073]
 [1.007]
 [0.091]
 [0.073]
 [0.073]]
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.4513101317594323, 0.09034024032872608, 0.11810715505416504, 0.34024247285767656]
using another actor
from probs:  [0.4513101317594323, 0.09034024032872608, 0.11810715505416504, 0.34024247285767656]
line 256 mcts: sample exp_bonus 1.096196597330439
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.4513101317594323, 0.09034024032872608, 0.11810715505416504, 0.34024247285767656]
siam score:  -0.9379513
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.4513101317594323, 0.09034024032872608, 0.11810715505416504, 0.34024247285767656]
siam score:  -0.93758285
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.4513101317594323, 0.09034024032872608, 0.11810715505416504, 0.34024247285767656]
using another actor
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.751]
 [0.825]
 [0.751]
 [0.751]] [[-0.015]
 [-0.015]
 [ 0.156]
 [-0.015]
 [-0.015]] [[0.699]
 [0.699]
 [0.802]
 [0.699]
 [0.699]]
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]] [[0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]] [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]]
siam score:  -0.94145906
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.4513101317594323, 0.09034024032872608, 0.11810715505416504, 0.34024247285767656]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.4513101317594323, 0.09034024032872608, 0.11810715505416504, 0.34024247285767656]
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.782]] [[3.124]
 [3.124]
 [3.124]
 [3.124]
 [3.134]] [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.629]]
maxi score, test score, baseline:  0.0301 0.35 0.35
probs:  [0.4513101317594323, 0.09034024032872608, 0.11810715505416504, 0.34024247285767656]
from probs:  [0.4513101317594323, 0.09034024032872608, 0.11810715505416504, 0.34024247285767656]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.4513101317594323, 0.09034024032872608, 0.11810715505416504, 0.34024247285767656]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.4513101317594323, 0.09034024032872608, 0.11810715505416504, 0.34024247285767656]
Printing some Q and Qe and total Qs values:  [[1.038]
 [1.109]
 [0.982]
 [1.132]
 [1.268]] [[1.509]
 [0.175]
 [0.684]
 [1.606]
 [0.734]] [[0.899]
 [0.747]
 [0.705]
 [1.009]
 [0.999]]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.46044499528179833, 0.09216625353865128, 0.11847187794887608, 0.3289168732306744]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.46044499528179833, 0.09216625353865128, 0.11847187794887608, 0.3289168732306744]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.46044499528179833, 0.09216625353865128, 0.11847187794887608, 0.3289168732306744]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.46044499528179833, 0.09216625353865128, 0.11847187794887608, 0.3289168732306744]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 5.587787248175085
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.341]] [[4.776]
 [4.776]
 [4.776]
 [4.776]
 [4.43 ]] [[0.79 ]
 [0.79 ]
 [0.79 ]
 [0.79 ]
 [0.699]]
first move QE:  -0.005896008746485564
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.46044499528179833, 0.09216625353865128, 0.11847187794887608, 0.3289168732306744]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.005896008746485564
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.468666447421052, 0.09380968041353431, 0.1188001315473688, 0.3187237406180449]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.468666447421052, 0.09380968041353431, 0.1188001315473688, 0.3187237406180449]
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.33 ]
 [0.643]
 [0.088]
 [0.286]] [[ 0.958]
 [ 0.083]
 [ 0.531]
 [-0.13 ]
 [ 0.425]] [[0.329]
 [0.308]
 [0.696]
 [0.03 ]
 [0.321]]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.4806823757572934, 0.09621174949513775, 0.12184312457928145, 0.3012627501682874]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.4806823757572934, 0.09621174949513775, 0.12184312457928145, 0.3012627501682874]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.4806823757572934, 0.09621174949513775, 0.12184312457928145, 0.3012627501682874]
start point for exploration sampling:  20031
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.4806823757572934, 0.09621174949513775, 0.12184312457928145, 0.3012627501682874]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.314]] [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [3.646]] [[0.264]
 [0.264]
 [0.264]
 [0.264]
 [1.044]]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.4806823757572934, 0.09621174949513775, 0.12184312457928145, 0.3012627501682874]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.4806823757572934, 0.09621174949513775, 0.12184312457928145, 0.3012627501682874]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.4806823757572934, 0.09621174949513775, 0.12184312457928145, 0.3012627501682874]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.4806823757572934, 0.09621174949513775, 0.12184312457928145, 0.3012627501682874]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.4806823757572934, 0.09621174949513775, 0.12184312457928145, 0.3012627501682874]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.521]
 [0.592]
 [0.475]
 [0.502]] [[0.292]
 [0.217]
 [0.214]
 [0.182]
 [0.338]] [[0.445]
 [0.596]
 [0.666]
 [0.545]
 [0.598]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.49990845562449976, 0.10005492662530009, 0.12226901156970009, 0.2777676061805]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.49990845562449976, 0.10005492662530009, 0.12226901156970009, 0.2777676061805]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.5052263767348724, 0.10111794690465778, 0.12238681163256382, 0.271268864727906]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.863]] [[2.895]
 [2.895]
 [2.895]
 [2.895]
 [3.424]] [[0.827]
 [0.827]
 [0.827]
 [0.827]
 [1.236]]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.5052263767348724, 0.10111794690465778, 0.12238681163256382, 0.271268864727906]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.5052263767348724, 0.10111794690465778, 0.12238681163256382, 0.271268864727906]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.5101102082636859, 0.10209419530104132, 0.12249499594917355, 0.26530060048609916]
using explorer policy with actor:  1
start point for exploration sampling:  20031
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.5146110165864658, 0.10299387967418573, 0.12259469571762763, 0.2598004080217209]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.714]
 [0.522]
 [0.522]
 [0.506]] [[2.184]
 [3.026]
 [2.603]
 [2.603]
 [3.12 ]] [[0.58 ]
 [1.127]
 [0.787]
 [0.787]
 [1.023]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.606]
 [0.606]
 [0.606]
 [0.558]] [[3.733]
 [4.773]
 [4.773]
 [4.773]
 [4.919]] [[0.836]
 [0.997]
 [0.997]
 [0.997]
 [1.013]]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.5146110165864658, 0.10299387967418573, 0.12259469571762763, 0.2598004080217209]
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.633]
 [0.596]
 [0.417]
 [0.594]] [[1.167]
 [1.72 ]
 [1.301]
 [1.2  ]
 [1.355]] [[0.57 ]
 [0.633]
 [0.596]
 [0.417]
 [0.594]]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.5146110165864658, 0.10299387967418573, 0.12259469571762763, 0.2598004080217209]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.5146110165864658, 0.10299387967418573, 0.12259469571762763, 0.2598004080217209]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.5146110165864658, 0.10299387967418573, 0.12259469571762764, 0.259800408021721]
maxi score, test score, baseline:  0.032100000000000004 0.35 0.35
probs:  [0.5146110165864658, 0.10299387967418573, 0.12259469571762764, 0.259800408021721]
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.5146110165864658, 0.10299387967418573, 0.12259469571762764, 0.259800408021721]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
using another actor
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.5146110165864658, 0.10299387967418573, 0.12259469571762764, 0.259800408021721]
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.5146110165864658, 0.10299387967418573, 0.12259469571762764, 0.259800408021721]
Printing some Q and Qe and total Qs values:  [[0.57]
 [0.57]
 [0.57]
 [0.57]
 [0.57]] [[4.276]
 [4.276]
 [4.276]
 [4.276]
 [4.276]] [[1.011]
 [1.011]
 [1.011]
 [1.011]
 [1.011]]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5146110165864658, 0.10299387967418573, 0.12259469571762764, 0.259800408021721]
maxi score, test score, baseline:  0.0361 0.35 0.35
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5187721603804133, 0.10382566716152962, 0.12268687139875159, 0.2547153010593055]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5187721603804133, 0.10382566716152962, 0.12268687139875159, 0.2547153010593055]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
siam score:  -0.93611664
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5226306919480961, 0.10459696429434873, 0.12277234375755515, 0.25]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5226306919480961, 0.10459696429434873, 0.12277234375755515, 0.25]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5226306919480961, 0.10459696429434873, 0.12277234375755515, 0.25]
maxi score, test score, baseline:  0.0361 0.35 0.35
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9354207
Printing some Q and Qe and total Qs values:  [[0.948]
 [1.041]
 [0.948]
 [0.948]
 [0.859]] [[1.635]
 [1.473]
 [1.635]
 [1.635]
 [1.807]] [[0.941]
 [1.006]
 [0.941]
 [0.941]
 [0.881]]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5262184635410618, 0.10531413814515805, 0.12285181836998738, 0.24561557994379266]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.006923523323611303
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5295630086798575, 0.10598269249825526, 0.12292590514551936, 0.241528393676368]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5295630086798575, 0.10598269249825526, 0.12292590514551936, 0.241528393676368]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5295630086798575, 0.10598269249825526, 0.12292590514551936, 0.241528393676368]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5295630086798575, 0.10598269249825526, 0.12292590514551936, 0.241528393676368]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5295630086798575, 0.10598269249825526, 0.12292590514551936, 0.241528393676368]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5295630086798575, 0.10598269249825526, 0.12292590514551936, 0.241528393676368]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5295630086798575, 0.10598269249825526, 0.12292590514551936, 0.241528393676368]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5295630086798575, 0.10598269249825526, 0.12292590514551936, 0.241528393676368]
siam score:  -0.93077004
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.532688250108096, 0.10660740936545857, 0.12299513400940616, 0.2377092065170393]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5240968009535554, 0.10488992890694128, 0.13713661137206543, 0.23387665876743793]
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5240968009535554, 0.10488992890694128, 0.13713661137206543, 0.23387665876743793]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5240968009535554, 0.10488992890694128, 0.13713661137206543, 0.23387665876743793]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5240968009535554, 0.10488992890694128, 0.13713661137206543, 0.23387665876743793]
maxi score, test score, baseline:  0.0361 0.35 0.35
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5240968009535554, 0.10488992890694128, 0.13713661137206543, 0.23387665876743793]
siam score:  -0.92782325
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5240968009535554, 0.10488992890694128, 0.13713661137206543, 0.23387665876743793]
first move QE:  -0.007849848845062117
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5240968009535554, 0.10488992890694128, 0.13713661137206543, 0.23387665876743793]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5240968009535554, 0.10488992890694128, 0.13713661137206543, 0.23387665876743793]
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5240968009535554, 0.10488992890694128, 0.13713661137206543, 0.23387665876743793]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5240968009535554, 0.10488992890694128, 0.13713661137206543, 0.23387665876743793]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5272463497892077, 0.10551950785632837, 0.13675853318468978, 0.23047560916977408]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5272463497892077, 0.10551950785632837, 0.13675853318468978, 0.23047560916977408]
start point for exploration sampling:  20031
siam score:  -0.93148255
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5272463497892077, 0.10551950785632837, 0.13675853318468978, 0.23047560916977408]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5302050263718054, 0.10611093240366745, 0.13640336768710587, 0.22728067353742118]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5302050263718054, 0.10611093240366745, 0.13640336768710587, 0.22728067353742118]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5302050263718054, 0.10611093240366745, 0.13640336768710587, 0.22728067353742118]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5302050263718054, 0.10611093240366745, 0.13640336768710587, 0.22728067353742118]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5329896715576596, 0.10666756895131531, 0.13606909326899422, 0.224273666222031]
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.579]
 [0.882]
 [0.579]
 [0.794]] [[1.21 ]
 [1.21 ]
 [1.991]
 [1.21 ]
 [1.639]] [[0.645]
 [0.645]
 [1.28 ]
 [0.645]
 [1.038]]
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5329896715576596, 0.10666756895131531, 0.13606909326899422, 0.224273666222031]
Printing some Q and Qe and total Qs values:  [[0.938]
 [0.938]
 [1.051]
 [0.929]
 [0.936]] [[2.014]
 [2.014]
 [2.132]
 [1.804]
 [2.408]] [[0.849]
 [0.849]
 [0.976]
 [0.764]
 [0.993]]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5329896715576596, 0.10666756895131531, 0.13606909326899422, 0.224273666222031]
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5329896715576596, 0.10666756895131531, 0.13606909326899422, 0.224273666222031]
maxi score, test score, baseline:  0.0361 0.35 0.35
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5356152016240521, 0.10719239918797403, 0.13575391935037923, 0.22143847983759482]
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.   ]
 [0.087]
 [0.087]
 [0.101]] [[-0.126]
 [-0.048]
 [ 0.323]
 [ 0.234]
 [ 0.587]] [[0.016]
 [0.006]
 [0.245]
 [0.202]
 [0.38 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5356152016240521, 0.10719239918797403, 0.13575391935037923, 0.22143847983759482]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.93429154
maxi score, test score, baseline:  0.0361 0.35 0.35
siam score:  -0.9339912
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5280715557862568, 0.10568438244004388, 0.1338435273297914, 0.2324005344439078]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5409341168370224, 0.10826286615632233, 0.12318256445565681, 0.22762045255099828]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.5409341168370224, 0.10826286615632233, 0.12318256445565681, 0.22762045255099828]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.543367359403677, 0.1087490491760074, 0.12323632618359637, 0.2246472652367193]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.543367359403677, 0.1087490491760074, 0.12323632618359637, 0.2246472652367193]
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.543367359403677, 0.1087490491760074, 0.12323632618359637, 0.2246472652367193]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.543367359403677, 0.1087490491760074, 0.12323632618359637, 0.2246472652367193]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.543367359403677, 0.1087490491760074, 0.12323632618359637, 0.2246472652367193]
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.543367359403677, 0.1087490491760074, 0.12323632618359637, 0.2246472652367193]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.92826277
maxi score, test score, baseline:  0.0361 0.35 0.35
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.547833888946913, 0.1096415006112249, 0.12333501274671514, 0.21918959769514693]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.547833888946913, 0.1096415006112249, 0.12333501274671514, 0.21918959769514693]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.547833888946913, 0.1096415006112249, 0.12333501274671514, 0.21918959769514693]
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.342]
 [1.016]
 [0.154]
 [0.436]] [[1.235]
 [0.754]
 [2.529]
 [0.319]
 [1.342]] [[0.225]
 [0.298]
 [1.221]
 [0.05 ]
 [0.481]]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.547833888946913, 0.1096415006112249, 0.12333501274671514, 0.21918959769514693]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.549888505357582, 0.11005203083312837, 0.1233804088490209, 0.21667905496026865]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.549888505357582, 0.11005203083312837, 0.1233804088490209, 0.21667905496026865]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.551836395908505, 0.11044123630036864, 0.12342344687707854, 0.21429892091404779]
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.052]
 [0.052]
 [0.052]
 [0.055]] [[-0.573]
 [-0.573]
 [-0.573]
 [-0.573]
 [-0.64 ]] [[0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.069]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.5536856658089594, 0.11081073650422697, 0.12346430591293363, 0.21203929177388012]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.5536856658089594, 0.11081073650422697, 0.12346430591293363, 0.21203929177388012]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.5536856658089594, 0.11081073650422697, 0.12346430591293363, 0.21203929177388012]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.5536856658089594, 0.11081073650422697, 0.12346430591293363, 0.21203929177388012]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.001]
 [0.08 ]
 [0.054]
 [0.058]] [[-0.702]
 [-0.491]
 [-0.786]
 [-0.86 ]
 [-0.862]] [[0.049]
 [0.027]
 [0.057]
 [0.019]
 [0.022]]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.5554436198069163, 0.11116199099685624, 0.12350314735269124, 0.20989124184353627]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.0381 0.35 0.35
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.0381 0.35 0.35
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
siam score:  -0.93169737
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
from probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.930045
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]] [[1.796]
 [1.796]
 [1.796]
 [1.796]
 [1.796]] [[0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
siam score:  -0.92933637
actor:  0 policy actor:  0  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0441 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.0441 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
line 256 mcts: sample exp_bonus 2.1031279867560433
maxi score, test score, baseline:  0.0441 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.0441 0.35 0.35
maxi score, test score, baseline:  0.0441 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.0441 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.0441 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.0441 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]] [[3.279]
 [3.279]
 [3.279]
 [3.279]
 [3.279]] [[1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.063]]
using another actor
maxi score, test score, baseline:  0.0441 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
maxi score, test score, baseline:  0.0441 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0461 0.35 0.35
probs:  [0.5571168586239084, 0.11149631865980603, 0.1235401170372142, 0.2078467056790714]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.048100000000000004 0.35 0.35
maxi score, test score, baseline:  0.048100000000000004 0.35 0.35
probs:  [0.5694953506079722, 0.11397722696888311, 0.11397722696888311, 0.20255019545426156]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.048100000000000004 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.6  ]
 [0.632]
 [0.6  ]
 [0.551]] [[1.05 ]
 [1.05 ]
 [1.908]
 [1.05 ]
 [1.249]] [[0.54 ]
 [0.54 ]
 [0.856]
 [0.54 ]
 [0.584]]
maxi score, test score, baseline:  0.048100000000000004 0.35 0.35
probs:  [0.5694953506079722, 0.11397722696888311, 0.11397722696888311, 0.20255019545426156]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.048100000000000004 0.35 0.35
probs:  [0.5694953506079722, 0.11397722696888311, 0.11397722696888311, 0.20255019545426156]
maxi score, test score, baseline:  0.048100000000000004 0.35 0.35
maxi score, test score, baseline:  0.048100000000000004 0.35 0.35
probs:  [0.5694953506079722, 0.11397722696888311, 0.11397722696888311, 0.20255019545426156]
maxi score, test score, baseline:  0.048100000000000004 0.35 0.35
probs:  [0.5694953506079722, 0.11397722696888311, 0.11397722696888311, 0.20255019545426156]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.048100000000000004 0.35 0.35
probs:  [0.5694953506079722, 0.11397722696888311, 0.11397722696888311, 0.20255019545426156]
maxi score, test score, baseline:  0.048100000000000004 0.35 0.35
probs:  [0.5694953506079722, 0.11397722696888311, 0.11397722696888311, 0.20255019545426156]
siam score:  -0.9295358
maxi score, test score, baseline:  0.048100000000000004 0.35 0.35
probs:  [0.5694953506079722, 0.11397722696888311, 0.11397722696888311, 0.20255019545426156]
maxi score, test score, baseline:  0.048100000000000004 0.35 0.35
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.048100000000000004 0.35 0.35
probs:  [0.5623770639140773, 0.12504917443436905, 0.11255409187780595, 0.2000196697737476]
maxi score, test score, baseline:  0.048100000000000004 0.35 0.35
probs:  [0.5623770639140773, 0.12504917443436905, 0.11255409187780595, 0.2000196697737476]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.5623770639140773, 0.12504917443436905, 0.11255409187780595, 0.2000196697737476]
maxi score, test score, baseline:  0.050100000000000006 0.35 0.35
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0521 0.35 0.35
probs:  [0.5639015704217679, 0.1250488894437623, 0.11285853719437326, 0.19819100294009656]
actor:  0 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0541 0.35 0.35
probs:  [0.5639015704217679, 0.1250488894437623, 0.11285853719437326, 0.1981910029400966]
maxi score, test score, baseline:  0.0541 0.35 0.35
probs:  [0.5639015704217679, 0.1250488894437623, 0.11285853719437326, 0.1981910029400966]
maxi score, test score, baseline:  0.0541 0.35 0.35
probs:  [0.5639015704217679, 0.1250488894437623, 0.11285853719437326, 0.1981910029400966]
maxi score, test score, baseline:  0.0541 0.35 0.35
probs:  [0.5639015704217679, 0.1250488894437623, 0.11285853719437326, 0.1981910029400966]
maxi score, test score, baseline:  0.0541 0.35 0.35
probs:  [0.5639015704217679, 0.1250488894437623, 0.11285853719437326, 0.1981910029400966]
from probs:  [0.5639015704217679, 0.1250488894437623, 0.11285853719437326, 0.1981910029400966]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.5639015704217679, 0.1250488894437623, 0.11285853719437326, 0.1981910029400966]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5639015704217679, 0.1250488894437623, 0.11285853719437326, 0.1981910029400966]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5653534878468955, 0.12504861802292816, 0.1131484864060642, 0.1964494077241121]
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5653534878468955, 0.12504861802292816, 0.1131484864060642, 0.1964494077241121]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5667378801035101, 0.1250483592252207, 0.11342495078105523, 0.19478880989021383]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5680593507889754, 0.12504811219004539, 0.1136888496618677, 0.19320368735911153]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.012]
 [0.018]
 [0.018]
 [0.023]] [[1.23 ]
 [0.202]
 [1.23 ]
 [1.23 ]
 [0.04 ]] [[0.018]
 [0.012]
 [0.018]
 [0.018]
 [0.023]]
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.569322094326015, 0.1250478761332985, 0.11394102067848061, 0.191689008862206]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.5705299404346009, 0.12504765033905388, 0.1141822286294064, 0.1902401805969388]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.571686392927031, 0.1250474341523103, 0.11441317322910265, 0.18885299969155608]
Printing some Q and Qe and total Qs values:  [[0.994]
 [1.142]
 [0.994]
 [0.994]
 [1.091]] [[2.362]
 [2.378]
 [2.362]
 [2.362]
 [2.696]] [[1.003]
 [1.079]
 [1.003]
 [1.003]
 [1.155]]
siam score:  -0.9353074
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.571686392927031, 0.1250474341523103, 0.11441317322910265, 0.18885299969155608]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5727946636539986, 0.12504722697264567, 0.11463449588703283, 0.18752361348632285]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5748782225701575, 0.1250468374730163, 0.11505058447085761, 0.1850243554859685]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9398741
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5758587253783339, 0.12504665417823285, 0.11524639176083935, 0.1838482286825939]
line 256 mcts: sample exp_bonus 2.2083203477373377
Printing some Q and Qe and total Qs values:  [[1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.155]] [[2.147]
 [2.147]
 [2.147]
 [2.147]
 [2.147]] [[0.935]
 [0.935]
 [0.935]
 [0.935]
 [0.935]]
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5702680654618968, 0.133538885286583, 0.11412869950101351, 0.18206434975050675]
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5702680654618968, 0.133538885286583, 0.11412869950101351, 0.18206434975050675]
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5702680654618968, 0.133538885286583, 0.11412869950101351, 0.18206434975050675]
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5702680654618968, 0.133538885286583, 0.11412869950101351, 0.18206434975050675]
siam score:  -0.9384101
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5702680654618968, 0.133538885286583, 0.11412869950101351, 0.18206434975050675]
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5702680654618968, 0.133538885286583, 0.11412869950101351, 0.18206434975050675]
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5702680654618968, 0.133538885286583, 0.11412869950101351, 0.18206434975050675]
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5702680654618968, 0.133538885286583, 0.11412869950101351, 0.18206434975050675]
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5702680654618968, 0.133538885286583, 0.11412869950101351, 0.18206434975050675]
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5702680654618968, 0.133538885286583, 0.11412869950101351, 0.18206434975050675]
maxi score, test score, baseline:  0.058100000000000006 0.35 0.35
probs:  [0.5702680654618968, 0.133538885286583, 0.11412869950101351, 0.18206434975050675]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.5702680654618968, 0.133538885286583, 0.11412869950101351, 0.18206434975050675]
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.011]
 [1.025]
 [0.906]
 [0.987]] [[2.713]
 [1.134]
 [2.45 ]
 [1.754]
 [3.237]] [[0.857]
 [0.148]
 [0.957]
 [0.731]
 [1.129]]
maxi score, test score, baseline:  0.0601 0.35 0.35
maxi score, test score, baseline:  0.0601 0.35 0.35
probs:  [0.5702680654618968, 0.133538885286583, 0.11412869950101351, 0.18206434975050675]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9414032
line 256 mcts: sample exp_bonus 0.8235449179778522
maxi score, test score, baseline:  0.0601 0.35 0.35
probs:  [0.5713083591886924, 0.13337696592410425, 0.11433647056477433, 0.1809782043224291]
maxi score, test score, baseline:  0.0601 0.35 0.35
probs:  [0.5713083591886924, 0.13337696592410425, 0.11433647056477433, 0.1809782043224291]
first move QE:  -0.014328276055758826
maxi score, test score, baseline:  0.0601 0.35 0.35
probs:  [0.5713083591886924, 0.13337696592410425, 0.11433647056477433, 0.1809782043224291]
from probs:  [0.5713083591886924, 0.13337696592410425, 0.11433647056477433, 0.1809782043224291]
maxi score, test score, baseline:  0.0601 0.35 0.35
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.5713083591886924, 0.13337696592410425, 0.11433647056477433, 0.1809782043224291]
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.5713083591886924, 0.13337696592410425, 0.11433647056477433, 0.1809782043224291]
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.5713083591886924, 0.13337696592410425, 0.11433647056477433, 0.1809782043224291]
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.5713083591886924, 0.13337696592410425, 0.11433647056477433, 0.1809782043224291]
siam score:  -0.93902206
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.5713083591886924, 0.13337696592410425, 0.11433647056477433, 0.1809782043224291]
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.5713083591886924, 0.13337696592410425, 0.11433647056477433, 0.1809782043224291]
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.5713083591886924, 0.13337696592410425, 0.11433647056477433, 0.1809782043224291]
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.5713083591886924, 0.13337696592410425, 0.11433647056477433, 0.1809782043224291]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.288]
 [0.221]
 [0.221]
 [0.201]] [[2.948]
 [1.289]
 [2.948]
 [2.948]
 [3.059]] [[0.461]
 [0.179]
 [0.461]
 [0.461]
 [0.474]]
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.565918866834319, 0.14155023974344275, 0.11325899793738434, 0.17927189548485398]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.565918866834319, 0.14155023974344275, 0.11325899793738434, 0.17927189548485398]
siam score:  -0.93306625
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.565918866834319, 0.14155023974344275, 0.11325899793738434, 0.17927189548485398]
maxi score, test score, baseline:  0.0621 0.35 0.35
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.565918866834319, 0.14155023974344275, 0.11325899793738434, 0.17927189548485398]
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.565918866834319, 0.14155023974344275, 0.11325899793738434, 0.17927189548485398]
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.565918866834319, 0.14155023974344275, 0.11325899793738434, 0.17927189548485398]
from probs:  [0.565918866834319, 0.14155023974344275, 0.11325899793738434, 0.17927189548485398]
maxi score, test score, baseline:  0.0621 0.35 0.35
probs:  [0.565918866834319, 0.14155023974344275, 0.11325899793738434, 0.17927189548485398]
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.754]
 [0.644]
 [0.433]
 [0.68 ]] [[2.695]
 [2.633]
 [3.228]
 [2.588]
 [2.738]] [[0.651]
 [0.754]
 [0.644]
 [0.433]
 [0.68 ]]
actor:  0 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0641 0.35 0.35
probs:  [0.5670107974756645, 0.14124447093900555, 0.11347710181704952, 0.17826762976828026]
Printing some Q and Qe and total Qs values:  [[1.064]
 [1.152]
 [1.041]
 [1.041]
 [1.068]] [[0.74 ]
 [0.403]
 [0.471]
 [0.471]
 [0.594]] [[0.773]
 [0.806]
 [0.706]
 [0.706]
 [0.754]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.   ]
 [0.1  ]
 [0.034]
 [0.031]] [[-0.142]
 [-0.232]
 [-0.383]
 [-0.146]
 [-0.104]] [[0.045]
 [0.   ]
 [0.1  ]
 [0.034]
 [0.031]]
maxi score, test score, baseline:  0.0641 0.35 0.35
maxi score, test score, baseline:  0.0641 0.35 0.35
probs:  [0.5670107974756645, 0.14124447093900555, 0.11347710181704952, 0.17826762976828026]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0661 0.35 0.35
probs:  [0.5680630244652249, 0.14094982018335142, 0.11368727522918927, 0.17729988012223427]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.027]
 [0.036]
 [0.036]
 [0.025]] [[0.024]
 [0.018]
 [0.   ]
 [0.004]
 [0.037]] [[0.036]
 [0.027]
 [0.036]
 [0.036]
 [0.025]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0901 0.35 0.35
probs:  [0.5680630244652249, 0.14094982018335142, 0.11368727522918927, 0.17729988012223427]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0961 0.8 0.8
siam score:  -0.93187606
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0961 0.8 0.8
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5690699401308766, 0.14066834219291635, 0.11389324232179385, 0.17636847535441302]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5690699401308766, 0.14066834219291635, 0.11389324232179385, 0.17636847535441302]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5690699401308766, 0.14066834219291635, 0.11389324232179385, 0.17636847535441302]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5700491034240869, 0.14039414266298392, 0.11408873690210006, 0.17546801701082904]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5700491034240869, 0.14039414266298392, 0.11408873690210006, 0.17546801701082904]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5700491034240869, 0.14039414266298392, 0.11408873690210006, 0.17546801701082904]
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]] [[0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]] [[0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5700491034240869, 0.14039414266298392, 0.11408873690210006, 0.17546801701082904]
maxi score, test score, baseline:  0.0961 0.8 0.8
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5700491034240869, 0.14039414266298392, 0.11408873690210006, 0.17546801701082904]
maxi score, test score, baseline:  0.0961 0.8 0.8
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
using explorer policy with actor:  1
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5700491034240869, 0.14039414266298392, 0.11408873690210006, 0.17546801701082904]
maxi score, test score, baseline:  0.0961 0.8 0.8
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5700491034240869, 0.14039414266298392, 0.11408873690210006, 0.17546801701082904]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5700491034240869, 0.14039414266298392, 0.11408873690210006, 0.17546801701082904]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5700491034240869, 0.14039414266298392, 0.11408873690210006, 0.17546801701082904]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5700491034240869, 0.14039414266298392, 0.11408873690210006, 0.17546801701082904]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5700491034240869, 0.14039414266298392, 0.11408873690210006, 0.17546801701082904]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5700491034240869, 0.14039414266298392, 0.11408873690210006, 0.17546801701082904]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5709945056081404, 0.14012939740929428, 0.11427749091736353, 0.17459860606520197]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5709945056081404, 0.14012939740929428, 0.11427749091736353, 0.17459860606520197]
Printing some Q and Qe and total Qs values:  [[1.083]
 [1.083]
 [1.083]
 [1.083]
 [1.083]] [[1.57]
 [1.57]
 [1.57]
 [1.57]
 [1.57]] [[0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5719078631925467, 0.13987362574991827, 0.11445984707682247, 0.17375866398071263]
line 256 mcts: sample exp_bonus 2.6825650972742863
using another actor
from probs:  [0.5719078631925467, 0.13987362574991827, 0.11445984707682247, 0.17375866398071263]
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.   ]
 [0.153]
 [0.066]
 [0.07 ]] [[ 0.002]
 [-0.005]
 [ 0.26 ]
 [-0.11 ]
 [-0.106]] [[0.07 ]
 [0.   ]
 [0.153]
 [0.066]
 [0.07 ]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.752]
 [0.58 ]
 [0.58 ]
 [0.645]] [[0.742]
 [0.817]
 [0.742]
 [0.742]
 [0.856]] [[0.58 ]
 [0.752]
 [0.58 ]
 [0.58 ]
 [0.645]]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.5719078631925467, 0.13987362574991827, 0.11445984707682247, 0.17375866398071263]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0981 0.8 0.8
using another actor
from probs:  [0.572790778263144, 0.13962637904550557, 0.11463612524448798, 0.17294671744686238]
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.572790778263144, 0.13962637904550557, 0.11463612524448798, 0.17294671744686238]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.572790778263144, 0.13962637904550557, 0.11463612524448798, 0.17294671744686238]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.165]
 [1.165]
 [1.331]
 [1.165]
 [1.308]] [[1.229]
 [1.229]
 [1.014]
 [1.229]
 [1.491]] [[0.974]
 [0.974]
 [1.069]
 [0.974]
 [1.204]]
start point for exploration sampling:  20031
using another actor
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.5736447478602575, 0.1393872380730765, 0.11480662431153797, 0.1721613897551279]
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.5736447478602575, 0.1393872380730765, 0.11480662431153797, 0.17216138975512787]
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.5736447478602575, 0.1393872380730765, 0.11480662431153797, 0.17216138975512787]
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.5736447478602575, 0.1393872380730765, 0.11480662431153797, 0.17216138975512787]
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.5736447478602575, 0.1393872380730765, 0.11480662431153797, 0.17216138975512787]
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.5689812105844874, 0.14638189974644036, 0.11387426045120601, 0.17076262921786617]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.5698756323541407, 0.14604041948490426, 0.1140528562494902, 0.1700310919114648]
actor:  1 policy actor:  1  step number:  86 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.5783788740446033, 0.14054037531846555, 0.11575706406981624, 0.16532368656711482]
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.5783788740446033, 0.14054037531846555, 0.11575706406981624, 0.16532368656711482]
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.5783788740446033, 0.14054037531846555, 0.11575706406981624, 0.16532368656711482]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9254662
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.5798669976265569, 0.140044334124481, 0.11605400702436777, 0.16403466122459426]
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.5798669976265569, 0.140044334124481, 0.11605400702436777, 0.16403466122459426]
using explorer policy with actor:  1
siam score:  -0.923717
line 256 mcts: sample exp_bonus 0.5834572802021192
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1021 0.8 0.8
probs:  [0.5798669976265569, 0.140044334124481, 0.11605400702436777, 0.16403466122459426]
maxi score, test score, baseline:  0.1021 0.8 0.8
probs:  [0.5798669976265569, 0.140044334124481, 0.11605400702436777, 0.16403466122459426]
siam score:  -0.92174685
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.5798669976265569, 0.140044334124481, 0.11605400702436777, 0.16403466122459426]
siam score:  -0.92184687
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.5805759104611534, 0.13980802984628218, 0.11619546481334266, 0.16342059487922173]
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.5805759104611534, 0.13980802984628218, 0.11619546481334266, 0.16342059487922173]
maxi score, test score, baseline:  0.1041 0.8 0.8
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.5805759104611534, 0.13980802984628218, 0.11619546481334266, 0.16342059487922173]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1041 0.8 0.8
Printing some Q and Qe and total Qs values:  [[1.383]
 [1.373]
 [1.373]
 [1.373]
 [1.383]] [[0.395]
 [0.133]
 [0.133]
 [0.133]
 [0.546]] [[0.98 ]
 [0.926]
 [0.926]
 [0.926]
 [1.006]]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.5819288039422459, 0.13935706535258469, 0.11646542370139533, 0.16224870700377406]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.582574737247168, 0.13914175425094402, 0.11659431443757666, 0.16168919406431134]
maxi score, test score, baseline:  0.1041 0.8 0.8
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.578227407883724, 0.1381042927669123, 0.11572515132029473, 0.167943148029069]
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.578227407883724, 0.1381042927669123, 0.11572515132029473, 0.167943148029069]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1041 0.8 0.8
Printing some Q and Qe and total Qs values:  [[0.753]
 [1.12 ]
 [1.105]
 [0.974]
 [1.05 ]] [[1.152]
 [1.139]
 [1.061]
 [1.439]
 [1.33 ]] [[0.706]
 [1.068]
 [1.028]
 [1.022]
 [1.062]]
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.578913519631911, 0.13791215252767283, 0.11586208417246094, 0.16731224366795536]
Printing some Q and Qe and total Qs values:  [[1.251]
 [1.251]
 [1.251]
 [1.251]
 [1.231]] [[0.781]
 [0.781]
 [0.781]
 [0.781]
 [1.064]] [[0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.92 ]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.5795797459845597, 0.1377255810382269, 0.11599504833594824, 0.16669962464126512]
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.5795797459845597, 0.1377255810382269, 0.11599504833594824, 0.16669962464126512]
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.5795797459845597, 0.1377255810382269, 0.11599504833594824, 0.16669962464126512]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.5795797459845597, 0.1377255810382269, 0.11599504833594824, 0.16669962464126512]
maxi score, test score, baseline:  0.1061 0.8 0.8
probs:  [0.5795797459845597, 0.1377255810382269, 0.11599504833594824, 0.16669962464126512]
maxi score, test score, baseline:  0.1061 0.8 0.8
probs:  [0.5795797459845597, 0.1377255810382269, 0.11599504833594824, 0.16669962464126512]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1081 0.8 0.8
probs:  [0.583810031082639, 0.13872998963912037, 0.11684080727304567, 0.16061917200519502]
maxi score, test score, baseline:  0.1081 0.8 0.8
probs:  [0.583810031082639, 0.13872998963912037, 0.11684080727304567, 0.16061917200519502]
maxi score, test score, baseline:  0.1081 0.8 0.8
probs:  [0.583810031082639, 0.13872998963912037, 0.11684080727304567, 0.16061917200519502]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1081 0.8 0.8
probs:  [0.584401019455667, 0.13853299351477763, 0.1169587341950572, 0.16010725283449811]
maxi score, test score, baseline:  0.1081 0.8 0.8
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1101 0.8 0.8
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.01884729281992476
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1101 0.8 0.8
probs:  [0.5849752437343678, 0.13834158542187744, 0.11707331597842548, 0.15960985486532933]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1101 0.8 0.8
siam score:  -0.91508055
maxi score, test score, baseline:  0.1101 0.8 0.8
probs:  [0.5855334072438866, 0.13815553091870444, 0.11718469296596153, 0.15912636887144735]
Printing some Q and Qe and total Qs values:  [[0.826]
 [1.001]
 [0.884]
 [0.884]
 [0.775]] [[2.044]
 [1.986]
 [1.563]
 [1.563]
 [2.331]] [[0.86 ]
 [1.017]
 [0.758]
 [0.758]
 [0.905]]
maxi score, test score, baseline:  0.1101 0.8 0.8
probs:  [0.5855334072438866, 0.13815553091870444, 0.11718469296596153, 0.15912636887144735]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.2682897397337878
Printing some Q and Qe and total Qs values:  [[1.267]
 [1.267]
 [1.267]
 [1.267]
 [1.267]] [[1.174]
 [1.174]
 [1.174]
 [1.174]
 [1.174]] [[1.403]
 [1.403]
 [1.403]
 [1.403]
 [1.403]]
maxi score, test score, baseline:  0.1101 0.8 0.8
probs:  [0.5855334072438866, 0.13815553091870444, 0.11718469296596153, 0.15912636887144735]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1101 0.8 0.8
probs:  [0.5860761745086739, 0.13797460849710874, 0.11729299775811339, 0.15865621923610404]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1101 0.8 0.8
probs:  [0.5866041738917563, 0.13779860870274793, 0.11739835573961119, 0.15819886166588468]
maxi score, test score, baseline:  0.1101 0.8 0.8
maxi score, test score, baseline:  0.1101 0.8 0.8
probs:  [0.5866041738917563, 0.13779860870274793, 0.11739835573961119, 0.15819886166588468]
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1121 0.8 0.8
probs:  [0.5866041738917563, 0.13779860870274793, 0.11739835573961119, 0.15819886166588468]
maxi score, test score, baseline:  0.1121 0.8 0.8
probs:  [0.5866041738917563, 0.13779860870274793, 0.11739835573961119, 0.15819886166588468]
maxi score, test score, baseline:  0.1121 0.8 0.8
probs:  [0.5866041738917563, 0.13779860870274793, 0.11739835573961119, 0.15819886166588468]
Printing some Q and Qe and total Qs values:  [[0.934]
 [1.295]
 [0.87 ]
 [0.87 ]
 [1.243]] [[1.028]
 [0.756]
 [1.551]
 [1.551]
 [0.875]] [[0.608]
 [0.925]
 [0.632]
 [0.632]
 [0.892]]
maxi score, test score, baseline:  0.1121 0.8 0.8
maxi score, test score, baseline:  0.1121 0.8 0.8
probs:  [0.5866041738917563, 0.13779860870274793, 0.11739835573961119, 0.15819886166588468]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1121 0.8 0.8
maxi score, test score, baseline:  0.1121 0.8 0.8
probs:  [0.5871180000214682, 0.13762733332617724, 0.11750088556370153, 0.15775378108865293]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.1121 0.8 0.8
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.9  ]
 [1.083]
 [0.9  ]
 [0.56 ]] [[0.811]
 [0.76 ]
 [0.603]
 [0.76 ]
 [0.402]] [[0.073]
 [0.751]
 [0.907]
 [0.751]
 [0.35 ]]
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5871180000214682, 0.13762733332617724, 0.11750088556370153, 0.15775378108865293]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5876182160253466, 0.13746059465821786, 0.11760069959790334, 0.15732048971853235]
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5876182160253466, 0.13746059465821786, 0.11760069959790334, 0.15732048971853235]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.]
 [ 0.]
 [-0.]
 [-0.]
 [ 0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5881053555888325, 0.13729821480372248, 0.11769790433480466, 0.1568985252726403]
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5881053555888325, 0.13729821480372248, 0.11769790433480466, 0.1568985252726403]
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5881053555888325, 0.13729821480372248, 0.11769790433480466, 0.1568985252726403]
siam score:  -0.9207866
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5881053555888325, 0.13729821480372248, 0.11769790433480466, 0.1568985252726403]
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5881053555888325, 0.13729821480372248, 0.11769790433480466, 0.1568985252726403]
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5881053555888325, 0.13729821480372248, 0.11769790433480466, 0.1568985252726403]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5881053555888325, 0.13729821480372248, 0.11769790433480466, 0.1568985252726403]
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5881053555888325, 0.13729821480372248, 0.11769790433480466, 0.1568985252726403]
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5881053555888325, 0.13729821480372248, 0.11769790433480466, 0.1568985252726403]
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5881053555888325, 0.13729821480372248, 0.11769790433480466, 0.1568985252726403]
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5881053555888325, 0.13729821480372248, 0.11769790433480466, 0.1568985252726403]
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5881053555888325, 0.13729821480372248, 0.11769790433480466, 0.1568985252726403]
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5881053555888325, 0.13729821480372248, 0.11769790433480466, 0.1568985252726403]
Printing some Q and Qe and total Qs values:  [[1.312]
 [1.427]
 [1.371]
 [1.286]
 [1.364]] [[0.673]
 [0.183]
 [0.23 ]
 [0.378]
 [0.581]] [[0.941]
 [0.975]
 [0.927]
 [0.866]
 [0.978]]
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
probs:  [0.5881053555888325, 0.13729821480372248, 0.11769790433480466, 0.1568985252726403]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.2079071613813929
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 0.8 0.8
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.035993474650382995
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.5894932497425509, 0.13683558341914967, 0.11797484732234127, 0.15569631951595803]
siam score:  -0.92591995
maxi score, test score, baseline:  0.11610000000000001 0.8 0.8
maxi score, test score, baseline:  0.11610000000000001 0.8 0.8
probs:  [0.5894932497425509, 0.13683558341914967, 0.11797484732234127, 0.15569631951595803]
maxi score, test score, baseline:  0.11610000000000001 0.8 0.8
maxi score, test score, baseline:  0.11610000000000001 0.8 0.8
probs:  [0.5894932497425509, 0.13683558341914967, 0.11797484732234127, 0.15569631951595803]
maxi score, test score, baseline:  0.11610000000000001 0.8 0.8
probs:  [0.5894932497425509, 0.13683558341914967, 0.11797484732234127, 0.15569631951595803]
maxi score, test score, baseline:  0.11610000000000001 0.8 0.8
probs:  [0.5894932497425509, 0.13683558341914967, 0.11797484732234127, 0.15569631951595803]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 0.8 0.8
probs:  [0.5894932497425509, 0.13683558341914967, 0.11797484732234127, 0.15569631951595803]
maxi score, test score, baseline:  0.1181 0.8 0.8
maxi score, test score, baseline:  0.1181 0.8 0.8
probs:  [0.5894932497425509, 0.13683558341914967, 0.11797484732234127, 0.15569631951595803]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 0.8 0.8
probs:  [0.5899328950734255, 0.13668903497552484, 0.11806257497150152, 0.15531549497954814]
Printing some Q and Qe and total Qs values:  [[0.94]
 [0.94]
 [0.94]
 [0.94]
 [0.94]] [[2.059]
 [2.059]
 [2.059]
 [2.059]
 [2.059]] [[0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]]
Printing some Q and Qe and total Qs values:  [[0.374]
 [1.255]
 [1.103]
 [0.962]
 [1.116]] [[1.624]
 [2.594]
 [2.581]
 [2.114]
 [2.473]] [[0.295]
 [0.845]
 [0.787]
 [0.624]
 [0.766]]
maxi score, test score, baseline:  0.1181 0.8 0.8
probs:  [0.5899328950734255, 0.13668903497552484, 0.11806257497150152, 0.15531549497954814]
maxi score, test score, baseline:  0.1181 0.8 0.8
probs:  [0.5899328950734255, 0.13668903497552484, 0.11806257497150152, 0.15531549497954814]
maxi score, test score, baseline:  0.1181 0.8 0.8
maxi score, test score, baseline:  0.1181 0.8 0.8
probs:  [0.5899328950734255, 0.13668903497552484, 0.11806257497150152, 0.15531549497954814]
maxi score, test score, baseline:  0.1181 0.8 0.8
maxi score, test score, baseline:  0.1181 0.8 0.8
probs:  [0.5899328950734255, 0.13668903497552484, 0.11806257497150152, 0.15531549497954814]
maxi score, test score, baseline:  0.1181 0.8 0.8
probs:  [0.5899328950734255, 0.13668903497552484, 0.11806257497150152, 0.15531549497954814]
maxi score, test score, baseline:  0.1181 0.8 0.8
probs:  [0.5899328950734255, 0.13668903497552484, 0.11806257497150152, 0.15531549497954814]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 0.8 0.8
probs:  [0.590361752415342, 0.13654608252821937, 0.1181481499652279, 0.15494401509121083]
first move QE:  -0.021035188877059043
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1201 0.8 0.8
using another actor
maxi score, test score, baseline:  0.1201 0.8 0.8
probs:  [0.590361752415342, 0.13654608252821937, 0.1181481499652279, 0.15494401509121083]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.590361752415342, 0.13654608252821937, 0.1181481499652279, 0.15494401509121083]
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.590361752415342, 0.13654608252821937, 0.1181481499652279, 0.15494401509121083]
siam score:  -0.9258331
siam score:  -0.9262542
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.5907802140281494, 0.1364065953239502, 0.11823165057578225, 0.1545815400721182]
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.5907802140281494, 0.1364065953239502, 0.11823165057578225, 0.1545815400721182]
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.5907802140281494, 0.1364065953239502, 0.11823165057578225, 0.1545815400721182]
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.5907802140281494, 0.1364065953239502, 0.11823165057578225, 0.1545815400721182]
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.5907802140281494, 0.1364065953239502, 0.11823165057578225, 0.1545815400721182]
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.5907802140281494, 0.1364065953239502, 0.11823165057578225, 0.1545815400721182]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.5907802140281494, 0.1364065953239502, 0.11823165057578225, 0.1545815400721182]
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.5911886533822569, 0.136270448872581, 0.11831315132614642, 0.15422774641901554]
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.5911886533822569, 0.136270448872581, 0.11831315132614642, 0.15422774641901554]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.5911886533822569, 0.136270448872581, 0.11831315132614642, 0.15422774641901554]
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.5911886533822569, 0.136270448872581, 0.11831315132614642, 0.15422774641901554]
siam score:  -0.9239107
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.5911886533822569, 0.136270448872581, 0.11831315132614642, 0.15422774641901554]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.5911886533822569, 0.136270448872581, 0.11831315132614642, 0.15422774641901554]
Printing some Q and Qe and total Qs values:  [[1.   ]
 [0.001]
 [1.   ]
 [1.   ]
 [1.   ]] [[ 0.  ]
 [-0.18]
 [ 0.  ]
 [ 0.  ]
 [ 0.  ]] [[1.   ]
 [0.001]
 [1.   ]
 [1.   ]
 [1.   ]]
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.5915874262703502, 0.13613752457654987, 0.11839272321185634, 0.15388232594124338]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.5919768718410983, 0.13600770938630058, 0.1184704339072699, 0.15354498486533125]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1221 0.8 0.8
probs:  [0.5919768718410983, 0.13600770938630058, 0.1184704339072699, 0.15354498486533125]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.5919768718410983, 0.13600770938630058, 0.1184704339072699, 0.15354498486533125]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1241 0.8 0.8
probs:  [0.5885352495106494, 0.14102942612318148, 0.11778237036279353, 0.15265295400337542]
using another actor
maxi score, test score, baseline:  0.1241 0.8 0.8
probs:  [0.5885352495106494, 0.14102942612318148, 0.11778237036279353, 0.15265295400337545]
maxi score, test score, baseline:  0.1241 0.8 0.8
probs:  [0.5885352495106494, 0.14102942612318148, 0.11778237036279353, 0.15265295400337545]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.901]
 [0.644]
 [0.838]
 [0.78 ]] [[1.611]
 [1.908]
 [2.129]
 [1.858]
 [2.022]] [[0.81 ]
 [1.069]
 [0.979]
 [0.99 ]
 [1.032]]
maxi score, test score, baseline:  0.1241 0.8 0.8
probs:  [0.5885352495106494, 0.14102942612318148, 0.11778237036279353, 0.15265295400337545]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.054]
 [1.054]
 [1.054]
 [1.054]
 [1.054]] [[1.961]
 [1.961]
 [1.961]
 [1.961]
 [1.961]] [[0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]]
maxi score, test score, baseline:  0.1241 0.8 0.8
probs:  [0.5889530601716365, 0.1408456246904899, 0.11786575620427724, 0.15233555893359624]
maxi score, test score, baseline:  0.1241 0.8 0.8
maxi score, test score, baseline:  0.1241 0.8 0.8
maxi score, test score, baseline:  0.1241 0.8 0.8
probs:  [0.5889530601716365, 0.1408456246904899, 0.11786575620427724, 0.15233555893359624]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.327]
 [0.377]
 [0.227]
 [0.149]] [[7.734]
 [7.734]
 [1.933]
 [5.667]
 [6.372]] [[1.404]
 [1.404]
 [0.046]
 [0.879]
 [1.018]]
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.06 ]] [[6.356]
 [6.356]
 [6.356]
 [6.356]
 [9.047]] [[0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.935]]
maxi score, test score, baseline:  0.1241 0.8 0.8
maxi score, test score, baseline:  0.1241 0.8 0.8
probs:  [0.5889530601716365, 0.1408456246904899, 0.11786575620427724, 0.15233555893359624]
using another actor
maxi score, test score, baseline:  0.1241 0.8 0.8
probs:  [0.5889530601716366, 0.14084562469048992, 0.11786575620427726, 0.15233555893359624]
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.009]
 [0.105]
 [0.036]
 [0.039]] [[-0.204]
 [ 0.044]
 [-0.39 ]
 [-0.127]
 [ 0.186]] [[0.076]
 [0.184]
 [0.029]
 [0.114]
 [0.283]]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.043]
 [0.03 ]
 [0.093]
 [0.034]] [[0.276]
 [0.335]
 [0.117]
 [0.319]
 [1.096]] [[0.063]
 [0.088]
 [0.006]
 [0.109]
 [0.349]]
maxi score, test score, baseline:  0.1261 0.8 0.8
probs:  [0.5889530601716366, 0.14084562469048992, 0.11786575620427726, 0.15233555893359624]
maxi score, test score, baseline:  0.1261 0.8 0.8
probs:  [0.5889530601716366, 0.14084562469048992, 0.11786575620427726, 0.15233555893359624]
start point for exploration sampling:  20031
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1261 0.8 0.8
probs:  [0.5893613758476934, 0.1406660002499063, 0.11794724705508162, 0.15202537684731865]
maxi score, test score, baseline:  0.1261 0.8 0.8
maxi score, test score, baseline:  0.1261 0.8 0.8
probs:  [0.5893613758476934, 0.1406660002499063, 0.11794724705508162, 0.15202537684731865]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1261 0.8 0.8
probs:  [0.5897605165707662, 0.14049041201438112, 0.11802690678656186, 0.15172216462829075]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1261 0.8 0.8
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1281 0.8 0.8
maxi score, test score, baseline:  0.1281 0.8 0.8
probs:  [0.5901507881502016, 0.14031872545360843, 0.11810479643155444, 0.1514256899646354]
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1301 0.8 0.8
probs:  [0.5905324829541523, 0.14015081195027349, 0.11818097434032816, 0.1511357307552461]
maxi score, test score, baseline:  0.1301 0.8 0.8
maxi score, test score, baseline:  0.1301 0.8 0.8
maxi score, test score, baseline:  0.1301 0.8 0.8
siam score:  -0.91818434
maxi score, test score, baseline:  0.1301 0.8 0.8
siam score:  -0.92038053
maxi score, test score, baseline:  0.1301 0.8 0.8
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1301 0.8 0.8
probs:  [0.5909058806400229, 0.13998654847871767, 0.11825549632636562, 0.15085207455489372]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.49944753343300463
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.1321 0.8 0.8
probs:  [0.5909058806400229, 0.13998654847871767, 0.11825549632636562, 0.15085207455489372]
maxi score, test score, baseline:  0.1321 0.8 0.8
probs:  [0.5909058806400229, 0.13998654847871767, 0.11825549632636562, 0.15085207455489372]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1321 0.8 0.8
probs:  [0.5912712488377967, 0.13982581730433336, 0.11832841580273984, 0.15057451805513009]
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.002]
 [0.185]
 [0.113]
 [0.084]] [[-0.47 ]
 [-0.019]
 [-0.575]
 [-0.722]
 [-0.598]] [[0.064]
 [0.061]
 [0.152]
 [0.055]
 [0.047]]
siam score:  -0.9223662
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1321 0.8 0.8
probs:  [0.5912712488377967, 0.13982581730433336, 0.11832841580273984, 0.15057451805513009]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.5912712488377967, 0.13982581730433336, 0.11832841580273984, 0.15057451805513009]
maxi score, test score, baseline:  0.1321 0.8 0.8
probs:  [0.5916288437897459, 0.1396685057021443, 0.11839978390978659, 0.15030286659832315]
maxi score, test score, baseline:  0.1321 0.8 0.8
probs:  [0.5916288437897459, 0.1396685057021443, 0.11839978390978659, 0.15030286659832315]
maxi score, test score, baseline:  0.1321 0.8 0.8
probs:  [0.5916288437897459, 0.1396685057021443, 0.11839978390978659, 0.15030286659832315]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.1321 0.8 0.8
probs:  [0.5916288437897459, 0.1396685057021443, 0.11839978390978659, 0.15030286659832315]
maxi score, test score, baseline:  0.1321 0.8 0.8
probs:  [0.5916288437897459, 0.1396685057021443, 0.11839978390978659, 0.15030286659832315]
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1341 0.8 0.8
probs:  [0.5916288437897459, 0.1396685057021443, 0.11839978390978659, 0.15030286659832315]
maxi score, test score, baseline:  0.1341 0.8 0.8
probs:  [0.5916288437897459, 0.1396685057021443, 0.11839978390978659, 0.15030286659832315]
maxi score, test score, baseline:  0.1341 0.8 0.8
first move QE:  -0.022347391888676207
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1341 0.8 0.8
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.195]
 [1.387]
 [1.214]
 [1.195]
 [1.256]] [[0.197]
 [0.14 ]
 [1.001]
 [0.197]
 [0.437]] [[0.809]
 [0.991]
 [0.961]
 [0.809]
 [0.909]]
first move QE:  -0.022619478907826555
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1341 0.8 0.8
probs:  [0.598271986064867, 0.13568171449779182, 0.11973032582306509, 0.1463159736142763]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.598271986064867, 0.13568171449779182, 0.11973032582306509, 0.1463159736142763]
maxi score, test score, baseline:  0.1341 0.8 0.8
probs:  [0.5985521276147986, 0.13556967885853782, 0.11978618628730166, 0.14609200723936194]
actor:  0 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1361 0.8 0.8
probs:  [0.5985521276147986, 0.13556967885853782, 0.11978618628730166, 0.14609200723936194]
maxi score, test score, baseline:  0.1361 0.8 0.8
probs:  [0.5985521276147986, 0.13556967885853782, 0.11978618628730166, 0.14609200723936194]
maxi score, test score, baseline:  0.1361 0.8 0.8
probs:  [0.5985521276147986, 0.13556967885853782, 0.11978618628730166, 0.14609200723936194]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1361 0.8 0.8
probs:  [0.5988264333382274, 0.13545997711282087, 0.119840883082751, 0.1458727064662008]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.9153552
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1381 0.8 0.8
probs:  [0.5993582518419086, 0.13524728954097898, 0.11994692814644282, 0.14544753047066974]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1381 0.8 0.8
probs:  [0.599616103847847, 0.13514416804998525, 0.1199983440565767, 0.14524138404559092]
maxi score, test score, baseline:  0.1381 0.8 0.8
maxi score, test score, baseline:  0.1381 0.8 0.8
probs:  [0.599616103847847, 0.13514416804998525, 0.1199983440565767, 0.14524138404559092]
maxi score, test score, baseline:  0.1381 0.8 0.8
probs:  [0.599616103847847, 0.13514416804998525, 0.1199983440565767, 0.14524138404559092]
Printing some Q and Qe and total Qs values:  [[1.226]
 [1.226]
 [1.226]
 [1.226]
 [1.212]] [[4.329]
 [4.329]
 [4.329]
 [4.329]
 [5.577]] [[0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.853]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.1381 0.8 0.8
probs:  [0.599616103847847, 0.13514416804998522, 0.1199983440565767, 0.14524138404559092]
maxi score, test score, baseline:  0.1381 0.8 0.8
probs:  [0.599616103847847, 0.13514416804998522, 0.1199983440565767, 0.14524138404559092]
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1401 0.8 0.8
probs:  [0.599616103847847, 0.13514416804998525, 0.1199983440565767, 0.14524138404559092]
maxi score, test score, baseline:  0.1401 0.8 0.8
probs:  [0.599616103847847, 0.13514416804998525, 0.1199983440565767, 0.14524138404559092]
maxi score, test score, baseline:  0.1401 0.8 0.8
probs:  [0.599616103847847, 0.13514416804998525, 0.1199983440565767, 0.14524138404559092]
maxi score, test score, baseline:  0.1401 0.8 0.8
probs:  [0.599616103847847, 0.13514416804998525, 0.1199983440565767, 0.14524138404559092]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1401 0.8 0.8
probs:  [0.5998687992002999, 0.1350431088341872, 0.12004873172560289, 0.14503936023991004]
siam score:  -0.9182625
actor:  0 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
first move QE:  -0.021706281618591693
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14209999999999998 0.8 0.8
probs:  [0.6001164910558228, 0.13494405064243284, 0.12009812169306933, 0.14484133660867518]
maxi score, test score, baseline:  0.14209999999999998 0.8 0.8
probs:  [0.6001164910558228, 0.13494405064243284, 0.12009812169306933, 0.14484133660867518]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14209999999999998 0.8 0.8
probs:  [0.6003593265652731, 0.1348469346253997, 0.12014654330098265, 0.14464719550834443]
maxi score, test score, baseline:  0.14209999999999998 0.8 0.8
probs:  [0.6003593265652731, 0.1348469346253997, 0.12014654330098265, 0.14464719550834443]
maxi score, test score, baseline:  0.14209999999999998 0.8 0.8
probs:  [0.6003593265652731, 0.1348469346253997, 0.12014654330098265, 0.14464719550834443]
maxi score, test score, baseline:  0.14209999999999998 0.8 0.8
probs:  [0.6003593265652731, 0.1348469346253997, 0.12014654330098265, 0.14464719550834443]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.14409999999999998 0.8 0.8
probs:  [0.6003593265652731, 0.1348469346253997, 0.12014654330098265, 0.14464719550834443]
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14609999999999998 0.8 0.8
probs:  [0.6003593265652731, 0.1348469346253997, 0.12014654330098265, 0.14464719550834443]
maxi score, test score, baseline:  0.14609999999999998 0.8 0.8
probs:  [0.6003593265652731, 0.1348469346253997, 0.12014654330098265, 0.14464719550834443]
maxi score, test score, baseline:  0.14609999999999998 0.8 0.8
probs:  [0.6003593265652731, 0.1348469346253997, 0.12014654330098265, 0.14464719550834443]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14609999999999998 0.8 0.8
probs:  [0.6005974471653287, 0.13475170421900964, 0.12019402475193715, 0.1444568238637246]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14809999999999998 0.8 0.8
probs:  [0.6005974471653287, 0.13475170421900964, 0.12019402475193715, 0.1444568238637246]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14809999999999998 0.8 0.8
maxi score, test score, baseline:  0.14809999999999998 0.8 0.8
probs:  [0.6008309888531863, 0.13465830503456885, 0.12024059316388996, 0.1442701129483548]
maxi score, test score, baseline:  0.14809999999999998 0.8 0.8
maxi score, test score, baseline:  0.14809999999999998 0.8 0.8
probs:  [0.6008309888531863, 0.13465830503456885, 0.12024059316388996, 0.1442701129483548]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.74]
 [0.74]
 [0.74]
 [0.74]
 [0.74]] [[3.151]
 [3.151]
 [3.151]
 [3.151]
 [3.151]] [[0.74]
 [0.74]
 [0.74]
 [0.74]
 [0.74]]
maxi score, test score, baseline:  0.14809999999999998 0.8 0.8
probs:  [0.6010600824455679, 0.13456668475518618, 0.12028627462180712, 0.14408695817743886]
actor:  0 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.792 0.042 0.083]
maxi score, test score, baseline:  0.15009999999999998 0.8 0.8
probs:  [0.6010600824455679, 0.13456668475518618, 0.12028627462180712, 0.14408695817743886]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.15009999999999998 0.8 0.8
probs:  [0.6012848538230637, 0.13447679303805288, 0.12033109422638587, 0.14390725891249753]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.91785914
maxi score, test score, baseline:  0.15009999999999998 0.8 0.8
probs:  [0.6015054241607778, 0.1343885814222026, 0.12037507614004535, 0.14373091827697412]
maxi score, test score, baseline:  0.15009999999999998 0.8 0.8
maxi score, test score, baseline:  0.15009999999999998 0.8 0.8
probs:  [0.6015054241607778, 0.1343885814222026, 0.12037507614004535, 0.14373091827697412]
maxi score, test score, baseline:  0.15009999999999998 0.8 0.8
probs:  [0.6015054241607778, 0.1343885814222026, 0.12037507614004535, 0.14373091827697412]
maxi score, test score, baseline:  0.15009999999999998 0.8 0.8
probs:  [0.6015054241607778, 0.1343885814222026, 0.12037507614004535, 0.14373091827697412]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.15009999999999998 0.8 0.8
probs:  [0.6015054241607778, 0.1343885814222026, 0.12037507614004535, 0.14373091827697412]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.15209999999999999 0.8 0.8
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.1786957154651991
Printing some Q and Qe and total Qs values:  [[1.414]
 [1.493]
 [1.469]
 [1.293]
 [1.428]] [[ 0.084]
 [-0.042]
 [ 0.124]
 [ 0.235]
 [ 0.059]] [[1.402]
 [1.46 ]
 [1.463]
 [1.309]
 [1.412]]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.15209999999999999 0.8 0.8
maxi score, test score, baseline:  0.15209999999999999 0.8 0.8
probs:  [0.601721910146159, 0.1343020032413951, 0.12041824363036248, 0.1435578429820835]
maxi score, test score, baseline:  0.15209999999999999 0.8 0.8
probs:  [0.601721910146159, 0.1343020032413951, 0.12041824363036248, 0.1435578429820835]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.15209999999999999 0.8 0.8
probs:  [0.6019344241848419, 0.1342170135417947, 0.12046061911111682, 0.1433879431622466]
maxi score, test score, baseline:  0.15209999999999999 0.8 0.8
probs:  [0.6019344241848419, 0.1342170135417947, 0.12046061911111682, 0.1433879431622466]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.15209999999999999 0.8 0.8
probs:  [0.6021430745952626, 0.1341335690041394, 0.12050222418109696, 0.14322113221950097]
maxi score, test score, baseline:  0.15209999999999999 0.8 0.8
probs:  [0.6021430745952626, 0.1341335690041394, 0.12050222418109696, 0.14322113221950097]
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9170355
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1541 0.8 0.8
probs:  [0.6023479657927535, 0.13405162787011618, 0.12054307966080934, 0.14305732667632076]
maxi score, test score, baseline:  0.1541 0.8 0.8
probs:  [0.6023479657927535, 0.13405162787011618, 0.12054307966080934, 0.14305732667632076]
maxi score, test score, baseline:  0.1541 0.8 0.8
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1561 0.8 0.8
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1561 0.8 0.8
probs:  [0.602549198463778, 0.13397114987268063, 0.12058320562722072, 0.14289644603632057]
maxi score, test score, baseline:  0.1561 0.8 0.8
probs:  [0.602549198463778, 0.13397114987268063, 0.12058320562722072, 0.14289644603632057]
siam score:  -0.92036474
maxi score, test score, baseline:  0.1561 0.8 0.8
probs:  [0.602549198463778, 0.13397114987268063, 0.12058320562722072, 0.14289644603632057]
maxi score, test score, baseline:  0.1561 0.8 0.8
probs:  [0.602549198463778, 0.13397114987268063, 0.12058320562722072, 0.14289644603632057]
maxi score, test score, baseline:  0.1561 0.8 0.8
probs:  [0.602549198463778, 0.13397114987268063, 0.12058320562722072, 0.14289644603632057]
maxi score, test score, baseline:  0.1561 0.8 0.8
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Printing some Q and Qe and total Qs values:  [[0.843]
 [0.843]
 [0.836]
 [0.843]
 [0.843]] [[0.404]
 [0.404]
 [0.123]
 [0.404]
 [0.404]] [[0.843]
 [0.843]
 [0.836]
 [0.843]
 [0.843]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1581 0.8 0.8
probs:  [0.602549198463778, 0.13397114987268063, 0.12058320562722072, 0.14289644603632057]
actor:  0 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.602549198463778, 0.13397114987268063, 0.12058320562722072, 0.14289644603632057]
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.602549198463778, 0.13397114987268063, 0.12058320562722072, 0.14289644603632057]
maxi score, test score, baseline:  0.1601 0.8 0.8
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.602549198463778, 0.13397114987268063, 0.12058320562722072, 0.14289644603632057]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6027468697309126, 0.13389209617007572, 0.12062262144665582, 0.14273841265235568]
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6027468697309126, 0.13389209617007572, 0.12062262144665582, 0.14273841265235568]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.1601 0.8 0.8
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6027468697309126, 0.13389209617007572, 0.12062262144665582, 0.14273841265235568]
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6027468697309126, 0.13389209617007572, 0.12062262144665582, 0.14273841265235568]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6029410733091491, 0.13381442928332354, 0.12066134580596394, 0.14258315160156326]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6031318996540395, 0.13373811303697777, 0.12069939674205941, 0.14243059056692337]
siam score:  -0.91758966
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6031318996540395, 0.13373811303697777, 0.12069939674205941, 0.14243059056692337]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9181819
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6033194361021799, 0.13366311250294072, 0.12073679166993416, 0.1422806597249451]
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6033194361021799, 0.13366311250294072, 0.12073679166993416, 0.1422806597249451]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.600730119286853, 0.13738023692623985, 0.12021913017214306, 0.14167051361476404]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.600730119286853, 0.13738023692623985, 0.12021913017214306, 0.14167051361476404]
maxi score, test score, baseline:  0.1601 0.8 0.8
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.600730119286853, 0.13738023692623985, 0.12021913017214306, 0.14167051361476404]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.013160353355379573
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.600935700922812, 0.137275199097521, 0.12026013481035436, 0.14152896516931268]
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.600935700922812, 0.137275199097521, 0.12026013481035436, 0.14152896516931268]
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.600935700922812, 0.137275199097521, 0.12026013481035436, 0.14152896516931268]
from probs:  [0.600935700922812, 0.137275199097521, 0.12026013481035436, 0.14152896516931268]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.600935700922812, 0.137275199097521, 0.12026013481035436, 0.14152896516931268]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6011378130504322, 0.137171933944756, 0.12030044743182232, 0.1413898055729894]
siam score:  -0.91772604
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6011378130504322, 0.137171933944756, 0.12030044743182232, 0.1413898055729894]
maxi score, test score, baseline:  0.1601 0.8 0.8
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6011378130504322, 0.137171933944756, 0.12030044743182232, 0.1413898055729894]
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6011378130504322, 0.137171933944756, 0.12030044743182232, 0.1413898055729894]
siam score:  -0.9174675
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6011378130504322, 0.137171933944756, 0.12030044743182232, 0.1413898055729894]
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.6011378130504322, 0.137171933944756, 0.12030044743182232, 0.1413898055729894]
maxi score, test score, baseline:  0.1601 0.8 0.8
maxi score, test score, baseline:  0.1601 0.8 0.8
probs:  [0.6011378130504322, 0.137171933944756, 0.12030044743182232, 0.1413898055729894]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6013365427647677, 0.13707039696846757, 0.12034008540824051, 0.14125297485852434]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6013365427647677, 0.13707039696846757, 0.12034008540824051, 0.14125297485852434]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6015319742699335, 0.1369705451462456, 0.12037906553468532, 0.1411184150491357]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6015319742699335, 0.1369705451462456, 0.12037906553468532, 0.1411184150491357]
Printing some Q and Qe and total Qs values:  [[1.243]
 [1.3  ]
 [1.243]
 [1.243]
 [1.243]] [[0.228]
 [0.146]
 [0.228]
 [0.228]
 [0.228]] [[0.896]
 [0.939]
 [0.896]
 [0.896]
 [0.896]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0000],
        [0.9218],
        [0.2207],
        [0.0000],
        [0.0534],
        [0.8410],
        [0.7580],
        [0.0562]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.9217840667694437
0.0 0.22067816575852778
0.0 0.0
0.0 0.053360933682524106
0.0 0.840976769622733
0.0 0.7579741317770617
0.0 0.05618771489686151
maxi score, test score, baseline:  0.1621 0.8 0.8
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6019132657224647, 0.1367757318979896, 0.12045511667607822, 0.14085588570346746]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6019132657224647, 0.1367757318979896, 0.12045511667607822, 0.14085588570346746]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6019132657224647, 0.1367757318979896, 0.12045511667607822, 0.14085588570346746]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6020992806652167, 0.13668069128016014, 0.1204922186058973, 0.14072780944872584]
maxi score, test score, baseline:  0.1621 0.8 0.8
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6020992806652167, 0.13668069128016014, 0.1204922186058973, 0.14072780944872584]
Printing some Q and Qe and total Qs values:  [[1.415]
 [1.451]
 [1.475]
 [1.249]
 [1.394]] [[0.318]
 [0.237]
 [0.251]
 [0.077]
 [0.17 ]] [[1.441]
 [1.463]
 [1.489]
 [1.239]
 [1.397]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6022823075996628, 0.1365871773254647, 0.12052872455738889, 0.14060179051748364]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6022823075996628, 0.1365871773254647, 0.12052872455738889, 0.14060179051748364]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6022823075996628, 0.1365871773254647, 0.12052872455738889, 0.14060179051748364]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6022823075996628, 0.1365871773254647, 0.12052872455738889, 0.14060179051748364]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6022823075996628, 0.1365871773254647, 0.12052872455738889, 0.14060179051748364]
maxi score, test score, baseline:  0.1621 0.8 0.8
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6022823075996628, 0.1365871773254647, 0.12052872455738889, 0.14060179051748364]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6022823075996628, 0.1365871773254647, 0.12052872455738889, 0.14060179051748364]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6022823075996628, 0.1365871773254647, 0.12052872455738889, 0.14060179051748364]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6022823075996628, 0.1365871773254647, 0.12052872455738889, 0.14060179051748364]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6022823075996628, 0.1365871773254647, 0.12052872455738889, 0.14060179051748364]
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.5998732059501637, 0.14003984955852003, 0.12004709493279636, 0.14003984955852003]
Printing some Q and Qe and total Qs values:  [[1.112]
 [1.112]
 [1.37 ]
 [1.112]
 [1.259]] [[1.686]
 [1.686]
 [1.811]
 [1.686]
 [2.087]] [[0.92 ]
 [0.92 ]
 [1.203]
 [0.92 ]
 [1.264]]
siam score:  -0.91145325
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6000717200759089, 0.1399207905710314, 0.12008669878202807, 0.1399207905710314]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6002671081868837, 0.1398036064131153, 0.12012567898688585, 0.1398036064131153]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0551],
        [0.3340],
        [0.2797],
        [0.3462],
        [0.7203],
        [0.3197],
        [0.7221],
        [0.0000],
        [0.5772],
        [0.8527]], dtype=torch.float64)
0.0 0.05513719854302823
0.0 0.33404146984670663
0.0 0.27969771512654695
0.0 0.34623990449747444
0.0 0.720302033063843
0.0 0.31974952480343133
0.0 0.7220620926545261
0.0 0.0
0.0 0.5771773584157717
0.0 0.8527260391691927
maxi score, test score, baseline:  0.1621 0.8 0.8
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6006487971390159, 0.13957468819379062, 0.12020182647340301, 0.13957468819379062]
siam score:  -0.91562593
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6008352377759392, 0.13946287028977258, 0.12023902164451566, 0.13946287028977258]
Printing some Q and Qe and total Qs values:  [[0.926]
 [0.926]
 [1.063]
 [0.926]
 [0.926]] [[1.358]
 [1.358]
 [1.693]
 [1.358]
 [1.358]] [[0.85 ]
 [0.85 ]
 [1.048]
 [0.85 ]
 [0.85 ]]
using another actor
siam score:  -0.9169788
Printing some Q and Qe and total Qs values:  [[1.318]
 [1.318]
 [1.379]
 [1.318]
 [1.318]] [[-0.023]
 [-0.023]
 [ 0.035]
 [-0.023]
 [-0.023]] [[0.88]
 [0.88]
 [0.95]
 [0.88]
 [0.88]]
maxi score, test score, baseline:  0.1621 0.8 0.8
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.91428345
maxi score, test score, baseline:  0.1621 0.8 0.8
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6010188321603455, 0.13935275942771722, 0.12027564898422016, 0.13935275942771722]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6010188321603455, 0.13935275942771722, 0.12027564898422016, 0.13935275942771722]
siam score:  -0.91659355
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6010188321603455, 0.13935275942771722, 0.12027564898422016, 0.13935275942771722]
maxi score, test score, baseline:  0.1621 0.8 0.8
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6010188321603455, 0.13935275942771722, 0.12027564898422016, 0.13935275942771722]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6010188321603455, 0.13935275942771722, 0.12027564898422016, 0.13935275942771722]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1621 0.8 0.8
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6011996449759707, 0.13924431681350788, 0.12031172139701349, 0.13924431681350788]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6011996449759707, 0.13924431681350788, 0.12031172139701349, 0.13924431681350788]
maxi score, test score, baseline:  0.1621 0.8 0.8
probs:  [0.6011996449759707, 0.13924431681350788, 0.12031172139701349, 0.13924431681350788]
Printing some Q and Qe and total Qs values:  [[0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]] [[1.566]
 [1.566]
 [1.566]
 [1.566]
 [1.566]] [[0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1641 0.8 0.8
probs:  [0.6011996449759707, 0.13924431681350788, 0.12031172139701349, 0.13924431681350788]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1641 0.8 0.8
probs:  [0.6013777389612899, 0.13913750481970003, 0.1203472513993102, 0.13913750481970003]
Printing some Q and Qe and total Qs values:  [[1.122]
 [1.122]
 [1.122]
 [1.122]
 [1.175]] [[1.453]
 [1.453]
 [1.453]
 [1.453]
 [1.202]] [[0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.906]]
using explorer policy with actor:  1
siam score:  -0.9199788
maxi score, test score, baseline:  0.1641 0.8 0.8
probs:  [0.6013777389612899, 0.13913750481970003, 0.1203472513993102, 0.13913750481970003]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1641 0.8 0.8
probs:  [0.6057856663207374, 0.13649197240705693, 0.12123038886514864, 0.13649197240705693]
maxi score, test score, baseline:  0.1641 0.8 0.8
probs:  [0.6057856663207374, 0.13649197240705693, 0.12123038886514864, 0.13649197240705693]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1661 0.8 0.8
probs:  [0.6057856663207374, 0.13649197240705693, 0.12123038886514864, 0.13649197240705693]
maxi score, test score, baseline:  0.1661 0.8 0.8
probs:  [0.6057856663207374, 0.13649197240705693, 0.12123038886514864, 0.13649197240705693]
actor:  0 policy actor:  0  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1681 0.8 0.8
probs:  [0.6057856663207374, 0.13649197240705693, 0.12123038886514864, 0.13649197240705693]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6057856663207374, 0.13649197240705693, 0.12123038886514864, 0.13649197240705693]
maxi score, test score, baseline:  0.1681 0.8 0.8
probs:  [0.6059303679032597, 0.13640520173300225, 0.12125922863073589, 0.13640520173300225]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1681 0.8 0.8
Printing some Q and Qe and total Qs values:  [[1.171]
 [1.244]
 [1.171]
 [1.171]
 [1.103]] [[ 0.   ]
 [-0.157]
 [ 0.   ]
 [ 0.   ]
 [-0.017]] [[0.777]
 [0.824]
 [0.777]
 [0.777]
 [0.706]]
maxi score, test score, baseline:  0.1681 0.8 0.8
maxi score, test score, baseline:  0.1681 0.8 0.8
probs:  [0.6060728936644204, 0.13631973579579193, 0.12128763474399583, 0.13631973579579193]
maxi score, test score, baseline:  0.1681 0.8 0.8
probs:  [0.6060728936644204, 0.13631973579579193, 0.12128763474399583, 0.13631973579579193]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1701 0.8 0.8
probs:  [0.6060728936644204, 0.13631973579579193, 0.12128763474399583, 0.13631973579579193]
maxi score, test score, baseline:  0.1701 0.8 0.8
probs:  [0.6060728936644204, 0.13631973579579193, 0.12128763474399583, 0.13631973579579193]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1701 0.8 0.8
probs:  [0.6063516111165412, 0.13615260216276734, 0.12134318455792406, 0.13615260216276734]
maxi score, test score, baseline:  0.1701 0.8 0.8
probs:  [0.6063516111165412, 0.13615260216276734, 0.12134318455792406, 0.13615260216276734]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1701 0.8 0.8
probs:  [0.6066221913494219, 0.13599034803407228, 0.12139711258243355, 0.13599034803407228]
maxi score, test score, baseline:  0.1701 0.8 0.8
probs:  [0.6066221913494219, 0.13599034803407228, 0.12139711258243355, 0.13599034803407228]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1701 0.8 0.8
probs:  [0.6067545405617236, 0.13591098449041328, 0.12142349045744985, 0.13591098449041328]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1701 0.8 0.8
maxi score, test score, baseline:  0.1701 0.8 0.8
probs:  [0.6110794235077588, 0.13331530080029164, 0.12228997489165779, 0.13331530080029164]
from probs:  [0.6110794235077588, 0.13331530080029164, 0.12228997489165779, 0.13331530080029164]
maxi score, test score, baseline:  0.1701 0.8 0.8
probs:  [0.6110794235077588, 0.13331530080029164, 0.12228997489165779, 0.13331530080029164]
maxi score, test score, baseline:  0.1701 0.8 0.8
probs:  [0.6110794235077588, 0.13331530080029164, 0.12228997489165779, 0.13331530080029164]
maxi score, test score, baseline:  0.1701 0.8 0.8
probs:  [0.6110794235077588, 0.13331530080029164, 0.12228997489165779, 0.13331530080029164]
Printing some Q and Qe and total Qs values:  [[0.802]
 [0.802]
 [0.877]
 [0.802]
 [0.802]] [[1.293]
 [1.293]
 [0.536]
 [1.293]
 [1.293]] [[0.802]
 [0.802]
 [0.877]
 [0.802]
 [0.802]]
maxi score, test score, baseline:  0.1701 0.8 0.8
probs:  [0.6110794235077588, 0.13331530080029164, 0.12228997489165779, 0.13331530080029164]
actor:  0 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17209999999999998 0.8 0.8
probs:  [0.6110794235077588, 0.13331530080029164, 0.12228997489165779, 0.13331530080029164]
maxi score, test score, baseline:  0.17209999999999998 0.8 0.8
probs:  [0.6110794235077588, 0.13331530080029164, 0.12228997489165779, 0.13331530080029164]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
probs:  [0.6110794235077588, 0.13331530080029164, 0.12228997489165779, 0.13331530080029164]
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
probs:  [0.6110794235077588, 0.13331530080029164, 0.12228997489165779, 0.13331530080029164]
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
probs:  [0.6110794235077588, 0.13331530080029164, 0.12228997489165779, 0.13331530080029164]
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
probs:  [0.6088410751464475, 0.13648904765775643, 0.12184247316198304, 0.13282740403381307]
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
probs:  [0.6088410751464475, 0.13648904765775643, 0.12184247316198304, 0.13282740403381307]
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
probs:  [0.6088410751464475, 0.13648904765775643, 0.12184247316198304, 0.13282740403381307]
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
probs:  [0.6088410751464475, 0.13648904765775643, 0.12184247316198304, 0.13282740403381307]
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
probs:  [0.6088410751464475, 0.13648904765775643, 0.12184247316198304, 0.13282740403381307]
Printing some Q and Qe and total Qs values:  [[0.972]
 [1.188]
 [1.146]
 [1.18 ]
 [1.185]] [[0.447]
 [0.246]
 [0.36 ]
 [0.436]
 [0.326]] [[0.623]
 [0.805]
 [0.782]
 [0.829]
 [0.816]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
probs:  [0.6089577683913604, 0.13640576949640498, 0.12186570799194484, 0.13277075412028994]
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
probs:  [0.6089577683913604, 0.13640576949640498, 0.12186570799194484, 0.13277075412028994]
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
probs:  [0.6089577683913604, 0.13640576949640498, 0.12186570799194484, 0.13277075412028994]
line 256 mcts: sample exp_bonus 2.2306842502249933
siam score:  -0.9257152
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
probs:  [0.6089577683913604, 0.13640576949640498, 0.12186570799194484, 0.13277075412028994]
using another actor
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.925]
 [0.925]
 [0.925]
 [0.076]
 [0.925]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.748]
 [ 0.   ]] [[0.925]
 [0.925]
 [0.925]
 [0.076]
 [0.925]]
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
probs:  [0.6089577683913604, 0.13640576949640498, 0.12186570799194484, 0.13277075412028994]
maxi score, test score, baseline:  0.17409999999999998 0.8 0.8
probs:  [0.6089577683913604, 0.13640576949640498, 0.12186570799194484, 0.13277075412028994]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.92618364
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.939]
 [1.092]
 [0.005]
 [0.989]] [[0.696]
 [0.849]
 [1.304]
 [0.23 ]
 [0.965]] [[0.519]
 [0.876]
 [1.109]
 [0.001]
 [0.944]]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.167 0.042 0.625 0.042 0.125]
Printing some Q and Qe and total Qs values:  [[0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.66 ]] [[0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.585]] [[0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.547]]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.869]
 [1.099]
 [0.869]
 [0.823]] [[1.834]
 [1.834]
 [1.676]
 [1.834]
 [2.389]] [[0.947]
 [0.947]
 [1.098]
 [0.947]
 [1.178]]
Printing some Q and Qe and total Qs values:  [[1.121]
 [1.128]
 [1.259]
 [1.121]
 [1.019]] [[2.514]
 [2.064]
 [2.161]
 [2.514]
 [2.364]] [[1.182]
 [1.051]
 [1.159]
 [1.182]
 [1.075]]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
siam score:  -0.9220147
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.318171380867716
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2181 1.0 1.0
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9155594
actor:  0 policy actor:  0  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.948]
 [0.972]
 [1.06 ]
 [0.948]
 [0.948]] [[ 0.   ]
 [ 0.096]
 [-0.135]
 [ 0.   ]
 [ 0.   ]] [[0.635]
 [0.675]
 [0.725]
 [0.635]
 [0.635]]
maxi score, test score, baseline:  0.2201 1.0 1.0
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2201 1.0 1.0
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.92340964
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.2221 1.0 1.0
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.92229533
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.163]
 [1.302]
 [1.163]
 [1.163]
 [1.163]] [[1.72 ]
 [1.481]
 [1.72 ]
 [1.72 ]
 [1.72 ]] [[1.008]
 [1.069]
 [1.008]
 [1.008]
 [1.008]]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2221 1.0 1.0
maxi score, test score, baseline:  0.2221 1.0 1.0
siam score:  -0.92667025
Printing some Q and Qe and total Qs values:  [[1.07 ]
 [1.265]
 [1.177]
 [1.103]
 [1.128]] [[1.667]
 [1.27 ]
 [2.153]
 [2.166]
 [2.082]] [[0.859]
 [0.856]
 [1.135]
 [1.082]
 [1.068]]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.   ]
 [0.695]
 [0.165]
 [0.107]] [[-0.267]
 [-0.434]
 [-0.607]
 [-0.194]
 [-0.459]] [[0.164]
 [0.017]
 [0.478]
 [0.193]
 [0.088]]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2241 1.0 1.0
maxi score, test score, baseline:  0.2241 1.0 1.0
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2261 1.0 1.0
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  -0.01655786784578546
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.905]
 [0.984]
 [0.845]
 [0.927]] [[2.235]
 [1.752]
 [0.104]
 [1.33 ]
 [1.228]] [[0.758]
 [0.905]
 [0.984]
 [0.845]
 [0.927]]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2341 1.0 1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.91861314
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.9208597
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2341 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.583]
 [0.977]
 [0.663]
 [0.663]] [[-0.431]
 [-0.354]
 [-0.216]
 [-0.431]
 [-0.431]] [[0.544]
 [0.477]
 [0.894]
 [0.544]
 [0.544]]
Printing some Q and Qe and total Qs values:  [[0.88 ]
 [0.868]
 [1.036]
 [0.88 ]
 [0.88 ]] [[-0.356]
 [-0.295]
 [-0.081]
 [-0.356]
 [-0.356]] [[0.788]
 [0.787]
 [0.991]
 [0.788]
 [0.788]]
siam score:  -0.9208644
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.3545447537900869
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.011]
 [0.583]
 [0.356]
 [0.224]] [[ 0.069]
 [ 0.477]
 [-0.454]
 [ 0.   ]
 [ 0.041]] [[0.1  ]
 [0.039]
 [0.301]
 [0.225]
 [0.107]]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
siam score:  -0.91774094
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.9123718
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
siam score:  -0.91633105
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.911]
 [0.838]
 [0.838]
 [0.868]] [[1.532]
 [1.618]
 [1.713]
 [1.713]
 [2.033]] [[0.809]
 [0.923]
 [0.907]
 [0.907]
 [1.025]]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2521 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.01937927960502512
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.910354
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.286]
 [0.333]
 [0.026]
 [0.272]] [[2.465]
 [4.479]
 [1.172]
 [1.357]
 [2.343]] [[0.406]
 [0.93 ]
 [0.113]
 [0.005]
 [0.38 ]]
siam score:  -0.90941864
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.01906372002998524
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.53]
 [1.53]
 [1.53]
 [1.53]
 [1.53]] [[1.274]
 [1.274]
 [1.274]
 [1.274]
 [1.274]] [[1.471]
 [1.471]
 [1.471]
 [1.471]
 [1.471]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.04 ]
 [1.197]
 [0.995]
 [1.048]
 [1.048]] [[1.772]
 [1.332]
 [2.091]
 [1.209]
 [1.445]] [[1.036]
 [1.044]
 [1.085]
 [0.888]
 [0.953]]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
line 256 mcts: sample exp_bonus 2.480070035912773
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.017397654261551702
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.123]
 [1.123]
 [1.123]
 [1.123]
 [1.167]] [[0.741]
 [0.741]
 [0.741]
 [0.741]
 [1.169]] [[0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.968]]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2681 1.0 1.0
maxi score, test score, baseline:  0.2681 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9021611
maxi score, test score, baseline:  0.2681 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.333]] [[2.835]
 [2.835]
 [2.835]
 [2.835]
 [3.799]] [[0.863]
 [0.863]
 [0.863]
 [0.863]
 [1.05 ]]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9020953
siam score:  -0.9025247
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9017907
siam score:  -0.901782
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.4903554677528329
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2681 1.0 1.0
first move QE:  -0.018831535549374456
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90539324
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.90818465
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.421]
 [1.421]
 [1.421]
 [1.421]
 [1.421]] [[0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]] [[0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.561]
 [1.042]
 [0.797]
 [0.9  ]] [[0.828]
 [1.235]
 [0.761]
 [0.811]
 [1.411]] [[0.565]
 [0.615]
 [0.841]
 [0.638]
 [1.011]]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.859]
 [0.486]
 [0.699]
 [0.773]] [[1.835]
 [1.922]
 [0.606]
 [1.623]
 [1.59 ]] [[0.688]
 [0.859]
 [0.486]
 [0.699]
 [0.773]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.791]
 [0.765]
 [0.765]
 [0.765]] [[1.696]
 [1.673]
 [1.104]
 [1.104]
 [1.104]] [[0.314]
 [0.791]
 [0.765]
 [0.765]
 [0.765]]
siam score:  -0.9010266
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.8063],
        [0.5574],
        [0.0000],
        [0.1768],
        [0.5151],
        [0.6684],
        [0.5157],
        [0.5668],
        [0.6397],
        [0.6614]], dtype=torch.float64)
0.0 0.806257714150547
0.0 0.5573783735026171
0.0 0.0
0.0 0.17679988303545602
0.0 0.5151407150769653
0.0 0.6683967627813263
0.0 0.5156847118526688
0.0 0.5667772190679459
0.0 0.6396640572254639
0.0 0.6613826833837304
maxi score, test score, baseline:  0.2721 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2721 1.0 1.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3401785627988212
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3276405294569908
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.732]
 [0.981]
 [0.732]
 [0.732]] [[0.479]
 [0.479]
 [1.228]
 [0.479]
 [0.479]] [[0.395]
 [0.395]
 [0.845]
 [0.395]
 [0.395]]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2761 1.0 1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.87]
 [0.87]
 [0.87]
 [0.87]
 [0.87]] [[5.109]
 [5.109]
 [5.109]
 [5.109]
 [5.109]] [[0.962]
 [0.962]
 [0.962]
 [0.962]
 [0.962]]
siam score:  -0.9021765
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2801 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.9012367
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
siam score:  -0.9006276
siam score:  -0.8991278
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
siam score:  -0.8983623
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.915]
 [0.977]
 [0.915]
 [0.915]
 [0.919]] [[0.687]
 [0.489]
 [0.687]
 [0.687]
 [0.878]] [[0.915]
 [0.977]
 [0.915]
 [0.915]
 [0.919]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.002]
 [0.663]
 [0.264]
 [0.245]] [[ 0.124]
 [ 0.334]
 [-0.194]
 [-0.162]
 [-0.314]] [[0.231]
 [0.02 ]
 [0.593]
 [0.199]
 [0.155]]
siam score:  -0.898515
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
first move QE:  -0.02295962474711734
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0312992487568409
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.010760827447474
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.63 ]
 [0.392]
 [0.63 ]
 [0.63 ]] [[2.807]
 [2.807]
 [0.585]
 [2.807]
 [2.807]] [[1.343]
 [1.343]
 [0.365]
 [1.343]
 [1.343]]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.076]
 [1.076]
 [1.076]
 [1.076]
 [1.107]] [[2.173]
 [2.173]
 [2.173]
 [2.173]
 [2.162]] [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.803]]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]] [[-0.242]
 [-0.242]
 [-0.242]
 [-0.242]
 [-0.242]] [[0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.496]
 [0.652]
 [0.518]
 [0.555]] [[0.128]
 [0.074]
 [0.474]
 [0.161]
 [0.131]] [[0.544]
 [0.496]
 [0.652]
 [0.518]
 [0.555]]
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.877]
 [0.935]
 [0.877]
 [0.877]
 [0.877]] [[0.913]
 [0.03 ]
 [0.913]
 [0.913]
 [0.913]] [[0.877]
 [0.935]
 [0.877]
 [0.877]
 [0.877]]
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.087]
 [1.087]
 [1.087]
 [1.087]
 [1.087]] [[2.096]
 [2.096]
 [2.096]
 [2.096]
 [2.096]] [[1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8986246
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.303]
 [1.358]
 [1.303]
 [1.303]
 [1.303]] [[2.06]
 [2.63]
 [2.06]
 [2.06]
 [2.06]] [[0.915]
 [1.159]
 [0.915]
 [0.915]
 [0.915]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.801]
 [1.339]
 [0.966]
 [0.805]] [[3.46 ]
 [4.082]
 [1.84 ]
 [0.992]
 [2.665]] [[0.625]
 [1.04 ]
 [0.718]
 [0.279]
 [0.651]]
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.347]
 [1.334]
 [1.353]
 [1.334]
 [1.346]] [[0.814]
 [1.113]
 [0.918]
 [1.113]
 [1.166]] [[1.089]
 [1.176]
 [1.13 ]
 [1.176]
 [1.206]]
actor:  1 policy actor:  1  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9001347
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.3361 1.0 1.0
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9258710806219198
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.732]
 [0.659]
 [0.659]
 [0.663]] [[0.947]
 [0.861]
 [0.869]
 [0.869]
 [1.314]] [[0.651]
 [0.732]
 [0.659]
 [0.659]
 [0.663]]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 1.0 1.0
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9518993696078074
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.301]
 [0.637]
 [0.301]
 [0.338]] [[5.171]
 [3.067]
 [1.909]
 [3.067]
 [4.575]] [[0.744]
 [0.237]
 [0.079]
 [0.237]
 [0.616]]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8902586
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8857057
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
line 256 mcts: sample exp_bonus 0.6425817134045065
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
siam score:  -0.8865959
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.022883507679996074
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]] [[4.487]
 [4.487]
 [4.487]
 [4.487]
 [4.487]] [[3.385]
 [3.385]
 [3.385]
 [3.385]
 [3.385]]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.89011765
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.024208740124401985
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.6835909056707528
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.496]
 [0.636]
 [0.599]
 [0.591]] [[1.846]
 [2.198]
 [2.176]
 [2.864]
 [3.046]] [[0.153]
 [0.341]
 [0.395]
 [0.578]
 [0.627]]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.03889433874852984
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.881636
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.965]
 [0.965]
 [0.965]
 [0.965]
 [0.965]] [[0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]] [[1.11]
 [1.11]
 [1.11]
 [1.11]
 [1.11]]
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.225]] [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [1.073]] [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.606]]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.129]
 [1.242]
 [1.129]
 [1.129]
 [1.129]] [[2.012]
 [2.253]
 [2.012]
 [2.012]
 [2.012]] [[0.975]
 [1.132]
 [0.975]
 [0.975]
 [0.975]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.042]
 [1.042]
 [1.161]
 [1.042]
 [0.849]] [[3.008]
 [3.008]
 [1.169]
 [3.008]
 [7.824]] [[0.826]
 [0.826]
 [0.721]
 [0.826]
 [1.167]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.025989379846078988
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8810057
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.88056153
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
Printing some Q and Qe and total Qs values:  [[0.842]
 [0.842]
 [0.842]
 [0.842]
 [0.842]] [[1.84]
 [1.84]
 [1.84]
 [1.84]
 [1.84]] [[2.069]
 [2.069]
 [2.069]
 [2.069]
 [2.069]]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.82]
 [0.82]
 [0.82]
 [0.82]
 [0.82]] [[2.136]
 [2.136]
 [2.136]
 [2.136]
 [2.136]] [[0.82]
 [0.82]
 [0.82]
 [0.82]
 [0.82]]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
maxi score, test score, baseline:  0.3641 1.0 1.0
maxi score, test score, baseline:  0.3641 1.0 1.0
maxi score, test score, baseline:  0.3641 1.0 1.0
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.089]
 [1.089]
 [1.089]
 [1.089]
 [1.112]] [[2.001]
 [2.001]
 [2.001]
 [2.001]
 [2.251]] [[1.107]
 [1.107]
 [1.107]
 [1.107]
 [1.232]]
Printing some Q and Qe and total Qs values:  [[1.066]
 [1.266]
 [0.835]
 [1.089]
 [1.127]] [[2.129]
 [1.763]
 [2.209]
 [2.001]
 [2.281]] [[0.935]
 [0.947]
 [0.802]
 [0.906]
 [1.029]]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.026785161200957856
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.11 ]
 [1.11 ]
 [1.11 ]
 [1.11 ]
 [0.939]] [[1.179]
 [1.179]
 [1.179]
 [1.179]
 [2.555]] [[0.776]
 [0.776]
 [0.776]
 [0.776]
 [1.053]]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.119]
 [1.291]
 [1.109]
 [0.299]
 [1.257]] [[1.252]
 [1.354]
 [0.997]
 [0.793]
 [1.257]] [[1.086]
 [1.257]
 [1.008]
 [0.281]
 [1.202]]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8880209
maxi score, test score, baseline:  0.3701 1.0 1.0
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3701 1.0 1.0
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3701 1.0 1.0
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.714]
 [0.647]
 [0.647]
 [0.662]] [[0.462]
 [0.842]
 [0.462]
 [0.462]
 [0.687]] [[0.647]
 [0.714]
 [0.647]
 [0.647]
 [0.662]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.764]
 [0.68 ]
 [0.652]
 [0.721]] [[0.954]
 [2.092]
 [2.228]
 [1.668]
 [2.025]] [[0.119]
 [0.764]
 [0.68 ]
 [0.652]
 [0.721]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8823517
start point for exploration sampling:  20031
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3781 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.535]
 [0.632]
 [0.55 ]
 [0.535]] [[0.592]
 [0.592]
 [1.08 ]
 [1.325]
 [0.592]] [[0.535]
 [0.535]
 [0.632]
 [0.55 ]
 [0.535]]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.88242507
maxi score, test score, baseline:  0.3801 1.0 1.0
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88110334
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.028435392369294647
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.028331601724379523
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3821 1.0 1.0
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
siam score:  -0.87481344
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3821 1.0 1.0
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.842]] [[2.82 ]
 [2.82 ]
 [2.82 ]
 [2.82 ]
 [3.599]] [[0.873]
 [0.873]
 [0.873]
 [0.873]
 [1.081]]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8757007
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.87197214
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]] [[2.408]
 [2.408]
 [2.408]
 [2.408]
 [2.408]] [[1.035]
 [1.035]
 [1.035]
 [1.035]
 [1.035]]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.874805
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.02876981657113091
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
maxi score, test score, baseline:  0.3921 1.0 1.0
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.859]
 [0.749]
 [0.749]
 [0.78 ]] [[1.517]
 [1.102]
 [1.517]
 [1.517]
 [1.386]] [[0.749]
 [0.859]
 [0.749]
 [0.749]
 [0.78 ]]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 1.0 1.0
maxi score, test score, baseline:  0.3941 1.0 1.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 1.0 1.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]] [[1.356]
 [1.356]
 [1.356]
 [1.356]
 [1.356]] [[1.315]
 [1.315]
 [1.315]
 [1.315]
 [1.315]]
maxi score, test score, baseline:  0.3961 1.0 1.0
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8669431
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.705]
 [0.705]
 [0.729]
 [0.599]] [[5.222]
 [5.222]
 [5.222]
 [3.414]
 [5.784]] [[0.865]
 [0.865]
 [0.865]
 [0.547]
 [0.91 ]]
siam score:  -0.8678437
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.843]
 [0.903]
 [0.005]
 [0.838]] [[-0.102]
 [-0.315]
 [-0.399]
 [ 0.046]
 [-0.226]] [[0.627]
 [0.78 ]
 [0.826]
 [0.002]
 [0.79 ]]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]] [[-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4021 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4021 1.0 1.0
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6662999273104497
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.312]
 [1.312]
 [1.312]
 [1.312]
 [1.312]] [[2.241]
 [2.241]
 [2.241]
 [2.241]
 [2.241]] [[1.402]
 [1.402]
 [1.402]
 [1.402]
 [1.402]]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.4041 1.0 1.0
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8695112
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.4735618335632896
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8751921
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 2.179482750786798
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8692569
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.929]
 [0.762]
 [0.762]
 [1.032]] [[3.516]
 [3.781]
 [3.516]
 [3.516]
 [3.198]] [[0.804]
 [0.949]
 [0.804]
 [0.804]
 [0.828]]
line 256 mcts: sample exp_bonus 0.13984635769260328
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.    0.208 0.208 0.042 0.542]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Starting evaluation
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4501 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.651]
 [0.926]
 [0.651]
 [0.383]] [[ 0.026]
 [ 0.   ]
 [-0.249]
 [ 0.   ]
 [-0.061]] [[0.206]
 [0.429]
 [0.58 ]
 [0.429]
 [0.13 ]]
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.4521 1.0 1.0
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.5771889228498304
maxi score, test score, baseline:  0.4541 1.0 1.0
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4541 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4541 1.0 1.0
maxi score, test score, baseline:  0.4541 1.0 1.0
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.289]
 [0.289]
 [0.289]
 [0.315]] [[1.677]
 [1.654]
 [1.654]
 [1.654]
 [0.825]] [[0.547]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.345]]
maxi score, test score, baseline:  0.4541 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4541 1.0 1.0
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.85822374
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]] [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86099267
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.912]
 [1.143]
 [1.077]
 [0.012]
 [0.912]] [[ 0.   ]
 [-0.1  ]
 [-0.233]
 [-0.035]
 [ 0.   ]] [[0.91 ]
 [1.124]
 [1.036]
 [0.005]
 [0.91 ]]
maxi score, test score, baseline:  0.4581 1.0 1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.942]
 [0.942]
 [1.064]
 [0.942]
 [0.942]] [[0.377]
 [0.377]
 [0.661]
 [0.377]
 [0.377]] [[0.719]
 [0.719]
 [0.922]
 [0.719]
 [0.719]]
Printing some Q and Qe and total Qs values:  [[1.186]
 [1.088]
 [1.088]
 [1.088]
 [1.088]] [[2.651]
 [1.507]
 [1.507]
 [1.507]
 [1.507]] [[1.286]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4581 1.0 1.0
siam score:  -0.87091637
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4581 1.0 1.0
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8725747
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4581 1.0 1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4581 1.0 1.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.805]] [[2.598]
 [2.598]
 [2.598]
 [2.598]
 [3.728]] [[0.433]
 [0.433]
 [0.433]
 [0.433]
 [1.001]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.919]
 [1.051]
 [0.844]
 [0.844]] [[1.306]
 [1.082]
 [1.027]
 [1.306]
 [1.306]] [[0.908]
 [0.871]
 [0.976]
 [0.908]
 [0.908]]
first move QE:  -0.029778570612100395
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.933]
 [1.006]
 [1.09 ]
 [0.933]
 [0.933]] [[0.995]
 [1.114]
 [1.008]
 [0.995]
 [0.995]] [[0.712]
 [0.805]
 [0.872]
 [0.712]
 [0.712]]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.317]
 [1.394]
 [1.317]
 [1.317]
 [1.317]] [[0.28 ]
 [1.067]
 [0.28 ]
 [0.28 ]
 [0.28 ]] [[0.874]
 [1.083]
 [0.874]
 [0.874]
 [0.874]]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4601 1.0 1.0
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.9  ]
 [1.069]
 [0.883]
 [0.913]
 [0.981]] [[2.382]
 [2.531]
 [2.963]
 [2.335]
 [2.838]] [[0.643]
 [0.807]
 [0.826]
 [0.636]
 [0.85 ]]
first move QE:  -0.029206888142445125
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.748]] [[3.998]
 [3.998]
 [3.998]
 [3.998]
 [4.821]] [[0.761]
 [0.761]
 [0.761]
 [0.761]
 [1.063]]
Printing some Q and Qe and total Qs values:  [[0.892]
 [1.016]
 [0.863]
 [0.892]
 [0.867]] [[2.246]
 [2.724]
 [3.712]
 [2.246]
 [3.272]] [[0.585]
 [0.801]
 [1.003]
 [0.585]
 [0.875]]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.029206888142445125
maxi score, test score, baseline:  0.4601 1.0 1.0
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4621 1.0 1.0
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4621 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8667691
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.86707574
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]] [[4.352]
 [4.352]
 [4.352]
 [4.352]
 [4.352]] [[0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]]
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4661 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4661 1.0 1.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.974]
 [0.968]
 [1.139]
 [0.974]
 [0.974]] [[0.074]
 [0.158]
 [1.823]
 [0.074]
 [0.074]] [[0.764]
 [0.771]
 [1.117]
 [0.764]
 [0.764]]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4681 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4681 1.0 1.0
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8533795
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.037]
 [1.197]
 [1.146]
 [1.037]
 [1.148]] [[0.272]
 [0.45 ]
 [0.47 ]
 [0.272]
 [0.498]] [[0.648]
 [0.838]
 [0.79 ]
 [0.648]
 [0.796]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.2104667169204912
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8612393
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]] [[3.12]
 [3.12]
 [3.12]
 [3.12]
 [3.12]] [[1.083]
 [1.083]
 [1.083]
 [1.083]
 [1.083]]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]] [[0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]] [[0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86401445
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.13 ]
 [1.209]
 [1.13 ]
 [1.13 ]
 [1.163]] [[0.063]
 [0.186]
 [0.063]
 [0.063]
 [0.392]] [[0.721]
 [0.821]
 [0.721]
 [0.721]
 [0.809]]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.559]
 [0.559]
 [0.559]
 [0.532]] [[4.514]
 [3.104]
 [3.104]
 [3.104]
 [4.621]] [[0.631]
 [0.383]
 [0.383]
 [0.383]
 [0.76 ]]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8684614
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.121]
 [1.201]
 [1.172]
 [1.098]
 [1.178]] [[0.432]
 [0.878]
 [0.747]
 [0.632]
 [0.797]] [[0.035]
 [1.074]
 [1.013]
 [0.918]
 [1.032]]
maxi score, test score, baseline:  0.4841 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.217]
 [1.207]
 [1.282]
 [1.228]
 [1.125]] [[0.535]
 [0.937]
 [0.74 ]
 [1.14 ]
 [1.447]] [[0.065]
 [1.052]
 [1.061]
 [1.131]
 [1.13 ]]
maxi score, test score, baseline:  0.4841 1.0 1.0
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.245]
 [1.397]
 [1.28 ]
 [1.28 ]
 [1.28 ]] [[0.503]
 [0.098]
 [0.116]
 [0.116]
 [0.116]] [[0.851]
 [0.935]
 [0.822]
 [0.822]
 [0.822]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.119]
 [1.183]
 [1.119]
 [1.119]
 [1.119]] [[2.242]
 [1.968]
 [2.242]
 [2.242]
 [2.242]] [[0.908]
 [0.921]
 [0.908]
 [0.908]
 [0.908]]
Printing some Q and Qe and total Qs values:  [[1.092]
 [1.279]
 [1.114]
 [1.092]
 [1.133]] [[2.174]
 [4.474]
 [2.187]
 [2.174]
 [2.304]] [[0.726]
 [1.177]
 [0.745]
 [0.726]
 [0.775]]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4861 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
siam score:  -0.8499045
maxi score, test score, baseline:  0.4901 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.013]
 [1.128]
 [1.343]
 [1.013]
 [1.013]] [[1.727]
 [2.177]
 [1.193]
 [1.727]
 [1.727]] [[0.878]
 [1.142]
 [1.03 ]
 [0.878]
 [0.878]]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4901 1.0 1.0
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.027383206850412522
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8656072
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
siam score:  -0.8628514
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8646173
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.181]
 [1.181]
 [1.181]
 [1.181]
 [1.181]] [[1.527]
 [1.527]
 [1.527]
 [1.527]
 [1.527]] [[1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.37]
 [1.37]
 [1.37]
 [1.37]
 [1.37]] [[2.75]
 [2.75]
 [2.75]
 [2.75]
 [2.75]] [[1.343]
 [1.343]
 [1.343]
 [1.343]
 [1.343]]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.693]
 [0.693]
 [0.767]
 [0.819]] [[2.383]
 [2.383]
 [2.383]
 [2.89 ]
 [3.001]] [[0.789]
 [0.789]
 [0.789]
 [1.065]
 [1.155]]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.012]
 [1.012]
 [1.067]
 [1.012]
 [1.012]] [[1.771]
 [1.771]
 [2.164]
 [1.771]
 [1.771]] [[0.919]
 [0.919]
 [1.048]
 [0.919]
 [0.919]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8657312
maxi score, test score, baseline:  0.4961 1.0 1.0
start point for exploration sampling:  20031
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8611499
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[1.039]
 [1.039]
 [1.039]
 [1.039]
 [1.039]] [[0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5001 1.0 1.0
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5021 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.792 0.042 0.042 0.083]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5021 1.0 1.0
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8568668
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5021 1.0 1.0
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5021 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5021 1.0 1.0
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.125 0.292 0.083 0.083 0.417]
maxi score, test score, baseline:  0.5021 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]] [[0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]] [[1.16]
 [1.16]
 [1.16]
 [1.16]
 [1.16]]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5041 1.0 1.0
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5041 1.0 1.0
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5041 1.0 1.0
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.02568312938465776
start point for exploration sampling:  20031
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.02566589588456574
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84961337
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5081 1.0 1.0
maxi score, test score, baseline:  0.5081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.025217868092311857
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.787]
 [0.787]
 [0.787]
 [1.031]] [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.017]] [[0.787]
 [0.787]
 [0.787]
 [0.787]
 [1.031]]
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5121 1.0 1.0
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.02474119319583413
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.09777187656919731
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5141 1.0 1.0
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.856]
 [0.654]
 [0.633]
 [0.739]] [[1.576]
 [0.999]
 [2.11 ]
 [1.694]
 [1.631]] [[0.757]
 [0.856]
 [0.654]
 [0.633]
 [0.739]]
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84445876
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[4.821]
 [4.821]
 [4.821]
 [4.821]
 [4.821]] [[1.081]
 [1.081]
 [1.081]
 [1.081]
 [1.081]]
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5201 1.0 1.0
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5201 1.0 1.0
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.    0.042 0.875 0.042 0.042]
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84807914
maxi score, test score, baseline:  0.5261 1.0 1.0
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5281 1.0 1.0
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.934]
 [0.025]
 [0.981]
 [0.82 ]
 [0.785]] [[1.669]
 [0.718]
 [1.589]
 [1.213]
 [1.496]] [[0.909]
 [0.005]
 [0.906]
 [0.664]
 [0.754]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.241]
 [1.284]
 [1.241]
 [1.241]
 [1.242]] [[1.711]
 [1.835]
 [1.711]
 [1.711]
 [1.689]] [[1.175]
 [1.237]
 [1.175]
 [1.175]
 [1.171]]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8441124
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.024334740863940694
Printing some Q and Qe and total Qs values:  [[0.754]
 [1.17 ]
 [1.347]
 [1.2  ]
 [1.097]] [[4.513]
 [3.693]
 [2.565]
 [2.679]
 [3.601]] [[0.828]
 [0.774]
 [0.538]
 [0.509]
 [0.719]]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.262]
 [1.262]
 [1.262]
 [1.262]
 [1.262]] [[2.849]
 [2.849]
 [2.849]
 [2.849]
 [2.849]] [[1.257]
 [1.257]
 [1.257]
 [1.257]
 [1.257]]
maxi score, test score, baseline:  0.5301 1.0 1.0
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5301 1.0 1.0
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5341 1.0 1.0
maxi score, test score, baseline:  0.5341 1.0 1.0
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5341 1.0 1.0
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.8470777
maxi score, test score, baseline:  0.5341 1.0 1.0
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.147]] [[3.299]
 [3.299]
 [3.299]
 [3.299]
 [3.299]] [[0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]]
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.633]
 [0.633]
 [0.76 ]
 [0.744]] [[2.768]
 [2.768]
 [2.768]
 [3.655]
 [3.183]] [[0.394]
 [0.394]
 [0.394]
 [0.764]
 [0.601]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.   ]
 [1.139]
 [1.068]
 [1.   ]
 [1.022]] [[3.107]
 [2.978]
 [4.062]
 [3.107]
 [3.614]] [[0.71 ]
 [0.758]
 [1.06 ]
 [0.71 ]
 [0.887]]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84350425
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8437243
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.023279182207854632
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.83843154
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5680999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5680999999999999 1.0 1.0
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.164]
 [1.092]
 [1.009]
 [0.507]
 [1.092]] [[ 1.507]
 [ 0.834]
 [ 0.894]
 [-0.129]
 [ 0.834]] [[1.259]
 [1.075]
 [1.001]
 [0.328]
 [1.075]]
Printing some Q and Qe and total Qs values:  [[1.157]
 [1.157]
 [1.157]
 [1.157]
 [1.157]] [[1.68]
 [1.68]
 [1.68]
 [1.68]
 [1.68]] [[0.935]
 [0.935]
 [0.935]
 [0.935]
 [0.935]]
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.022544408849405666
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5761 1.0 1.0
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5761 1.0 1.0
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.431]
 [1.431]
 [1.431]
 [1.431]
 [1.431]] [[0.776]
 [0.776]
 [0.776]
 [0.776]
 [0.776]] [[1.266]
 [1.266]
 [1.266]
 [1.266]
 [1.266]]
maxi score, test score, baseline:  0.5761 1.0 1.0
maxi score, test score, baseline:  0.5761 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.167]
 [1.167]
 [1.167]
 [1.167]
 [1.113]] [[2.675]
 [2.675]
 [2.675]
 [2.675]
 [2.936]] [[1.262]
 [1.262]
 [1.262]
 [1.262]
 [1.295]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8519743
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.466]
 [1.051]
 [0.666]
 [0.505]] [[2.923]
 [3.961]
 [3.601]
 [2.923]
 [4.191]] [[0.524]
 [0.675]
 [0.868]
 [0.524]
 [0.748]]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.474]] [[4.383]
 [4.383]
 [4.383]
 [4.383]
 [4.338]] [[1.06 ]
 [1.06 ]
 [1.06 ]
 [1.06 ]
 [1.058]]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
maxi score, test score, baseline:  0.5801 1.0 1.0
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5801 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.105]
 [1.233]
 [1.191]
 [1.105]
 [1.184]] [[0.48 ]
 [0.496]
 [0.327]
 [0.48 ]
 [0.502]] [[0.784]
 [0.915]
 [0.844]
 [0.784]
 [0.867]]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.5801 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.792 0.042 0.042 0.042 0.083]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5801 1.0 1.0
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5801 1.0 1.0
maxi score, test score, baseline:  0.5801 1.0 1.0
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5821 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5821 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84401095
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.583 0.292 0.083]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.937]
 [1.051]
 [0.002]
 [0.937]
 [0.857]] [[0.35 ]
 [0.39 ]
 [0.024]
 [0.35 ]
 [0.289]] [[0.63 ]
 [0.707]
 [0.001]
 [0.63 ]
 [0.567]]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.829077
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5841 1.0 1.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]] [[4.669]
 [4.669]
 [4.669]
 [4.669]
 [4.669]] [[1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.155]]
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5861 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5861 1.0 1.0
maxi score, test score, baseline:  0.5861 1.0 1.0
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5861 1.0 1.0
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.215]
 [1.241]
 [1.215]
 [1.215]
 [1.215]] [[0.391]
 [0.492]
 [0.391]
 [0.391]
 [0.391]] [[0.874]
 [0.917]
 [0.874]
 [0.874]
 [0.874]]
maxi score, test score, baseline:  0.5901 1.0 1.0
maxi score, test score, baseline:  0.5901 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5901 1.0 1.0
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5901 1.0 1.0
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.392]
 [1.392]
 [1.392]
 [1.392]
 [1.392]] [[2.244]
 [2.244]
 [2.244]
 [2.244]
 [2.244]] [[1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]]
maxi score, test score, baseline:  0.5901 1.0 1.0
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.019189005546100833
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.83406603
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.019379065658969494
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5941 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.644]
 [1.128]
 [1.303]
 [1.128]
 [1.161]] [[4.631]
 [3.503]
 [2.562]
 [3.503]
 [3.763]] [[0.838]
 [0.813]
 [0.722]
 [0.813]
 [0.863]]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.358]
 [0.716]
 [0.844]
 [0.742]] [[2.788]
 [1.246]
 [2.788]
 [3.511]
 [3.644]] [[0.685]
 [0.231]
 [0.685]
 [0.882]
 [0.867]]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5941 1.0 1.0
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.01902210589952046
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.773]
 [0.711]
 [0.711]
 [0.734]] [[1.188]
 [1.595]
 [1.188]
 [1.188]
 [1.941]] [[0.711]
 [0.773]
 [0.711]
 [0.711]
 [0.734]]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5961 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.7167448323310506
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5981 1.0 1.0
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5981 1.0 1.0
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.83187014
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5981 1.0 1.0
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5981 1.0 1.0
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5981 1.0 1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6021 1.0 1.0
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.6021 1.0 1.0
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.018520912129656373
Printing some Q and Qe and total Qs values:  [[1.159]
 [1.159]
 [1.159]
 [1.159]
 [1.102]] [[2.577]
 [2.577]
 [2.577]
 [2.577]
 [2.664]] [[1.13 ]
 [1.13 ]
 [1.13 ]
 [1.13 ]
 [1.131]]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.803]
 [0.85 ]
 [0.66 ]
 [0.794]] [[3.24 ]
 [2.069]
 [2.47 ]
 [3.24 ]
 [2.902]] [[0.718]
 [0.665]
 [0.779]
 [0.718]
 [0.795]]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8184209
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8182109
maxi score, test score, baseline:  0.6061 1.0 1.0
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[6.087]
 [6.087]
 [6.087]
 [6.087]
 [6.087]] [[2.681]
 [2.681]
 [2.681]
 [2.681]
 [2.681]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]] [[3.265]
 [3.801]
 [3.265]
 [3.265]
 [3.265]] [[0.917]
 [1.053]
 [0.917]
 [0.917]
 [0.917]]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6081 1.0 1.0
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.6081 1.0 1.0
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.03 ]
 [1.03 ]
 [0.977]
 [1.03 ]
 [1.021]] [[2.389]
 [2.389]
 [2.616]
 [2.389]
 [2.3  ]] [[1.187]
 [1.187]
 [1.263]
 [1.187]
 [1.133]]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6081 1.0 1.0
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6081 1.0 1.0
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.044]
 [0.135]
 [0.046]
 [0.029]] [[1.655]
 [1.393]
 [0.549]
 [1.156]
 [1.601]] [[1.021]
 [0.833]
 [0.376]
 [0.682]
 [0.953]]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  -0.01850946355814433
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.606]
 [0.655]
 [0.606]
 [0.606]] [[-0.017]
 [-0.017]
 [ 0.264]
 [-0.017]
 [-0.017]] [[0.606]
 [0.606]
 [0.655]
 [0.606]
 [0.606]]
maxi score, test score, baseline:  0.6081 1.0 1.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.80989325
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.8084213
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6161 1.0 1.0
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8157474
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6161 1.0 1.0
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]] [[-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]] [[0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]]
maxi score, test score, baseline:  0.6181 1.0 1.0
maxi score, test score, baseline:  0.6181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6181 1.0 1.0
maxi score, test score, baseline:  0.6181 1.0 1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80769217
maxi score, test score, baseline:  0.6201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6221 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.   ]] [[0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.033]]
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81708723
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6241 1.0 1.0
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6261 1.0 1.0
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6281 1.0 1.0
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80775374
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6281 1.0 1.0
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.655]
 [0.774]
 [0.655]
 [0.655]] [[0.557]
 [0.557]
 [0.284]
 [0.557]
 [0.557]] [[0.655]
 [0.655]
 [0.774]
 [0.655]
 [0.655]]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8451790674808282
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.865]
 [0.845]
 [0.816]
 [0.823]] [[0.432]
 [0.469]
 [0.309]
 [0.432]
 [0.359]] [[0.816]
 [0.865]
 [0.845]
 [0.816]
 [0.823]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.918]
 [1.005]
 [1.083]
 [0.918]
 [0.998]] [[0.951]
 [1.084]
 [1.02 ]
 [0.951]
 [1.04 ]] [[0.719]
 [0.859]
 [0.9  ]
 [0.719]
 [0.832]]
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.997]
 [0.997]
 [0.997]
 [0.997]
 [0.997]] [[0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]] [[1.08]
 [1.08]
 [1.08]
 [1.08]
 [1.08]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6461 1.0 1.0
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.015131119101008198
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.935]
 [0.935]
 [0.935]
 [0.935]
 [0.932]] [[4.514]
 [4.514]
 [4.514]
 [4.514]
 [3.489]] [[1.189]
 [1.189]
 [1.189]
 [1.189]
 [0.946]]
maxi score, test score, baseline:  0.6481 1.0 1.0
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.014625896321497947
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]] [[0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]] [[1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.101]]
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6501 1.0 1.0
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.817]
 [0.91 ]
 [0.817]
 [0.889]] [[3.297]
 [2.505]
 [1.369]
 [2.505]
 [2.122]] [[0.973]
 [0.785]
 [0.51 ]
 [0.785]
 [0.716]]
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6501 1.0 1.0
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.998]
 [1.236]
 [1.173]
 [1.11 ]
 [1.313]] [[2.005]
 [1.871]
 [1.569]
 [1.834]
 [1.781]] [[0.851]
 [0.978]
 [0.82 ]
 [0.871]
 [1.001]]
siam score:  -0.8108864
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.01422483167668708
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.448]
 [0.584]
 [0.448]
 [0.436]] [[0.215]
 [0.215]
 [0.547]
 [0.215]
 [0.208]] [[0.448]
 [0.448]
 [0.584]
 [0.448]
 [0.436]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.039]
 [1.156]
 [1.039]
 [1.039]
 [1.116]] [[5.248]
 [3.921]
 [5.248]
 [5.248]
 [4.969]] [[1.093]
 [0.788]
 [1.093]
 [1.093]
 [1.05 ]]
maxi score, test score, baseline:  0.6541 1.0 1.0
first move QE:  -0.014095152360426647
Printing some Q and Qe and total Qs values:  [[1.051]
 [0.951]
 [0.911]
 [0.821]
 [0.927]] [[2.493]
 [2.407]
 [2.604]
 [0.951]
 [2.385]] [[1.037]
 [0.931]
 [0.993]
 [0.189]
 [0.905]]
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.802165
maxi score, test score, baseline:  0.6561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.6561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 4.412251665346356
maxi score, test score, baseline:  0.6561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6561 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.77 ]] [[4.278]
 [4.278]
 [4.278]
 [4.278]
 [4.421]] [[0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.943]]
maxi score, test score, baseline:  0.6561 1.0 1.0
maxi score, test score, baseline:  0.6561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.108]
 [1.108]
 [1.108]
 [1.108]
 [1.108]] [[2.035]
 [2.035]
 [2.035]
 [2.035]
 [2.035]] [[0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6581 1.0 1.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6581 1.0 1.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  20031
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6601 1.0 1.0
maxi score, test score, baseline:  0.6601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6601 1.0 1.0
maxi score, test score, baseline:  0.6601 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6601 1.0 1.0
maxi score, test score, baseline:  0.6601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6601 1.0 1.0
maxi score, test score, baseline:  0.6601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6641 1.0 1.0
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81595314
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.375 0.208 0.208 0.042 0.167]
maxi score, test score, baseline:  0.6661 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[4.213]
 [4.213]
 [4.213]
 [4.213]
 [4.213]] [[0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]]
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.65 ]
 [0.675]
 [0.581]
 [0.59 ]] [[0.526]
 [3.75 ]
 [3.657]
 [3.292]
 [3.696]] [[0.092]
 [0.917]
 [0.903]
 [0.771]
 [0.88 ]]
Printing some Q and Qe and total Qs values:  [[1.045]
 [1.045]
 [1.396]
 [1.045]
 [1.045]] [[2.148]
 [2.148]
 [2.072]
 [2.148]
 [2.148]] [[1.147]
 [1.147]
 [1.4  ]
 [1.147]
 [1.147]]
maxi score, test score, baseline:  0.6661 1.0 1.0
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81834793
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.81542915
maxi score, test score, baseline:  0.6661 1.0 1.0
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8157697
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8141754
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.999]
 [0.01 ]
 [0.883]
 [0.796]] [[0.538]
 [0.711]
 [0.522]
 [0.688]
 [0.759]] [[0.643]
 [0.804]
 [0.096]
 [0.721]
 [0.687]]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8036907
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.753617597745394
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.962]
 [0.017]
 [1.159]
 [0.972]
 [1.046]] [[2.117]
 [1.061]
 [2.079]
 [2.041]
 [2.038]] [[1.062]
 [0.005]
 [1.189]
 [1.042]
 [1.094]]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.102]
 [1.197]
 [1.102]
 [1.102]
 [1.102]] [[0.593]
 [0.543]
 [0.593]
 [0.593]
 [0.593]] [[0.817]
 [0.903]
 [0.817]
 [0.817]
 [0.817]]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.949]
 [1.001]
 [1.153]
 [0.902]
 [0.983]] [[ 0.346]
 [-0.009]
 [-0.444]
 [ 0.721]
 [ 0.185]] [[0.655]
 [0.648]
 [0.727]
 [0.67 ]
 [0.662]]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8069487
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]] [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]]
siam score:  -0.80641043
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8115321
Printing some Q and Qe and total Qs values:  [[0.962]
 [0.962]
 [0.962]
 [0.962]
 [0.958]] [[2.38 ]
 [2.38 ]
 [2.38 ]
 [2.38 ]
 [2.692]] [[1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.259]]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20031
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.945]
 [0.945]
 [0.945]
 [0.945]
 [0.945]] [[-0.234]
 [-0.234]
 [-0.234]
 [-0.234]
 [-0.234]] [[0.66]
 [0.66]
 [0.66]
 [0.66]
 [0.66]]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20031
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.012097971046578181
maxi score, test score, baseline:  0.6621 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.    0.25  0.167 0.375 0.208]
maxi score, test score, baseline:  0.6621 1.0 1.0
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6601 1.0 1.0
maxi score, test score, baseline:  0.6601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6601 1.0 1.0
maxi score, test score, baseline:  0.6601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6581 1.0 1.0
maxi score, test score, baseline:  0.6581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
