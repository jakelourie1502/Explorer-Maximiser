dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:255000
exp_gamma:0.95
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
UNIT TEST: sample policy line 217 mcts : [0.2 0.4 0.2 0.2 0. ]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
2 27
in main func line 156:  4
using explorer policy with actor:  1
Starting evaluation
siam score:  0.0003164349420165474
rdn probs:  [0.25, 0.25, 0.25, 0.25]
8 99
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  1088 train batches done:  31 episodes:  110
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.07434575282905992
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.0032584511970381266
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.2119246
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.46234772
10 105
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.4 0.  0.2]
19 182
19 184
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.2716510992600642
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.2716510992600642
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.24589436
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.20301372
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
26 235
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.34900257
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.2 0.2 0.2]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.57069546
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72860414
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7368793
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.1398563596213292
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
50 372
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70936084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6693589
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
55 412
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.1214242484643124
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.1214242484643124
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
65 478
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  66
66 489
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
66 498
siam score:  -0.71703875
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
67 502
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.067]
 [1.238]
 [0.674]
 [0.836]
 [1.015]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.184]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]] [[0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]]
siam score:  -0.7245216
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.133]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.396]
 [0.593]
 [0.419]
 [0.615]
 [0.616]] [[0.501]
 [0.764]
 [0.531]
 [0.793]
 [0.794]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
73 553
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.56868523
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
83 599
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.075]] [[0.189]
 [0.189]
 [0.189]
 [0.189]
 [0.139]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.57786644
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
86 631
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6086755
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
87 653
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5647187
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
89 663
89 664
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -7.434710507184379e-05
90 707
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.24 ]
 [0.101]
 [0.083]
 [0.143]
 [0.165]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.022]
 [1.201]
 [1.272]
 [0.438]
 [0.608]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
first move QE:  -0.02862577830392211
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.48597187
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.4864773
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.47579184
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
100 819
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.07 ]
 [-0.088]
 [-0.056]
 [-0.165]
 [-0.06 ]] [[0.099]
 [0.088]
 [0.109]
 [0.036]
 [0.106]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.50353014
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
102 833
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5541008
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
106 849
line 256 mcts: sample exp_bonus 0.0034962754592618842
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
108 864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
111 876
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
111 884
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.63692075
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.   ]
 [-0.005]
 [-0.009]
 [-0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6486569
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.02612968402773016
116 933
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.025708604105119427
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.01973598328939327
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  127
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.024599223431491603
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
129 990
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.006707841153925166
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.66047525
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.001]
 [-0.005]
 [-0.001]
 [-0.004]] [[0.022]
 [0.022]
 [0.018]
 [0.022]
 [0.019]]
siam score:  -0.67988175
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6947638
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.020568801128205597
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.021865652032998926
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.021865652032998926
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.017]] [[0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.   ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.016]
 [-0.015]
 [-0.016]
 [-0.016]
 [-0.016]] [[0.002]
 [0.003]
 [0.002]
 [0.002]
 [0.002]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.016]
 [ 0.   ]
 [-0.015]
 [ 0.   ]
 [-0.017]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
147 1149
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
147 1172
siam score:  -0.6836758
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.60469925
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.66137433
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
156 1247
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
158 1256
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
164 1310
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.56996053
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
171 1362
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.014]
 [-0.007]
 [-0.006]
 [-0.004]] [[0.004]
 [0.001]
 [0.004]
 [0.004]
 [0.005]]
siam score:  -0.6151624
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6519014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.65399164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.008]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.007]
 [0.004]
 [0.004]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
174 1398
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.0028461888197817255
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.583 0.083 0.167]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7054057
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.017379590289673993
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.68578565
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.006]
 [-0.008]
 [-0.015]
 [-0.007]
 [-0.007]] [[0.011]
 [0.009]
 [0.002]
 [0.01 ]
 [0.01 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.015]
 [ 0.   ]
 [-0.015]
 [ 0.   ]
 [ 0.   ]] [[0.   ]
 [0.006]
 [0.001]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6641152
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.083 0.417 0.167 0.167 0.167]
Printing some Q and Qe and total Qs values:  [[1.5]
 [0. ]
 [0. ]
 [0. ]
 [0. ]] [[0.   ]
 [0.002]
 [0.002]
 [0.001]
 [0.001]] [[3.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 1.3107920736408092e-08
0.0 1.3386425109011818e-08
0.0 1.5313848351528985e-08
0.0 1.5010140198121005e-08
0.0 0.0
0.0 0.0
0.0 0.0
0.0 1.2707843150703599e-08
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.53383756
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.0009105172709565154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.39000514
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.38069475
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.37700593
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
192 1544
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.39683518
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.003]
 [0.002]
 [0.002]] [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.66460824
siam score:  -0.6780739
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.007]
 [-0.008]
 [ 0.007]
 [ 0.007]
 [ 0.007]] [[0.014]
 [0.004]
 [0.014]
 [0.014]
 [0.014]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6995369
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70602524
first move QE:  -0.015672036174125385
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.009]
 [-0.008]
 [-0.008]
 [-0.009]
 [-0.009]] [[0.   ]
 [0.001]
 [0.001]
 [0.001]
 [0.   ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70851535
202 1615
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.015472622362310994
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6946047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.014]
 [-0.014]
 [-0.014]
 [-0.013]
 [-0.013]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]]
line 256 mcts: sample exp_bonus -0.0138903072234318
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
STARTED EXPV TRAINING ON FRAME NO.  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.332]
 [-0.055]
 [-0.063]
 [-0.052]
 [-0.086]] [[0.   ]
 [0.093]
 [0.09 ]
 [0.093]
 [0.082]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20032
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
217 1721
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20032
siam score:  -0.7299087
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
219 1777
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7308701
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
219 1814
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71996915
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
221 1843
first move QE:  -0.0369872828828999
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.078]
 [ 0.078]
 [-0.029]
 [ 0.078]
 [ 0.078]] [[0.563]
 [0.563]
 [0.42 ]
 [0.563]
 [0.563]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20032
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
226 1888
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20032
siam score:  -0.71200997
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
229 1895
line 256 mcts: sample exp_bonus 2.5478925636193677
start point for exploration sampling:  20032
233 1898
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7071947
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6632852
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.138]
 [1.138]
 [3.138]
 [1.138]
 [1.138]] [[0.394]
 [0.394]
 [1.312]
 [0.394]
 [0.394]]
241 1910
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.738]
 [1.137]
 [2.521]
 [2.059]
 [1.137]] [[0.3  ]
 [0.529]
 [1.324]
 [1.058]
 [0.529]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.08260230756357
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.204]
 [3.204]
 [3.204]
 [3.363]
 [3.204]] [[1.708]
 [1.708]
 [1.708]
 [1.838]
 [1.708]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7066675
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.037251932636919616
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.077]
 [3.077]
 [3.077]
 [3.077]
 [3.077]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.164]
 [6.164]
 [4.9  ]
 [6.164]
 [6.164]] [[2.   ]
 [2.   ]
 [1.593]
 [2.   ]
 [2.   ]]
siam score:  -0.67232287
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.65075624
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6553424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.468]
 [2.468]
 [2.468]
 [2.468]
 [2.468]] [[1.834]
 [1.834]
 [1.834]
 [1.834]
 [1.834]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.0576804765726977
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.70426404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.401]
 [3.976]
 [4.161]
 [4.14 ]
 [4.361]] [[1.198]
 [1.678]
 [1.833]
 [1.816]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72055763
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72498167
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7286136
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.01986367533723892
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.438]
 [1.438]
 [1.438]
 [1.438]
 [1.438]] [[1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.232]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.]
 [1.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.865]
 [0.636]
 [0.438]] [[1.]
 [1.]
 [0.]
 [0.]
 [0.]]
start point for exploration sampling:  20032
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.054]
 [3.054]
 [3.054]
 [3.054]
 [2.618]] [[1.092]
 [1.092]
 [1.092]
 [1.092]
 [0.801]]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.705352
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.082]
 [2.236]
 [3.138]
 [3.21 ]
 [3.089]] [[0.987]
 [0.424]
 [1.025]
 [1.073]
 [0.992]]
siam score:  -0.69918996
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.017]
 [1.235]
 [2.523]
 [2.337]
 [2.432]] [[1.378]
 [0.773]
 [1.77 ]
 [1.626]
 [1.7  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.475]
 [7.772]
 [4.475]
 [4.475]
 [4.475]] [[0.408]
 [1.509]
 [0.408]
 [0.408]
 [0.408]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.523]
 [2.523]
 [2.523]
 [2.523]
 [2.36 ]] [[1.07 ]
 [1.07 ]
 [1.07 ]
 [1.07 ]
 [0.961]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
314 2020
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.349]
 [2.349]
 [2.349]
 [2.349]
 [2.349]] [[0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.37 ]
 [5.37 ]
 [5.37 ]
 [5.37 ]
 [5.997]] [[1.532]
 [1.532]
 [1.532]
 [1.532]
 [1.812]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.816]
 [1.816]
 [1.816]
 [1.816]
 [1.816]] [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  319
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.803]
 [5.452]
 [3.537]
 [5.452]
 [5.452]] [[1.971]
 [1.756]
 [0.58 ]
 [1.756]
 [1.756]]
using explorer policy with actor:  1
in main func line 156:  322
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.675235
328 2043
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.705]
 [5.705]
 [5.705]
 [5.705]
 [4.53 ]] [[1.986]
 [1.986]
 [1.986]
 [1.986]
 [1.321]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6790415
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6841283
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.641]
 [1.35 ]
 [1.751]
 [1.764]
 [1.603]] [[0.   ]
 [0.472]
 [0.739]
 [0.748]
 [0.641]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
331 2052
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.289]
 [4.289]
 [3.488]
 [4.289]
 [4.284]] [[1.467]
 [1.467]
 [0.933]
 [1.467]
 [1.463]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.47 ]
 [3.932]
 [3.872]
 [3.2  ]
 [3.615]] [[1.452]
 [1.886]
 [1.83 ]
 [1.198]
 [1.589]]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.041]
 [ 0.349]
 [ 0.118]
 [ 0.118]
 [ 0.118]] [[0.   ]
 [0.26 ]
 [0.106]
 [0.106]
 [0.106]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.896]
 [3.896]
 [3.896]
 [3.896]
 [3.896]] [[1.692]
 [1.692]
 [1.692]
 [1.692]
 [1.692]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150181725206463, 0.07150181725206463, 0.07150181725206463, 0.785494548243806]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150181725206463, 0.07150181725206463, 0.07150181725206463, 0.785494548243806]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150184172666571, 0.07150184172666571, 0.07150184172666571, 0.7854944748200028]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150184172666571, 0.07150184172666571, 0.07150184172666571, 0.7854944748200028]
siam score:  -0.6708233
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150184172666571, 0.07150184172666571, 0.07150184172666571, 0.7854944748200028]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150189067584772, 0.07150189067584772, 0.07150189067584772, 0.7854943279724568]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.62809294
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150193962500291, 0.07150193962500291, 0.07150193962500291, 0.7854941811249913]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.059]
 [5.059]
 [6.059]
 [5.059]
 [4.609]] [[1.534]
 [1.534]
 [1.851]
 [1.534]
 [1.391]]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.001]
 [0.   ]] [[-0.197]
 [-0.176]
 [ 0.038]
 [-0.017]
 [-0.005]] [[0.262]
 [0.291]
 [0.575]
 [0.502]
 [0.517]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
UNIT TEST: sample policy line 217 mcts : [0.25  0.042 0.083 0.5   0.125]
siam score:  -0.62748545
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.659534
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
359 2093
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
361 2109
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
siam score:  -0.6658176
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]] [[0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.117]
 [0.503]
 [0.117]
 [0.117]
 [0.235]] [[0.093]
 [0.608]
 [0.093]
 [0.093]
 [0.25 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
using another actor
from probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
using explorer policy with actor:  1
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
rdn probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.001]
 [0.002]
 [0.002]] [[1.091]
 [1.5  ]
 [1.704]
 [0.851]
 [1.287]] [[0.841]
 [1.277]
 [1.496]
 [0.584]
 [1.051]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[ 0.205]
 [ 0.684]
 [ 0.273]
 [-0.139]
 [ 0.37 ]] [[1.389]
 [2.001]
 [1.476]
 [0.949]
 [1.599]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[2.218]
 [2.218]
 [2.218]
 [2.218]
 [2.218]] [[1.195]
 [1.195]
 [1.195]
 [1.195]
 [1.195]]
start point for exploration sampling:  20032
first move QE:  0.014057434417281238
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.011]
 [0.01 ]
 [0.007]
 [0.014]] [[-0.102]
 [-0.421]
 [-0.136]
 [-0.058]
 [-0.19 ]] [[0.542]
 [0.113]
 [0.492]
 [0.591]
 [0.428]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
siam score:  -0.7292266
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.76017046
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.7621166
387 2176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.75471425
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[2.076]
 [2.076]
 [2.076]
 [2.076]
 [2.076]] [[2.002]
 [2.002]
 [2.002]
 [2.002]
 [2.002]]
first move QE:  0.013457922264922118
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.7305919
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[4.263]
 [4.263]
 [4.436]
 [4.263]
 [4.263]] [[1.902]
 [1.902]
 [2.002]
 [1.902]
 [1.902]]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.002]] [[5.495]
 [5.495]
 [5.478]
 [5.495]
 [5.665]] [[1.766]
 [1.766]
 [1.755]
 [1.766]
 [1.877]]
in main func line 156:  398
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.004]
 [0.002]
 [0.002]] [[4.255]
 [4.255]
 [5.855]
 [4.255]
 [4.255]] [[1.112]
 [1.112]
 [2.002]
 [1.112]
 [1.112]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.01658431460432013
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
403 2202
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.7282047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.7247947
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.725701
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.01 ]
 [0.011]
 [0.009]
 [0.009]] [[1.534]
 [1.854]
 [1.523]
 [1.534]
 [1.534]] [[1.533]
 [1.906]
 [1.524]
 [1.533]
 [1.533]]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[1.972]
 [1.972]
 [1.972]
 [1.972]
 [1.972]] [[1.752]
 [1.752]
 [1.752]
 [1.752]
 [1.752]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.012]
 [0.011]
 [0.011]
 [0.011]] [[2.439]
 [2.727]
 [2.439]
 [2.439]
 [2.439]] [[1.84 ]
 [2.004]
 [1.84 ]
 [1.84 ]
 [1.84 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
first move QE:  0.01281694059815207
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.708 0.042 0.083]
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.74940085
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7413658
siam score:  -0.7412874
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.7646632
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
423 2248
using another actor
424 2257
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
426 2268
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.7738013
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.7803833
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[2.006]
 [1.791]
 [1.791]
 [1.791]
 [1.791]] [[1.09 ]
 [0.874]
 [0.874]
 [0.874]
 [0.874]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  446
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.75265604
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.002]
 [0.001]
 [0.001]] [[2.839]
 [2.599]
 [2.288]
 [2.839]
 [3.446]] [[0.754]
 [0.579]
 [0.351]
 [0.754]
 [1.197]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
454 2320
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.002]] [[4.25 ]
 [3.657]
 [3.657]
 [3.657]
 [4.834]] [[1.609]
 [1.212]
 [1.212]
 [1.212]
 [2.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.75979596
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.003]
 [0.   ]
 [0.001]
 [0.001]] [[0.648]
 [1.42 ]
 [1.051]
 [0.787]
 [0.963]] [[0.085]
 [0.347]
 [0.22 ]
 [0.132]
 [0.19 ]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[2.646]
 [2.646]
 [2.646]
 [2.646]
 [2.646]] [[2.005]
 [2.005]
 [2.005]
 [2.005]
 [2.005]]
in main func line 156:  462
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.035]
 [0.035]
 [0.035]
 [0.033]] [[3.215]
 [2.281]
 [2.281]
 [2.281]
 [2.729]] [[1.647]
 [0.879]
 [0.879]
 [0.879]
 [1.241]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using another actor
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.872]
 [1.385]
 [0.872]
 [1.508]
 [0.872]] [[0.025]
 [0.196]
 [0.025]
 [0.237]
 [0.025]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.895]
 [1.091]
 [1.33 ]
 [1.34 ]
 [1.116]] [[1.137]
 [1.323]
 [1.551]
 [1.561]
 [1.347]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.0001967868976736
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
line 256 mcts: sample exp_bonus 5.004157412530538
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.022]
 [0.012]
 [0.011]] [[3.938]
 [4.489]
 [5.051]
 [4.838]
 [4.892]] [[0.866]
 [1.171]
 [1.505]
 [1.371]
 [1.4  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0002],
        [0.0000],
        [0.0003],
        [0.0003],
        [0.0003],
        [0.0118],
        [0.0000],
        [0.0000],
        [0.0003],
        [0.0000]], dtype=torch.float64)
0.0 0.00021016793356592317
0.99 0.99
0.0 0.00030527695041395386
0.0 0.00028196218173614563
0.0 0.0003136315978551498
0.0 0.011760388430967344
0.0 0.0
0.0 0.0
0.0 0.0003132083697405206
0.99 0.99
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
UNIT TEST: sample policy line 217 mcts : [0.125 0.167 0.125 0.417 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
478 2362
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.006]
 [0.002]
 [0.004]] [[4.939]
 [4.776]
 [4.73 ]
 [3.573]
 [5.384]] [[1.417]
 [1.336]
 [1.317]
 [0.728]
 [1.644]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.016]] [[4.032]
 [4.032]
 [4.032]
 [4.032]
 [4.372]] [[1.713]
 [1.713]
 [1.713]
 [1.713]
 [2.007]]
siam score:  -0.753082
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.001]] [[0.254]
 [0.058]
 [0.564]
 [0.254]
 [1.057]] [[0.219]
 [0.035]
 [0.509]
 [0.219]
 [0.969]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.7445384
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.168]
 [2.568]
 [2.518]
 [2.357]
 [2.573]] [[0.466]
 [0.6  ]
 [0.583]
 [0.529]
 [0.601]]
siam score:  -0.7502939
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
first move QE:  -0.020320971642669477
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
492 2385
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.7739521
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
first move QE:  -0.020384598784675877
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[-1.569]
 [-0.538]
 [-0.646]
 [-1.309]
 [-0.752]] [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.031]
 [0.021]
 [0.019]] [[2.05 ]
 [2.05 ]
 [2.09 ]
 [2.05 ]
 [1.833]] [[1.036]
 [1.036]
 [1.071]
 [1.036]
 [0.895]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.943]
 [1.383]
 [1.383]
 [1.383]
 [1.383]] [[1.366]
 [1.952]
 [1.952]
 [1.952]
 [1.952]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.005]
 [0.008]
 [0.008]] [[1.078]
 [1.078]
 [0.438]
 [1.078]
 [1.078]] [[1.653]
 [1.653]
 [0.795]
 [1.653]
 [1.653]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.008]
 [0.002]
 [0.002]] [[-1.948]
 [-0.793]
 [-0.765]
 [-1.173]
 [-1.155]] [[0.001]
 [0.889]
 [0.92 ]
 [0.599]
 [0.613]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.06339985633329384
507 2408
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.78685355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]] [[1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.576]]
508 2412
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.018]
 [0.171]
 [0.026]
 [0.022]] [[0.671]
 [0.206]
 [2.011]
 [2.493]
 [2.16 ]] [[0.385]
 [0.115]
 [1.338]
 [1.497]
 [1.293]]
siam score:  -0.7827841
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.003]
 [0.003]
 [0.002]
 [0.002]] [[5.11 ]
 [7.161]
 [4.642]
 [4.812]
 [4.203]] [[1.093]
 [1.628]
 [0.974]
 [1.017]
 [0.859]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
first move QE:  -0.029733801106018948
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.014]
 [0.02 ]
 [0.014]
 [0.018]] [[2.411]
 [3.801]
 [3.683]
 [3.801]
 [3.772]] [[0.481]
 [1.56 ]
 [1.475]
 [1.56 ]
 [1.543]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[-1.976]
 [-0.807]
 [-0.841]
 [-1.726]
 [-1.458]] [[0.   ]
 [1.153]
 [1.12 ]
 [0.246]
 [0.511]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.039]
 [0.036]
 [0.023]
 [0.023]] [[2.812]
 [3.069]
 [2.973]
 [2.812]
 [2.812]] [[1.399]
 [1.702]
 [1.594]
 [1.399]
 [1.399]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.77091044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.76555437
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.76449144
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.708]
 [2.708]
 [2.708]
 [2.708]
 [2.708]] [[1.805]
 [1.805]
 [1.805]
 [1.805]
 [1.805]]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]] [[1.964]
 [1.964]
 [1.964]
 [1.964]
 [1.964]] [[1.579]
 [1.579]
 [1.579]
 [1.579]
 [1.579]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.025]] [[1.692]
 [1.692]
 [1.692]
 [1.692]
 [1.993]] [[0.886]
 [0.886]
 [0.886]
 [0.886]
 [1.282]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[5.835]
 [5.835]
 [6.098]
 [5.835]
 [6.521]] [[0.978]
 [0.978]
 [1.082]
 [0.978]
 [1.249]]
533 2441
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Starting evaluation
siam score:  -0.7582901
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.043791415226642524
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.   ]
 [0.001]] [[-0.319]
 [-0.018]
 [-0.089]
 [-0.329]
 [-0.336]] [[0.001]
 [0.   ]
 [0.001]
 [0.   ]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
rdn probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.266953650148831
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.7688395
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.016]
 [0.018]
 [0.018]] [[1.177]
 [1.177]
 [1.554]
 [1.177]
 [1.177]] [[1.166]
 [1.166]
 [1.665]
 [1.166]
 [1.166]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[2.892]
 [2.892]
 [2.892]
 [2.892]
 [2.892]] [[1.931]
 [1.931]
 [1.931]
 [1.931]
 [1.931]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
siam score:  -0.78057116
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
line 256 mcts: sample exp_bonus 1.972868581019065
siam score:  -0.78189164
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.01 ]
 [0.017]
 [0.017]] [[1.939]
 [1.939]
 [1.693]
 [1.939]
 [1.939]] [[1.564]
 [1.564]
 [1.222]
 [1.564]
 [1.564]]
555 2505
siam score:  -0.7835664
556 2506
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.78377974
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.205]
 [1.205]
 [1.026]
 [1.262]
 [1.095]] [[0.44 ]
 [0.44 ]
 [0.321]
 [0.478]
 [0.367]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.004]] [[1.041]
 [1.041]
 [1.041]
 [1.041]
 [0.167]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.7677081
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.76761746
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
564 2522
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.7646083
first move QE:  -0.06189552296734667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.76454973
using another actor
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]] [[0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.007]
 [0.009]
 [0.009]] [[0.883]
 [0.883]
 [1.016]
 [0.883]
 [0.883]] [[1.302]
 [1.302]
 [1.476]
 [1.302]
 [1.302]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.77864677
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.77778137
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.001]
 [0.002]
 [0.003]] [[-0.251]
 [-0.28 ]
 [-0.173]
 [-0.377]
 [-0.316]] [[0.344]
 [0.304]
 [0.446]
 [0.176]
 [0.258]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.002]
 [0.005]
 [0.006]
 [0.006]] [[ 0.938]
 [-0.021]
 [ 1.213]
 [ 1.072]
 [ 0.997]] [[0.667]
 [0.021]
 [0.85 ]
 [0.756]
 [0.707]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[-0.803]
 [-0.265]
 [ 0.214]
 [-0.369]
 [ 0.075]] [[0.002]
 [0.719]
 [1.357]
 [0.581]
 [1.175]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
first move QE:  -0.06330958488270093
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
581 2571
UNIT TEST: sample policy line 217 mcts : [0.417 0.042 0.042 0.    0.5  ]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
using another actor
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.80565786
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.002]
 [0.003]
 [0.002]] [[0.842]
 [0.945]
 [1.59 ]
 [0.688]
 [1.465]] [[0.66 ]
 [0.728]
 [1.156]
 [0.558]
 [1.074]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.80253404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
593 2580
in main func line 156:  594
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
597 2586
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.008]
 [0.003]
 [0.006]] [[3.023]
 [3.023]
 [3.019]
 [3.023]
 [3.48 ]] [[1.278]
 [1.278]
 [1.282]
 [1.278]
 [1.649]]
599 2591
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.625 0.042 0.25 ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[4.427]
 [4.427]
 [4.427]
 [4.427]
 [4.427]] [[1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[ 0.   ]
 [-1.409]
 [-1.   ]
 [-1.883]
 [-1.472]] [[0.001]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
siam score:  -0.84286904
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.125 0.25  0.167 0.042 0.417]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
line 256 mcts: sample exp_bonus -0.10710429332668918
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[0.115]
 [0.   ]
 [0.258]
 [0.119]
 [0.121]] [[0.124]
 [0.047]
 [0.219]
 [0.126]
 [0.128]]
line 256 mcts: sample exp_bonus 0.6495547924010616
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
using explorer policy with actor:  1
627 2651
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.01 ]
 [0.009]
 [0.007]] [[2.702]
 [2.702]
 [1.534]
 [2.702]
 [2.699]] [[1.766]
 [1.766]
 [0.876]
 [1.766]
 [1.761]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[4.32 ]
 [4.32 ]
 [4.32 ]
 [4.32 ]
 [4.126]] [[1.356]
 [1.356]
 [1.356]
 [1.356]
 [1.218]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.8042678
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.031]
 [-0.087]
 [-0.085]
 [-0.031]
 [-0.087]] [[1.995]
 [1.937]
 [1.939]
 [1.995]
 [1.937]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.023]
 [0.02 ]
 [0.011]
 [0.011]] [[0.197]
 [0.451]
 [2.332]
 [1.961]
 [2.879]] [[0.004]
 [0.188]
 [1.459]
 [1.198]
 [1.82 ]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[2.662]
 [2.662]
 [2.662]
 [2.662]
 [2.662]] [[1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0002],
        [0.0000],
        [0.0019],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0002],
        [0.0044],
        [0.0254],
        [0.0002]], dtype=torch.float64)
0.0 0.00016687749913865086
0.0 0.0
0.0 0.0019089569930751155
0.0 0.0
0.9801 0.9801
0.0 0.0
0.0 0.0002094208796988661
0.0 0.004395181987049562
0.0 0.025432788712318986
0.0 0.0001789512417693685
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
line 256 mcts: sample exp_bonus 2.790134055515957
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
start point for exploration sampling:  20032
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[1.54]
 [1.54]
 [1.54]
 [1.54]
 [1.65]] [[1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.214]]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
siam score:  -0.80919397
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
line 256 mcts: sample exp_bonus 3.9260896342888643
using explorer policy with actor:  1
from probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
using explorer policy with actor:  1
start point for exploration sampling:  20032
using another actor
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus -0.342912624168567
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
siam score:  -0.81022906
siam score:  -0.81296766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
first move QE:  -0.10610185572402275
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.003]] [[0.052]
 [0.052]
 [0.052]
 [0.052]
 [1.355]] [[0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [1.125]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.833 0.042 0.042]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.10846817796875281
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
siam score:  -0.8800866
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.016]
 [0.005]
 [0.005]
 [0.011]] [[0.221]
 [0.482]
 [0.221]
 [0.221]
 [0.554]] [[1.415]
 [1.629]
 [1.415]
 [1.415]
 [1.677]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[-0.582]
 [-0.582]
 [-0.369]
 [-0.582]
 [-0.582]] [[0.058]
 [0.058]
 [0.343]
 [0.058]
 [0.058]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.07150201304868534, 0.7854939608539441]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8850705
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.87588716
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.012]
 [0.008]
 [0.003]
 [0.003]] [[-1.277]
 [-0.624]
 [-0.619]
 [-0.909]
 [-1.012]] [[0.003]
 [0.576]
 [0.575]
 [0.319]
 [0.231]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.053]
 [0.058]
 [0.053]
 [0.053]] [[2.878]
 [2.878]
 [3.265]
 [2.878]
 [2.878]] [[1.523]
 [1.523]
 [1.963]
 [1.523]
 [1.523]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.012]
 [0.012]
 [0.012]
 [0.011]] [[2.703]
 [2.809]
 [2.809]
 [2.809]
 [1.913]] [[0.006]
 [0.012]
 [0.012]
 [0.012]
 [0.011]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.376479031901113
using explorer policy with actor:  0
siam score:  -0.86632454
using explorer policy with actor:  0
first move QE:  -0.1192917198979011
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.015]] [[2.841]
 [2.841]
 [2.841]
 [2.841]
 [2.874]] [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.015]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.005]
 [0.003]
 [0.003]
 [0.004]] [[-0.218]
 [ 0.934]
 [-0.076]
 [-0.076]
 [ 0.285]] [[0.003]
 [0.005]
 [0.003]
 [0.003]
 [0.004]]
rdn probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.8945415
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.8908906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.037]
 [0.025]
 [0.025]
 [0.025]] [[1.485]
 [1.479]
 [1.485]
 [1.485]
 [1.485]] [[1.379]
 [1.396]
 [1.379]
 [1.379]
 [1.379]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0644999321704576
first move QE:  -0.12354091428949326
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.027]
 [0.04 ]
 [0.027]
 [0.034]] [[3.065]
 [1.539]
 [1.175]
 [1.539]
 [1.33 ]] [[1.099]
 [0.611]
 [0.5  ]
 [0.611]
 [0.547]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.8797133
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
721 2778
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
start point for exploration sampling:  20032
siam score:  -0.90419215
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
724 2779
using another actor
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
UNIT TEST: sample policy line 217 mcts : [0.042 0.125 0.5   0.25  0.083]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.062]
 [0.031]
 [0.042]
 [0.037]] [[2.046]
 [1.973]
 [1.138]
 [0.848]
 [2.045]] [[1.316]
 [1.289]
 [0.724]
 [0.548]
 [1.311]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
start point for exploration sampling:  20032
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.025]
 [0.03 ]
 [0.025]
 [0.027]] [[1.23 ]
 [1.23 ]
 [1.213]
 [1.23 ]
 [1.279]] [[0.895]
 [0.895]
 [0.883]
 [0.895]
 [0.965]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.031]
 [0.008]
 [0.008]
 [0.011]] [[-0.268]
 [-0.363]
 [-0.268]
 [-0.268]
 [-0.061]] [[1.514]
 [1.468]
 [1.514]
 [1.514]
 [1.678]]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.9142691
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.9105849
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.024]] [[3.251]
 [3.251]
 [3.251]
 [3.251]
 [4.207]] [[1.079]
 [1.079]
 [1.079]
 [1.079]
 [1.759]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5106471432139508
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.91096663
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.018]
 [0.016]
 [0.016]
 [0.021]] [[3.239]
 [2.497]
 [3.239]
 [3.239]
 [2.287]] [[0.752]
 [0.508]
 [0.752]
 [0.752]
 [0.443]]
siam score:  -0.90001905
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.89376974
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.89420134
first move QE:  -0.13425960589130126
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.01 ]
 [0.015]
 [0.013]
 [0.013]] [[4.138]
 [4.869]
 [1.915]
 [1.559]
 [3.579]] [[1.614]
 [2.003]
 [0.427]
 [0.235]
 [1.315]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
first move QE:  -0.1363189419124026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
siam score:  -0.881737
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
760 2823
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.011]
 [0.007]
 [0.003]
 [0.01 ]] [[1.509]
 [1.189]
 [1.05 ]
 [1.1  ]
 [1.026]] [[1.239]
 [0.833]
 [0.656]
 [0.709]
 [0.632]]
763 2823
siam score:  -0.8759567
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
764 2824
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.07149589420125588, 0.7855123173962324]
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[2.736]
 [2.736]
 [2.736]
 [2.736]
 [3.447]] [[1.36 ]
 [1.36 ]
 [1.36 ]
 [1.36 ]
 [2.072]]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.014]
 [0.016]
 [0.014]] [[4.448]
 [4.448]
 [2.322]
 [2.821]
 [3.603]] [[1.579]
 [1.579]
 [0.156]
 [0.493]
 [1.009]]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[4.083]
 [4.083]
 [4.083]
 [4.083]
 [4.083]] [[2.745]
 [2.745]
 [2.745]
 [2.745]
 [2.745]]
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
768 2828
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.015]
 [0.01 ]
 [0.01 ]
 [0.01 ]] [[0.62 ]
 [0.951]
 [0.62 ]
 [0.62 ]
 [0.62 ]] [[1.285]
 [1.735]
 [1.285]
 [1.285]
 [1.285]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.87267774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
using another actor
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
using explorer policy with actor:  0
siam score:  -0.8702524
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
line 256 mcts: sample exp_bonus 3.201595842879609
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.8677
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
siam score:  -0.8808897
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.33495022741078384
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
using another actor
from probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.07149385449222193, 0.7855184365233342]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 2.9092867311930735
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.077]
 [0.078]
 [0.074]
 [0.065]] [[2.534]
 [2.228]
 [2.501]
 [2.318]
 [2.538]] [[1.406]
 [1.091]
 [1.383]
 [1.181]
 [1.402]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.13812781425400963
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.3053023235146353
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
siam score:  -0.9007179
siam score:  -0.8987088
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
from probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]] [[4.159]
 [4.159]
 [4.159]
 [4.159]
 [4.159]] [[2.023]
 [2.023]
 [2.023]
 [2.023]
 [2.023]]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.91112053
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.88964325
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]] [[2.051]
 [2.051]
 [2.051]
 [2.051]
 [2.879]] [[1.365]
 [1.365]
 [1.365]
 [1.365]
 [1.984]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.01 ]
 [0.013]] [[0.533]
 [0.533]
 [0.533]
 [0.667]
 [0.533]] [[1.229]
 [1.229]
 [1.229]
 [1.401]
 [1.229]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
820 2894
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]] [[1.051]
 [1.051]
 [1.051]
 [1.051]
 [1.051]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]] [[2.026]
 [2.026]
 [2.026]
 [2.026]
 [2.026]] [[1.764]
 [1.764]
 [1.764]
 [1.764]
 [1.764]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
line 256 mcts: sample exp_bonus -0.10718768057282704
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.06 ]
 [0.06 ]
 [0.059]
 [0.06 ]
 [0.06 ]] [[1.525]
 [1.525]
 [1.888]
 [1.525]
 [1.525]] [[1.277]
 [1.277]
 [1.758]
 [1.277]
 [1.277]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.415127736415116
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.01 ]
 [0.01 ]
 [0.006]
 [0.006]] [[ 0.   ]
 [ 0.   ]
 [-0.044]
 [ 0.029]
 [-0.578]] [[0.783]
 [0.783]
 [0.726]
 [0.814]
 [0.004]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using another actor
siam score:  -0.893035
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.8949944
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.006]
 [0.009]
 [0.004]
 [0.004]] [[-0.952]
 [-1.134]
 [-0.388]
 [-0.683]
 [-0.791]] [[0.004]
 [0.006]
 [0.009]
 [0.004]
 [0.004]]
start point for exploration sampling:  20032
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.075]
 [0.065]
 [0.065]
 [0.052]] [[1.853]
 [1.767]
 [1.853]
 [1.853]
 [3.48 ]] [[0.91 ]
 [0.861]
 [0.91 ]
 [0.91 ]
 [2.02 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.081]
 [0.063]
 [0.066]
 [0.071]] [[1.507]
 [1.744]
 [1.623]
 [1.363]
 [1.572]] [[0.563]
 [0.721]
 [0.605]
 [0.438]
 [0.587]]
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.004]
 [0.002]
 [0.004]
 [0.006]] [[-0.22 ]
 [ 0.   ]
 [-0.201]
 [-0.13 ]
 [-0.664]] [[0.006]
 [0.004]
 [0.002]
 [0.004]
 [0.006]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[1.958]
 [1.958]
 [1.958]
 [1.958]
 [1.958]] [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.008]] [[2.216]
 [1.576]
 [2.216]
 [2.216]
 [2.006]] [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.008]]
rdn probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.208 0.667]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 1.1806088306854687
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.8818139
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.008]
 [0.018]
 [0.018]] [[0.595]
 [0.595]
 [0.522]
 [0.595]
 [0.595]] [[1.213]
 [1.213]
 [1.097]
 [1.213]
 [1.213]]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.013]
 [0.014]
 [0.016]] [[1.068]
 [1.068]
 [0.35 ]
 [0.525]
 [0.57 ]] [[1.707]
 [1.707]
 [0.813]
 [1.032]
 [1.093]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.007]
 [0.007]
 [0.008]
 [0.01 ]] [[1.299]
 [1.047]
 [0.432]
 [0.68 ]
 [0.129]] [[1.409]
 [1.182]
 [0.642]
 [0.861]
 [0.38 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.8866032
using explorer policy with actor:  1
first move QE:  -0.11374643248206033
873 2944
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.89301664
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.9036963
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.90343076
siam score:  -0.9043107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
first move QE:  -0.11237745942402369
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]] [[1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.048]] [[0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
start point for exploration sampling:  20032
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]] [[1.794]
 [1.794]
 [1.794]
 [1.794]
 [1.794]] [[1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.802]]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.049]
 [0.044]
 [0.046]
 [0.046]] [[1.513]
 [2.258]
 [1.91 ]
 [1.513]
 [1.513]] [[0.902]
 [1.368]
 [1.147]
 [0.902]
 [0.902]]
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.031]
 [0.033]
 [0.035]
 [0.033]] [[ 1.551]
 [-0.322]
 [ 0.282]
 [ 1.874]
 [ 1.551]] [[1.678]
 [0.016]
 [0.554]
 [1.967]
 [1.678]]
line 256 mcts: sample exp_bonus 0.2078141709906311
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
using explorer policy with actor:  1
from probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
UNIT TEST: sample policy line 217 mcts : [0.667 0.042 0.083 0.125 0.083]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.009]
 [0.038]
 [0.039]
 [0.038]] [[ 0.956]
 [-0.343]
 [ 0.367]
 [ 0.155]
 [ 0.67 ]] [[1.185]
 [0.009]
 [0.671]
 [0.487]
 [0.938]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.9254115
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.033]
 [0.035]
 [0.035]] [[0.371]
 [0.371]
 [0.987]
 [0.371]
 [0.371]] [[1.274]
 [1.274]
 [1.903]
 [1.274]
 [1.274]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
first move QE:  -0.10778529877371361
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.91228974
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.007]
 [0.021]
 [0.023]
 [0.027]] [[1.209]
 [0.117]
 [1.236]
 [1.258]
 [0.96 ]] [[1.137]
 [0.009]
 [1.157]
 [1.182]
 [0.892]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]] [[3.335]
 [2.823]
 [2.823]
 [2.823]
 [2.823]] [[0.638]
 [0.472]
 [0.472]
 [0.472]
 [0.472]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.035]
 [0.023]
 [0.023]
 [0.023]] [[2.347]
 [1.537]
 [1.551]
 [1.551]
 [1.551]] [[2.014]
 [1.406]
 [1.403]
 [1.403]
 [1.403]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
siam score:  -0.9093585
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
910 2966
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.036]
 [0.025]
 [0.054]
 [0.04 ]] [[ 2.679]
 [ 2.518]
 [-0.064]
 [ 1.927]
 [ 2.462]] [[1.545]
 [1.448]
 [0.016]
 [1.153]
 [1.423]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149181473657325, 0.07149181473657325, 0.07149181473657325, 0.7855245557902802]
first move QE:  -0.10257398325414346
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.039]
 [0.038]
 [0.038]
 [0.038]] [[3.809]
 [4.296]
 [3.809]
 [3.809]
 [3.809]] [[1.455]
 [1.763]
 [1.455]
 [1.455]
 [1.455]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149222269143224, 0.07149222269143224, 0.07149222269143224, 0.7855233319257032]
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.038]] [[4.054]
 [4.054]
 [4.054]
 [4.054]
 [4.054]] [[2.018]
 [2.018]
 [2.018]
 [2.018]
 [2.018]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
siam score:  -0.92075574
first move QE:  -0.10083199999280097
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
using explorer policy with actor:  0
first move QE:  -0.10135486912177553
using explorer policy with actor:  1
siam score:  -0.91330504
siam score:  -0.9131601
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
first move QE:  -0.10218754626780108
siam score:  -0.91256696
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
siam score:  -0.91234046
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
start point for exploration sampling:  20032
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.041]] [[5.172]
 [5.172]
 [5.172]
 [5.172]
 [5.743]] [[1.589]
 [1.589]
 [1.589]
 [1.589]
 [1.809]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.053]
 [0.053]
 [0.06 ]
 [0.046]] [[3.117]
 [3.229]
 [3.229]
 [2.02 ]
 [3.678]] [[1.024]
 [1.104]
 [1.104]
 [0.348]
 [1.381]]
from probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
siam score:  -0.8973674
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
using explorer policy with actor:  1
from probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
949 2998
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
from probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.03 ]
 [0.009]
 [0.016]
 [0.024]] [[ 1.309]
 [ 1.552]
 [-0.253]
 [ 0.792]
 [ 1.002]] [[0.826]
 [0.952]
 [0.005]
 [0.55 ]
 [0.664]]
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.04 ]
 [0.031]
 [0.026]
 [0.029]] [[2.422]
 [3.458]
 [0.945]
 [2.572]
 [2.921]] [[0.825]
 [1.401]
 [0.012]
 [0.903]
 [1.097]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
954 3005
from probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
from probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.024]
 [0.018]
 [0.019]
 [0.027]] [[-0.272]
 [ 0.129]
 [ 0.288]
 [ 0.56 ]
 [ 0.833]] [[0.014]
 [0.562]
 [0.762]
 [1.128]
 [1.508]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.032]
 [0.036]
 [0.035]
 [0.033]] [[2.194]
 [2.112]
 [1.934]
 [2.268]
 [2.246]] [[0.346]
 [0.328]
 [0.277]
 [0.385]
 [0.374]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.084]
 [0.084]
 [0.084]
 [0.084]] [[2.332]
 [1.565]
 [1.565]
 [1.565]
 [1.565]] [[0.814]
 [0.566]
 [0.566]
 [0.566]
 [0.566]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
961 3013
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
siam score:  -0.900519
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
siam score:  -0.90043724
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.024]
 [0.024]
 [0.026]
 [0.029]] [[1.704]
 [1.704]
 [1.704]
 [1.77 ]
 [1.999]] [[0.251]
 [0.251]
 [0.251]
 [0.277]
 [0.358]]
965 3017
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
from probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[-0.248]
 [-0.248]
 [-0.248]
 [-0.248]
 [-0.248]] [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
using another actor
from probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.001]
 [0.022]
 [0.021]
 [0.017]] [[-0.221]
 [-0.669]
 [ 0.002]
 [-0.338]
 [-0.616]] [[0.18 ]
 [0.002]
 [0.268]
 [0.153]
 [0.052]]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]] [[1.449]
 [1.449]
 [1.449]
 [1.449]
 [1.449]] [[0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]] [[4.576]
 [4.576]
 [4.576]
 [4.576]
 [4.576]] [[2.009]
 [2.009]
 [2.009]
 [2.009]
 [2.009]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
from probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.083]] [[3.288]
 [3.288]
 [3.288]
 [3.288]
 [4.086]] [[1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [2.031]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]] [[3.755]
 [3.755]
 [3.755]
 [3.755]
 [3.755]] [[1.695]
 [1.695]
 [1.695]
 [1.695]
 [1.695]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
using another actor
from probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.028]
 [0.019]
 [0.022]
 [0.023]] [[2.174]
 [0.767]
 [0.95 ]
 [0.843]
 [0.925]] [[0.621]
 [0.162]
 [0.205]
 [0.176]
 [0.205]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.039]
 [0.026]
 [0.028]
 [0.031]] [[0.281]
 [0.731]
 [0.829]
 [2.015]
 [0.333]] [[0.017]
 [0.197]
 [0.204]
 [0.604]
 [0.047]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.044]
 [0.044]
 [0.039]
 [0.042]] [[1.766]
 [1.601]
 [1.575]
 [1.303]
 [1.672]] [[1.65 ]
 [1.516]
 [1.494]
 [1.261]
 [1.573]]
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.03 ]
 [0.027]
 [0.03 ]
 [0.03 ]] [[0.799]
 [0.799]
 [1.099]
 [0.956]
 [0.799]] [[0.499]
 [0.499]
 [0.806]
 [0.662]
 [0.499]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
siam score:  -0.9045001
first move QE:  -0.09361237258065934
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
first move QE:  -0.09361237258065934
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
first move QE:  -0.09361237258065934
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.9074721
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.042]
 [0.048]
 [0.048]
 [0.048]] [[1.079]
 [1.326]
 [1.079]
 [1.079]
 [1.079]] [[0.438]
 [0.508]
 [0.438]
 [0.438]
 [0.438]]
1002 3049
siam score:  -0.90370786
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.045]] [[2.763]
 [2.763]
 [2.763]
 [2.763]
 [2.887]] [[1.355]
 [1.355]
 [1.355]
 [1.355]
 [1.439]]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]] [[2.376]
 [2.376]
 [2.376]
 [2.376]
 [2.376]] [[1.106]
 [1.106]
 [1.106]
 [1.106]
 [1.106]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.9091296
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
line 256 mcts: sample exp_bonus 1.7963627900766455
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1875232415041272, 0.06256972451238153, 0.06256972451238153, 0.6873373094711097]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]] [[1.947]
 [1.947]
 [1.947]
 [1.947]
 [1.947]] [[1.664]
 [1.664]
 [1.664]
 [1.664]
 [1.664]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.023]
 [0.016]
 [0.018]
 [0.018]] [[0.829]
 [1.35 ]
 [1.433]
 [0.829]
 [0.829]] [[0.018]
 [0.023]
 [0.016]
 [0.018]
 [0.018]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.031]
 [0.023]
 [0.025]
 [0.023]] [[1.742]
 [1.751]
 [1.287]
 [1.566]
 [1.303]] [[0.023]
 [0.031]
 [0.023]
 [0.025]
 [0.023]]
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.029]
 [0.043]
 [0.029]
 [0.035]] [[1.42 ]
 [1.42 ]
 [1.383]
 [1.42 ]
 [1.53 ]] [[0.029]
 [0.029]
 [0.043]
 [0.029]
 [0.035]]
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.023]
 [0.02 ]
 [0.02 ]
 [0.023]] [[1.838]
 [2.222]
 [1.838]
 [1.838]
 [2.162]] [[0.02 ]
 [0.023]
 [0.02 ]
 [0.02 ]
 [0.023]]
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.943]
 [0.29 ]
 [0.127]
 [0.254]] [[1.498]
 [1.94 ]
 [1.827]
 [1.455]
 [1.989]] [[0.323]
 [0.943]
 [0.29 ]
 [0.127]
 [0.254]]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.187523250581757, 0.06256975174527092, 0.06256975174527092, 0.6873372459277012]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.187523250581757, 0.06256975174527092, 0.06256975174527092, 0.6873372459277012]
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.18752325672542702, 0.06256977017628114, 0.06256977017628114, 0.6873372029220107]
actor:  0 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
rdn probs:  [0.18752326294402588, 0.06256978883207767, 0.06256978883207767, 0.6873371593918188]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
line 256 mcts: sample exp_bonus 2.603807371984499
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.035]
 [0.01 ]
 [0.031]
 [0.031]] [[ 3.202]
 [ 0.251]
 [-0.146]
 [-0.052]
 [-0.018]] [[2.014]
 [0.267]
 [0.009]
 [0.083]
 [0.103]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
siam score:  -0.86943024
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.012]
 [0.011]
 [0.012]] [[1.077]
 [1.077]
 [1.077]
 [1.256]
 [1.077]] [[0.889]
 [0.889]
 [0.889]
 [1.125]
 [0.889]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.026]
 [0.006]
 [0.026]
 [0.025]] [[ 0.383]
 [ 0.383]
 [-0.112]
 [ 0.383]
 [ 0.351]] [[0.026]
 [0.026]
 [0.006]
 [0.026]
 [0.025]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
siam score:  -0.89496785
start point for exploration sampling:  20032
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
from probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
in main func line 156:  1032
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.047]
 [0.045]
 [0.047]
 [0.047]] [[2.057]
 [2.617]
 [0.892]
 [2.617]
 [2.617]] [[1.207]
 [1.639]
 [0.306]
 [1.639]
 [1.639]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
using another actor
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.046]
 [0.047]
 [0.045]
 [0.047]] [[0.659]
 [0.432]
 [0.588]
 [0.442]
 [0.349]] [[0.16 ]
 [0.087]
 [0.14 ]
 [0.088]
 [0.06 ]]
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]] [[2.752]
 [2.752]
 [2.752]
 [2.752]
 [2.752]] [[0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.018]
 [0.004]
 [0.004]
 [0.017]] [[1.304]
 [0.504]
 [0.045]
 [0.216]
 [0.107]] [[0.005]
 [0.018]
 [0.004]
 [0.004]
 [0.017]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.333 0.    0.208 0.208 0.25 ]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.047]
 [0.047]
 [0.052]
 [0.047]] [[2.893]
 [2.561]
 [2.561]
 [2.323]
 [2.561]] [[1.547]
 [1.355]
 [1.355]
 [1.221]
 [1.355]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
siam score:  -0.9029175
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
first move QE:  -0.07987347240815319
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
from probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
from probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[0.65 ]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[0.01 ]
 [0.009]
 [0.009]
 [0.009]
 [0.009]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
from probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
UNIT TEST: sample policy line 217 mcts : [0.083 0.042 0.042 0.75  0.083]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
using explorer policy with actor:  1
from probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
using another actor
Printing some Q and Qe and total Qs values:  [[0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]] [[3.244]
 [3.244]
 [3.244]
 [3.244]
 [3.244]] [[2.013]
 [2.013]
 [2.013]
 [2.013]
 [2.013]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0347],
        [0.0335],
        [0.0670],
        [0.0307],
        [0.0337],
        [0.0288],
        [0.0275],
        [0.0190],
        [0.0000],
        [0.0356]], dtype=torch.float64)
0.0 0.0346909724116745
0.0 0.033475974540400105
0.0 0.06695243893491043
0.0 0.030727736036796197
0.0 0.03367644987421609
0.0 0.02884999857733474
0.0 0.027468038528376433
0.0 0.018991439907853336
0.0 0.0
0.0 0.03562953635405604
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
siam score:  -0.9055208
first move QE:  -0.07328383734586737
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.111]
 [0.049]
 [0.049]
 [0.049]] [[2.052]
 [2.892]
 [2.052]
 [2.052]
 [2.052]] [[0.875]
 [1.84 ]
 [0.875]
 [0.875]
 [0.875]]
1063 3108
siam score:  -0.9119241
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0319],
        [0.1890],
        [0.0327],
        [0.0677],
        [0.0000],
        [0.1542],
        [0.0371],
        [0.0364],
        [0.0363],
        [0.0340]], dtype=torch.float64)
0.0 0.0319029129154331
0.0 0.18897652767889409
0.0 0.03267343492542451
0.0 0.06766329390600903
0.0 0.0
0.0 0.15415319497018687
0.0 0.03714316890395329
0.0 0.03635964631649118
0.0 0.03629706608117506
0.0 0.03404129450734306
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.044]
 [0.044]
 [0.044]
 [0.045]] [[2.431]
 [2.2  ]
 [1.715]
 [2.181]
 [1.243]] [[2.001]
 [1.751]
 [1.24 ]
 [1.731]
 [0.742]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
siam score:  -0.9095684
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.054]
 [0.049]
 [0.049]
 [0.052]] [[2.544]
 [2.204]
 [2.006]
 [3.118]
 [2.606]] [[1.615]
 [1.379]
 [1.235]
 [2.019]
 [1.66 ]]
Printing some Q and Qe and total Qs values:  [[0.04]
 [0.04]
 [0.04]
 [0.07]
 [0.04]] [[2.833]
 [2.833]
 [2.833]
 [5.115]
 [2.833]] [[0.93 ]
 [0.93 ]
 [0.93 ]
 [2.017]
 [0.93 ]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.127]
 [0.164]
 [0.164]
 [0.149]] [[4.368]
 [3.29 ]
 [4.368]
 [4.368]
 [4.413]] [[1.429]
 [0.787]
 [1.429]
 [1.429]
 [1.428]]
siam score:  -0.9190879
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.0768692759355063
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
1071 3111
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
1072 3112
in main func line 156:  1077
Printing some Q and Qe and total Qs values:  [[0.06]
 [0.06]
 [0.06]
 [0.06]
 [0.06]] [[2.253]
 [2.253]
 [2.253]
 [2.253]
 [2.253]] [[1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.155]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.06]
 [0.06]
 [0.06]
 [0.06]
 [0.06]] [[3.4]
 [3.4]
 [3.4]
 [3.4]
 [3.4]] [[2.035]
 [2.035]
 [2.035]
 [2.035]
 [2.035]]
from probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
from probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.012]
 [0.038]
 [0.039]
 [0.041]] [[ 2.344]
 [-0.102]
 [ 1.599]
 [ 1.67 ]
 [ 1.822]] [[0.896]
 [0.008]
 [0.628]
 [0.654]
 [0.708]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
siam score:  -0.9028588
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
first move QE:  -0.04825666317912687
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
1094 3127
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.087]
 [0.087]
 [0.081]
 [0.087]] [[2.434]
 [2.434]
 [2.434]
 [3.226]
 [2.434]] [[1.349]
 [1.349]
 [1.349]
 [1.815]
 [1.349]]
using another actor
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
siam score:  -0.92974055
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
line 256 mcts: sample exp_bonus 1.2949739273311622
siam score:  -0.93058366
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
line 256 mcts: sample exp_bonus 1.5225205417948222
1101 3131
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.141]
 [0.141]
 [0.141]
 [0.141]] [[2.341]
 [2.124]
 [2.124]
 [2.124]
 [2.124]] [[1.948]
 [1.747]
 [1.747]
 [1.747]
 [1.747]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
siam score:  -0.94348633
siam score:  -0.94289106
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.097]
 [0.097]
 [0.089]
 [0.097]] [[1.247]
 [1.247]
 [1.247]
 [3.063]
 [1.247]] [[0.535]
 [0.535]
 [0.535]
 [1.81 ]
 [0.535]]
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.081]
 [0.073]
 [0.081]
 [0.086]] [[1.217]
 [1.217]
 [1.305]
 [1.217]
 [1.566]] [[0.758]
 [0.758]
 [0.845]
 [0.758]
 [1.165]]
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.121]
 [0.079]
 [0.087]
 [0.085]] [[1.315]
 [0.687]
 [1.095]
 [1.298]
 [1.16 ]] [[0.369]
 [0.215]
 [0.267]
 [0.352]
 [0.301]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.076]
 [0.067]
 [0.077]
 [0.076]] [[1.917]
 [1.917]
 [1.43 ]
 [2.015]
 [1.917]] [[1.52 ]
 [1.52 ]
 [0.858]
 [1.651]
 [1.52 ]]
line 256 mcts: sample exp_bonus 2.747347215851974
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
using another actor
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[4.047]
 [4.047]
 [4.047]
 [4.047]
 [4.047]] [[1.893]
 [1.893]
 [1.893]
 [1.893]
 [1.893]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.119]
 [0.148]
 [0.107]
 [0.127]] [[2.758]
 [2.758]
 [2.722]
 [2.908]
 [3.007]] [[1.422]
 [1.422]
 [1.435]
 [1.505]
 [1.598]]
UNIT TEST: sample policy line 217 mcts : [0.583 0.    0.167 0.    0.25 ]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.075]
 [0.075]
 [0.089]
 [0.092]] [[2.266]
 [2.266]
 [2.266]
 [2.747]
 [2.459]] [[1.145]
 [1.145]
 [1.145]
 [1.681]
 [1.379]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
using another actor
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.115]
 [0.048]
 [0.115]
 [0.124]] [[3.873]
 [4.296]
 [2.216]
 [4.296]
 [3.96 ]] [[1.544]
 [1.828]
 [0.366]
 [1.828]
 [1.612]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
siam score:  -0.93495566
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
siam score:  -0.933981
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.1875249693593536, 0.06257490807806071, 0.06257490807806071, 0.6873252144845251]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.882813527488766
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
first move QE:  -0.022504201090982487
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
siam score:  -0.94233644
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
siam score:  -0.9434748
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.769]] [[1.196]
 [1.196]
 [1.196]
 [1.196]
 [1.196]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
using another actor
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.083]
 [0.093]
 [0.094]
 [0.09 ]] [[2.661]
 [2.661]
 [2.138]
 [2.168]
 [3.137]] [[1.504]
 [1.504]
 [1.136]
 [1.159]
 [1.855]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
1142 3150
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
first move QE:  -0.01813464125758233
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.27776643255112216, 0.05563497214214474, 0.05563497214214474, 0.6109636231645883]
first move QE:  -0.015656173057467006
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
siam score:  -0.94844556
siam score:  -0.9447229
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.7492],
        [0.1394],
        [0.1095],
        [0.0958],
        [0.1192],
        [0.1007],
        [0.1185],
        [0.1075],
        [0.0000],
        [0.1041]], dtype=torch.float64)
0.0 0.7492130079802254
0.0 0.1394147148747366
0.0 0.10952525591438841
0.0 0.09582307315648794
0.0 0.11919333203143448
0.0 0.10071915601119197
0.0 0.11848602156206395
0.0 0.10746095226104385
0.0 0.0
0.0 0.10409394767409104
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
from probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.061]
 [0.112]
 [0.111]
 [0.145]] [[3.371]
 [1.721]
 [2.454]
 [3.52 ]
 [2.507]] [[1.651]
 [0.747]
 [1.166]
 [1.717]
 [1.219]]
using another actor
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.077]
 [0.133]
 [0.145]
 [0.136]] [[2.127]
 [0.735]
 [2.558]
 [2.427]
 [3.411]] [[1.09 ]
 [0.042]
 [1.406]
 [1.325]
 [2.019]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.583 0.208 0.125]
from probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.136]
 [0.121]
 [0.13 ]
 [0.13 ]] [[1.673]
 [1.616]
 [1.55 ]
 [2.336]
 [1.44 ]] [[0.541]
 [0.519]
 [0.466]
 [0.747]
 [0.448]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
siam score:  -0.93777573
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.041]
 [0.063]
 [0.061]
 [0.066]] [[1.2  ]
 [0.154]
 [1.033]
 [1.248]
 [1.585]] [[0.057]
 [0.041]
 [0.063]
 [0.061]
 [0.066]]
using explorer policy with actor:  1
using another actor
from probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.022]
 [0.16 ]
 [0.16 ]
 [0.146]] [[1.689]
 [0.384]
 [2.024]
 [1.668]
 [1.689]] [[1.606]
 [0.016]
 [1.985]
 [1.606]
 [1.606]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.136]
 [0.133]
 [0.136]
 [0.136]] [[1.231]
 [1.231]
 [1.612]
 [1.231]
 [1.231]] [[0.34 ]
 [0.34 ]
 [0.461]
 [0.34 ]
 [0.34 ]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
siam score:  -0.94527143
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.063]
 [0.063]
 [0.07 ]
 [0.067]] [[1.135]
 [1.368]
 [1.368]
 [1.362]
 [1.527]] [[0.683]
 [0.979]
 [0.979]
 [0.983]
 [1.196]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.078]
 [0.078]
 [0.078]
 [0.078]] [[2.534]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]] [[0.989]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
first move QE:  0.00461885051281864
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
siam score:  -0.9452664
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
first move QE:  0.006276503525681944
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
1189 3176
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.2608188405783887
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.078]
 [0.079]
 [0.078]
 [0.084]] [[1.71 ]
 [1.71 ]
 [0.65 ]
 [1.71 ]
 [1.934]] [[1.758]
 [1.758]
 [0.649]
 [1.758]
 [2.002]]
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.079]
 [0.079]
 [0.087]
 [0.084]] [[0.298]
 [1.652]
 [1.652]
 [0.547]
 [1.796]] [[0.292]
 [1.698]
 [1.698]
 [0.553]
 [1.857]]
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]] [[1.64]
 [1.64]
 [1.64]
 [1.64]
 [1.64]] [[1.249]
 [1.249]
 [1.249]
 [1.249]
 [1.249]]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.45 0.45
probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
rdn probs:  [0.3157618073857749, 0.05271457784267532, 0.05271457784267532, 0.5788090369288745]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]] [[2.956]
 [2.956]
 [2.956]
 [2.956]
 [3.147]] [[1.774]
 [1.774]
 [1.774]
 [1.774]
 [1.896]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]] [[2.745]
 [1.816]
 [1.816]
 [1.816]
 [1.816]] [[1.724]
 [1.1  ]
 [1.1  ]
 [1.1  ]
 [1.1  ]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]] [[1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]] [[1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7491482288231701
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.5845893570903433
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.185]] [[2.801]
 [2.801]
 [2.801]
 [2.801]
 [4.874]] [[0.488]
 [0.488]
 [0.488]
 [0.488]
 [2.072]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]] [[1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]] [[1.95]
 [1.95]
 [1.95]
 [1.95]
 [1.95]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
siam score:  -0.94849855
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
siam score:  -0.94743013
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]] [[0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]] [[0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]]
line 256 mcts: sample exp_bonus 0.24095483536596538
from probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.078]
 [0.078]
 [0.072]
 [0.078]] [[0.891]
 [0.607]
 [0.607]
 [0.484]
 [0.607]] [[0.419]
 [0.323]
 [0.323]
 [0.269]
 [0.323]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
line 256 mcts: sample exp_bonus 3.093783538724449
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
in main func line 156:  1226
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.099]] [[1.396]
 [1.396]
 [1.396]
 [1.396]
 [1.696]] [[1.593]
 [1.593]
 [1.593]
 [1.593]
 [1.859]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.381966677350594
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
siam score:  -0.92662644
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.13379708942862686
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.103]
 [0.079]
 [0.081]
 [0.083]] [[ 1.761]
 [ 0.173]
 [-0.019]
 [ 0.403]
 [ 0.222]] [[1.904]
 [0.316]
 [0.081]
 [0.517]
 [0.335]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
using another actor
from probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.113]
 [0.102]
 [0.102]
 [0.102]] [[1.635]
 [1.298]
 [1.635]
 [1.635]
 [1.468]] [[1.145]
 [0.83 ]
 [1.145]
 [1.145]
 [0.979]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.3157636910993665, 0.05270892670190034, 0.05270892670190034, 0.5788184554968328]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
using explorer policy with actor:  1
from probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
from probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
from probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
1243 3212
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
siam score:  -0.938574
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.052]
 [0.052]
 [0.055]
 [0.065]] [[0.83 ]
 [0.83 ]
 [0.83 ]
 [1.138]
 [0.488]] [[0.754]
 [0.754]
 [0.754]
 [1.171]
 [0.325]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
from probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.062]] [[1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.882]] [[1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.521]]
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.078]
 [0.059]
 [0.064]
 [0.065]] [[1.447]
 [0.486]
 [0.879]
 [1.321]
 [1.346]] [[1.171]
 [0.079]
 [0.502]
 [1.025]
 [1.055]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
from probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
siam score:  -0.92684877
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.39529039040124836, 0.04659345343825231, 0.04659345343825231, 0.511522702722247]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
siam score:  -0.9269453
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.4582478742219127, 0.041752125778087276, 0.041752125778087276, 0.4582478742219127]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.4582478742219127, 0.041752125778087276, 0.041752125778087276, 0.4582478742219127]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.4582478742219127, 0.041752125778087276, 0.041752125778087276, 0.4582478742219127]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.4582478742219127, 0.041752125778087276, 0.041752125778087276, 0.4582478742219127]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.4582478742219127, 0.041752125778087276, 0.041752125778087276, 0.4582478742219127]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.4582478742219127, 0.041752125778087276, 0.041752125778087276, 0.4582478742219127]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.4582478742219127, 0.041752125778087276, 0.041752125778087276, 0.4582478742219127]
using another actor
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.4582478742219127, 0.041752125778087276, 0.041752125778087276, 0.4582478742219127]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.49989959552495045, 0.04553669457049513, 0.04553669457049513, 0.4090270153340594]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
siam score:  -0.9221271
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
first move QE:  0.02712401547975802
from probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
UNIT TEST: sample policy line 217 mcts : [0.833 0.042 0.042 0.042 0.042]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
siam score:  -0.92009276
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
siam score:  -0.9161742
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
from probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
using another actor
from probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
using another actor
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.227]
 [0.227]
 [0.291]
 [0.227]] [[2.713]
 [2.681]
 [2.681]
 [2.905]
 [2.681]] [[1.585]
 [1.556]
 [1.556]
 [1.795]
 [1.556]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
siam score:  -0.9140622
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
siam score:  -0.9083975
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.012]
 [0.017]
 [0.017]] [[1.063]
 [1.063]
 [1.097]
 [1.063]
 [1.063]] [[0.21 ]
 [0.21 ]
 [0.213]
 [0.21 ]
 [0.21 ]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.028]
 [0.028]
 [0.028]
 [0.028]] [[1.207]
 [1.201]
 [1.201]
 [1.201]
 [1.201]] [[1.139]
 [1.13 ]
 [1.13 ]
 [1.13 ]
 [1.13 ]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5498818593859033, 0.0500787604093978, 0.0500787604093978, 0.349960619795301]
1299 3250
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
1301 3250
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5788198909522733, 0.05270806542863605, 0.05270806542863605, 0.31576397819045454]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.004]
 [0.006]
 [0.024]
 [0.022]] [[1.454]
 [0.119]
 [0.592]
 [1.586]
 [1.927]] [[0.489]
 [0.003]
 [0.165]
 [0.532]
 [0.643]]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]] [[3.095]
 [3.095]
 [3.095]
 [3.095]
 [3.095]] [[1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5788198909522733, 0.05270806542863605, 0.05270806542863605, 0.31576397819045454]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5788198909522733, 0.05270806542863605, 0.05270806542863605, 0.31576397819045454]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5788198909522733, 0.05270806542863605, 0.05270806542863605, 0.31576397819045454]
siam score:  -0.90531
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5788198909522733, 0.05270806542863605, 0.05270806542863605, 0.31576397819045454]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
siam score:  -0.89933723
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5788198909522733, 0.05270806542863605, 0.05270806542863605, 0.31576397819045454]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.5788198909522733, 0.05270806542863605, 0.05270806542863605, 0.31576397819045454]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.2890475534144729]
siam score:  -0.9018408
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.2890475534144729]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.28904755341447297]
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.045]
 [0.045]
 [0.044]
 [0.045]] [[1.565]
 [1.182]
 [1.182]
 [1.367]
 [1.182]] [[1.599]
 [1.088]
 [1.088]
 [1.334]
 [1.088]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.28904755341447297]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.28904755341447297]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.28904755341447297]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.28904755341447297]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.28904755341447297]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.28904755341447297]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.28904755341447297]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.28904755341447297]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.28904755341447297]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.28904755341447297]
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.042]
 [0.045]
 [0.044]
 [0.043]] [[0.99 ]
 [0.928]
 [0.495]
 [0.682]
 [1.068]] [[0.595]
 [0.529]
 [0.103]
 [0.287]
 [0.671]]
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.044]
 [0.04 ]
 [0.04 ]
 [0.042]] [[1.348]
 [0.634]
 [1.348]
 [1.348]
 [1.328]] [[0.43 ]
 [0.198]
 [0.43 ]
 [0.43 ]
 [0.426]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.28904755341447297]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.601427980730257, 0.05476223292763496, 0.05476223292763496, 0.28904755341447297]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
siam score:  -0.8925778
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
1319 3261
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.073]
 [0.089]
 [0.072]
 [0.082]] [[2.032]
 [0.802]
 [2.019]
 [1.853]
 [1.617]] [[0.495]
 [0.087]
 [0.526]
 [0.436]
 [0.376]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
siam score:  -0.8876473
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.372219396351015
from probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6195782976747795, 0.05641136788463937, 0.05641136788463937, 0.2675989665559419]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6574576800577302, 0.059853082639725975, 0.059853082639725975, 0.222836154662818]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6665131127572675, 0.06067585783760563, 0.06067585783760563, 0.21213517156752112]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6665131127572675, 0.06067585783760563, 0.06067585783760563, 0.21213517156752112]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6665131127572675, 0.06067585783760563, 0.06067585783760563, 0.21213517156752112]
siam score:  -0.89814925
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6665131127572675, 0.06067585783760563, 0.06067585783760563, 0.21213517156752112]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6439975562550915, 0.10365805053382317, 0.058629758390384146, 0.1937146348207012]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6162413451481962, 0.14228195730935406, 0.056107523156837334, 0.18536917438561246]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6162413451481962, 0.14228195730935406, 0.056107523156837334, 0.18536917438561246]
siam score:  -0.90369684
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6162413451481962, 0.14228195730935406, 0.056107523156837334, 0.18536917438561246]
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.003]
 [0.055]
 [0.052]
 [0.051]] [[1.481]
 [0.201]
 [1.488]
 [1.209]
 [1.93 ]] [[0.528]
 [0.007]
 [0.541]
 [0.44 ]
 [0.68 ]]
from probs:  [0.6439975562550916, 0.1486863426772622, 0.05862975839038415, 0.1486863426772622]
siam score:  -0.8999181
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6439975562550916, 0.1486863426772622, 0.05862975839038415, 0.1486863426772622]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6439975562550916, 0.1486863426772622, 0.05862975839038415, 0.1486863426772622]
1337 3272
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6439975562550916, 0.1486863426772622, 0.05862975839038415, 0.1486863426772622]
siam score:  -0.8961609
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6439975562550916, 0.1486863426772622, 0.05862975839038415, 0.1486863426772622]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6439975562550916, 0.1486863426772622, 0.05862975839038415, 0.1486863426772622]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6439975562550916, 0.1486863426772622, 0.05862975839038415, 0.1486863426772622]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6439975562550916, 0.1486863426772622, 0.05862975839038415, 0.1486863426772622]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6439975562550916, 0.1486863426772622, 0.05862975839038415, 0.1486863426772622]
from probs:  [0.6439975562550916, 0.1486863426772622, 0.05862975839038415, 0.1486863426772622]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6439975562550916, 0.1486863426772622, 0.05862975839038415, 0.1486863426772622]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6439975562550916, 0.1486863426772622, 0.05862975839038415, 0.1486863426772622]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.241589067001565
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.042]] [[2.291]
 [2.291]
 [2.291]
 [2.291]
 [2.023]] [[0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.775]]
using another actor
using another actor
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
line 256 mcts: sample exp_bonus 1.43043644286457
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6888178801915381
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.055]
 [0.055]
 [0.054]
 [0.055]] [[2.302]
 [2.302]
 [2.302]
 [2.355]
 [2.302]] [[1.574]
 [1.574]
 [1.574]
 [1.623]
 [1.574]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
from probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
siam score:  -0.8818605
from probs:  [0.6523933582339463, 0.1441070109910668, 0.05939261978392019, 0.1441070109910668]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
UNIT TEST: sample policy line 217 mcts : [0.042 0.167 0.292 0.375 0.125]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6812583181823689, 0.15047884965022257, 0.06201560489486482, 0.1062472272725437]
1356 3285
start point for exploration sampling:  20032
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6873407428808728, 0.1458712516950303, 0.06256825305105457, 0.10421975237304243]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6873407428808728, 0.1458712516950303, 0.06256825305105457, 0.10421975237304243]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6873407428808728, 0.1458712516950303, 0.06256825305105457, 0.10421975237304243]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6873407428808728, 0.1458712516950303, 0.06256825305105457, 0.10421975237304243]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6873407428808728, 0.1458712516950303, 0.06256825305105457, 0.10421975237304243]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6873407428808728, 0.1458712516950303, 0.06256825305105457, 0.10421975237304243]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6873407428808728, 0.1458712516950303, 0.06256825305105457, 0.10421975237304243]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6873407428808728, 0.1458712516950303, 0.06256825305105457, 0.10421975237304243]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6873407428808728, 0.1458712516950303, 0.06256825305105457, 0.10421975237304243]
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[2.674]
 [2.674]
 [2.674]
 [2.674]
 [2.674]] [[0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.784]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6873407428808728, 0.1458712516950303, 0.06256825305105457, 0.10421975237304243]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1360 3286
from probs:  [0.6927526775536861, 0.1417715677090989, 0.06305998058844361, 0.10241577414877125]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6927526775536861, 0.1417715677090989, 0.06305998058844361, 0.10241577414877125]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6927526775536861, 0.1417715677090989, 0.06305998058844361, 0.10241577414877125]
Starting evaluation
using explorer policy with actor:  0
using explorer policy with actor:  0
using another actor
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]] [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.21 ]
 [0.202]
 [0.092]
 [0.167]] [[2.012]
 [1.891]
 [2.055]
 [1.583]
 [2.676]] [[0.204]
 [0.21 ]
 [0.202]
 [0.092]
 [0.167]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6927526775536861, 0.1417715677090989, 0.06305998058844361, 0.10241577414877125]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6927526775536861, 0.1417715677090989, 0.06305998058844361, 0.10241577414877125]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.6927526775536861, 0.1417715677090989, 0.06305998058844361, 0.10241577414877125]
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [0.6927526704639874, 0.14177156944213642, 0.06305998358187201, 0.10241577651200422]
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [0.6927526704639874, 0.14177156944213642, 0.06305998358187201, 0.10241577651200422]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [0.6927526704639874, 0.14177156944213642, 0.06305998358187201, 0.10241577651200422]
maxi score, test score, baseline:  0.0201 0.0 0.0201
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [0.6927526704639874, 0.14177156944213642, 0.06305998358187201, 0.10241577651200422]
actor:  0 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.031]] [[1.819]
 [1.819]
 [1.819]
 [1.819]
 [2.161]] [[0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.031]]
rdn probs:  [0.6927526633307448, 0.14177157118581793, 0.06305998659368556, 0.10241577888975176]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6927523474109971, 0.14177164841064516, 0.06306011998202346, 0.10241588419633431]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6927523474109971, 0.14177164841064516, 0.06306011998202346, 0.10241588419633431]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6927523474109971, 0.14177164841064516, 0.06306011998202346, 0.10241588419633431]
siam score:  -0.89003295
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6927523474109971, 0.14177164841064516, 0.06306011998202346, 0.10241588419633431]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6927523474109971, 0.14177164841064516, 0.06306011998202346, 0.10241588419633431]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
siam score:  -0.88726306
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6927523474109971, 0.14177164841064516, 0.06306011998202346, 0.10241588419633431]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6927523474109971, 0.14177164841064516, 0.06306011998202346, 0.10241588419633431]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6975988806712435, 0.1381002798321891, 0.06350046638698183, 0.10080037310958546]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8831747
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7019642069030564, 0.13479343745608363, 0.06389709127521205, 0.09934526436564785]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7019642069030564, 0.13479343745608363, 0.06389709127521205, 0.09934526436564785]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7019642069030564, 0.13479343745608363, 0.06389709127521205, 0.09934526436564785]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7019642069030564, 0.13479343745608363, 0.06389709127521205, 0.09934526436564785]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.038]
 [0.038]
 [0.038]
 [0.038]] [[3.268]
 [2.497]
 [2.497]
 [2.497]
 [2.497]] [[1.968]
 [1.424]
 [1.424]
 [1.424]
 [1.424]]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7019642069030564, 0.13479343745608363, 0.06389709127521205, 0.09934526436564785]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7059166042600847, 0.13179939889553358, 0.06425619826440991, 0.09802779857997175]
Printing some Q and Qe and total Qs values:  [[1.03 ]
 [1.418]
 [1.048]
 [1.03 ]
 [1.03 ]] [[1.887]
 [1.306]
 [1.987]
 [1.887]
 [1.887]] [[2.354]
 [2.867]
 [2.416]
 [2.354]
 [2.354]]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7059166042600847, 0.13179939889553358, 0.06425619826440991, 0.09802779857997175]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]] [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]] [[0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]] [[2.7  ]
 [2.596]
 [2.596]
 [2.596]
 [2.596]] [[1.51 ]
 [1.434]
 [1.434]
 [1.434]
 [1.434]]
line 256 mcts: sample exp_bonus 0.7775047789752075
line 256 mcts: sample exp_bonus 2.8051436152965463
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7095120169886758, 0.12907578500298006, 0.06458287033790275, 0.09682932767044139]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
from probs:  [0.7095120169886758, 0.12907578500298006, 0.06458287033790275, 0.09682932767044139]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7095120169886758, 0.12907578500298006, 0.06458287033790275, 0.09682932767044139]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7095120169886758, 0.12907578500298006, 0.06458287033790275, 0.09682932767044139]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.034]] [[1.116]
 [1.116]
 [1.116]
 [1.116]
 [1.272]] [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.424]]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7095120169886758, 0.12907578500298006, 0.06458287033790275, 0.09682932767044139]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7095120169886758, 0.12907578500298006, 0.06458287033790275, 0.09682932767044139]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7095120169886758, 0.12907578500298006, 0.06458287033790275, 0.09682932767044139]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7095120169886758, 0.12907578500298006, 0.06458287033790275, 0.09682932767044139]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7095120169886758, 0.12907578500298006, 0.06458287033790275, 0.09682932767044139]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7095120169886758, 0.12907578500298006, 0.06458287033790275, 0.09682932767044139]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7095120169886758, 0.12907578500298006, 0.06458287033790275, 0.09682932767044139]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7095120169886758, 0.12907578500298006, 0.06458287033790275, 0.09682932767044139]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
siam score:  -0.89488024
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
using explorer policy with actor:  1
first move QE:  0.06881547092492753
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
first move QE:  0.06959964522377746
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6914565928175096, 0.1527299032774979, 0.06294212168749597, 0.09287138221749662]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7049644352141278, 0.1410648535402793, 0.06416945603929997, 0.08980125520629308]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7049644352141278, 0.1410648535402793, 0.06416945603929997, 0.08980125520629308]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7049644352141278, 0.1410648535402793, 0.06416945603929997, 0.08980125520629308]
rdn beta is 0 so we're just using the maxi policy
using another actor
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7049644352141278, 0.1410648535402793, 0.06416945603929997, 0.08980125520629308]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]] [[1.47]
 [1.47]
 [1.47]
 [1.47]
 [1.47]] [[0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7049644352141278, 0.1410648535402793, 0.06416945603929997, 0.08980125520629308]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7049644352141278, 0.1410648535402793, 0.06416945603929997, 0.08980125520629308]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7049644352141278, 0.1410648535402793, 0.06416945603929997, 0.08980125520629308]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7049644352141278, 0.1410648535402793, 0.06416945603929997, 0.08980125520629308]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7049644352141278, 0.1410648535402793, 0.06416945603929997, 0.08980125520629308]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7077562885358396, 0.1386538757615525, 0.06442312626925417, 0.08916670943335361]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.05 ]
 [0.05 ]
 [0.05 ]
 [0.05 ]] [[2.241]
 [2.105]
 [2.105]
 [2.105]
 [2.105]] [[1.645]
 [1.558]
 [1.558]
 [1.558]
 [1.558]]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7103611307429509, 0.13640439631018098, 0.06465980450608472, 0.08857466844078349]
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.186]
 [0.806]
 [0.228]
 [0.225]] [[1.971]
 [2.193]
 [1.217]
 [1.971]
 [1.82 ]] [[1.13 ]
 [1.185]
 [1.695]
 [1.13 ]
 [1.038]]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7103611307429509, 0.13640439631018098, 0.06465980450608472, 0.08857466844078349]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7127971431909444, 0.1343007142022639, 0.06488114272362219, 0.08802099988316943]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7127971431909444, 0.1343007142022639, 0.06488114272362219, 0.08802099988316943]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7127971431909444, 0.1343007142022639, 0.06488114272362219, 0.08802099988316943]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7127971431909444, 0.1343007142022639, 0.06488114272362219, 0.08802099988316943]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7127971431909444, 0.1343007142022639, 0.06488114272362219, 0.08802099988316943]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7127971431909444, 0.1343007142022639, 0.06488114272362219, 0.08802099988316943]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7127971431909444, 0.1343007142022639, 0.06488114272362219, 0.08802099988316943]
siam score:  -0.89225495
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7127971431909444, 0.1343007142022639, 0.06488114272362219, 0.08802099988316943]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]] [[0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]]
siam score:  -0.8916099
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7127971431909444, 0.1343007142022639, 0.06488114272362219, 0.08802099988316943]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7127971431909444, 0.1343007142022639, 0.06488114272362219, 0.08802099988316943]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.037]
 [0.037]
 [0.037]
 [0.037]] [[3.451]
 [1.145]
 [1.145]
 [1.145]
 [1.145]] [[1.098]
 [0.352]
 [0.352]
 [0.352]
 [0.352]]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7127971431909444, 0.1343007142022639, 0.06488114272362219, 0.08802099988316943]
actor:  1 policy actor:  1  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7150802244086587, 0.13232909984841168, 0.06508858547607546, 0.0875020902668542]
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]] [[0.863]
 [1.055]
 [1.055]
 [1.055]
 [0.242]] [[0.377]
 [0.442]
 [0.442]
 [0.442]
 [0.169]]
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]] [[0.564]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[0.247]
 [0.064]
 [0.064]
 [0.064]
 [0.064]]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7150802244086587, 0.13232909984841168, 0.06508858547607546, 0.0875020902668542]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8844075
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6993993977999354, 0.15135135170245323, 0.06366366432685606, 0.08558558617075536]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6993993977999354, 0.15135135170245323, 0.06366366432685606, 0.08558558617075536]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6993993977999354, 0.15135135170245323, 0.06366366432685606, 0.08558558617075536]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6993993977999354, 0.15135135170245323, 0.06366366432685606, 0.08558558617075536]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6993993977999354, 0.15135135170245323, 0.06366366432685606, 0.08558558617075536]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.6993993977999354, 0.15135135170245323, 0.06366366432685606, 0.08558558617075536]
from probs:  [0.6993993977999354, 0.15135135170245323, 0.06366366432685606, 0.08558558617075536]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7019649664328869, 0.14897253691500176, 0.06389677852763481, 0.08516571812447654]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7043821162053042, 0.14673133722606727, 0.06411640700692106, 0.0847701395617076]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.019]
 [0.011]
 [0.019]
 [0.019]] [[0.26 ]
 [0.26 ]
 [0.764]
 [0.26 ]
 [0.26 ]] [[0.074]
 [0.074]
 [0.226]
 [0.074]
 [0.074]]
first move QE:  0.07071586642690449
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7066633641684134, 0.14461614673036613, 0.0643236870963594, 0.08439680200486108]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7066633641684134, 0.14461614673036613, 0.0643236870963594, 0.08439680200486108]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7066633641684134, 0.14461614673036613, 0.0643236870963594, 0.08439680200486108]
first move QE:  0.07051868646101446
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.70881985834246, 0.14261662889857316, 0.0645196317338991, 0.08404388102506762]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.163]
 [0.596]
 [0.593]
 [0.182]
 [0.338]] [[0.71 ]
 [1.609]
 [1.901]
 [2.078]
 [4.805]] [[0.281]
 [1.155]
 [1.266]
 [0.846]
 [2.115]]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7229391344407722, 0.14545555975519767, 0.065802652902015, 0.065802652902015]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7229391344407722, 0.14545555975519767, 0.065802652902015, 0.065802652902015]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7229391344407722, 0.14545555975519767, 0.065802652902015, 0.065802652902015]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.147]
 [1.212]
 [1.147]
 [1.147]
 [1.147]] [[1.421]
 [0.72 ]
 [1.421]
 [1.421]
 [1.421]] [[2.43 ]
 [2.324]
 [2.43 ]
 [2.43 ]
 [2.43 ]]
start point for exploration sampling:  20032
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.073]
 [0.082]
 [0.06 ]
 [0.056]] [[ 0.12 ]
 [ 0.224]
 [ 0.115]
 [-0.209]
 [-0.266]] [[0.193]
 [0.248]
 [0.229]
 [0.077]
 [0.05 ]]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.726245815743033, 0.14154798255356668, 0.06610310085170004, 0.06610310085170004]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
probs:  [0.7277715103351098, 0.13974503607651315, 0.06624172679418856, 0.06624172679418856]
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.264]
 [1.179]
 [0.264]
 [0.264]] [[1.542]
 [1.542]
 [0.609]
 [1.542]
 [1.542]] [[0.91 ]
 [0.91 ]
 [2.267]
 [0.91 ]
 [0.91 ]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.022099999999999998 0.1 0.1
Printing some Q and Qe and total Qs values:  [[0.911]
 [0.911]
 [0.911]
 [0.911]
 [1.   ]] [[0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.599]] [[0.911]
 [0.911]
 [0.911]
 [0.911]
 [1.   ]]
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [0.7252913030300167, 0.1273441798632215, 0.08134824731192955, 0.06601626979483224]
maxi score, test score, baseline:  0.0241 0.1 0.1
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [0.7252913030300167, 0.1273441798632215, 0.08134824731192955, 0.06601626979483224]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [0.7252913030300167, 0.1273441798632215, 0.08134824731192955, 0.06601626979483224]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [0.7252913030300167, 0.1273441798632215, 0.08134824731192955, 0.06601626979483224]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [0.7252913030300167, 0.1273441798632215, 0.08134824731192955, 0.06601626979483224]
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.141]
 [0.128]
 [0.141]
 [0.12 ]] [[0.811]
 [0.811]
 [1.359]
 [0.811]
 [1.489]] [[0.818]
 [0.818]
 [1.352]
 [0.818]
 [1.471]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7252913030300167, 0.1273441798632215, 0.08134824731192955, 0.06601626979483224]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [0.7265575808528508, 0.12617007741618833, 0.08114101465843865, 0.06613132707252209]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [0.7265575808528508, 0.12617007741618833, 0.08114101465843865, 0.06613132707252209]
using another actor
from probs:  [0.7265575808528508, 0.12617007741618833, 0.08114101465843865, 0.06613132707252209]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [0.7265575808528508, 0.1261700774161883, 0.08114101465843865, 0.06613132707252209]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [0.7265575808528508, 0.1261700774161883, 0.08114101465843865, 0.06613132707252209]
maxi score, test score, baseline:  0.0241 0.1 0.1
siam score:  -0.8835863
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [0.7265575808528508, 0.1261700774161883, 0.08114101465843865, 0.06613132707252209]
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [0.7265575808528508, 0.1261700774161883, 0.08114101465843865, 0.06613132707252209]
siam score:  -0.87955046
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [0.7265575808528508, 0.1261700774161883, 0.08114101465843865, 0.06613132707252209]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0241 0.1 0.1
probs:  [0.7265575808528508, 0.12617007741618833, 0.08114101465843865, 0.06613132707252209]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1444 3380
line 256 mcts: sample exp_bonus 1.4470341546711285
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [0.7289368712176391, 0.12396398125851603, 0.08075163197572152, 0.06634751554812336]
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [0.7185830771395241, 0.12220461532558435, 0.09380564095349198, 0.06540666658139963]
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [0.7185830771395241, 0.12220461532558435, 0.09380564095349198, 0.06540666658139963]
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [0.7185830771395241, 0.12220461532558435, 0.09380564095349198, 0.06540666658139963]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [0.7100003059548746, 0.13328350445921094, 0.09208944720952068, 0.06462674237639383]
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [0.700380170529911, 0.14502417077874255, 0.09084309763228708, 0.06375256105905938]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [0.703492672235091, 0.14233627205929494, 0.09013567669410458, 0.06403537901150941]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
maxi score, test score, baseline:  0.026099999999999998 0.1 0.1
probs:  [0.7049651262704087, 0.14106468807609931, 0.0898010118766166, 0.06416917377687525]
siam score:  -0.89517885
Printing some Q and Qe and total Qs values:  [[1.182]
 [1.182]
 [1.359]
 [1.182]
 [1.182]] [[1.423]
 [1.423]
 [0.931]
 [1.423]
 [1.423]] [[2.338]
 [2.338]
 [2.527]
 [2.338]
 [2.338]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7005593504099168, 0.1478732139070855, 0.0877986338524299, 0.06376880183056767]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7005593504099168, 0.1478732139070855, 0.0877986338524299, 0.06376880183056767]
siam score:  -0.8989852
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7005593504099168, 0.1478732139070855, 0.0877986338524299, 0.06376880183056767]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7019654727875254, 0.14660920557148113, 0.08752875161232747, 0.063896570028666]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.693765361374829, 0.14489767756911945, 0.09818553426650586, 0.06315142678954568]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.709070834325716, 0.13751244456919542, 0.08886917735587449, 0.06454754374921404]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.709070834325716, 0.13751244456919542, 0.08886917735587449, 0.06454754374921404]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.709070834325716, 0.13751244456919542, 0.08886917735587449, 0.06454754374921404]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.709070834325716, 0.13751244456919542, 0.08886917735587449, 0.06454754374921404]
maxi score, test score, baseline:  0.0281 0.1 0.1
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.709070834325716, 0.13751244456919542, 0.08886917735587449, 0.06454754374921404]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.709070834325716, 0.13751244456919542, 0.08886917735587449, 0.06454754374921404]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7127874973055482, 0.1343031256736129, 0.08802437594305806, 0.06488500107778065]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7139473698946941, 0.13330158180562912, 0.08776073568099657, 0.0649903126186803]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]] [[0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [1.262]] [[0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7150708352066217, 0.13233147542964993, 0.0875053708314213, 0.06509231853230701]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7150708352066217, 0.13233147542964993, 0.0875053708314213, 0.06509231853230701]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7150708352066217, 0.13233147542964993, 0.0875053708314213, 0.06509231853230701]
maxi score, test score, baseline:  0.0281 0.1 0.1
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7150708352066217, 0.13233147542964993, 0.0875053708314213, 0.06509231853230701]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7161595809375135, 0.1313913492289167, 0.08725789777921127, 0.06519117205435855]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7172151920577184, 0.130479834589886, 0.08701795625893545, 0.06528701709346017]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7182391585104326, 0.12959564495446016, 0.08678520760493488, 0.06537998893017223]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7192328822282407, 0.1287375697612412, 0.08655933315645552, 0.06547021485406268]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7211348070086164, 0.12709526773688262, 0.08612702364917685, 0.06564290160532395]
siam score:  -0.90746874
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7211348070086164, 0.12709526773688262, 0.08612702364917685, 0.06564290160532395]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7211348070086164, 0.12709526773688262, 0.08612702364917685, 0.06564290160532395]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7211348070086164, 0.12709526773688262, 0.08612702364917685, 0.06564290160532395]
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7211348070086164, 0.12709526773688262, 0.08612702364917685, 0.06564290160532395]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0281 0.1 0.1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20032
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0281 0.1 0.1
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7176752529557289, 0.12288826458126342, 0.09410824901475703, 0.06532823344825063]
maxi score, test score, baseline:  0.0281 0.1 0.1
first move QE:  0.071911265598697
maxi score, test score, baseline:  0.0281 0.1 0.1
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.0281 0.1 0.1
probs:  [0.7185748234863039, 0.12220686632191714, 0.09380839217123206, 0.06540991802054698]
actor:  0 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [0.7185748234863039, 0.12220686632191714, 0.09380839217123206, 0.06540991802054698]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [0.719450854573215, 0.12154329849986656, 0.09351638180892835, 0.06548946511799013]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [0.7211358999722427, 0.12026692609459985, 0.09295470000925245, 0.06564247392390504]
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [0.7211358999722427, 0.12026692609459985, 0.09295470000925245, 0.06564247392390504]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [0.7211358999722427, 0.12026692609459985, 0.09295470000925245, 0.06564247392390504]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [0.7155125225062889, 0.11858737402932516, 0.10076871288075906, 0.0651313905836269]
Printing some Q and Qe and total Qs values:  [[1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.382]] [[0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]] [[2.717]
 [2.717]
 [2.717]
 [2.717]
 [2.717]]
maxi score, test score, baseline:  0.0301 0.1 0.1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.242]
 [0.193]
 [0.193]
 [0.193]] [[1.542]
 [1.804]
 [1.542]
 [1.542]
 [1.542]] [[1.38 ]
 [1.788]
 [1.38 ]
 [1.38 ]
 [1.38 ]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [0.717217261779818, 0.1174406838671214, 0.10005585552182585, 0.06528619883123474]
maxi score, test score, baseline:  0.0301 0.1 0.1
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [0.717217261779818, 0.1174406838671214, 0.10005585552182585, 0.06528619883123474]
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [0.717217261779818, 0.1174406838671214, 0.10005585552182585, 0.06528619883123474]
maxi score, test score, baseline:  0.0301 0.1 0.1
probs:  [0.717217261779818, 0.1174406838671214, 0.10005585552182585, 0.06528619883123474]
maxi score, test score, baseline:  0.0301 0.1 0.1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.0716551379100472
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [0.7188409631612287, 0.11634850371421987, 0.09937688513824779, 0.06543364798630362]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]] [[3.145]
 [3.145]
 [3.145]
 [3.145]
 [3.145]] [[1.293]
 [1.293]
 [1.293]
 [1.293]
 [1.293]]
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.282]] [[2.362]
 [2.362]
 [2.362]
 [2.362]
 [3.008]] [[0.781]
 [0.781]
 [0.781]
 [0.781]
 [1.307]]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[3.325]
 [3.325]
 [3.325]
 [3.325]
 [3.325]] [[2.215]
 [2.215]
 [2.215]
 [2.215]
 [2.215]]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [0.7127904741028809, 0.1237844161537598, 0.09854129938451175, 0.06488381035884769]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [0.7127904741028809, 0.1237844161537598, 0.09854129938451175, 0.06488381035884769]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [0.7127904741028809, 0.1237844161537598, 0.09854129938451175, 0.06488381035884769]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [0.7127904741028809, 0.1237844161537598, 0.09854129938451175, 0.06488381035884769]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [0.7127904741028809, 0.1237844161537598, 0.09854129938451175, 0.06488381035884769]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [0.7127904741028809, 0.1237844161537598, 0.09854129938451175, 0.06488381035884769]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [0.7127904741028809, 0.1237844161537598, 0.09854129938451175, 0.06488381035884769]
line 256 mcts: sample exp_bonus 0.24757178330378998
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [0.7127904741028809, 0.1237844161537598, 0.09854129938451175, 0.06488381035884769]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [0.7136376727507346, 0.12317534512199635, 0.09822623268697925, 0.06496074944028976]
Printing some Q and Qe and total Qs values:  [[1.212]
 [1.212]
 [1.212]
 [1.212]
 [1.212]] [[1.466]
 [1.466]
 [1.466]
 [1.466]
 [1.466]] [[2.123]
 [2.123]
 [2.123]
 [2.123]
 [2.123]]
using another actor
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [0.7144653641038222, 0.12258029834319922, 0.09791842060317327, 0.06503591694980533]
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [0.7144653641038222, 0.12258029834319922, 0.09791842060317327, 0.06503591694980533]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7160648592762692, 0.1214303836479257, 0.09732358058191178, 0.06518117649389321]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6358785870850807
maxi score, test score, baseline:  0.032100000000000004 0.1 0.1
probs:  [0.7168379069802375, 0.12087462147355134, 0.09703609005328391, 0.0652513814929273]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [0.7111849463814323, 0.12780569796731284, 0.09627168453952258, 0.06473767111173234]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
siam score:  -0.89834166
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [0.7111849463814323, 0.12780569796731284, 0.09627168453952258, 0.06473767111173234]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [0.7111849463814323, 0.12780569796731284, 0.09627168453952258, 0.06473767111173234]
line 256 mcts: sample exp_bonus 1.4777961802545656
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [0.7111849463814323, 0.12780569796731284, 0.09627168453952258, 0.06473767111173234]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [0.7111849463814323, 0.12780569796731284, 0.09627168453952258, 0.06473767111173234]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7119968108921124, 0.12719072115526123, 0.0960010630359625, 0.06481140491666376]
1505 3434
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [0.7119968108921124, 0.12719072115526123, 0.0960010630359625, 0.06481140491666376]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.45 ]
 [1.35 ]
 [1.127]
 [1.11 ]] [[2.185]
 [1.173]
 [1.015]
 [2.408]
 [2.45 ]] [[1.767]
 [0.34 ]
 [2.088]
 [2.108]
 [2.087]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.277020363062285
siam score:  -0.9022478
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [0.71432939124904, 0.1254238218600137, 0.09522353625032001, 0.06502325064062636]
Printing some Q and Qe and total Qs values:  [[1.253]
 [1.344]
 [1.253]
 [1.253]
 [1.253]] [[1.528]
 [1.139]
 [1.528]
 [1.528]
 [1.528]] [[2.251]
 [2.305]
 [2.251]
 [2.251]
 [2.251]]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.07362543678622303
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [0.7150743767032113, 0.12485950506379456, 0.09497520776559626, 0.06509091046739793]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [0.7150743767032113, 0.12485950506379456, 0.09497520776559626, 0.06509091046739793]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [0.7150743767032113, 0.12485950506379456, 0.09497520776559626, 0.06509091046739793]
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.757]
 [0.935]
 [0.682]
 [0.757]] [[1.444]
 [1.444]
 [1.214]
 [1.766]
 [1.444]] [[1.955]
 [1.955]
 [2.065]
 [2.104]
 [1.955]]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [0.7150743767032113, 0.12485950506379456, 0.09497520776559623, 0.06509091046739793]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.7150743767032113, 0.12485950506379456, 0.09497520776559623, 0.06509091046739793]
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [0.7158039338848249, 0.12430687498346002, 0.09473202203839177, 0.06515716909332353]
using another actor
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.11586168659187768
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.124]
 [1.124]
 [1.292]
 [1.124]
 [1.124]] [[1.307]
 [1.307]
 [0.232]
 [1.307]
 [1.307]] [[2.187]
 [2.187]
 [2.165]
 [2.187]
 [2.187]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
maxi score, test score, baseline:  0.034100000000000005 0.1 0.1
probs:  [0.7172186416047387, 0.12323525227778413, 0.09426045279842049, 0.06528565331905685]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [0.7185770840120952, 0.12220624981488316, 0.09380763866263495, 0.06540902751038674]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [0.7185770840120952, 0.12220624981488316, 0.09380763866263495, 0.06540902751038674]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [0.7185770840120952, 0.12220624981488316, 0.09380763866263495, 0.06540902751038674]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [0.7185770840120952, 0.12220624981488316, 0.09380763866263495, 0.06540902751038674]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [0.7185770840120952, 0.12220624981488316, 0.09380763866263495, 0.06540902751038674]
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[1.113]
 [1.113]
 [1.113]
 [1.113]
 [1.113]] [[1.142]
 [1.142]
 [1.142]
 [1.142]
 [1.142]]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [0.7185770840120952, 0.12220624981488316, 0.09380763866263495, 0.06540902751038674]
siam score:  -0.90978193
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [0.7185770840120952, 0.12220624981488316, 0.09380763866263495, 0.06540902751038674]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [0.7185770840120952, 0.12220624981488316, 0.09380763866263495, 0.06540902751038674]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [0.7185770840120952, 0.12220624981488316, 0.09380763866263495, 0.06540902751038674]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [0.7185770840120952, 0.12220624981488316, 0.09380763866263495, 0.06540902751038674]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [0.7185770840120952, 0.12220624981488316, 0.09380763866263495, 0.06540902751038674]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [0.7185770840120952, 0.12220624981488316, 0.09380763866263495, 0.06540902751038674]
maxi score, test score, baseline:  0.0361 0.1 0.1
probs:  [0.7185770840120952, 0.12220624981488316, 0.09380763866263495, 0.06540902751038674]
maxi score, test score, baseline:  0.0361 0.1 0.1
maxi score, test score, baseline:  0.0361 0.1 0.1
1524 3444
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1525 3446
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [0.7198825523081764, 0.12121737455257382, 0.09337248256394114, 0.06552759057530845]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.1 0.1
probs:  [0.7211380859147081, 0.12026632416841372, 0.09295397136176396, 0.06564161855511423]
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [0. ]
 [1.5]
 [1.5]] [[0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]] [[3.]
 [3.]
 [0.]
 [3.]
 [3.]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7217480140517057, 0.1198043115341171, 0.09275066198276478, 0.06569701243141249]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7217480140517057, 0.1198043115341171, 0.09275066198276478, 0.06569701243141249]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7217480140517057, 0.1198043115341171, 0.09275066198276478, 0.06569701243141249]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7217480140517057, 0.1198043115341171, 0.09275066198276478, 0.06569701243141249]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7217480140517057, 0.1198043115341171, 0.09275066198276478, 0.06569701243141249]
first move QE:  0.0749373002785527
Printing some Q and Qe and total Qs values:  [[1.344]
 [1.474]
 [1.375]
 [1.316]
 [1.299]] [[1.012]
 [0.016]
 [0.72 ]
 [1.677]
 [1.68 ]] [[2.156]
 [2.083]
 [2.12 ]
 [2.322]
 [2.289]]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7217480140517057, 0.1198043115341171, 0.09275066198276478, 0.06569701243141249]
start point for exploration sampling:  20032
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.242]
 [0.215]
 [0.217]
 [0.217]] [[0.597]
 [1.157]
 [0.61 ]
 [0.597]
 [0.597]] [[0.395]
 [0.633]
 [0.396]
 [0.395]
 [0.395]]
Printing some Q and Qe and total Qs values:  [[1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]] [[1.181]
 [1.181]
 [1.181]
 [1.181]
 [1.181]] [[1.925]
 [1.925]
 [1.925]
 [1.925]
 [1.925]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9141048
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7235103869862043, 0.11846933694827662, 0.09216320433793194, 0.06585707172758727]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7235103869862042, 0.11846933694827662, 0.09216320433793193, 0.06585707172758726]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7235103869862042, 0.11846933694827662, 0.09216320433793193, 0.06585707172758726]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7240763994952529, 0.11804058983121825, 0.09197453350158231, 0.06590847717194641]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.7246321742955152, 0.1176195976454685, 0.09178927523482824, 0.06595895282418794]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7205651378967819, 0.12282023300086971, 0.09102529125108713, 0.06558933785126109]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7205651378967819, 0.12282023300086971, 0.09102529125108713, 0.06558933785126109]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7205651378967819, 0.12282023300086971, 0.09102529125108713, 0.06558933785126109]
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7205651378967819, 0.12282023300086971, 0.09102529125108713, 0.06558933785126109]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.040100000000000004 0.1 0.1
probs:  [0.7205651378967819, 0.12282023300086971, 0.09102529125108713, 0.06558933785126109]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.214]
 [0.229]
 [0.214]
 [0.214]] [[1.033]
 [0.963]
 [0.892]
 [0.963]
 [0.963]] [[0.7  ]
 [0.701]
 [0.709]
 [0.701]
 [0.701]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  0
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [0.7217020076563032, 0.12192197143106996, 0.0906834278776724, 0.06569259303495435]
maxi score, test score, baseline:  0.042100000000000005 0.1 0.1
probs:  [0.7217020076563032, 0.12192197143106996, 0.0906834278776724, 0.06569259303495435]
using explorer policy with actor:  0
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [0.7217020076563032, 0.12192197143106996, 0.0906834278776724, 0.06569259303495435]
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [0.7217020076563032, 0.12192197143106996, 0.0906834278776724, 0.06569259303495435]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[3.481]
 [2.402]
 [2.402]
 [2.402]
 [2.402]] [[1.303]
 [1.062]
 [1.062]
 [1.062]
 [1.062]]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.688]
 [1.087]
 [0.688]
 [0.688]
 [0.688]] [[1.178]
 [2.513]
 [1.178]
 [1.178]
 [1.178]] [[1.288]
 [2.33 ]
 [1.288]
 [1.288]
 [1.288]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.741]
 [0.741]
 [0.85 ]
 [0.741]
 [0.741]] [[1.567]
 [1.567]
 [1.279]
 [1.567]
 [1.567]] [[0.741]
 [0.741]
 [0.85 ]
 [0.741]
 [0.741]]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [0.7183873092117051, 0.12641898356303544, 0.08980238610023111, 0.06539132112502823]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.53 ]
 [0.778]
 [0.303]
 [0.477]] [[0.192]
 [4.57 ]
 [1.857]
 [1.202]
 [1.38 ]] [[0.076]
 [2.1  ]
 [1.085]
 [0.493]
 [0.683]]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [0.7146306922121627, 0.1312109233502885, 0.08910846580355529, 0.06504991863399347]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [0.7146306922121627, 0.1312109233502885, 0.08910846580355529, 0.06504991863399347]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [0.7146306922121627, 0.1312109233502885, 0.08910846580355529, 0.06504991863399347]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [0.7146306922121627, 0.1312109233502885, 0.08910846580355529, 0.06504991863399347]
1559 3468
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.237]
 [1.237]
 [1.237]
 [1.237]
 [1.231]] [[2.094]
 [2.094]
 [2.094]
 [2.094]
 [2.293]] [[2.598]
 [2.598]
 [2.598]
 [2.598]
 [2.653]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.716378185016452, 0.1297389585806633, 0.08867421273015812, 0.06520864367272658]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [0.7127929547965782, 0.13430176130085553, 0.08802246582119765, 0.06488281808136874]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [0.7127929547965782, 0.13430176130085553, 0.08802246582119765, 0.06488281808136874]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [0.7127929547965782, 0.13430176130085553, 0.08802246582119765, 0.06488281808136874]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [0.7127929547965782, 0.13430176130085553, 0.08802246582119765, 0.06488281808136874]
first move QE:  0.07563092665476016
maxi score, test score, baseline:  0.0461 0.1 0.1
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [0.7127929547965782, 0.13430176130085553, 0.08802246582119765, 0.06488281808136874]
maxi score, test score, baseline:  0.0461 0.1 0.1
probs:  [0.7127929547965782, 0.13430176130085553, 0.08802246582119765, 0.06488281808136874]
from probs:  [0.7127929547965782, 0.13430176130085553, 0.08802246582119765, 0.06488281808136874]
Printing some Q and Qe and total Qs values:  [[1.302]
 [1.501]
 [1.5  ]
 [1.5  ]
 [1.302]] [[1.178]
 [0.096]
 [1.069]
 [0.977]
 [1.178]] [[2.568]
 [2.599]
 [2.878]
 [2.852]
 [2.568]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [0.7156247237653657, 0.13185641337296694, 0.08737882781926035, 0.06514003504240708]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [0.7122343849533556, 0.13615592299368393, 0.08677776911142634, 0.06483192294153409]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [0.7122343849533556, 0.13615592299368393, 0.08677776911142634, 0.06483192294153409]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [0.7127932758294708, 0.1356628377362484, 0.08666119676606915, 0.06488268966821172]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7133437080163871, 0.13517721521459794, 0.08654638871725119, 0.06493268805176375]
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.097]
 [0.053]
 [0.053]
 [0.053]] [[0.185]
 [0.324]
 [0.185]
 [0.185]
 [0.185]] [[0.053]
 [0.097]
 [0.053]
 [0.053]
 [0.053]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.048100000000000004 0.1 0.1
probs:  [0.7133437080163871, 0.13517721521459794, 0.08654638871725119, 0.06493268805176375]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.038]] [[0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.312]] [[0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.038]]
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0521 0.1 0.1
maxi score, test score, baseline:  0.0521 0.1 0.1
probs:  [0.7133437080163871, 0.13517721521459794, 0.08654638871725119, 0.06493268805176375]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.669]
 [0.804]
 [0.669]
 [0.669]] [[1.843]
 [1.843]
 [1.066]
 [1.843]
 [1.843]] [[0.669]
 [0.669]
 [0.804]
 [0.669]
 [0.669]]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.08146508617399206
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.804]
 [0.978]
 [0.804]
 [0.804]
 [0.804]] [[1.447]
 [0.814]
 [1.447]
 [1.447]
 [1.447]] [[0.804]
 [0.978]
 [0.804]
 [0.804]
 [0.804]]
actor:  0 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0861 0.1 0.1
maxi score, test score, baseline:  0.0861 0.1 0.1
probs:  [0.7133437080163871, 0.13517721521459794, 0.08654638871725119, 0.06493268805176375]
actor:  0 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.7133437080163871, 0.13517721521459794, 0.08654638871725119, 0.06493268805176375]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88726133
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8889381
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.165]
 [0.153]
 [0.153]
 [0.138]] [[1.224]
 [1.161]
 [1.438]
 [1.438]
 [1.341]] [[1.183]
 [1.193]
 [1.539]
 [1.539]
 [1.379]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.042 0.    0.833]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88680536
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0881 1.0 1.0
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8928253
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.025]
 [1.025]
 [1.156]
 [1.025]
 [1.025]] [[1.711]
 [1.711]
 [1.118]
 [1.711]
 [1.711]] [[1.86 ]
 [1.86 ]
 [1.924]
 [1.86 ]
 [1.86 ]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0881 1.0 1.0
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8880095
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.194]
 [0.188]
 [0.194]
 [0.163]] [[1.045]
 [0.854]
 [0.936]
 [0.854]
 [1.085]] [[0.701]
 [0.637]
 [0.679]
 [0.637]
 [0.73 ]]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.08800350450025915
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.45 ]
 [0.226]
 [0.283]
 [0.289]] [[1.127]
 [1.484]
 [1.281]
 [1.203]
 [1.009]] [[0.287]
 [0.45 ]
 [0.226]
 [0.283]
 [0.289]]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.062107796818468
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
siam score:  -0.9112795
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9148853
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1617 3515
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.866]
 [0.76 ]
 [0.502]
 [0.676]] [[1.139]
 [2.654]
 [2.692]
 [2.308]
 [2.977]] [[0.254]
 [1.676]
 [1.576]
 [1.061]
 [1.643]]
siam score:  -0.92425555
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9182646
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
siam score:  -0.9246472
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.475]] [[3.867]
 [3.867]
 [3.867]
 [3.867]
 [7.921]] [[0.956]
 [0.956]
 [0.956]
 [0.956]
 [2.048]]
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.93019867
siam score:  -0.92959017
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.08318583087318607
maxi score, test score, baseline:  0.0921 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.883]
 [1.02 ]
 [1.095]
 [0.976]
 [1.02 ]] [[2.535]
 [2.744]
 [2.424]
 [3.09 ]
 [2.744]] [[1.877]
 [2.181]
 [2.029]
 [2.386]
 [2.181]]
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.064]
 [1.169]
 [1.111]
 [1.064]
 [1.003]] [[3.523]
 [2.164]
 [2.385]
 [3.523]
 [3.701]] [[2.15 ]
 [1.663]
 [1.684]
 [2.15 ]
 [2.148]]
maxi score, test score, baseline:  0.0921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  1646
Printing some Q and Qe and total Qs values:  [[0.973]
 [1.082]
 [1.119]
 [0.973]
 [0.991]] [[3.551]
 [2.08 ]
 [2.654]
 [3.551]
 [3.006]] [[2.356]
 [2.083]
 [2.349]
 [2.356]
 [2.21 ]]
maxi score, test score, baseline:  0.0941 1.0 1.0
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.982]
 [1.107]
 [1.07 ]
 [0.982]
 [0.982]] [[3.394]
 [1.978]
 [2.397]
 [3.394]
 [2.863]] [[2.113]
 [1.891]
 [1.956]
 [2.113]
 [1.935]]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.93252516
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.671]
 [0.558]
 [0.558]
 [0.558]] [[1.681]
 [1.708]
 [1.504]
 [1.504]
 [1.504]] [[0.412]
 [0.671]
 [0.558]
 [0.558]
 [0.558]]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.917]] [[3.609]
 [3.609]
 [3.609]
 [3.609]
 [8.288]] [[0.921]
 [0.921]
 [0.921]
 [0.921]
 [2.072]]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.498]
 [0.589]
 [0.498]
 [0.498]] [[2.364]
 [2.364]
 [3.23 ]
 [2.364]
 [2.364]] [[1.254]
 [1.254]
 [2.007]
 [1.254]
 [1.254]]
start point for exploration sampling:  20032
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.496]
 [0.342]
 [0.386]
 [0.349]] [[3.507]
 [1.514]
 [1.682]
 [2.338]
 [1.589]] [[2.131]
 [0.864]
 [0.83 ]
 [1.222]
 [0.785]]
siam score:  -0.93127143
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.94334966
maxi score, test score, baseline:  0.0961 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.94781613
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.926]
 [0.926]
 [1.267]
 [0.926]
 [0.926]] [[1.897]
 [1.897]
 [2.578]
 [1.897]
 [1.897]] [[1.312]
 [1.312]
 [1.93 ]
 [1.312]
 [1.312]]
siam score:  -0.9453312
Printing some Q and Qe and total Qs values:  [[1.184]
 [1.184]
 [1.184]
 [1.184]
 [1.184]] [[1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.609]] [[1.91]
 [1.91]
 [1.91]
 [1.91]
 [1.91]]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.92232955
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
siam score:  -0.9199573
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.534]
 [0.892]
 [0.102]
 [0.497]] [[1.611]
 [1.273]
 [1.49 ]
 [0.866]
 [0.758]] [[1.122]
 [0.976]
 [1.609]
 [0.111]
 [0.575]]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]] [[2.092]
 [2.092]
 [2.092]
 [2.092]
 [2.13 ]] [[2.211]
 [2.211]
 [2.211]
 [2.211]
 [2.252]]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.975]
 [0.975]
 [1.12 ]
 [0.975]
 [0.975]] [[1.776]
 [1.776]
 [1.681]
 [1.776]
 [1.776]] [[1.945]
 [1.945]
 [2.203]
 [1.945]
 [1.945]]
siam score:  -0.9282249
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9321367
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.119]
 [1.281]
 [1.127]
 [1.07 ]
 [1.063]] [[2.011]
 [1.247]
 [1.591]
 [1.491]
 [1.848]] [[2.425]
 [2.239]
 [2.16 ]
 [1.98 ]
 [2.204]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.857]
 [0.744]
 [0.744]
 [0.47 ]] [[2.192]
 [2.686]
 [2.214]
 [2.214]
 [2.434]] [[1.039]
 [2.065]
 [1.605]
 [1.605]
 [1.262]]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.516]
 [0.639]
 [0.388]
 [0.388]] [[1.231]
 [1.234]
 [1.169]
 [1.231]
 [1.231]] [[0.388]
 [0.516]
 [0.639]
 [0.388]
 [0.388]]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9294063
maxi score, test score, baseline:  0.0981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9256718
maxi score, test score, baseline:  0.0981 1.0 1.0
maxi score, test score, baseline:  0.0981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.346]
 [1.5  ]
 [0.308]
 [0.333]
 [0.345]] [[1.076]
 [0.   ]
 [0.65 ]
 [0.766]
 [1.515]] [[1.445]
 [2.318]
 [0.8  ]
 [1.007]
 [2.03 ]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.    0.042 0.042 0.042 0.875]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.92222387
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.504]
 [0.21 ]
 [0.27 ]
 [0.267]] [[0.72 ]
 [0.62 ]
 [0.513]
 [0.688]
 [0.79 ]] [[0.776]
 [0.954]
 [0.433]
 [0.685]
 [0.777]]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
siam score:  -0.9310746
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9284706
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.346]
 [0.274]
 [0.239]
 [0.257]] [[0.325]
 [1.2  ]
 [1.842]
 [1.42 ]
 [1.548]] [[0.113]
 [1.648]
 [2.144]
 [1.652]
 [1.816]]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
1703 3551
siam score:  -0.9226155
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.    0.792 0.125]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.294]
 [0.255]
 [0.259]
 [0.267]] [[0.399]
 [0.279]
 [0.266]
 [0.255]
 [2.106]] [[0.464]
 [0.442]
 [0.357]
 [0.358]
 [1.606]]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.637]
 [0.822]
 [0.508]
 [0.455]] [[1.504]
 [1.73 ]
 [1.254]
 [1.447]
 [2.016]] [[0.941]
 [1.525]
 [1.578]
 [1.079]
 [1.351]]
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1021 1.0 1.0
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.91905123
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.997]
 [1.256]
 [0.779]
 [0.997]] [[3.075]
 [1.654]
 [1.541]
 [2.774]
 [1.654]] [[1.61 ]
 [1.636]
 [2.116]
 [1.575]
 [1.636]]
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9139778
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.1041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
maxi score, test score, baseline:  0.1061 1.0 1.0
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[2.58 ]
 [2.58 ]
 [2.58 ]
 [2.58 ]
 [2.384]] [[2.106]
 [2.106]
 [2.106]
 [2.106]
 [1.909]]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[3.352]
 [3.352]
 [3.352]
 [3.352]
 [3.352]] [[2.321]
 [2.321]
 [2.321]
 [2.321]
 [2.321]]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[1.287]
 [1.287]
 [1.287]
 [1.287]
 [1.287]] [[1.589]
 [1.589]
 [1.589]
 [1.589]
 [1.589]]
line 256 mcts: sample exp_bonus 1.3329411357097833
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1061 1.0 1.0
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.244]
 [0.211]
 [0.245]
 [0.244]] [[1.177]
 [1.177]
 [1.149]
 [1.136]
 [1.177]] [[0.796]
 [0.796]
 [0.709]
 [0.77 ]
 [0.796]]
using explorer policy with actor:  1
siam score:  -0.9188553
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.91354907
siam score:  -0.9135968
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1081 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.709]
 [0.709]
 [0.584]
 [0.652]] [[1.477]
 [1.477]
 [1.477]
 [2.384]
 [1.979]] [[0.709]
 [0.709]
 [0.709]
 [0.584]
 [0.652]]
Printing some Q and Qe and total Qs values:  [[0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]] [[1.494]
 [1.494]
 [1.494]
 [1.494]
 [1.494]] [[0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]]
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.101]
 [1.417]
 [1.272]
 [1.183]
 [1.101]] [[0.796]
 [0.617]
 [1.385]
 [0.981]
 [0.796]] [[1.6  ]
 [2.173]
 [2.139]
 [1.827]
 [1.6  ]]
UNIT TEST: sample policy line 217 mcts : [0.    0.458 0.167 0.375 0.   ]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.611]
 [0.789]
 [0.789]
 [0.701]] [[2.886]
 [1.889]
 [1.963]
 [1.963]
 [1.88 ]] [[2.178]
 [1.66 ]
 [2.09 ]
 [2.09 ]
 [1.831]]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90685195
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]] [[1.11]
 [1.11]
 [1.11]
 [1.11]
 [1.11]] [[1.392]
 [1.392]
 [1.392]
 [1.392]
 [1.392]]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.909]
 [0.666]
 [0.727]
 [0.727]] [[1.574]
 [0.466]
 [1.472]
 [1.574]
 [1.574]] [[0.727]
 [0.909]
 [0.666]
 [0.727]
 [0.727]]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.206]] [[1.295]
 [1.295]
 [1.295]
 [1.295]
 [1.069]] [[1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.185]]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9026179
maxi score, test score, baseline:  0.1121 1.0 1.0
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.064]
 [0.055]
 [0.069]
 [0.067]] [[1.745]
 [1.389]
 [1.35 ]
 [1.617]
 [1.198]] [[0.067]
 [0.064]
 [0.055]
 [0.069]
 [0.067]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.05 ]
 [0.051]
 [0.06 ]
 [0.06 ]] [[1.686]
 [1.686]
 [1.926]
 [1.867]
 [2.169]] [[0.05 ]
 [0.05 ]
 [0.051]
 [0.06 ]
 [0.06 ]]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]] [[1.657]
 [1.657]
 [1.657]
 [1.657]
 [1.657]] [[0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.127]
 [0.124]
 [0.127]
 [0.124]] [[2.204]
 [2.204]
 [1.97 ]
 [2.204]
 [2.454]] [[0.127]
 [0.127]
 [0.124]
 [0.127]
 [0.124]]
actor:  0 policy actor:  0  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.157]] [[2.987]
 [2.987]
 [2.987]
 [2.987]
 [3.051]] [[1.72 ]
 [1.72 ]
 [1.72 ]
 [1.72 ]
 [1.771]]
maxi score, test score, baseline:  0.1241 0.3 0.3
probs:  [0.6907694878701865, 0.162677742969114, 0.08367189136974089, 0.06288087779095851]
from probs:  [0.6907694878701865, 0.162677742969114, 0.08367189136974089, 0.06288087779095851]
maxi score, test score, baseline:  0.1241 0.3 0.3
probs:  [0.6907694878701865, 0.162677742969114, 0.08367189136974089, 0.06288087779095851]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6907694878701865, 0.162677742969114, 0.08367189136974089, 0.06288087779095851]
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6907694878701865, 0.162677742969114, 0.08367189136974089, 0.06288087779095851]
maxi score, test score, baseline:  0.1261 0.3 0.3
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6907694878701865, 0.162677742969114, 0.08367189136974089, 0.06288087779095851]
maxi score, test score, baseline:  0.1261 0.3 0.3
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[3.005]
 [3.005]
 [3.005]
 [3.005]
 [3.005]] [[0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]]
maxi score, test score, baseline:  0.1261 0.3 0.3
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.036]
 [0.104]
 [0.128]
 [0.097]] [[1.961]
 [1.135]
 [1.961]
 [2.001]
 [2.226]] [[0.831]
 [0.145]
 [0.831]
 [0.905]
 [0.995]]
siam score:  -0.88094515
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6913181327025971, 0.1621497862301622, 0.08360135980066014, 0.06293072126658064]
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6913181327025971, 0.1621497862301622, 0.08360135980066014, 0.06293072126658064]
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6913181327025971, 0.1621497862301622, 0.08360135980066014, 0.06293072126658064]
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6913181327025971, 0.1621497862301622, 0.08360135980066014, 0.06293072126658064]
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6913181327025971, 0.1621497862301622, 0.08360135980066014, 0.06293072126658064]
1758 3587
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.88711584
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6890274647060766, 0.16505995438087567, 0.08319003089256302, 0.06272255002048487]
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6890274647060766, 0.16505995438087567, 0.08319003089256302, 0.06272255002048487]
Printing some Q and Qe and total Qs values:  [[0.963]
 [0.963]
 [0.943]
 [0.963]
 [0.963]] [[2.091]
 [2.091]
 [1.663]
 [2.091]
 [2.091]] [[2.268]
 [2.268]
 [2.084]
 [2.268]
 [2.268]]
siam score:  -0.88875306
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1765 3590
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.689577550490748, 0.16452658740457682, 0.08312333731369753, 0.06277252479097772]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.1261 0.3 0.3
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.689577550490748, 0.16452658740457682, 0.08312333731369753, 0.06277252479097772]
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.689577550490748, 0.16452658740457682, 0.08312333731369753, 0.06277252479097772]
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.689577550490748, 0.16452658740457682, 0.08312333731369753, 0.06277252479097772]
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.689577550490748, 0.16452658740457682, 0.08312333731369753, 0.06277252479097772]
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.229]
 [0.926]
 [0.153]
 [0.161]] [[1.312]
 [1.299]
 [0.253]
 [1.246]
 [1.477]] [[0.277]
 [0.435]
 [1.481]
 [0.266]
 [0.359]]
using explorer policy with actor:  1
using another actor
first move QE:  0.1313348039718825
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.689577550490748, 0.16452658740457682, 0.08312333731369753, 0.06277252479097772]
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.08 ]
 [0.08 ]
 [0.086]
 [0.085]] [[1.386]
 [1.174]
 [1.031]
 [1.221]
 [1.323]] [[1.121]
 [0.904]
 [0.762]
 [0.964]
 [1.064]]
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.689577550490748, 0.16452658740457682, 0.08312333731369753, 0.06277252479097772]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.674]
 [0.315]
 [0.315]
 [0.315]] [[1.745]
 [1.359]
 [1.745]
 [1.745]
 [1.745]] [[1.183]
 [1.772]
 [1.183]
 [1.183]
 [1.183]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6906591205917334, 0.16347788956418022, 0.08299220543783621, 0.06287078440625021]
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6906591205917334, 0.16347788956418022, 0.08299220543783621, 0.06287078440625021]
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6906591205917334, 0.16347788956418022, 0.08299220543783621, 0.06287078440625021]
maxi score, test score, baseline:  0.1261 0.3 0.3
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6906591205917334, 0.16347788956418022, 0.08299220543783621, 0.06287078440625021]
maxi score, test score, baseline:  0.1261 0.3 0.3
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6906591205917334, 0.16347788956418022, 0.08299220543783621, 0.06287078440625021]
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6906591205917334, 0.16347788956418022, 0.08299220543783621, 0.06287078440625021]
maxi score, test score, baseline:  0.1261 0.3 0.3
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6906591205917334, 0.16347788956418022, 0.08299220543783621, 0.06287078440625021]
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6906591205917334, 0.16347788956418022, 0.08299220543783621, 0.06287078440625021]
siam score:  -0.869209
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6906591205917333, 0.16347788956418022, 0.08299220543783621, 0.06287078440625021]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.6917165798551329, 0.16245256975844213, 0.08286399681157133, 0.06296685357485361]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1261 0.3 0.3
probs:  [0.692236518474552, 0.16194843368180062, 0.08280095833959891, 0.06301408950404849]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8679806
actor:  0 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1281 0.3 0.3
probs:  [0.6927507256276781, 0.16144985487446434, 0.08273861476287715, 0.06306080473498034]
maxi score, test score, baseline:  0.1281 0.3 0.3
maxi score, test score, baseline:  0.1281 0.3 0.3
maxi score, test score, baseline:  0.1281 0.3 0.3
probs:  [0.6927507256276781, 0.16144985487446434, 0.08273861476287715, 0.06306080473498034]
siam score:  -0.86556464
maxi score, test score, baseline:  0.1281 0.3 0.3
probs:  [0.6927507256276781, 0.16144985487446434, 0.08273861476287715, 0.06306080473498034]
maxi score, test score, baseline:  0.1281 0.3 0.3
probs:  [0.6927507256276781, 0.16144985487446434, 0.08273861476287715, 0.06306080473498034]
maxi score, test score, baseline:  0.1281 0.3 0.3
probs:  [0.6927507256276781, 0.16144985487446434, 0.08273861476287715, 0.06306080473498034]
maxi score, test score, baseline:  0.1281 0.3 0.3
probs:  [0.6927507256276781, 0.16144985487446434, 0.08273861476287715, 0.06306080473498034]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.5226966896435006
in main func line 156:  1784
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.299]
 [0.56 ]
 [0.299]
 [0.299]] [[2.066]
 [2.066]
 [2.463]
 [2.066]
 [2.066]] [[0.299]
 [0.299]
 [0.56 ]
 [0.299]
 [0.299]]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1301 0.3 0.3
probs:  [0.6905556404539949, 0.1642281053983373, 0.08235493327856833, 0.06286132086909953]
maxi score, test score, baseline:  0.1301 0.3 0.3
probs:  [0.6905556404539949, 0.1642281053983373, 0.08235493327856833, 0.06286132086909953]
maxi score, test score, baseline:  0.1301 0.3 0.3
maxi score, test score, baseline:  0.1301 0.3 0.3
probs:  [0.6905556404539949, 0.1642281053983373, 0.08235493327856833, 0.06286132086909953]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1301 0.3 0.3
probs:  [0.687872995113519, 0.16747404748414835, 0.0820354142912666, 0.06261754311106621]
Printing some Q and Qe and total Qs values:  [[1.286]
 [1.286]
 [1.286]
 [1.286]
 [1.286]] [[0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]] [[2.145]
 [2.145]
 [2.145]
 [2.145]
 [2.145]]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1301 0.3 0.3
probs:  [0.6884012691364775, 0.16695482567018266, 0.08197836821641612, 0.06266553697692372]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.741]
 [1.154]
 [0.741]
 [0.741]
 [0.741]] [[1.623]
 [0.563]
 [1.623]
 [1.623]
 [1.623]] [[1.279]
 [1.751]
 [1.279]
 [1.279]
 [1.279]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.154]
 [0.246]
 [0.154]
 [0.154]] [[1.524]
 [1.387]
 [1.523]
 [1.387]
 [1.387]] [[0.145]
 [0.154]
 [0.246]
 [0.154]
 [0.154]]
maxi score, test score, baseline:  0.1301 0.3 0.3
probs:  [0.6894408546766676, 0.16593305388794186, 0.08186610777588368, 0.06275998365950684]
maxi score, test score, baseline:  0.1301 0.3 0.3
probs:  [0.6894408546766676, 0.16593305388794186, 0.08186610777588368, 0.06275998365950684]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1321 0.3 0.3
probs:  [0.6894408546766676, 0.16593305388794186, 0.08186610777588368, 0.06275998365950684]
maxi score, test score, baseline:  0.1321 0.3 0.3
probs:  [0.6894408546766676, 0.16593305388794186, 0.08186610777588368, 0.06275998365950684]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.689952346778849, 0.16543032642911976, 0.08181087390959768, 0.06280645288243358]
using another actor
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1321 0.3 0.3
probs:  [0.6904584223051373, 0.16493292273076748, 0.08175622495640676, 0.06285243000768843]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1321 0.3 0.3
probs:  [0.6909591668426743, 0.16444075867231694, 0.08170215167411794, 0.06289792281089089]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1321 0.3 0.3
probs:  [0.6919449963783106, 0.16347182176172023, 0.08159569611689639, 0.06298748574307278]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.07 ]
 [1.07 ]
 [1.232]
 [1.07 ]
 [1.07 ]] [[1.765]
 [1.765]
 [1.088]
 [1.765]
 [1.765]] [[2.088]
 [2.088]
 [2.185]
 [2.088]
 [2.088]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1321 0.3 0.3
probs:  [0.6947829070253989, 0.16068253996853046, 0.08128924216277973, 0.06324531084329092]
maxi score, test score, baseline:  0.1321 0.3 0.3
probs:  [0.6947829070253989, 0.16068253996853046, 0.08128924216277973, 0.06324531084329092]
maxi score, test score, baseline:  0.1321 0.3 0.3
maxi score, test score, baseline:  0.1321 0.3 0.3
probs:  [0.6947829070253989, 0.16068253996853046, 0.08128924216277973, 0.06324531084329092]
maxi score, test score, baseline:  0.1321 0.3 0.3
probs:  [0.6947829070253989, 0.16068253996853046, 0.08128924216277973, 0.06324531084329092]
maxi score, test score, baseline:  0.1321 0.3 0.3
probs:  [0.6947829070253989, 0.16068253996853046, 0.08128924216277973, 0.06324531084329092]
maxi score, test score, baseline:  0.1321 0.3 0.3
actor:  0 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1361 0.3 0.3
probs:  [0.695239242902646, 0.16023402360833747, 0.08123996438367445, 0.06328676910534195]
maxi score, test score, baseline:  0.1361 0.3 0.3
probs:  [0.695239242902646, 0.16023402360833747, 0.08123996438367445, 0.06328676910534195]
maxi score, test score, baseline:  0.1361 0.3 0.3
probs:  [0.695239242902646, 0.16023402360833747, 0.08123996438367445, 0.06328676910534195]
maxi score, test score, baseline:  0.1361 0.3 0.3
probs:  [0.695239242902646, 0.16023402360833747, 0.08123996438367445, 0.06328676910534195]
maxi score, test score, baseline:  0.1361 0.3 0.3
probs:  [0.695239242902646, 0.16023402360833747, 0.08123996438367445, 0.06328676910534195]
using another actor
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1361 0.3 0.3
probs:  [0.6956910122653699, 0.15978999551342213, 0.08119117972313641, 0.06332781249807148]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1361 0.3 0.3
probs:  [0.6961382833174025, 0.15935038864865533, 0.08114288081612261, 0.06336844721781972]
maxi score, test score, baseline:  0.1361 0.3 0.3
probs:  [0.6961382833174025, 0.15935038864865533, 0.08114288081612261, 0.06336844721781972]
maxi score, test score, baseline:  0.1361 0.3 0.3
maxi score, test score, baseline:  0.1361 0.3 0.3
maxi score, test score, baseline:  0.1361 0.3 0.3
probs:  [0.6961382833174025, 0.15935038864865533, 0.08114288081612261, 0.06336844721781972]
maxi score, test score, baseline:  0.1361 0.3 0.3
probs:  [0.6961382833174025, 0.15935038864865533, 0.08114288081612261, 0.06336844721781972]
maxi score, test score, baseline:  0.1361 0.3 0.3
probs:  [0.6961382833174025, 0.15935038864865533, 0.08114288081612261, 0.06336844721781972]
maxi score, test score, baseline:  0.1361 0.3 0.3
probs:  [0.6961382833174025, 0.15935038864865533, 0.08114288081612261, 0.06336844721781972]
maxi score, test score, baseline:  0.1361 0.3 0.3
from probs:  [0.6961382833174025, 0.15935038864865533, 0.08114288081612261, 0.06336844721781972]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.207]
 [0.166]
 [0.171]
 [0.169]] [[1.77 ]
 [1.213]
 [0.947]
 [1.733]
 [1.557]] [[1.027]
 [0.735]
 [0.477]
 [1.011]
 [0.889]]
siam score:  -0.8498672
maxi score, test score, baseline:  0.1381 0.3 0.3
maxi score, test score, baseline:  0.1381 0.3 0.3
probs:  [0.6961382833174025, 0.15935038864865533, 0.08114288081612261, 0.06336844721781972]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1814 3616
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.1381 0.3 0.3
probs:  [0.6978837018856034, 0.157634878599467, 0.0809544004556283, 0.0635270190593013]
maxi score, test score, baseline:  0.1381 0.3 0.3
probs:  [0.6978837018856034, 0.157634878599467, 0.0809544004556283, 0.0635270190593013]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.41 ]
 [0.627]
 [0.358]
 [0.41 ]] [[1.711]
 [1.711]
 [1.216]
 [1.74 ]
 [1.711]] [[0.41 ]
 [0.41 ]
 [0.627]
 [0.358]
 [0.41 ]]
maxi score, test score, baseline:  0.1381 0.3 0.3
probs:  [0.6978837018856034, 0.157634878599467, 0.0809544004556283, 0.0635270190593013]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1401 0.3 0.3
from probs:  [0.6978837018856034, 0.157634878599467, 0.0809544004556283, 0.0635270190593013]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8656576
maxi score, test score, baseline:  0.1401 0.3 0.3
maxi score, test score, baseline:  0.1401 0.3 0.3
probs:  [0.6983094582848298, 0.1572164177244163, 0.08090842482487075, 0.06356569916588314]
maxi score, test score, baseline:  0.1401 0.3 0.3
probs:  [0.6983094582848298, 0.1572164177244163, 0.08090842482487075, 0.06356569916588314]
using another actor
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1401 0.3 0.3
probs:  [0.6991486814824467, 0.156391574987406, 0.08081780066531169, 0.06364194286483572]
maxi score, test score, baseline:  0.1401 0.3 0.3
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1401 0.3 0.3
probs:  [0.699562265962496, 0.1559850774603145, 0.0807731394285661, 0.06367951714862327]
maxi score, test score, baseline:  0.1401 0.3 0.3
probs:  [0.699562265962496, 0.1559850774603145, 0.0807731394285661, 0.06367951714862327]
maxi score, test score, baseline:  0.1401 0.3 0.3
start point for exploration sampling:  20032
siam score:  -0.85998
maxi score, test score, baseline:  0.1401 0.3 0.3
probs:  [0.699562265962496, 0.1559850774603145, 0.0807731394285661, 0.06367951714862327]
maxi score, test score, baseline:  0.1401 0.3 0.3
probs:  [0.699562265962496, 0.1559850774603145, 0.0807731394285661, 0.06367951714862327]
maxi score, test score, baseline:  0.1401 0.3 0.3
maxi score, test score, baseline:  0.1401 0.3 0.3
probs:  [0.699562265962496, 0.1559850774603145, 0.0807731394285661, 0.06367951714862327]
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.092]
 [0.107]
 [0.103]
 [0.092]] [[1.353]
 [1.353]
 [1.474]
 [1.421]
 [1.953]] [[0.343]
 [0.343]
 [0.414]
 [0.388]
 [0.543]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14209999999999998 0.3 0.3
probs:  [0.7003776665177469, 0.15518364915415853, 0.0806850877752831, 0.0637535965528114]
maxi score, test score, baseline:  0.14209999999999998 0.3 0.3
probs:  [0.7003776665177469, 0.15518364915415853, 0.0806850877752831, 0.0637535965528114]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14209999999999998 0.3 0.3
probs:  [0.7007795936880635, 0.15478860918364265, 0.08064168536205461, 0.06379011176623915]
siam score:  -0.8522535
maxi score, test score, baseline:  0.14209999999999998 0.3 0.3
probs:  [0.7007795936880635, 0.15478860918364265, 0.08064168536205461, 0.06379011176623915]
maxi score, test score, baseline:  0.14209999999999998 0.3 0.3
probs:  [0.7007795936880635, 0.15478860918364265, 0.08064168536205461, 0.06379011176623915]
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.894]
 [0.467]
 [0.467]
 [0.467]] [[1.445]
 [1.917]
 [1.445]
 [1.445]
 [1.445]] [[1.103]
 [2.115]
 [1.103]
 [1.103]
 [1.103]]
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.658]
 [1.097]
 [0.658]
 [0.658]] [[2.449]
 [2.449]
 [2.17 ]
 [2.449]
 [2.449]] [[1.507]
 [1.507]
 [2.292]
 [1.507]
 [1.507]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14209999999999998 0.3 0.3
probs:  [0.701177744443878, 0.1543972809170965, 0.08059869074783761, 0.06382628389118787]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.14209999999999998 0.3 0.3
probs:  [0.7015721717597942, 0.15400961228765925, 0.08055609821212892, 0.06386211774041749]
maxi score, test score, baseline:  0.14209999999999998 0.3 0.3
probs:  [0.7015721717597942, 0.15400961228765925, 0.08055609821212892, 0.06386211774041749]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14209999999999998 0.3 0.3
probs:  [0.7019629276242085, 0.1536255521977791, 0.0805139021409218, 0.0638976180370906]
maxi score, test score, baseline:  0.14209999999999998 0.3 0.3
probs:  [0.7019629276242085, 0.1536255521977791, 0.0805139021409218, 0.0638976180370906]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14209999999999998 0.3 0.3
probs:  [0.7019629276242085, 0.1536255521977791, 0.0805139021409218, 0.0638976180370906]
start point for exploration sampling:  20032
siam score:  -0.8469551
maxi score, test score, baseline:  0.14209999999999998 0.3 0.3
probs:  [0.7019629276242085, 0.1536255521977791, 0.0805139021409218, 0.0638976180370906]
maxi score, test score, baseline:  0.14209999999999998 0.3 0.3
probs:  [0.7019629276242085, 0.1536255521977791, 0.0805139021409218, 0.0638976180370906]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8449751
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14409999999999998 0.3 0.3
probs:  [0.7027336281575037, 0.15286805795893557, 0.08043067745373497, 0.06396763642982574]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7027336281575037, 0.15286805795893557, 0.08043067745373497, 0.06396763642982574]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7027336281575037, 0.15286805795893557, 0.08043067745373497, 0.06396763642982574]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7027336281575037, 0.15286805795893557, 0.08043067745373497, 0.06396763642982574]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7027336281575037, 0.15286805795893557, 0.08043067745373497, 0.06396763642982574]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7027336281575037, 0.15286805795893557, 0.08043067745373497, 0.06396763642982574]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7027336281575037, 0.15286805795893557, 0.08043067745373497, 0.06396763642982574]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.495]
 [0.636]
 [0.636]
 [0.481]] [[2.03 ]
 [2.896]
 [1.898]
 [2.588]
 [1.9  ]] [[0.879]
 [1.244]
 [1.194]
 [1.424]
 [0.884]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.84712005
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.8849001612662764
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7038633885589652, 0.1517576565015478, 0.08030867941176438, 0.06407027552772268]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7038633885589652, 0.1517576565015478, 0.08030867941176438, 0.06407027552772268]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7038633885589652, 0.1517576565015478, 0.08030867941176438, 0.06407027552772268]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7038633885589652, 0.1517576565015478, 0.08030867941176438, 0.06407027552772268]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7038633885589652, 0.1517576565015478, 0.08030867941176438, 0.06407027552772268]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7038633885589652, 0.1517576565015478, 0.08030867941176438, 0.06407027552772268]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.446]
 [1.446]
 [1.467]
 [1.446]
 [1.446]] [[0.442]
 [0.442]
 [0.171]
 [0.442]
 [0.442]] [[2.911]
 [2.911]
 [2.862]
 [2.911]
 [2.911]]
line 256 mcts: sample exp_bonus 0.5392608849298919
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7038633885589652, 0.1517576565015478, 0.08030867941176438, 0.06407027552772268]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7049627329681203, 0.15067714984498784, 0.08018996586401146, 0.06417015132288045]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7049627329681203, 0.15067714984498784, 0.08018996586401146, 0.06417015132288045]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7049627329681203, 0.15067714984498784, 0.08018996586401146, 0.06417015132288045]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.101939073994639
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7056793326464028, 0.14997282941908235, 0.08011258329907633, 0.0642352546354386]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7056793326464028, 0.14997282941908235, 0.08011258329907633, 0.0642352546354386]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7060328733825096, 0.14962534675982894, 0.08007440593199382, 0.06426737392566766]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7060328733825096, 0.14962534675982894, 0.08007440593199382, 0.06426737392566766]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7063832973245521, 0.14928092748699545, 0.08003656513430478, 0.06429921005414782]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7063832973245521, 0.14928092748699545, 0.08003656513430478, 0.06429921005414782]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7063832973245521, 0.14928092748699545, 0.08003656513430478, 0.06429921005414782]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7063832973245521, 0.14928092748699545, 0.08003656513430478, 0.06429921005414782]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7063832973245521, 0.14928092748699545, 0.08003656513430478, 0.06429921005414782]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7067306455078536, 0.14893953126841664, 0.07999905647477838, 0.06433076674895151]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7067306455078536, 0.14893953126841664, 0.07999905647477838, 0.06433076674895151]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7067306455078536, 0.14893953126841664, 0.07999905647477838, 0.06433076674895151]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7067306455078536, 0.14893953126841664, 0.07999905647477838, 0.06433076674895151]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7067306455078536, 0.14893953126841664, 0.07999905647477838, 0.06433076674895151]
in main func line 156:  1870
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7070749582505307, 0.14860111847684473, 0.07996187559963192, 0.06436204767299265]
in main func line 156:  1872
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.633]
 [1.1  ]
 [0.633]
 [0.633]] [[2.502]
 [2.502]
 [1.499]
 [2.502]
 [2.502]] [[1.582]
 [1.582]
 [2.181]
 [1.582]
 [1.582]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.707416275169094, 0.14826565017461574, 0.0799250182308462, 0.06439305642544403]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.707416275169094, 0.14826565017461574, 0.0799250182308462, 0.06439305642544403]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.707416275169094, 0.14826565017461574, 0.0799250182308462, 0.06439305642544403]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.707416275169094, 0.14826565017461574, 0.0799250182308462, 0.06439305642544403]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.707416275169094, 0.14826565017461574, 0.0799250182308462, 0.06439305642544403]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.844]
 [0.868]
 [0.322]
 [0.495]] [[2.037]
 [1.406]
 [0.703]
 [2.073]
 [2.181]] [[0.862]
 [1.588]
 [1.402]
 [0.768]
 [1.149]]
Printing some Q and Qe and total Qs values:  [[1.083]
 [1.378]
 [1.15 ]
 [1.234]
 [1.234]] [[2.151]
 [0.534]
 [2.089]
 [1.834]
 [1.834]] [[1.978]
 [2.028]
 [2.091]
 [2.175]
 [2.175]]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.7084226369374719, 0.14727653286016515, 0.07981634548475122, 0.0644844847176117]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]] [[2.077]
 [2.077]
 [2.077]
 [2.077]
 [2.077]] [[1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.707261591253337, 0.14888935110899812, 0.07947009963159382, 0.06437895800607116]
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
maxi score, test score, baseline:  0.14609999999999998 0.3 0.3
probs:  [0.707261591253337, 0.14888935110899812, 0.07947009963159382, 0.06437895800607116]
siam score:  -0.84758085
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.546]
 [0.697]
 [0.697]
 [0.697]] [[2.326]
 [2.329]
 [2.326]
 [2.326]
 [2.326]] [[2.411]
 [2.161]
 [2.411]
 [2.411]
 [2.411]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.847122
siam score:  -0.8463983
maxi score, test score, baseline:  0.14809999999999998 0.3 0.3
maxi score, test score, baseline:  0.14809999999999998 0.3 0.3
probs:  [0.7082416856044642, 0.14792014483282667, 0.07937016910012634, 0.06446800046258279]
maxi score, test score, baseline:  0.14809999999999998 0.3 0.3
maxi score, test score, baseline:  0.14809999999999998 0.3 0.3
using another actor
maxi score, test score, baseline:  0.14809999999999998 0.3 0.3
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.967]
 [0.821]
 [0.745]
 [0.662]] [[ 1.12 ]
 [-0.025]
 [ 1.002]
 [ 1.483]
 [ 1.538]] [[0.342]
 [0.967]
 [0.821]
 [0.745]
 [0.662]]
actor:  0 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
probs:  [0.7088815582316581, 0.14728738068566752, 0.07930492761431078, 0.06452613346836367]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
probs:  [0.7095109228912679, 0.14666500777086322, 0.07924075752206475, 0.0645833118158042]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
probs:  [0.7095109228912679, 0.14666500777086322, 0.07924075752206475, 0.0645833118158042]
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
probs:  [0.7095109228912679, 0.14666500777086322, 0.07924075752206475, 0.0645833118158042]
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
probs:  [0.7095109228912679, 0.14666500777086322, 0.07924075752206475, 0.0645833118158042]
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
probs:  [0.7095109228912679, 0.14666500777086322, 0.07924075752206475, 0.0645833118158042]
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
probs:  [0.7095109228912679, 0.14666500777086322, 0.07924075752206475, 0.0645833118158042]
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
probs:  [0.7095109228912679, 0.14666500777086322, 0.07924075752206475, 0.0645833118158042]
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.22 ]
 [0.214]
 [0.22 ]
 [0.22 ]] [[3.284]
 [3.284]
 [2.419]
 [3.284]
 [3.284]] [[1.11 ]
 [1.11 ]
 [0.809]
 [1.11 ]
 [1.11 ]]
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
probs:  [0.7095109228912679, 0.14666500777086322, 0.07924075752206475, 0.0645833118158042]
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
probs:  [0.7095109228912679, 0.14666500777086322, 0.07924075752206475, 0.0645833118158042]
UNIT TEST: sample policy line 217 mcts : [0.833 0.042 0.042 0.042 0.042]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
probs:  [0.7095109228912679, 0.14666500777086322, 0.07924075752206475, 0.0645833118158042]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
probs:  [0.7098217452907056, 0.1463576383630473, 0.07920906603488079, 0.06461155031136633]
siam score:  -0.8446352
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
probs:  [0.7098217452907056, 0.1463576383630473, 0.07920906603488079, 0.06461155031136633]
maxi score, test score, baseline:  0.15009999999999998 0.3 0.3
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7107391469486696, 0.14545042831368224, 0.07911552765753577, 0.06469489708011263]
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7107391469486696, 0.14545042831368224, 0.07911552765753577, 0.06469489708011263]
siam score:  -0.843922
first move QE:  0.16926569054079488
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7107391469486696, 0.14545042831368224, 0.07911552765753577, 0.06469489708011263]
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.1  ]
 [0.1  ]
 [0.1  ]
 [0.096]] [[1.456]
 [2.19 ]
 [2.19 ]
 [2.19 ]
 [1.498]] [[0.328]
 [0.585]
 [0.585]
 [0.585]
 [0.344]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.259594593760911
1903 3662
1904 3663
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7104358267699411, 0.14575037884454164, 0.07914645421744325, 0.06466734016807403]
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7104358267699411, 0.14575037884454164, 0.07914645421744325, 0.06466734016807403]
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7104358267699411, 0.14575037884454164, 0.07914645421744325, 0.06466734016807403]
1905 3668
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7104358267699411, 0.14575037884454164, 0.07914645421744325, 0.06466734016807403]
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7104358267699411, 0.14575037884454164, 0.07914645421744325, 0.06466734016807403]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7107391469486696, 0.14545042831368224, 0.07911552765753577, 0.06469489708011263]
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7107391469486696, 0.14545042831368224, 0.07911552765753577, 0.06469489708011263]
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7116345815938623, 0.14456494124090796, 0.07902422903931026, 0.06477624812591945]
siam score:  -0.84732926
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7116345815938623, 0.14456494124090796, 0.07902422903931026, 0.06477624812591945]
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7116345815938623, 0.14456494124090796, 0.07902422903931026, 0.06477624812591945]
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7116345815938623, 0.14456494124090796, 0.07902422903931026, 0.06477624812591945]
maxi score, test score, baseline:  0.15209999999999999 0.3 0.3
probs:  [0.7116345815938623, 0.14456494124090796, 0.07902422903931026, 0.06477624812591945]
actor:  0 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.093]] [[1.443]
 [1.443]
 [1.443]
 [1.443]
 [2.299]] [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.932]]
maxi score, test score, baseline:  0.1541 0.3 0.3
probs:  [0.7116345815938623, 0.14456494124090796, 0.07902422903931026, 0.06477624812591945]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1561 0.3 0.3
maxi score, test score, baseline:  0.1561 0.3 0.3
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1561 0.3 0.3
maxi score, test score, baseline:  0.1561 0.3 0.3
probs:  [0.7113384952595125, 0.1448577382896925, 0.07905441803562252, 0.06474934841517252]
maxi score, test score, baseline:  0.1561 0.3 0.3
maxi score, test score, baseline:  0.1561 0.3 0.3
probs:  [0.7113384952595125, 0.1448577382896925, 0.0790544180356225, 0.06474934841517252]
maxi score, test score, baseline:  0.1561 0.3 0.3
probs:  [0.7113384952595125, 0.1448577382896925, 0.0790544180356225, 0.06474934841517252]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1561 0.3 0.3
probs:  [0.7119283140778199, 0.14427447189309497, 0.07899428004185162, 0.06480293398723348]
siam score:  -0.8422374
maxi score, test score, baseline:  0.1561 0.3 0.3
probs:  [0.7119283140778199, 0.14427447189309497, 0.07899428004185162, 0.06480293398723348]
maxi score, test score, baseline:  0.1561 0.3 0.3
probs:  [0.7119283140778199, 0.14427447189309497, 0.07899428004185162, 0.06480293398723348]
maxi score, test score, baseline:  0.1561 0.3 0.3
probs:  [0.7119283140778199, 0.14427447189309497, 0.07899428004185162, 0.06480293398723348]
maxi score, test score, baseline:  0.1561 0.3 0.3
probs:  [0.7119283140778199, 0.14427447189309497, 0.07899428004185162, 0.06480293398723348]
maxi score, test score, baseline:  0.1561 0.3 0.3
probs:  [0.7119283140778199, 0.14427447189309497, 0.07899428004185162, 0.06480293398723348]
maxi score, test score, baseline:  0.1561 0.3 0.3
maxi score, test score, baseline:  0.1561 0.3 0.3
probs:  [0.7119283140778199, 0.14427447189309497, 0.07899428004185162, 0.06480293398723348]
actor:  0 policy actor:  0  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8452827
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7122197206694906, 0.14398630259874068, 0.07896456819263496, 0.06482940853913371]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7125088288859587, 0.14370040614645394, 0.07893509068601529, 0.06485567428157209]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.431]
 [0.979]
 [0.431]
 [0.431]] [[3.64 ]
 [3.062]
 [1.847]
 [3.062]
 [3.062]] [[2.203]
 [2.001]
 [2.224]
 [2.001]
 [2.001]]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7125088288859587, 0.14370040614645394, 0.07893509068601529, 0.06485567428157209]
first move QE:  0.1789186489975093
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7125088288859587, 0.14370040614645394, 0.07893509068601529, 0.06485567428157209]
siam score:  -0.84795135
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7125088288859587, 0.14370040614645394, 0.07893509068601529, 0.06485567428157209]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7122197206694906, 0.14398630259874068, 0.07896456819263496, 0.06482940853913371]
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.078]
 [0.076]
 [0.076]
 [0.076]] [[2.247]
 [2.38 ]
 [2.247]
 [2.247]
 [2.247]] [[0.076]
 [0.078]
 [0.076]
 [0.076]
 [0.076]]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7119283140778199, 0.14427447189309497, 0.07899428004185162, 0.06480293398723348]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.849815
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8495734
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7122197206694906, 0.14398630259874068, 0.07896456819263496, 0.06482940853913371]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7122197206694906, 0.14398630259874068, 0.07896456819263496, 0.06482940853913371]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7122197206694906, 0.14398630259874068, 0.07896456819263496, 0.06482940853913371]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7122197206694906, 0.14398630259874068, 0.07896456819263496, 0.06482940853913371]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7122197206694906, 0.14398630259874068, 0.07896456819263496, 0.06482940853913371]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7125088288859587, 0.14370040614645394, 0.07893509068601529, 0.06485567428157209]
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.065]
 [0.058]
 [0.069]
 [0.072]] [[1.527]
 [1.593]
 [2.078]
 [2.026]
 [2.319]] [[0.072]
 [0.065]
 [0.058]
 [0.069]
 [0.072]]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7125088288859587, 0.14370040614645394, 0.07893509068601529, 0.06485567428157209]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1581 0.3 0.3
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1581 0.3 0.3
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8441945
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7130802581082866, 0.14313532505193388, 0.07887682769754116, 0.06490758914223839]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7130802581082866, 0.14313532505193388, 0.07887682769754116, 0.06490758914223839]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.506]
 [0.681]
 [0.506]
 [0.506]] [[3.742]
 [3.742]
 [3.078]
 [3.742]
 [3.742]] [[1.515]
 [1.515]
 [1.643]
 [1.515]
 [1.515]]
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.873]
 [0.213]
 [0.367]
 [0.367]] [[4.305]
 [2.368]
 [2.569]
 [5.022]
 [4.297]] [[1.081]
 [1.791]
 [0.538]
 [1.664]
 [1.424]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7130802581082866, 0.14313532505193385, 0.07887682769754116, 0.06490758914223839]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7130802581082866, 0.14313532505193385, 0.07887682769754116, 0.06490758914223839]
maxi score, test score, baseline:  0.1581 0.3 0.3
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7130802581082866, 0.14313532505193385, 0.07887682769754116, 0.06490758914223839]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7130802581082866, 0.14313532505193388, 0.07887682769754116, 0.06490758914223839]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7130802581082866, 0.14313532505193388, 0.07887682769754116, 0.06490758914223839]
first move QE:  0.1881058925817732
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7136428133851688, 0.14257901932032715, 0.07881946949755357, 0.06495869779695061]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7141966998343261, 0.14203128611260857, 0.07876299517222632, 0.06500901888083886]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7141966998343261, 0.14203128611260857, 0.07876299517222632, 0.06500901888083886]
siam score:  -0.8487308
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7141966998343261, 0.14203128611260857, 0.07876299517222632, 0.06500901888083886]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.601]
 [0.635]
 [0.39 ]
 [0.601]] [[3.318]
 [3.287]
 [3.014]
 [3.481]
 [3.287]] [[1.492]
 [1.688]
 [1.559]
 [1.545]
 [1.688]]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7150117087717278, 0.1412253312814672, 0.07867989676831082, 0.06508306317849422]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.715279255591936, 0.1409607566953396, 0.07865261766410508, 0.06510737004861933]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [0.7150117087717278, 0.1412253312814672, 0.07867989676831082, 0.06508306317849422]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.004]
 [0.056]
 [0.059]
 [0.061]] [[2.206]
 [0.305]
 [2.206]
 [2.543]
 [2.603]] [[1.177]
 [0.005]
 [1.177]
 [1.378]
 [1.415]]
maxi score, test score, baseline:  0.1601 0.3 0.3
probs:  [0.715279255591936, 0.1409607566953396, 0.07865261766410508, 0.06510737004861933]
maxi score, test score, baseline:  0.1601 0.3 0.3
probs:  [0.715279255591936, 0.1409607566953396, 0.07865261766410508, 0.06510737004861933]
maxi score, test score, baseline:  0.1601 0.3 0.3
maxi score, test score, baseline:  0.1601 0.3 0.3
probs:  [0.715279255591936, 0.1409607566953396, 0.07865261766410508, 0.06510737004861933]
maxi score, test score, baseline:  0.1601 0.3 0.3
probs:  [0.715279255591936, 0.1409607566953396, 0.07865261766410508, 0.06510737004861933]
maxi score, test score, baseline:  0.1601 0.3 0.3
probs:  [0.715279255591936, 0.1409607566953396, 0.07865261766410508, 0.06510737004861933]
maxi score, test score, baseline:  0.1601 0.3 0.3
probs:  [0.715279255591936, 0.1409607566953396, 0.07865261766410508, 0.06510737004861933]
maxi score, test score, baseline:  0.1601 0.3 0.3
probs:  [0.715279255591936, 0.1409607566953396, 0.07865261766410508, 0.06510737004861933]
maxi score, test score, baseline:  0.1601 0.3 0.3
probs:  [0.715279255591936, 0.1409607566953396, 0.07865261766410508, 0.06510737004861933]
Printing some Q and Qe and total Qs values:  [[1.286]
 [1.425]
 [1.286]
 [1.286]
 [1.286]] [[2.174]
 [0.832]
 [2.174]
 [2.174]
 [2.174]] [[2.746]
 [2.576]
 [2.746]
 [2.746]
 [2.746]]
maxi score, test score, baseline:  0.1601 0.3 0.3
probs:  [0.715279255591936, 0.1409607566953396, 0.07865261766410508, 0.06510737004861933]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.106]] [[1.82 ]
 [1.82 ]
 [1.82 ]
 [1.82 ]
 [2.322]] [[0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.106]]
maxi score, test score, baseline:  0.1601 0.3 0.3
probs:  [0.715279255591936, 0.1409607566953396, 0.07865261766410508, 0.06510737004861933]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.715279255591936, 0.1409607566953396, 0.07865261766410508, 0.06510737004861933]
Starting evaluation
maxi score, test score, baseline:  0.1601 0.3 0.3
probs:  [0.7155447799562349, 0.1406981820972318, 0.07862554476973381, 0.06513149317679945]
1966 3693
maxi score, test score, baseline:  0.1601 0.3 0.3
probs:  [0.7155447799562349, 0.1406981820972318, 0.07862554476973381, 0.06513149317679945]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.379458380558595
line 256 mcts: sample exp_bonus 3.431279815183656
maxi score, test score, baseline:  0.1601 0.3 0.3
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]] [[3.559]
 [3.559]
 [3.559]
 [3.559]
 [3.559]] [[0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.2336161787630213
maxi score, test score, baseline:  0.1601 0.3 0.3
probs:  [0.7155447799562349, 0.1406981820972318, 0.07862554476973381, 0.06513149317679945]
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]] [[2.76]
 [2.76]
 [2.76]
 [2.76]
 [2.76]] [[0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.034]
 [0.024]
 [0.025]
 [0.031]] [[4.136]
 [3.073]
 [2.42 ]
 [2.841]
 [2.895]] [[0.032]
 [0.034]
 [0.024]
 [0.025]
 [0.031]]
actor:  0 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.096]
 [0.081]
 [0.089]
 [0.088]] [[2.523]
 [2.415]
 [1.942]
 [2.538]
 [4.254]] [[0.596]
 [0.591]
 [0.403]
 [0.617]
 [1.19 ]]
maxi score, test score, baseline:  0.1621 0.3 0.3
probs:  [0.7155447799562349, 0.1406981820972318, 0.07862554476973381, 0.06513149317679945]
siam score:  -0.843119
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.146]
 [0.142]
 [0.141]
 [0.141]] [[3.288]
 [3.198]
 [3.032]
 [3.288]
 [3.288]] [[0.141]
 [0.146]
 [0.142]
 [0.141]
 [0.141]]
actor:  0 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1641 0.3 0.3
probs:  [0.7155447799562349, 0.1406981820972318, 0.07862554476973381, 0.06513149317679945]
actor:  0 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]] [[3.077]
 [3.077]
 [3.077]
 [3.077]
 [3.077]] [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]]
maxi score, test score, baseline:  0.1661 0.3 0.3
probs:  [0.7155447799562349, 0.1406981820972318, 0.07862554476973381, 0.06513149317679945]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.021]
 [0.117]
 [0.069]
 [0.09 ]] [[3.288]
 [0.865]
 [3.196]
 [2.953]
 [4.55 ]] [[0.073]
 [0.021]
 [0.117]
 [0.069]
 [0.09 ]]
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]] [[2.455]
 [2.455]
 [2.455]
 [2.455]
 [2.455]] [[0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]]
rdn probs:  [0.7155447799562349, 0.1406981820972318, 0.07862554476973381, 0.06513149317679945]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7155448526603044, 0.1406981650275807, 0.07862551800620673, 0.06513146430590806]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7155448526603044, 0.1406981650275807, 0.07862551800620673, 0.06513146430590806]
first move QE:  0.19933087398972532
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7155448526603044, 0.1406981650275807, 0.07862551800620673, 0.06513146430590806]
maxi score, test score, baseline:  0.1661 0.15 0.1661
maxi score, test score, baseline:  0.1661 0.15 0.1661
maxi score, test score, baseline:  0.1661 0.15 0.1661
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7155448526603044, 0.1406981650275807, 0.07862551800620673, 0.06513146430590806]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7155448526603044, 0.1406981650275807, 0.07862551800620673, 0.06513146430590806]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7155448526603044, 0.1406981650275807, 0.07862551800620673, 0.06513146430590806]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.601171911472729
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]] [[3.609]
 [3.609]
 [3.609]
 [3.609]
 [3.609]] [[0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]]
maxi score, test score, baseline:  0.1661 0.15 0.1661
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7152793285302184, 0.14096073960208855, 0.078652590803282, 0.06510734106441102]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7152793285302184, 0.14096073960208855, 0.078652590803282, 0.06510734106441102]
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]] [[2.552]
 [2.552]
 [2.552]
 [2.552]
 [2.552]] [[1.543]
 [1.543]
 [1.543]
 [1.543]
 [1.543]]
maxi score, test score, baseline:  0.1661 0.15 0.1661
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7152793285302185, 0.14096073960208855, 0.07865259080328202, 0.06510734106441102]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7152793285302185, 0.14096073960208855, 0.07865259080328202, 0.06510734106441102]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7152793285302185, 0.14096073960208855, 0.07865259080328202, 0.06510734106441102]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7152793285302185, 0.14096073960208855, 0.07865259080328202, 0.06510734106441102]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7152793285302185, 0.14096073960208855, 0.07865259080328202, 0.06510734106441102]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7152793285302185, 0.14096073960208855, 0.07865259080328202, 0.06510734106441102]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7152793285302185, 0.14096073960208855, 0.07865259080328202, 0.06510734106441102]
maxi score, test score, baseline:  0.1661 0.15 0.1661
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7152793285302185, 0.14096073960208855, 0.07865259080328202, 0.06510734106441102]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7152793285302185, 0.14096073960208855, 0.07865259080328202, 0.06510734106441102]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7152793285302185, 0.14096073960208855, 0.07865259080328202, 0.06510734106441102]
maxi score, test score, baseline:  0.1661 0.15 0.1661
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.075]
 [0.076]
 [0.075]
 [0.075]] [[3.348]
 [3.462]
 [1.616]
 [3.462]
 [3.462]] [[0.683]
 [0.725]
 [0.11 ]
 [0.725]
 [0.725]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7152793285302184, 0.14096073960208855, 0.078652590803282, 0.06510734106441102]
maxi score, test score, baseline:  0.1661 0.15 0.1661
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7152793285302184, 0.14096073960208855, 0.078652590803282, 0.06510734106441102]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7150117819456887, 0.14122531416475118, 0.07867986980948308, 0.06508303408007697]
maxi score, test score, baseline:  0.1661 0.15 0.1661
maxi score, test score, baseline:  0.1661 0.15 0.1661
maxi score, test score, baseline:  0.1661 0.15 0.1661
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7147421897113142, 0.1414919116533055, 0.0787073573898093, 0.065058541245571]
maxi score, test score, baseline:  0.1661 0.15 0.1661
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7147421897113142, 0.1414919116533055, 0.0787073573898093, 0.065058541245571]
line 256 mcts: sample exp_bonus 1.4729154707011534
using another actor
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7147421897113142, 0.1414919116533055, 0.0787073573898093, 0.065058541245571]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7147421897113142, 0.1414919116533055, 0.0787073573898093, 0.065058541245571]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7147421897113142, 0.1414919116533055, 0.0787073573898093, 0.065058541245571]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7147421897113142, 0.1414919116533055, 0.0787073573898093, 0.065058541245571]
siam score:  -0.84918195
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[2.146]
 [2.146]
 [2.146]
 [2.146]
 [2.146]] [[1.543]
 [1.543]
 [1.543]
 [1.543]
 [1.543]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7150117819456887, 0.14122531416475118, 0.07867986980948308, 0.06508303408007697]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7152793285302184, 0.14096073960208855, 0.078652590803282, 0.06510734106441102]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7158083771819888, 0.14043756784896944, 0.07859864908887858, 0.06515540588016316]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7158083771819888, 0.14043756784896944, 0.07859864908887858, 0.06515540588016316]
maxi score, test score, baseline:  0.1661 0.15 0.1661
probs:  [0.7158083771819888, 0.14043756784896944, 0.07859864908887858, 0.06515540588016316]
from probs:  [0.7158083771819888, 0.14043756784896944, 0.07859864908887858, 0.06515540588016316]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1681 0.15 0.1681
probs:  [0.7158083763496594, 0.140437568044741, 0.07859864939514698, 0.06515540621045261]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.6236348730456761
maxi score, test score, baseline:  0.1681 0.15 0.1681
probs:  [0.7160699237688332, 0.14017892600849333, 0.07857198206203833, 0.06517916816063508]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1681 0.15 0.1681
probs:  [0.7165871756288258, 0.1396674200079985, 0.0785192431449615, 0.06522616121821433]
maxi score, test score, baseline:  0.1681 0.15 0.1681
probs:  [0.7165871756288258, 0.1396674200079985, 0.0785192431449615, 0.06522616121821433]
maxi score, test score, baseline:  0.1681 0.15 0.1681
probs:  [0.7165871756288258, 0.1396674200079985, 0.0785192431449615, 0.06522616121821433]
maxi score, test score, baseline:  0.1681 0.15 0.1681
probs:  [0.7165871756288258, 0.1396674200079985, 0.0785192431449615, 0.06522616121821433]
maxi score, test score, baseline:  0.1681 0.15 0.1681
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.21081433551125323
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1681 0.15 0.1681
probs:  [0.7170967808148894, 0.1391634757388398, 0.07846728388153777, 0.065272459564733]
from probs:  [0.7170967808148894, 0.1391634757388398, 0.07846728388153777, 0.065272459564733]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.582708053870417
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1701 0.15 0.1701
maxi score, test score, baseline:  0.1701 0.15 0.1701
probs:  [0.7170967799897512, 0.1391634759346353, 0.07846728418455463, 0.06527245989105883]
maxi score, test score, baseline:  0.1701 0.15 0.1701
probs:  [0.7170967799897512, 0.1391634759346353, 0.07846728418455463, 0.06527245989105883]
maxi score, test score, baseline:  0.1701 0.15 0.1701
probs:  [0.7170967799897512, 0.1391634759346353, 0.07846728418455463, 0.06527245989105883]
maxi score, test score, baseline:  0.1701 0.15 0.1701
probs:  [0.7170967799897512, 0.1391634759346353, 0.07846728418455463, 0.06527245989105883]
maxi score, test score, baseline:  0.1701 0.15 0.1701
probs:  [0.7170967799897512, 0.1391634759346353, 0.07846728418455463, 0.06527245989105883]
using another actor
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7168429217575151, 0.1394145135694964, 0.07849316775149445, 0.06524939692149402]
start point for exploration sampling:  20032
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7157234101244074, 0.1409175182457559, 0.078211420949903, 0.06514765067993364]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7157234101244074, 0.1409175182457559, 0.078211420949903, 0.06514765067993364]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8403438
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7162305635593357, 0.14041312205628967, 0.07816258783974417, 0.06519372654463053]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7162305635593357, 0.14041312205628967, 0.07816258783974417, 0.06519372654463053]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7162305635593357, 0.14041312205628967, 0.07816258783974417, 0.06519372654463053]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7162305635593357, 0.14041312205628967, 0.07816258783974417, 0.06519372654463053]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7162305635593357, 0.14041312205628967, 0.07816258783974417, 0.06519372654463053]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7162305635593357, 0.14041312205628967, 0.07816258783974417, 0.06519372654463053]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7164813869063008, 0.1401636623627824, 0.07813843640294185, 0.06521651432797507]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7164813869063008, 0.1401636623627824, 0.07813843640294185, 0.06521651432797507]
first move QE:  0.21602294479642356
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7164813869063008, 0.1401636623627824, 0.07813843640294185, 0.06521651432797507]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7164813869063008, 0.1401636623627824, 0.07813843640294185, 0.06521651432797507]
siam score:  -0.823682
siam score:  -0.82395285
line 256 mcts: sample exp_bonus 7.77074124524097
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.001]
 [0.064]
 [0.064]
 [0.083]] [[7.206]
 [1.583]
 [7.206]
 [7.206]
 [3.303]] [[2.005]
 [0.002]
 [2.005]
 [2.005]
 [0.74 ]]
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.089]] [[2.901]
 [3.276]
 [3.276]
 [3.276]
 [4.233]] [[0.751]
 [0.874]
 [0.874]
 [0.874]
 [1.193]]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7162305635593357, 0.14041312205628967, 0.07816258783974417, 0.06519372654463053]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7162305635593357, 0.14041312205628967, 0.07816258783974417, 0.06519372654463053]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7162305635593357, 0.14041312205628967, 0.07816258783974417, 0.06519372654463053]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7162305635593357, 0.14041312205628967, 0.07816258783974417, 0.06519372654463053]
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.1690],
        [0.2093],
        [0.0000],
        [0.1736],
        [0.0000],
        [0.0000],
        [0.0544],
        [0.0000],
        [0.0782],
        [0.2218]], dtype=torch.float64)
0.0 0.1689703933239388
0.0 0.2092940598948917
0.96059601 0.96059601
0.0 0.1735883688097636
0.9801 0.9801
0.0 0.0
0.0 0.05436386956776453
0.96059601 0.96059601
0.0 0.07819108941854881
0.0 0.22176710996153456
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7159779113241408, 0.1406644006948944, 0.07818691537769114, 0.0651707726032738]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7159779113241408, 0.1406644006948944, 0.07818691537769114, 0.0651707726032738]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.864]
 [0.864]
 [1.148]
 [0.864]
 [0.864]] [[1.753]
 [1.753]
 [1.227]
 [1.753]
 [1.753]] [[1.932]
 [1.932]
 [2.324]
 [1.932]
 [1.932]]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7159779113241408, 0.1406644006948944, 0.07818691537769114, 0.0651707726032738]
2033 3729
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7159779113241408, 0.1406644006948944, 0.07818691537769114, 0.0651707726032738]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7159779113241408, 0.1406644006948944, 0.07818691537769114, 0.0651707726032738]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7159779113241408, 0.1406644006948944, 0.07818691537769114, 0.0651707726032738]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
using another actor
from probs:  [0.7159779113241408, 0.1406644006948944, 0.07818691537769114, 0.0651707726032738]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7159779113241408, 0.1406644006948944, 0.07818691537769114, 0.0651707726032738]
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]] [[1.552]
 [1.552]
 [1.552]
 [1.552]
 [1.552]] [[0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7159779113241408, 0.1406644006948944, 0.07818691537769114, 0.0651707726032738]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7159779113241408, 0.1406644006948944, 0.07818691537769114, 0.0651707726032738]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.7159779113241408, 0.1406644006948944, 0.07818691537769114, 0.0651707726032738]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.1509292780100688
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7164813869063008, 0.14016366236278238, 0.07813843640294185, 0.06521651432797507]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7164813869063008, 0.14016366236278238, 0.07813843640294185, 0.06521651432797507]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7164813869063008, 0.14016366236278238, 0.07813843640294185, 0.06521651432797507]
siam score:  -0.85211825
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7164813869063008, 0.14016366236278238, 0.07813843640294185, 0.06521651432797507]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7164813869063008, 0.14016366236278238, 0.07813843640294185, 0.06521651432797507]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2043 3732
Printing some Q and Qe and total Qs values:  [[0.06 ]
 [0.078]
 [0.063]
 [0.061]
 [0.064]] [[1.404]
 [1.276]
 [1.15 ]
 [1.497]
 [1.372]] [[0.324]
 [0.317]
 [0.245]
 [0.357]
 [0.32 ]]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7169776257979446, 0.13967012137740872, 0.07809065423921821, 0.06526159858542852]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7169776257979446, 0.13967012137740872, 0.07809065423921821, 0.06526159858542852]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7169776257979446, 0.13967012137740872, 0.07809065423921821, 0.06526159858542852]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7169776257979446, 0.13967012137740872, 0.07809065423921821, 0.06526159858542852]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7169776257979446, 0.13967012137740872, 0.07809065423921821, 0.06526159858542852]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7169776257979446, 0.13967012137740872, 0.07809065423921821, 0.06526159858542852]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7169776257979446, 0.13967012137740872, 0.07809065423921821, 0.06526159858542852]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7169776257979446, 0.13967012137740872, 0.07809065423921821, 0.06526159858542852]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7169776257979446, 0.13967012137740872, 0.07809065423921821, 0.06526159858542852]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.046]
 [0.039]
 [0.039]
 [0.041]] [[1.962]
 [1.874]
 [1.962]
 [1.962]
 [2.393]] [[0.754]
 [0.686]
 [0.754]
 [0.754]
 [1.135]]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7169776257979446, 0.13967012137740872, 0.07809065423921821, 0.06526159858542852]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7172230800679076, 0.13942600157079615, 0.07806701978349229, 0.06528389857780398]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7167304011517376, 0.1399160019352453, 0.07811445916204972, 0.06523913775096729]
siam score:  -0.83922714
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7167304011517376, 0.1399160019352453, 0.07811445916204972, 0.06523913775096729]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7167304011517376, 0.1399160019352453, 0.07811445916204972, 0.06523913775096729]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7167304011517376, 0.1399160019352453, 0.07811445916204972, 0.06523913775096729]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7172230800679076, 0.13942600157079615, 0.07806701978349229, 0.06528389857780398]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7174667829102833, 0.13918362366976933, 0.07804355397033172, 0.06530603944961556]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7174667829102833, 0.13918362366976933, 0.07804355397033172, 0.06530603944961556]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7174667829102833, 0.13918362366976933, 0.07804355397033172, 0.06530603944961556]
siam score:  -0.8406789
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7174667829102833, 0.13918362366976933, 0.07804355397033172, 0.06530603944961556]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7174667829102833, 0.13918362366976933, 0.07804355397033172, 0.06530603944961556]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7174667829102833, 0.13918362366976933, 0.07804355397033172, 0.06530603944961556]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7174667829102833, 0.13918362366976933, 0.07804355397033172, 0.06530603944961556]
siam score:  -0.839856
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7172230800679076, 0.13942600157079615, 0.07806701978349229, 0.06528389857780398]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7172230800679076, 0.13942600157079615, 0.07806701978349229, 0.06528389857780398]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7172230800679076, 0.13942600157079615, 0.07806701978349229, 0.06528389857780398]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7167304011517376, 0.1399160019352453, 0.07811445916204972, 0.06523913775096729]
Printing some Q and Qe and total Qs values:  [[0.318]
 [1.224]
 [0.318]
 [0.318]
 [0.318]] [[3.405]
 [0.632]
 [3.405]
 [3.405]
 [3.405]] [[1.554]
 [2.378]
 [1.554]
 [1.554]
 [1.554]]
first move QE:  0.23210126156580935
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7164813869063008, 0.14016366236278238, 0.07813843640294185, 0.06521651432797507]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7164813869063008, 0.1401636623627824, 0.07813843640294185, 0.06521651432797507]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7164813869063008, 0.1401636623627824, 0.07813843640294185, 0.06521651432797507]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7159779113241408, 0.1406644006948944, 0.07818691537769114, 0.0651707726032738]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.146]
 [0.248]
 [0.146]
 [0.146]] [[2.284]
 [2.284]
 [2.093]
 [2.284]
 [2.284]] [[0.582]
 [0.582]
 [0.72 ]
 [0.582]
 [0.582]]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7157234101244074, 0.1409175182457559, 0.078211420949903, 0.06514765067993364]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7157234101244074, 0.1409175182457559, 0.078211420949903, 0.06514765067993364]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7157234101244074, 0.1409175182457559, 0.078211420949903, 0.06514765067993364]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7157234101244074, 0.1409175182457559, 0.078211420949903, 0.06514765067993364]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7157234101244074, 0.1409175182457559, 0.078211420949903, 0.06514765067993364]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7157234101244074, 0.1409175182457559, 0.078211420949903, 0.06514765067993364]
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7157234101244074, 0.1409175182457559, 0.078211420949903, 0.06514765067993364]
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
maxi score, test score, baseline:  0.17209999999999998 0.15 0.17209999999999998
probs:  [0.7157234101244074, 0.1409175182457559, 0.078211420949903, 0.06514765067993364]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.828]
 [0.636]
 [0.636]
 [0.636]] [[2.092]
 [1.122]
 [2.092]
 [2.092]
 [2.092]] [[0.636]
 [0.828]
 [0.636]
 [0.636]
 [0.636]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17409999999999998 0.15 0.17409999999999998
using another actor
actor:  0 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17609999999999998 0.15 0.17609999999999998
probs:  [0.7154670379248638, 0.1411724953584121, 0.07823610713195166, 0.06512435958477239]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.17609999999999998 0.15 0.17609999999999998
maxi score, test score, baseline:  0.17609999999999998 0.15 0.17609999999999998
probs:  [0.7154670379248638, 0.1411724953584121, 0.07823610713195166, 0.06512435958477239]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.47 ]
 [0.685]
 [0.47 ]
 [0.47 ]] [[2.113]
 [2.113]
 [1.764]
 [2.113]
 [2.113]] [[0.47 ]
 [0.47 ]
 [0.685]
 [0.47 ]
 [0.47 ]]
maxi score, test score, baseline:  0.17609999999999998 0.15 0.17609999999999998
probs:  [0.7154670379248638, 0.1411724953584121, 0.07823610713195166, 0.06512435958477239]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.086]
 [0.086]
 [0.074]
 [0.086]] [[2.119]
 [2.103]
 [2.103]
 [2.51 ]
 [2.103]] [[1.695]
 [1.682]
 [1.682]
 [2.036]
 [1.682]]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17809999999999998 0.15 0.17809999999999998
probs:  [0.715467037083719, 0.14117249555507422, 0.07823610744234599, 0.06512435991886094]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.086]
 [0.086]
 [0.071]
 [0.086]] [[2.119]
 [2.103]
 [2.103]
 [2.862]
 [2.103]] [[1.16 ]
 [1.147]
 [1.147]
 [1.857]
 [1.147]]
maxi score, test score, baseline:  0.17809999999999998 0.15 0.17809999999999998
maxi score, test score, baseline:  0.17809999999999998 0.15 0.17809999999999998
probs:  [0.715467037083719, 0.14117249555507422, 0.07823610744234599, 0.06512435991886094]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
probs:  [0.715467036236411, 0.14117249575317714, 0.07823610775501452, 0.0651243602553973]
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.01 ]
 [0.05 ]
 [0.051]
 [0.058]] [[1.999]
 [0.603]
 [2.251]
 [2.17 ]
 [1.661]] [[1.76 ]
 [0.008]
 [2.032]
 [1.937]
 [1.346]]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
probs:  [0.715467036236411, 0.14117249575317714, 0.07823610775501452, 0.0651243602553973]
UNIT TEST: sample policy line 217 mcts : [0.792 0.    0.    0.167 0.042]
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
probs:  [0.715467036236411, 0.14117249575317714, 0.07823610775501452, 0.0651243602553973]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
probs:  [0.715467036236411, 0.14117249575317714, 0.07823610775501452, 0.0651243602553973]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
probs:  [0.715467036236411, 0.1411724957531771, 0.07823610775501452, 0.0651243602553973]
first move QE:  0.24029063037101114
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
siam score:  -0.83276886
siam score:  -0.83411986
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
probs:  [0.715467036236411, 0.1411724957531771, 0.07823610775501452, 0.0651243602553973]
maxi score, test score, baseline:  0.18009999999999998 0.15 0.18009999999999998
probs:  [0.715467036236411, 0.14117249575317714, 0.07823610775501452, 0.0651243602553973]
using another actor
Printing some Q and Qe and total Qs values:  [[0.874]
 [0.972]
 [0.806]
 [0.874]
 [0.846]] [[1.449]
 [0.225]
 [1.935]
 [1.449]
 [1.365]] [[0.874]
 [0.972]
 [0.806]
 [0.874]
 [0.846]]
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1861 0.15 0.1861
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7149486015490479, 0.14168810986641497, 0.07828602783699935, 0.06507726074753778]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7149486015490479, 0.14168810986641497, 0.07828602783699935, 0.06507726074753778]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7149486015490478, 0.14168810986641497, 0.07828602783699935, 0.06507726074753777]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.337]
 [1.472]
 [1.263]
 [1.26 ]
 [1.237]] [[0.399]
 [0.123]
 [0.925]
 [1.017]
 [0.736]] [[0.661]
 [2.644]
 [2.506]
 [2.529]
 [2.402]]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7133306901702848, 0.14368332605157771, 0.07805574954020593, 0.06493023423793157]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7133306901702848, 0.14368332605157771, 0.07805574954020593, 0.06493023423793157]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7133306901702848, 0.14368332605157771, 0.07805574954020593, 0.06493023423793157]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7133306901702848, 0.14368332605157771, 0.07805574954020593, 0.06493023423793157]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7133306901702848, 0.14368332605157771, 0.07805574954020593, 0.06493023423793157]
maxi score, test score, baseline:  0.1861 0.15 0.1861
maxi score, test score, baseline:  0.1861 0.15 0.1861
maxi score, test score, baseline:  0.1861 0.15 0.1861
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7133306901702848, 0.14368332605157771, 0.07805574954020593, 0.06493023423793157]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7133306901702848, 0.14368332605157771, 0.07805574954020593, 0.06493023423793157]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
siam score:  -0.8411731
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7138576879323216, 0.14315637525154387, 0.07800782357565597, 0.06497811324047839]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7138576879323216, 0.14315637525154387, 0.07800782357565597, 0.06497811324047839]
maxi score, test score, baseline:  0.1861 0.15 0.1861
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7138576879323216, 0.14315637525154387, 0.07800782357565597, 0.06497811324047839]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.183]
 [0.124]
 [0.132]
 [0.127]] [[2.101]
 [1.059]
 [1.306]
 [1.469]
 [1.51 ]] [[0.121]
 [0.183]
 [0.124]
 [0.132]
 [0.127]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7130642743802449, 0.14394971810068366, 0.07807997779054923, 0.06490602972852236]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.884]
 [0.696]
 [1.042]
 [0.576]
 [0.629]] [[1.804]
 [2.012]
 [1.335]
 [2.291]
 [1.986]] [[1.622]
 [1.315]
 [1.781]
 [1.168]
 [1.172]]
maxi score, test score, baseline:  0.1861 0.15 0.1861
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7130642743802449, 0.14394971810068366, 0.07807997779054923, 0.06490602972852236]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7130642743802449, 0.14394971810068366, 0.07807997779054923, 0.06490602972852236]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.9710092050941779
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
maxi score, test score, baseline:  0.1861 0.15 0.1861
siam score:  -0.84820044
maxi score, test score, baseline:  0.1861 0.15 0.1861
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.651]
 [0.835]
 [0.651]
 [0.651]] [[2.617]
 [2.617]
 [0.26 ]
 [2.617]
 [2.617]] [[0.651]
 [0.651]
 [0.835]
 [0.651]
 [0.651]]
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.492]
 [0.48 ]
 [0.424]
 [0.471]] [[1.738]
 [1.933]
 [1.937]
 [2.05 ]
 [1.931]] [[0.922]
 [1.099]
 [1.076]
 [1.003]
 [1.057]]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
siam score:  -0.85170275
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.602]
 [0.602]
 [0.068]
 [0.438]] [[2.029]
 [1.84 ]
 [1.84 ]
 [0.916]
 [1.993]] [[1.302]
 [1.655]
 [1.655]
 [0.279]
 [1.379]]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1861 0.15 0.1861
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
maxi score, test score, baseline:  0.1861 0.15 0.1861
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
maxi score, test score, baseline:  0.1861 0.15 0.1861
siam score:  -0.85287356
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
from probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
UNIT TEST: sample policy line 217 mcts : [0.125 0.125 0.083 0.375 0.292]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7135951542313393, 0.1434188855575341, 0.07803169878301514, 0.06495426142811135]
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7114624464591756, 0.14592549079856892, 0.07785159798756999, 0.06476046475468558]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7114624464591756, 0.14592549079856892, 0.07785159798756999, 0.06476046475468558]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.349]
 [0.352]
 [0.335]
 [0.329]] [[2.621]
 [2.529]
 [2.607]
 [2.621]
 [3.076]] [[0.969]
 [0.967]
 [0.999]
 [0.969]
 [1.109]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2120 3772
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7117330456922901, 0.14565354899609262, 0.0778283558435528, 0.06478504946806438]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7117330456922901, 0.14565354899609262, 0.0778283558435528, 0.06478504946806438]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7114624464591756, 0.14592549079856892, 0.07785159798756999, 0.06476046475468558]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7114624464591756, 0.14592549079856892, 0.07785159798756999, 0.06476046475468558]
maxi score, test score, baseline:  0.1861 0.15 0.1861
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7114624464591756, 0.14592549079856892, 0.07785159798756999, 0.06476046475468558]
siam score:  -0.85323817
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7114624464591756, 0.14592549079856892, 0.07785159798756999, 0.06476046475468558]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.1861 0.15 0.1861
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7114624464591756, 0.14592549079856892, 0.07785159798756999, 0.06476046475468558]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7114624464591756, 0.14592549079856892, 0.07785159798756999, 0.06476046475468558]
from probs:  [0.7114624464591756, 0.14592549079856892, 0.07785159798756999, 0.06476046475468558]
maxi score, test score, baseline:  0.1861 0.15 0.1861
probs:  [0.7111898554702483, 0.14619943423889004, 0.07787501120626064, 0.06473569908460115]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.711189854603878, 0.14619943443388503, 0.0778750115296068, 0.06473569943263022]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.711189854603878, 0.14619943443388503, 0.0778750115296068, 0.06473569943263022]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.711189854603878, 0.14619943443388503, 0.0778750115296068, 0.06473569943263022]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.711189854603878, 0.14619943443388503, 0.0778750115296068, 0.06473569943263022]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.711189854603878, 0.14619943443388503, 0.0778750115296068, 0.06473569943263022]
maxi score, test score, baseline:  0.1881 0.15 0.1881
Printing some Q and Qe and total Qs values:  [[1.207]
 [1.482]
 [1.207]
 [1.207]
 [1.207]] [[1.023]
 [0.457]
 [1.023]
 [1.023]
 [1.023]] [[2.312]
 [2.671]
 [2.312]
 [2.312]
 [2.312]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]] [[1.618]
 [1.454]
 [1.454]
 [1.454]
 [1.454]] [[1.857]
 [1.644]
 [1.644]
 [1.644]
 [1.644]]
siam score:  -0.84843564
siam score:  -0.8481996
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7109152497846352, 0.14647540169357978, 0.07789859771990013, 0.06471075080188482]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7109152497846352, 0.14647540169357978, 0.07789859771990013, 0.06471075080188482]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7109152497846352, 0.14647540169357978, 0.07789859771990013, 0.06471075080188482]
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.044]
 [0.154]
 [0.146]
 [0.146]] [[2.357]
 [0.994]
 [4.009]
 [2.357]
 [2.357]] [[0.895]
 [0.236]
 [1.463]
 [0.895]
 [0.895]]
maxi score, test score, baseline:  0.1881 0.15 0.1881
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.84605944
maxi score, test score, baseline:  0.1881 0.15 0.1881
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.711189854603878, 0.14619943443388503, 0.0778750115296068, 0.06473569943263022]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.711189854603878, 0.14619943443388503, 0.0778750115296068, 0.06473569943263022]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.711189854603878, 0.14619943443388503, 0.0778750115296068, 0.06473569943263022]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.711189854603878, 0.14619943443388503, 0.0778750115296068, 0.06473569943263022]
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.335]
 [0.276]
 [0.276]
 [0.276]] [[3.076]
 [2.685]
 [3.076]
 [3.076]
 [3.076]] [[1.314]
 [1.301]
 [1.314]
 [1.314]
 [1.314]]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7109152497846352, 0.14647540169357978, 0.07789859771990013, 0.06471075080188482]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.243]
 [0.107]
 [0.144]
 [0.144]] [[0.908]
 [1.205]
 [1.249]
 [0.908]
 [0.908]] [[0.889]
 [1.384]
 [1.156]
 [0.889]
 [0.889]]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.556]
 [0.46 ]
 [0.46 ]
 [0.46 ]] [[1.171]
 [1.481]
 [1.171]
 [1.171]
 [1.171]] [[1.101]
 [1.603]
 [1.101]
 [1.101]
 [1.101]]
Printing some Q and Qe and total Qs values:  [[0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]] [[1.728]
 [1.728]
 [1.728]
 [1.728]
 [1.728]] [[2.008]
 [2.008]
 [2.008]
 [2.008]
 [2.008]]
maxi score, test score, baseline:  0.1881 0.15 0.1881
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1881 0.15 0.1881
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7087616867127297, 0.1463660650159733, 0.08035718922996903, 0.06451505904132801]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7087616867127297, 0.1463660650159733, 0.08035718922996903, 0.06451505904132801]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7087616867127297, 0.1463660650159733, 0.08035718922996903, 0.06451505904132801]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7087616867127297, 0.1463660650159733, 0.08035718922996903, 0.06451505904132801]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7087616867127297, 0.1463660650159733, 0.08035718922996903, 0.06451505904132801]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7087616867127297, 0.1463660650159733, 0.08035718922996903, 0.06451505904132801]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7087616867127297, 0.1463660650159733, 0.08035718922996903, 0.06451505904132801]
maxi score, test score, baseline:  0.1881 0.15 0.1881
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7087616867127297, 0.1463660650159733, 0.08035718922996903, 0.06451505904132801]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7087616867127297, 0.1463660650159733, 0.08035718922996903, 0.06451505904132801]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7087616867127297, 0.1463660650159733, 0.08035718922996903, 0.06451505904132801]
maxi score, test score, baseline:  0.1881 0.15 0.1881
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7084767594852177, 0.14664396751489311, 0.08039010053726049, 0.06448917246262867]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.657]
 [1.048]
 [0.657]
 [0.657]] [[1.56 ]
 [1.56 ]
 [1.722]
 [1.56 ]
 [1.56 ]] [[1.604]
 [1.604]
 [2.442]
 [1.604]
 [1.604]]
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.214]
 [0.789]
 [0.261]
 [0.236]] [[1.759]
 [1.759]
 [1.119]
 [0.854]
 [0.791]] [[0.808]
 [0.808]
 [1.623]
 [0.627]
 [0.566]]
2146 3784
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5183896452103459
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7076091448648069, 0.14749019156029947, 0.0804903167631096, 0.06441034681178404]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7073155817669428, 0.14777651701680103, 0.0805242255804859, 0.06438367563577027]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7073155817669428, 0.14777651701680103, 0.0805242255804859, 0.06438367563577027]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7073155817669428, 0.14777651701680103, 0.0805242255804859, 0.06438367563577027]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7073155817669428, 0.14777651701680103, 0.0805242255804859, 0.06438367563577027]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7073155817669428, 0.14777651701680103, 0.0805242255804859, 0.06438367563577027]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7073155817669428, 0.14777651701680103, 0.0805242255804859, 0.06438367563577027]
siam score:  -0.83466536
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7073155817669428, 0.14777651701680103, 0.0805242255804859, 0.06438367563577027]
siam score:  -0.8373958
maxi score, test score, baseline:  0.1881 0.15 0.1881
first move QE:  0.25692729581723334
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.366]
 [0.25 ]
 [0.288]
 [0.299]] [[0.949]
 [0.835]
 [1.222]
 [0.949]
 [1.025]] [[0.288]
 [0.366]
 [0.25 ]
 [0.288]
 [0.299]]
from probs:  [0.7070197983535017, 0.14806500804818498, 0.08055839086155249, 0.0643568027367607]
maxi score, test score, baseline:  0.1881 0.15 0.1881
probs:  [0.7067217693393837, 0.1483556893161609, 0.0805928155269348, 0.06432972581752056]
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.84428316
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [0.7067217684478575, 0.14835568951457181, 0.08059281585761965, 0.06432972617995114]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [0.7070197974647675, 0.1480650082464108, 0.08055839119105374, 0.06435680309776803]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.499]
 [0.587]
 [0.499]
 [0.499]] [[1.668]
 [1.668]
 [2.264]
 [1.668]
 [1.668]] [[1.447]
 [1.447]
 [2.077]
 [1.447]
 [1.447]]
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [0.7070197974647675, 0.1480650082464108, 0.08055839119105374, 0.06435680309776803]
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [0.7070197974647675, 0.1480650082464108, 0.08055839119105374, 0.06435680309776803]
maxi score, test score, baseline:  0.1901 0.15 0.1901
siam score:  -0.847939
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [0.7070197974647675, 0.1480650082464108, 0.08055839119105374, 0.06435680309776803]
2158 3787
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
siam score:  -0.8466333
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [0.7070197974647675, 0.1480650082464108, 0.08055839119105374, 0.06435680309776803]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [0.7070197974647675, 0.1480650082464108, 0.08055839119105374, 0.06435680309776803]
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [0.7070197974647675, 0.1480650082464108, 0.08055839119105374, 0.06435680309776803]
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [0.7070197974647675, 0.1480650082464108, 0.08055839119105374, 0.06435680309776803]
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [0.7070197974647675, 0.1480650082464108, 0.08055839119105374, 0.06435680309776803]
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [0.7070197974647675, 0.1480650082464108, 0.08055839119105374, 0.06435680309776803]
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [0.7070197974647675, 0.1480650082464108, 0.08055839119105374, 0.06435680309776803]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.25837255358298583
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.416]
 [1.025]
 [0.339]
 [0.676]] [[1.636]
 [2.269]
 [1.286]
 [2.217]
 [1.61 ]] [[1.88 ]
 [2.234]
 [2.468]
 [2.029]
 [2.096]]
first move QE:  0.25852226088304925
maxi score, test score, baseline:  0.1901 0.15 0.1901
Starting evaluation
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [0.7073155808809836, 0.14777651721483895, 0.08052422590881193, 0.06438367599536547]
maxi score, test score, baseline:  0.1901 0.15 0.1901
maxi score, test score, baseline:  0.1901 0.15 0.1901
probs:  [0.7073155808809836, 0.14777651721483895, 0.08052422590881193, 0.06438367599536547]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1941 0.15 0.1941
probs:  [0.7073155790892363, 0.1477765176153472, 0.08052422657281248, 0.06438367672260414]
actor:  0 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.924]
 [0.62 ]
 [0.935]
 [0.638]] [[1.401]
 [0.207]
 [1.735]
 [1.183]
 [1.305]] [[0.709]
 [0.924]
 [0.62 ]
 [0.935]
 [0.638]]
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.915]
 [0.625]
 [0.904]
 [0.627]] [[1.548]
 [0.477]
 [1.707]
 [1.312]
 [1.489]] [[0.673]
 [0.915]
 [0.625]
 [0.904]
 [0.627]]
first move QE:  0.25860037858964674
maxi score, test score, baseline:  0.20809999999999998 0.15 0.20809999999999998
actor:  0 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.84705764
actor:  0 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2261 0.15 0.2261
probs:  [0.7073155637404381, 0.14777652104625502, 0.08052423226089647, 0.06438368295241043]
actor:  0 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.7070197792441599, 0.14806501231038682, 0.08055839794640453, 0.06435681049904879]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204098, 0.0643489757917569]
maxi score, test score, baseline:  0.2281 0.95 0.95
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204098, 0.0643489757917569]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204098, 0.0643489757917569]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204098, 0.0643489757917569]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204098, 0.0643489757917569]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204098, 0.0643489757917569]
line 256 mcts: sample exp_bonus 1.3641131696157296
Printing some Q and Qe and total Qs values:  [[0.283]
 [1.067]
 [0.344]
 [0.278]
 [0.271]] [[1.224]
 [0.279]
 [1.079]
 [1.209]
 [1.217]] [[0.883]
 [2.128]
 [0.957]
 [0.868]
 [0.857]]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204098, 0.0643489757917569]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.7069725944962626, 0.1480755365303757, 0.0805758918485053, 0.06437597712485642]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.7069725944962626, 0.1480755365303757, 0.0805758918485053, 0.06437597712485642]
maxi score, test score, baseline:  0.2281 0.95 0.95
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.7069725944962626, 0.1480755365303757, 0.0805758918485053, 0.06437597712485642]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.7069725944962626, 0.1480755365303757, 0.0805758918485053, 0.06437597712485642]
from probs:  [0.7069725944962626, 0.1480755365303757, 0.0805758918485053, 0.06437597712485642]
UNIT TEST: sample policy line 217 mcts : [0.417 0.042 0.042 0.25  0.25 ]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.7069725944962626, 0.1480755365303757, 0.0805758918485053, 0.06437597712485642]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.7069725944962626, 0.1480755365303757, 0.0805758918485053, 0.06437597712485642]
actor:  0 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.261]
 [0.261]
 [0.262]
 [0.261]
 [0.261]] [[0.113]
 [0.113]
 [0.114]
 [0.113]
 [0.113]]
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.211]
 [0.309]
 [0.211]
 [0.211]] [[1.865]
 [1.534]
 [1.312]
 [1.534]
 [1.534]] [[0.626]
 [0.56 ]
 [0.682]
 [0.56 ]
 [0.56 ]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.706373967765062, 0.14865913383458387, 0.08064512969672066, 0.0643217687036335]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.706373967765062, 0.14865913383458387, 0.08064512969672066, 0.0643217687036335]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204096, 0.0643489757917569]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204096, 0.0643489757917569]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204096, 0.0643489757917569]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204096, 0.0643489757917569]
maxi score, test score, baseline:  0.2301 0.95 0.95
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204096, 0.0643489757917569]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204096, 0.0643489757917569]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204096, 0.0643489757917569]
from probs:  [0.7066744172129776, 0.14836622762322457, 0.08061037937204096, 0.0643489757917569]
maxi score, test score, baseline:  0.2301 0.95 0.95
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.031]
 [0.174]
 [0.135]
 [0.141]] [[1.244]
 [0.533]
 [1.146]
 [1.005]
 [1.102]] [[0.54 ]
 [0.063]
 [0.552]
 [0.426]
 [0.471]]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7057661476918083, 0.1492516936681266, 0.08071543085732834, 0.06426672778273677]
from probs:  [0.7057661476918083, 0.1492516936681266, 0.08071543085732834, 0.06426672778273677]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.149]
 [0.098]
 [0.104]
 [0.102]] [[0.739]
 [0.879]
 [0.497]
 [0.713]
 [0.494]] [[0.28 ]
 [0.423]
 [0.195]
 [0.278]
 [0.202]]
maxi score, test score, baseline:  0.2301 0.95 0.95
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7057661476918083, 0.1492516936681266, 0.08071543085732834, 0.06426672778273677]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2301 0.95 0.95
siam score:  -0.85139656
siam score:  -0.8490973
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7057661476918083, 0.1492516936681266, 0.08071543085732834, 0.06426672778273677]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7057661476918083, 0.1492516936681266, 0.08071543085732834, 0.06426672778273677]
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.101]
 [0.094]
 [0.099]
 [0.1  ]] [[1.181]
 [1.043]
 [1.512]
 [1.383]
 [1.272]] [[0.337]
 [0.292]
 [0.433]
 [0.402]
 [0.367]]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7054587237160412, 0.14955139930129607, 0.08075098786382759, 0.06423888911883516]
using another actor
from probs:  [0.7054587237160412, 0.14955139930129607, 0.08075098786382759, 0.06423888911883516]
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.066]
 [0.066]
 [0.063]
 [0.073]] [[1.47 ]
 [1.47 ]
 [1.47 ]
 [2.163]
 [1.534]] [[1.357]
 [1.357]
 [1.357]
 [2.033]
 [1.433]]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]] [[1.755]
 [1.755]
 [1.755]
 [1.518]
 [1.665]] [[0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7054587237160412, 0.14955139930129607, 0.08075098786382759, 0.06423888911883516]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7054587237160412, 0.14955139930129607, 0.08075098786382759, 0.06423888911883516]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7054587237160412, 0.14955139930129607, 0.0807509878638276, 0.06423888911883516]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7054587237160412, 0.14955139930129607, 0.0807509878638276, 0.06423888911883516]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7054587237160412, 0.14955139930129607, 0.0807509878638276, 0.06423888911883516]
siam score:  -0.8367552
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7054587237160412, 0.14955139930129607, 0.0807509878638276, 0.06423888911883516]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7054587237160412, 0.14955139930129607, 0.0807509878638276, 0.06423888911883516]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.704836711401891, 0.1501577950581215, 0.0808229305151503, 0.06418256302483719]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.704836711401891, 0.1501577950581215, 0.0808229305151503, 0.06418256302483719]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7045220671925331, 0.15046453965002718, 0.08085932262207418, 0.06415407053536547]
2194 3811
maxi score, test score, baseline:  0.2301 0.95 0.95
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7045220671925331, 0.15046453965002718, 0.08085932262207418, 0.06415407053536547]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7045220671925331, 0.15046453965002718, 0.08085932262207418, 0.06415407053536547]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.318]
 [0.118]
 [0.257]
 [0.459]
 [0.245]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.8450686
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]] [[1.606]
 [1.606]
 [1.606]
 [1.606]
 [1.606]] [[1.325]
 [1.325]
 [1.325]
 [1.325]
 [1.325]]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.148]
 [0.317]
 [0.148]
 [0.148]] [[1.26 ]
 [1.26 ]
 [1.162]
 [1.26 ]
 [1.26 ]] [[0.544]
 [0.544]
 [0.849]
 [0.544]
 [0.544]]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7045220671925331, 0.15046453965002718, 0.08085932262207418, 0.06415407053536547]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2301 0.95 0.95
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7045220671925331, 0.15046453965002718, 0.08085932262207418, 0.06415407053536547]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7045220671925331, 0.15046453965002718, 0.08085932262207418, 0.06415407053536547]
maxi score, test score, baseline:  0.2301 0.95 0.95
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7042049596386485, 0.15077368574047986, 0.08089599964222625, 0.06412535497864538]
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
actor:  0 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
maxi score, test score, baseline:  0.2301 0.95 0.95
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
siam score:  -0.8441136
maxi score, test score, baseline:  0.2301 0.95 0.95
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
maxi score, test score, baseline:  0.2301 0.95 0.95
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
from probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7019151127129457, 0.1534608582749435, 0.08070672248214726, 0.06391730652996351]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2301 0.95 0.95
from probs:  [0.7019151127129457, 0.1534608582749435, 0.08070672248214726, 0.06391730652996351]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2321 0.95 0.95
probs:  [0.7022415124093397, 0.1531408779277377, 0.08067074342762777, 0.06394686623529469]
siam score:  -0.8331735
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]] [[1.292]
 [1.292]
 [1.292]
 [1.292]
 [1.292]] [[1.503]
 [1.503]
 [1.503]
 [1.503]
 [1.503]]
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [1.06 ]
 [1.139]
 [0.834]
 [1.095]] [[0.958]
 [0.82 ]
 [0.904]
 [1.2  ]
 [0.98 ]] [[0.511]
 [2.304]
 [2.49 ]
 [1.978]
 [2.427]]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2321 0.95 0.95
maxi score, test score, baseline:  0.2321 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  0.2321 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.2301 0.95 0.95
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
maxi score, test score, baseline:  0.2301 0.95 0.95
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
siam score:  -0.8354808
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7022415124093399, 0.1531408779277377, 0.08067074342762777, 0.0639468662352947]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2220 3828
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.7019151127129457, 0.1534608582749435, 0.08070672248214726, 0.06391730652996351]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2321 0.95 0.95
probs:  [0.7015861447134277, 0.15378335641409083, 0.0807429846408459, 0.06388751423163552]
using another actor
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2321 0.95 0.95
probs:  [0.7015861447134277, 0.15378335641409083, 0.0807429846408459, 0.06388751423163552]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4861763861300057
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2321 0.95 0.95
probs:  [0.6999558666543441, 0.15303276672100183, 0.08327216723970812, 0.0637391993849459]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6983472879521059, 0.1522921675405766, 0.08576768586607557, 0.0635928586412419]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6983472879521059, 0.1522921675405766, 0.08576768586607557, 0.0635928586412419]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6983472879521059, 0.1522921675405766, 0.08576768586607557, 0.0635928586412419]
maxi score, test score, baseline:  0.2301 0.95 0.95
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6983472879521059, 0.1522921675405766, 0.08576768586607557, 0.0635928586412419]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6983472879521059, 0.1522921675405766, 0.08576768586607557, 0.0635928586412419]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6983472879521059, 0.1522921675405766, 0.08576768586607557, 0.0635928586412419]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6983472879521059, 0.1522921675405766, 0.08576768586607557, 0.0635928586412419]
siam score:  -0.8306599
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6941432203322291, 0.154529961984661, 0.08811602249572958, 0.06321079518738028]
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.649]
 [0.81 ]
 [0.649]
 [0.649]] [[1.969]
 [1.525]
 [1.09 ]
 [1.525]
 [1.525]] [[1.488]
 [1.269]
 [1.301]
 [1.269]
 [1.269]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2301 0.95 0.95
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6918650728570404, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.2301 0.95 0.95
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6918650728570404, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6918650728570404, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6918650728570404, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6918650728570404, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6918650728570404, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6918650728570404, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.59 ]
 [0.972]
 [0.59 ]
 [0.572]] [[1.695]
 [1.695]
 [1.235]
 [1.695]
 [1.864]] [[1.227]
 [1.227]
 [1.837]
 [1.227]
 [1.247]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6922270693191276, 0.15686325373154103, 0.08787307131046024, 0.06303660563887117]
UNIT TEST: sample policy line 217 mcts : [0.25  0.167 0.042 0.292 0.25 ]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.6918650728570402, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
line 256 mcts: sample exp_bonus 1.0201955734618786
maxi score, test score, baseline:  0.2281 0.95 0.95
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6918650728570402, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6918650728570402, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
maxi score, test score, baseline:  0.2281 0.95 0.95
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6918650728570402, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6918650728570402, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.2281 0.95 0.95
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6918650728570402, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
siam score:  -0.83161145
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6918650728570402, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6918650728570402, 0.1571944831303395, 0.08793663472014511, 0.06300380929247512]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.328]
 [1.129]
 [1.221]
 [0.822]
 [1.129]] [[1.   ]
 [1.04 ]
 [1.347]
 [1.978]
 [1.04 ]] [[1.966]
 [1.582]
 [1.868]
 [1.282]
 [1.582]]
maxi score, test score, baseline:  0.2281 0.95 0.95
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.84180313
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.6903214698987438, 0.15918369683338415, 0.08763145797483828, 0.06286337529303396]
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.6903214698987438, 0.15918369683338415, 0.08763145797483828, 0.06286337529303396]
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.6903214698987438, 0.15918369683338415, 0.08763145797483828, 0.06286337529303396]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2251 3846
maxi score, test score, baseline:  0.2261 0.95 0.95
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.859]
 [0.57 ]
 [0.57 ]
 [0.57 ]] [[1.581]
 [0.774]
 [1.581]
 [1.581]
 [1.581]] [[0.57 ]
 [0.859]
 [0.57 ]
 [0.57 ]
 [0.57 ]]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6880516695995007, 0.16183865769066652, 0.08745252511716642, 0.0626571475926664]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6880516695995007, 0.16183865769066652, 0.08745252511716642, 0.0626571475926664]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6880516695995007, 0.16183865769066652, 0.08745252511716642, 0.0626571475926664]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6880516695995007, 0.16183865769066652, 0.08745252511716642, 0.0626571475926664]
maxi score, test score, baseline:  0.2281 0.95 0.95
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6880516695995007, 0.16183865769066652, 0.08745252511716642, 0.0626571475926664]
siam score:  -0.8366432
maxi score, test score, baseline:  0.2281 0.95 0.95
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6880516695995007, 0.16183865769066652, 0.08745252511716642, 0.0626571475926664]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6880516695995007, 0.16183865769066652, 0.08745252511716642, 0.0626571475926664]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6880516695995007, 0.16183865769066652, 0.08745252511716642, 0.0626571475926664]
first move QE:  0.2771526179121835
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6880516695995007, 0.16183865769066652, 0.08745252511716642, 0.0626571475926664]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6880516695995007, 0.16183865769066652, 0.08745252511716642, 0.0626571475926664]
maxi score, test score, baseline:  0.2281 0.95 0.95
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.6880516695995007, 0.16183865769066652, 0.08745252511716642, 0.0626571475926664]
actor:  0 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.2744265773688537
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.6880516695995007, 0.16183865769066652, 0.08745252511716642, 0.0626571475926664]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.6865415791211495, 0.16378645929582314, 0.08715220089211036, 0.06251976069091697]
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.6865415791211495, 0.16378645929582314, 0.08715220089211036, 0.06251976069091697]
maxi score, test score, baseline:  0.2261 0.95 0.95
maxi score, test score, baseline:  0.2261 0.95 0.95
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.6865415791211495, 0.16378645929582314, 0.08715220089211036, 0.06251976069091697]
maxi score, test score, baseline:  0.2261 0.95 0.95
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.505]
 [0.377]
 [0.377]
 [0.377]] [[0.962]
 [1.421]
 [0.962]
 [0.962]
 [0.962]] [[0.926]
 [1.335]
 [0.926]
 [0.926]
 [0.926]]
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.6865415791211495, 0.16378645929582314, 0.08715220089211036, 0.06251976069091697]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.6857768610650695, 0.16449947662647366, 0.0872731974503854, 0.06245046485807133]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.686160691324724, 0.16414159619592048, 0.08721246638746523, 0.062485246091890316]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.686160691324724, 0.16414159619592048, 0.08721246638746523, 0.062485246091890316]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.686160691324724, 0.16414159619592048, 0.08721246638746523, 0.062485246091890316]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.686160691324724, 0.16414159619592048, 0.08721246638746523, 0.062485246091890316]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.686160691324724, 0.16414159619592048, 0.08721246638746523, 0.062485246091890316]
maxi score, test score, baseline:  0.2281 0.95 0.95
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.686160691324724, 0.16414159619592048, 0.08721246638746523, 0.062485246091890316]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.686160691324724, 0.16414159619592048, 0.08721246638746523, 0.062485246091890316]
maxi score, test score, baseline:  0.2281 0.95 0.95
maxi score, test score, baseline:  0.2281 0.95 0.95
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.078]] [[1.672]
 [1.672]
 [1.672]
 [1.672]
 [1.655]] [[0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.078]]
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.03 ]
 [0.05 ]
 [0.305]
 [0.151]] [[1.355]
 [1.359]
 [0.994]
 [1.082]
 [1.686]] [[0.784]
 [0.147]
 [0.065]
 [0.605]
 [0.498]]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.686160691324724, 0.16414159619592048, 0.08721246638746523, 0.062485246091890316]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.686160691324724, 0.16414159619592048, 0.08721246638746523, 0.062485246091890316]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.686160691324724, 0.16414159619592048, 0.08721246638746523, 0.062485246091890316]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.686160691324724, 0.16414159619592048, 0.08721246638746523, 0.062485246091890316]
from probs:  [0.686160691324724, 0.16414159619592048, 0.08721246638746523, 0.062485246091890316]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2301 0.95 0.95
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.686160691324724, 0.16414159619592048, 0.08721246638746523, 0.062485246091890316]
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2321 0.95 0.95
probs:  [0.6853900541129616, 0.16486013250255283, 0.08733439949674723, 0.06241541388773831]
maxi score, test score, baseline:  0.2321 0.95 0.95
probs:  [0.6853900541129616, 0.16486013250255283, 0.08733439949674723, 0.06241541388773831]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.2321 0.95 0.95
maxi score, test score, baseline:  0.2321 0.95 0.95
probs:  [0.6853900541129616, 0.16486013250255283, 0.08733439949674723, 0.06241541388773831]
line 256 mcts: sample exp_bonus 0.6899542827839755
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2321 0.95 0.95
probs:  [0.6853900541129616, 0.16486013250255283, 0.08733439949674723, 0.06241541388773831]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.2321 0.95 0.95
probs:  [0.6853900541129616, 0.16486013250255283, 0.08733439949674723, 0.06241541388773831]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.6857768610650695, 0.16449947662647366, 0.0872731974503854, 0.06245046485807133]
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.6857768610650695, 0.16449947662647366, 0.0872731974503854, 0.06245046485807133]
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.6857768610650695, 0.16449947662647366, 0.0872731974503854, 0.06245046485807133]
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.6857768610650695, 0.16449947662647366, 0.0872731974503854, 0.06245046485807133]
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.6857768610650695, 0.16449947662647366, 0.0872731974503854, 0.06245046485807133]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.686160691324724, 0.16414159619592048, 0.08721246638746523, 0.062485246091890316]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2283 3856
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.6865415791211495, 0.16378645929582314, 0.08715220089211037, 0.06251976069091697]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.6865415791211495, 0.16378645929582314, 0.08715220089211037, 0.06251976069091697]
maxi score, test score, baseline:  0.2341 0.95 0.95
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.6865415791211495, 0.16378645929582314, 0.08715220089211037, 0.06251976069091697]
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.6865415791211495, 0.16378645929582314, 0.08715220089211037, 0.06251976069091697]
maxi score, test score, baseline:  0.2341 0.95 0.95
siam score:  -0.8287021
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.6865415791211495, 0.16378645929582314, 0.08715220089211037, 0.06251976069091697]
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.6865415791211495, 0.16378645929582314, 0.08715220089211037, 0.06251976069091697]
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.264]
 [0.518]
 [0.264]
 [0.264]] [[1.722]
 [1.722]
 [1.71 ]
 [1.722]
 [1.722]] [[0.745]
 [0.745]
 [1.25 ]
 [0.745]
 [0.745]]
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.6865415791211495, 0.16378645929582314, 0.08715220089211037, 0.06251976069091697]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.6869195581607686, 0.1634340344985685, 0.08709239563116432, 0.0625540117094987]
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.6869195581607686, 0.1634340344985685, 0.08709239563116432, 0.0625540117094987]
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[2.449]
 [2.439]
 [2.439]
 [2.439]
 [1.988]] [[1.969]
 [1.96 ]
 [1.96 ]
 [1.96 ]
 [1.462]]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.6869195581607686, 0.1634340344985685, 0.08709239563116432, 0.0625540117094987]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.84 ]
 [0.216]
 [0.294]
 [0.356]] [[2.752]
 [3.625]
 [2.485]
 [2.541]
 [2.567]] [[1.295]
 [2.413]
 [0.853]
 [1.02 ]
 [1.146]]
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.6876669222406988, 0.1627371978840029, 0.08697414488406738, 0.06262173499123097]
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.6876669222406988, 0.1627371978840029, 0.08697414488406738, 0.06262173499123097]
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.6876669222406988, 0.1627371978840029, 0.08697414488406738, 0.06262173499123097]
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.6869195581607686, 0.16343403449856847, 0.08709239563116432, 0.0625540117094987]
using another actor
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.106]
 [0.112]
 [0.112]
 [0.113]] [[1.474]
 [1.385]
 [1.242]
 [1.481]
 [1.435]] [[1.023]
 [0.889]
 [0.71 ]
 [1.029]
 [0.971]]
actor:  0 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]] [[1.854]
 [1.854]
 [1.854]
 [1.854]
 [1.854]] [[1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.321]
 [0.424]
 [0.321]
 [0.321]] [[2.467]
 [2.467]
 [1.805]
 [2.467]
 [2.467]] [[0.984]
 [0.984]
 [0.97 ]
 [0.984]
 [0.984]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6876669222406988, 0.1627371978840029, 0.08697414488406738, 0.06262173499123097]
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
siam score:  -0.8314305
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6872946616371443, 0.16308429085472909, 0.08703304535261704, 0.06258800215550958]
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[1.478]
 [1.779]
 [1.779]
 [1.779]
 [1.779]] [[0.424]
 [0.526]
 [0.526]
 [0.526]
 [0.526]]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.105]
 [0.105]
 [0.105]
 [0.105]] [[1.55 ]
 [1.693]
 [1.693]
 [1.693]
 [1.539]] [[0.688]
 [0.788]
 [0.788]
 [0.788]
 [0.685]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.6876669222406988, 0.1627371978840029, 0.08697414488406738, 0.06262173499123097]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.688403043132092, 0.16205084433337813, 0.08685767307641902, 0.06268843945811074]
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.688403043132092, 0.16205084433337813, 0.08685767307641902, 0.06268843945811074]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6887669663694752, 0.16171152505980074, 0.08680009177720742, 0.06272141679351671]
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6887669663694752, 0.16171152505980074, 0.08680009177720742, 0.06272141679351671]
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6887669663694752, 0.16171152505980074, 0.08680009177720742, 0.06272141679351671]
first move QE:  0.29156595932779306
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.6887669663694752, 0.16171152505980074, 0.08680009177720742, 0.06272141679351671]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.293]
 [0.295]
 [0.193]
 [0.177]] [[1.188]
 [1.044]
 [0.922]
 [1.179]
 [1.203]] [[0.717]
 [0.798]
 [0.721]
 [0.688]
 [0.671]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]] [[3.145]
 [3.147]
 [3.147]
 [3.147]
 [3.147]] [[1.637]
 [1.638]
 [1.638]
 [1.638]
 [1.638]]
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
probs:  [0.6876600471809449, 0.16326253225309567, 0.08645683562224903, 0.06262058494371044]
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
probs:  [0.6876600471809449, 0.16326253225309567, 0.08645683562224903, 0.06262058494371044]
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
probs:  [0.6876600471809449, 0.16326253225309567, 0.08645683562224903, 0.06262058494371044]
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.043]
 [0.043]
 [0.043]
 [0.043]] [[3.638]
 [3.14 ]
 [3.14 ]
 [3.14 ]
 [3.14 ]] [[1.028]
 [0.876]
 [0.876]
 [0.876]
 [0.876]]
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
probs:  [0.6876600471809449, 0.16326253225309567, 0.08645683562224903, 0.06262058494371044]
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
probs:  [0.6876600471809449, 0.16326253225309567, 0.08645683562224903, 0.06262058494371044]
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.184]
 [0.342]
 [0.187]
 [0.29 ]] [[1.969]
 [2.261]
 [1.559]
 [1.969]
 [1.923]] [[0.55 ]
 [0.643]
 [0.725]
 [0.55 ]
 [0.742]]
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.534]] [[1.06 ]
 [1.06 ]
 [1.06 ]
 [1.06 ]
 [1.393]]
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.108]
 [0.108]
 [0.106]
 [0.107]] [[1.466]
 [1.327]
 [1.327]
 [1.263]
 [1.11 ]] [[1.308]
 [1.117]
 [1.117]
 [1.027]
 [0.826]]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.219]
 [0.244]
 [0.219]
 [0.219]] [[2.048]
 [2.048]
 [2.001]
 [2.048]
 [2.048]] [[0.653]
 [0.653]
 [0.686]
 [0.653]
 [0.653]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
probs:  [0.6883807312069663, 0.16258675074883575, 0.08634662358240683, 0.06268589446179096]
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
probs:  [0.6883807312069663, 0.16258675074883575, 0.08634662358240683, 0.06268589446179096]
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
probs:  [0.6883807312069663, 0.16258675074883575, 0.08634662358240683, 0.06268589446179096]
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
probs:  [0.6883807312069663, 0.16258675074883575, 0.08634662358240683, 0.06268589446179096]
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
probs:  [0.6883807312069663, 0.16258675074883575, 0.08634662358240683, 0.06268589446179096]
Printing some Q and Qe and total Qs values:  [[1.033]
 [1.342]
 [0.84 ]
 [1.033]
 [0.879]] [[1.393]
 [0.012]
 [1.801]
 [1.393]
 [1.457]] [[2.433]
 [2.589]
 [2.184]
 [2.433]
 [2.146]]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6887371076773493, 0.16225257846453017, 0.08629212400098907, 0.06271818985713148]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.6  ]
 [1.169]
 [0.6  ]
 [0.6  ]] [[1.681]
 [1.681]
 [0.354]
 [1.681]
 [1.681]] [[1.365]
 [1.365]
 [2.06 ]
 [1.365]
 [1.365]]
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
probs:  [0.6887371076773493, 0.16225257846453017, 0.08629212400098907, 0.06271818985713148]
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
probs:  [0.6887371076773493, 0.16225257846453017, 0.08629212400098907, 0.06271818985713148]
maxi score, test score, baseline:  0.24009999999999998 0.95 0.95
probs:  [0.6887371076773493, 0.16225257846453017, 0.08629212400098907, 0.06271818985713148]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.159]
 [0.159]
 [0.154]
 [0.151]] [[1.48 ]
 [1.168]
 [1.168]
 [1.458]
 [1.452]] [[0.617]
 [0.523]
 [0.523]
 [0.609]
 [0.601]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6883807312069663, 0.16258675074883575, 0.08634662358240683, 0.06268589446179096]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6887371076773493, 0.16225257846453017, 0.08629212400098907, 0.06271818985713148]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6887371076773493, 0.16225257846453017, 0.08629212400098907, 0.06271818985713148]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6887371076773493, 0.16225257846453017, 0.08629212400098907, 0.06271818985713148]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6887371076773493, 0.16225257846453017, 0.08629212400098907, 0.06271818985713148]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6887371076773493, 0.16225257846453017, 0.08629212400098907, 0.06271818985713148]
siam score:  -0.81982666
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6894420739419275, 0.1615915354199672, 0.08618431563111577, 0.06278207500698947]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.546]
 [0.63 ]
 [0.578]
 [0.634]] [[2.463]
 [2.463]
 [4.622]
 [2.129]
 [2.029]] [[1.102]
 [1.102]
 [2.2  ]
 [0.996]
 [1.026]]
using explorer policy with actor:  1
siam score:  -0.82073474
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6876534575115909, 0.16376605948290135, 0.08596100036972962, 0.06261948263577813]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6876534575115909, 0.16376605948290135, 0.08596100036972962, 0.06261948263577813]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6876534575115909, 0.16376605948290135, 0.08596100036972962, 0.06261948263577813]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6876534575115909, 0.16376605948290135, 0.08596100036972962, 0.06261948263577813]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6876534575115909, 0.16376605948290135, 0.08596100036972962, 0.06261948263577813]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6876534575115909, 0.16376605948290135, 0.08596100036972962, 0.06261948263577813]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6876534575115909, 0.16376605948290135, 0.08596100036972962, 0.06261948263577813]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6876534575115909, 0.16376605948290135, 0.08596100036972962, 0.06261948263577813]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6876534575115909, 0.16376605948290135, 0.08596100036972962, 0.06261948263577813]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6876534575115909, 0.16376605948290135, 0.08596100036972962, 0.06261948263577813]
using explorer policy with actor:  1
siam score:  -0.8251166
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.438]
 [0.457]
 [0.457]
 [0.425]] [[1.407]
 [2.21 ]
 [1.407]
 [1.407]
 [2.835]] [[0.457]
 [0.438]
 [0.457]
 [0.457]
 [0.425]]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6872966570544696, 0.1641024423643006, 0.08601375360457386, 0.06258714697665585]
maxi score, test score, baseline:  0.24209999999999998 0.95 0.95
probs:  [0.6872966570544696, 0.1641024423643006, 0.08601375360457386, 0.06258714697665585]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.6872966570544696, 0.1641024423643006, 0.08601375360457386, 0.06258714697665585]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.518]
 [0.563]
 [0.518]
 [0.518]] [[1.149]
 [1.149]
 [1.489]
 [1.149]
 [1.149]] [[1.297]
 [1.297]
 [1.5  ]
 [1.297]
 [1.297]]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.6869372457273416, 0.1644412867110886, 0.08606689285865063, 0.06255457470291925]
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.6869372457273416, 0.1644412867110886, 0.08606689285865063, 0.06255457470291925]
siam score:  -0.8270879
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.6869372457273416, 0.1644412867110886, 0.08606689285865063, 0.06255457470291925]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.6872966570544696, 0.1641024423643006, 0.08601375360457386, 0.06258714697665585]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.699]
 [0.572]
 [0.699]
 [0.701]] [[2.562]
 [2.562]
 [2.257]
 [2.562]
 [2.135]] [[2.082]
 [2.082]
 [1.625]
 [2.082]
 [1.802]]
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]] [[2.114]
 [2.114]
 [2.114]
 [2.114]
 [2.114]] [[1.676]
 [1.676]
 [1.676]
 [1.676]
 [1.676]]
using another actor
siam score:  -0.82873565
siam score:  -0.8288999
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.6869372457273416, 0.1644412867110886, 0.08606689285865063, 0.06255457470291925]
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.82673436
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.6865751947676004, 0.16478261963995786, 0.08612042238453439, 0.06252176320790734]
first move QE:  0.2987162227626283
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.6865751947676004, 0.16478261963995786, 0.08612042238453439, 0.06252176320790734]
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.6865751947676004, 0.16478261963995786, 0.08612042238453439, 0.06252176320790734]
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.6865751947676004, 0.16478261963995786, 0.08612042238453439, 0.06252176320790734]
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.6865751947676004, 0.16478261963995786, 0.08612042238453439, 0.06252176320790734]
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.6865751947676004, 0.16478261963995786, 0.08612042238453439, 0.06252176320790734]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.6862104749885993, 0.1651264686673766, 0.08617434649749439, 0.06248870984652971]
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.685843056771807, 0.16547286171698283, 0.08622866957665427, 0.0624554119345557]
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.685843056771807, 0.16547286171698283, 0.08622866957665427, 0.0624554119345557]
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.685843056771807, 0.16547286171698283, 0.08622866957665427, 0.0624554119345557]
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.685843056771807, 0.16547286171698283, 0.08622866957665427, 0.0624554119345557]
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.685843056771807, 0.16547286171698283, 0.08622866957665427, 0.0624554119345557]
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.685843056771807, 0.16547286171698283, 0.08622866957665427, 0.0624554119345557]
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.685843056771807, 0.16547286171698283, 0.08622866957665427, 0.0624554119345557]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.493]
 [0.033]
 [0.229]
 [0.345]] [[0.834]
 [0.451]
 [0.194]
 [1.047]
 [0.701]] [[0.844]
 [1.105]
 [0.1  ]
 [0.776]
 [0.894]]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.448]
 [0.58 ]
 [0.448]
 [0.429]] [[1.152]
 [1.152]
 [1.027]
 [1.152]
 [1.311]] [[1.25 ]
 [1.25 ]
 [1.473]
 [1.25 ]
 [1.265]]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.592]
 [0.569]
 [0.533]
 [0.533]] [[1.197]
 [1.132]
 [1.117]
 [1.197]
 [1.197]] [[1.283]
 [1.38 ]
 [1.328]
 [1.283]
 [1.283]]
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
siam score:  -0.8294705
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.9256119785944841
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.367]
 [0.194]
 [0.206]
 [0.249]] [[1.26 ]
 [0.232]
 [1.174]
 [1.315]
 [0.986]] [[1.086]
 [0.618]
 [0.899]
 [1.017]
 [0.883]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
Starting evaluation
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
maxi score, test score, baseline:  0.24409999999999998 0.95 0.95
probs:  [0.6858430567718071, 0.1654728617169828, 0.08622866957665427, 0.062455411934555706]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2601 0.95 0.95
probs:  [0.6858430567718071, 0.1654728617169828, 0.08622866957665427, 0.062455411934555706]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.648]
 [0.66 ]
 [0.648]
 [0.648]] [[2.198]
 [0.767]
 [1.542]
 [0.767]
 [0.767]] [[0.507]
 [0.648]
 [0.66 ]
 [0.648]
 [0.648]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2661 0.95 0.95
probs:  [0.6858430567718071, 0.1654728617169828, 0.08622866957665427, 0.062455411934555706]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.6858430567718071, 0.1654728617169828, 0.08622866957665427, 0.062455411934555706]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.123]
 [1.476]
 [1.123]
 [1.123]
 [1.123]] [[1.768]
 [1.046]
 [1.768]
 [1.768]
 [1.768]] [[1.815]
 [2.281]
 [1.815]
 [1.815]
 [1.815]]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.197]
 [0.187]
 [0.197]
 [0.206]] [[1.682]
 [1.682]
 [1.701]
 [1.682]
 [1.954]] [[0.732]
 [0.732]
 [0.731]
 [0.732]
 [1.021]]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.194]
 [0.194]
 [0.209]
 [0.207]] [[1.71 ]
 [1.704]
 [1.704]
 [1.917]
 [1.834]] [[0.783]
 [0.748]
 [0.748]
 [0.992]
 [0.903]]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8306151
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]] [[2.323]
 [2.323]
 [2.323]
 [2.323]
 [2.323]] [[1.822]
 [1.822]
 [1.822]
 [1.822]
 [1.822]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.047952736753414
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.3015327958032978
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.197]
 [0.197]
 [0.19 ]
 [0.197]] [[2.178]
 [2.178]
 [2.178]
 [2.35 ]
 [2.178]] [[1.762]
 [1.762]
 [1.762]
 [1.893]
 [1.762]]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3861291667121112
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20032
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.544]
 [0.766]
 [0.544]
 [0.501]] [[1.777]
 [1.645]
 [1.055]
 [1.645]
 [1.879]] [[0.464]
 [0.544]
 [0.766]
 [0.544]
 [0.501]]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.17 ]
 [0.179]
 [0.179]
 [0.179]
 [0.179]] [[2.075]
 [2.036]
 [2.036]
 [2.036]
 [2.036]] [[1.275]
 [1.268]
 [1.268]
 [1.268]
 [1.268]]
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[1.951]
 [1.806]
 [1.806]
 [1.806]
 [1.806]] [[1.592]
 [1.528]
 [1.528]
 [1.528]
 [1.528]]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.536]
 [0.607]
 [0.536]
 [0.536]] [[2.937]
 [2.937]
 [3.103]
 [2.937]
 [2.937]] [[1.754]
 [1.754]
 [1.951]
 [1.754]
 [1.754]]
Sims:  25 1 epoch:  141114 pick best:  False frame count:  141114
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
siam score:  -0.8314862
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.176]
 [0.176]
 [0.176]
 [0.175]] [[2.543]
 [2.569]
 [2.569]
 [2.569]
 [1.932]] [[1.715]
 [1.752]
 [1.752]
 [1.752]
 [1.237]]
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.264]
 [0.264]
 [0.264]
 [0.264]] [[3.588]
 [2.755]
 [2.755]
 [2.755]
 [2.755]] [[1.545]
 [1.183]
 [1.183]
 [1.183]
 [1.183]]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.368]
 [0.424]
 [0.368]
 [0.368]] [[3.45 ]
 [3.007]
 [2.131]
 [3.007]
 [3.007]] [[1.728]
 [1.563]
 [1.139]
 [1.563]
 [1.563]]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.83005404
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.782]
 [0.776]
 [0.661]
 [0.661]] [[1.287]
 [1.238]
 [1.004]
 [1.287]
 [1.287]] [[1.383]
 [1.608]
 [1.517]
 [1.383]
 [1.383]]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84955704
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20032
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8515207
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[2.008]
 [2.06 ]
 [2.06 ]
 [2.06 ]
 [2.06 ]] [[1.974]
 [2.038]
 [2.038]
 [2.038]
 [2.038]]
in main func line 156:  2410
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.318]] [[1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.437]] [[1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.003]]
using explorer policy with actor:  1
2412 3899
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2941 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.343]
 [0.346]
 [0.346]
 [0.298]] [[0.813]
 [1.031]
 [0.868]
 [0.868]
 [1.386]] [[0.533]
 [0.773]
 [0.672]
 [0.672]
 [0.919]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2961 1.0 1.0
maxi score, test score, baseline:  0.2961 1.0 1.0
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2961 1.0 1.0
maxi score, test score, baseline:  0.2961 1.0 1.0
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.3639052551018875
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
2422 3903
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2324117512274821
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.583]
 [0.423]
 [0.325]
 [0.32 ]] [[2.222]
 [1.365]
 [1.833]
 [2.222]
 [1.412]] [[0.825]
 [1.054]
 [0.892]
 [0.825]
 [0.543]]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.262]
 [0.232]
 [0.231]
 [0.228]] [[1.345]
 [0.984]
 [1.07 ]
 [1.354]
 [1.488]] [[0.964]
 [0.785]
 [0.782]
 [0.969]
 [1.053]]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.138]
 [0.138]
 [0.138]
 [0.138]] [[1.872]
 [1.569]
 [1.569]
 [1.569]
 [1.569]] [[1.173]
 [0.833]
 [0.833]
 [0.833]
 [0.833]]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.83366305
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.2981 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.097]
 [0.238]
 [0.217]
 [0.19 ]] [[2.022]
 [0.774]
 [1.817]
 [1.82 ]
 [2.256]] [[1.392]
 [0.062]
 [1.27 ]
 [1.234]
 [1.581]]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.83367467
maxi score, test score, baseline:  0.2981 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.192]
 [0.29 ]
 [0.192]
 [0.2  ]] [[1.907]
 [1.667]
 [1.511]
 [1.667]
 [1.847]] [[1.057]
 [0.77 ]
 [0.81 ]
 [0.77 ]
 [0.966]]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8345864
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8355739
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.154]
 [0.172]
 [0.163]
 [0.161]] [[1.415]
 [1.272]
 [1.359]
 [1.573]
 [1.153]] [[0.687]
 [0.58 ]
 [0.674]
 [0.799]
 [0.515]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.532]
 [0.971]
 [0.532]
 [0.532]] [[1.806]
 [1.806]
 [1.814]
 [1.806]
 [1.806]] [[1.03]
 [1.03]
 [1.91]
 [1.03]
 [1.03]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.519]
 [1.107]
 [0.519]
 [0.519]] [[1.757]
 [1.757]
 [1.212]
 [1.757]
 [1.757]] [[1.096]
 [1.096]
 [2.089]
 [1.096]
 [1.096]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2981 1.0 1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20032
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.208]] [[2.028]
 [2.028]
 [2.028]
 [2.028]
 [2.229]] [[1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.233]]
siam score:  -0.8258043
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
using explorer policy with actor:  1
