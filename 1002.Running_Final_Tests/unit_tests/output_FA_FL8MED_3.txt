dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:255000
exp_gamma:0.95
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[ 0.002]
 [ 0.   ]
 [ 0.   ]
 [ 0.002]
 [-0.   ]] [[0.   ]
 [0.001]
 [0.001]
 [0.   ]
 [0.001]] [[ 0.002]
 [ 0.   ]
 [ 0.   ]
 [ 0.002]
 [-0.   ]]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
11 63
from probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
siam score:  -0.0031914386940612035
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.03584262791320614
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.03584262791320614
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.04998439759321988
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  1115 train batches done:  31 episodes:  102
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.43157402
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
19 123
from probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  1798 train batches done:  144 episodes:  168
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.1794692
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.32487786
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
28 189
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.384]
 [ 0.   ]
 [ 0.   ]
 [-0.105]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.377432530727138
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.464823
35 239
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.42917228
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.41120598
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
43 323
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.2849503245091903
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
49 360
49 366
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.37251267
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.37307724
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.3545256
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
59 427
UNIT TEST: sample policy line 217 mcts : [0.  0.2 0.6 0.  0.2]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.04761634043012874
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.4750997
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
66 479
66 483
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.44757167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.43537512
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.027]
 [-0.054]
 [-0.107]
 [-0.059]] [[0.143]
 [0.106]
 [0.07 ]
 [0.   ]
 [0.064]]
line 256 mcts: sample exp_bonus -0.04260804245790597
71 514
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.039]
 [-0.247]
 [-0.247]
 [-0.247]
 [-0.066]] [[0.281]
 [0.073]
 [0.073]
 [0.073]
 [0.254]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.011]
 [-0.051]
 [ 0.007]
 [-0.009]
 [ 0.011]] [[0.027]
 [0.   ]
 [0.038]
 [0.028]
 [0.041]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
72 538
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.03586932181221494
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.018]
 [-0.054]
 [ 0.   ]
 [-0.061]
 [ 0.   ]] [[0.091]
 [0.054]
 [0.109]
 [0.048]
 [0.109]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.057]
 [-0.12 ]
 [-0.081]
 [-0.228]
 [-0.209]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6443128
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
76 626
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
77 641
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.597508
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
79 651
siam score:  -0.59773856
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.208]
 [0.208]
 [0.208]
 [0.208]
 [0.208]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.273]
 [0.273]
 [0.273]
 [0.36 ]
 [0.273]] [[0.181]
 [0.181]
 [0.181]
 [0.239]
 [0.181]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
82 673
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
83 680
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.551]
 [0.   ]
 [0.   ]
 [0.029]
 [0.   ]] [[ 0.728]
 [-0.007]
 [-0.007]
 [ 0.031]
 [-0.007]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
83 688
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.04416962708265595
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.64343226
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6691834
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.265]
 [0.023]
 [0.023]
 [0.023]
 [0.023]] [[0.451]
 [0.128]
 [0.128]
 [0.128]
 [0.128]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
91 774
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.66847444
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.002]
 [ 0.   ]
 [-0.   ]
 [-0.001]] [[0.   ]
 [0.002]
 [0.004]
 [0.004]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
in main func line 156:  95
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]] [[1.589]
 [1.589]
 [1.589]
 [1.589]
 [1.589]]
97 834
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
98 853
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
98 860
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
rdn probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
107 900
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.62275857
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
108 914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
109 936
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
112 952
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.002]] [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
113 967
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
114 996
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[0.   ]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.001]
 [ 0.002]
 [ 0.002]
 [-0.001]
 [ 0.001]] [[0.001]
 [0.003]
 [0.002]
 [0.   ]
 [0.001]]
siam score:  -0.6855398
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6814917
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.01 ]
 [ 0.001]
 [-0.001]
 [ 0.002]
 [ 0.013]] [[0.009]
 [0.003]
 [0.001]
 [0.003]
 [0.011]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.01 ]
 [-0.001]
 [ 0.   ]
 [ 0.015]
 [ 0.01 ]] [[0.016]
 [0.001]
 [0.003]
 [0.022]
 [0.016]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
121 1045
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.003]
 [-0.002]
 [ 0.004]
 [-0.003]
 [-0.001]] [[0.009]
 [0.002]
 [0.01 ]
 [0.   ]
 [0.003]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.003]
 [-0.002]
 [-0.002]
 [-0.003]] [[0.001]
 [0.001]
 [0.002]
 [0.002]
 [0.001]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.00029928352591857525
129 1105
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.639123
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.64834267
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.68455625
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
137 1170
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.673293
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.008]
 [-0.007]
 [-0.008]
 [-0.008]
 [-0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6846704
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
141 1234
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.001789701663648332
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.125 0.125 0.125 0.083 0.542]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6491892
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.0017423777898782561
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.   ]
 [-0.001]
 [ 0.   ]
 [ 0.   ]
 [-0.001]] [[0.   ]
 [0.   ]
 [0.001]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
147 1290
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.004]
 [-0.008]
 [-0.008]
 [-0.008]] [[0.002]
 [0.004]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6882431
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.68905044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.0004999818753605259
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
160 1357
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.375 0.083 0.167 0.083 0.292]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [ 0.009]
 [ 0.009]
 [ 0.009]
 [ 0.009]] [[0.   ]
 [0.007]
 [0.007]
 [0.007]
 [0.007]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.010299878279565404
164 1418
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6947716
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6790152
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]]
167 1437
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.003]
 [0.002]
 [0.001]] [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.68396276
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
170 1465
170 1467
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.006]
 [-0.002]
 [-0.007]
 [-0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.012899870479724258
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.0013264394897501538
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.001346706875360797
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.001]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
siam score:  -0.70338273
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69985825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
185 1589
185 1595
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.0008550124410898508
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6740333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6805847
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69001544
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
196 1684
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.68451154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
197 1711
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.67832685
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -1.5291826859580835e-06
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
STARTED EXPV TRAINING ON FRAME NO.  20019
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.0008000005351411274
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
200 1729
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
202 1752
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6507745
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6542238
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
206 1791
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.011309602526930635
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.582]
 [-0.094]
 [-0.344]
 [-0.094]
 [-0.094]] [[0.   ]
 [0.488]
 [0.239]
 [0.488]
 [0.488]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.285]
 [-0.159]
 [-0.249]
 [-0.159]
 [-0.057]] [[0.226]
 [0.352]
 [0.262]
 [0.352]
 [0.454]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
210 1830
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.011]
 [-0.098]
 [-0.02 ]
 [ 0.004]
 [ 0.139]] [[0.087]
 [0.   ]
 [0.079]
 [0.102]
 [0.237]]
214 1876
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7070206
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.410695179062484
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6733515
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.019]
 [3.94 ]
 [3.019]
 [3.019]
 [3.268]] [[1.026]
 [1.801]
 [1.026]
 [1.026]
 [1.236]]
using explorer policy with actor:  1
siam score:  -0.69503385
first move QE:  -0.0092567903809275
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7002279
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  -0.005365055316619744
Sims:  25 1 epoch:  23442 pick best:  False frame count:  23442
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7060466
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.083 0.792 0.042 0.042 0.042]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 6.4441592382104886e-21
0.0 4.833119428657867e-21
0.0 4.833119428657867e-21
0.0 1.494582473021776e-11
0.0 1.1019086058093363e-11
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.38 ]
 [0.648]
 [1.324]
 [1.4  ]
 [1.77 ]] [[0.155]
 [0.334]
 [0.784]
 [0.835]
 [1.081]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.106]
 [5.104]
 [5.104]
 [5.602]
 [5.104]] [[1.149]
 [0.815]
 [0.815]
 [0.981]
 [0.815]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.709]
 [ 0.   ]
 [ 0.   ]
 [-0.172]] [[0.14 ]
 [0.717]
 [0.14 ]
 [0.14 ]
 [0.   ]]
254 2025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.66468096
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
262 2032
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.35 ]
 [2.959]
 [3.35 ]
 [3.35 ]
 [2.904]] [[2.   ]
 [1.643]
 [2.   ]
 [2.   ]
 [1.593]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
siam score:  -0.6866909
siam score:  -0.6893126
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.6994668847767969
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.675]
 [2.675]
 [2.675]
 [2.784]
 [3.188]] [[1.149]
 [1.149]
 [1.149]
 [1.257]
 [1.654]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
line 256 mcts: sample exp_bonus 0.8581427103443946
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.657]
 [2.657]
 [2.657]
 [2.657]
 [4.072]] [[0.798]
 [0.798]
 [0.798]
 [0.798]
 [1.735]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.256]
 [3.256]
 [3.256]
 [3.256]
 [3.256]] [[0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
282 2037
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.0072049639594540795
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.671]
 [4.671]
 [4.671]
 [4.671]
 [4.671]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.006025655234677252
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.005582288613426845
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  303
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.0052412530972716495
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.218]
 [1.218]
 [2.121]
 [1.218]
 [1.218]] [[0.14 ]
 [0.14 ]
 [0.742]
 [0.14 ]
 [0.14 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
309 2075
310 2075
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70226014
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1145332576722993
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.993]
 [2.862]
 [3.563]
 [2.727]
 [2.862]] [[0.233]
 [0.523]
 [0.757]
 [0.478]
 [0.523]]
siam score:  -0.6976075
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[     0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 6.4441592382104886e-21
0.0 -1.2888318476420977e-20
0.0 4.833119428657867e-21
0.0 0.0
0.0 0.0
0.0 0.0
0.0 3.2220796191052443e-21
0.0 6.444159238210488e-20
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.274]
 [8.067]
 [7.721]
 [7.274]
 [8.685]] [[1.092]
 [1.408]
 [1.27 ]
 [1.092]
 [1.655]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.0035036208715469113
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  329
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71500045
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.25, 0.25, 0.25, 0.25]
334 2104
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7223227
siam score:  -0.7215323
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  345
siam score:  -0.7109106
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.692]
 [2.69 ]
 [2.692]
 [2.692]
 [2.692]] [[2.   ]
 [1.998]
 [2.   ]
 [2.   ]
 [2.   ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
348 2113
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.1455979431428912
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.02783571375572837
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71701795
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
353 2128
siam score:  -0.7186461
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.830948228460575
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.04 ]
 [5.658]
 [6.04 ]
 [6.04 ]
 [4.794]] [[1.61 ]
 [1.421]
 [1.61 ]
 [1.61 ]
 [0.996]]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.669]
 [5.257]
 [5.781]
 [4.44 ]
 [4.943]] [[1.249]
 [1.446]
 [1.621]
 [1.173]
 [1.341]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  366
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.023038378819638945
siam score:  -0.7212403
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 7.098757444861502
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6938901
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.678]
 [3.678]
 [4.371]
 [3.678]
 [3.678]] [[1.053]
 [1.053]
 [1.495]
 [1.053]
 [1.053]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.71202284
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7169354
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7155035
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.442]
 [4.915]
 [4.442]
 [4.442]
 [4.442]] [[1.328]
 [1.592]
 [1.328]
 [1.328]
 [1.328]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.11 ]
 [4.808]
 [4.888]
 [4.888]
 [3.404]] [[1.799]
 [1.622]
 [1.669]
 [1.669]
 [0.8  ]]
using explorer policy with actor:  1
first move QE:  0.021428192618243837
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73867005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
383 2164
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.017683123767660156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7434674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7337808
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.261299474283028
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.571]
 [1.039]
 [3.452]
 [1.417]
 [2.671]] [[0.997]
 [0.262]
 [1.421]
 [0.444]
 [1.046]]
siam score:  -0.7358639
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.009179596231018527
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.581]
 [5.547]
 [5.547]
 [5.547]
 [5.547]] [[1.057]
 [0.711]
 [0.711]
 [0.711]
 [0.711]]
line 256 mcts: sample exp_bonus 6.344653229632102
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.729899
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7287043
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.79 ]
 [6.463]
 [6.369]
 [5.79 ]
 [5.79 ]] [[0.763]
 [0.987]
 [0.956]
 [0.763]
 [0.763]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 1.2888318476420977e-20
0.0 2.8369389518099236e-11
0.0 0.0
0.0 0.0
0.0 5.155327390568391e-20
0.0 0.0
0.0 0.0
0.0 0.0
410 2209
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7426185
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7356316
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73557615
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.314]
 [5.756]
 [5.314]
 [5.314]
 [5.314]] [[0.986]
 [1.218]
 [0.986]
 [0.986]
 [0.986]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.883]
 [0.198]
 [2.105]
 [2.677]
 [2.538]] [[0.969]
 [0.121]
 [1.081]
 [1.368]
 [1.298]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.989]
 [2.989]
 [2.465]
 [2.579]
 [3.034]] [[1.525]
 [1.525]
 [1.262]
 [1.319]
 [1.548]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.176]
 [5.168]
 [5.049]
 [4.814]
 [4.864]] [[1.077]
 [1.84 ]
 [1.795]
 [1.705]
 [1.724]]
siam score:  -0.7481289
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75704974
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.301]
 [4.682]
 [4.301]
 [4.301]
 [4.301]] [[0.673]
 [0.8  ]
 [0.673]
 [0.673]
 [0.673]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.237]
 [4.237]
 [4.237]
 [4.237]
 [4.237]] [[4.237]
 [4.237]
 [4.237]
 [4.237]
 [4.237]]
line 256 mcts: sample exp_bonus -0.37310815777252665
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.736082
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.774]
 [6.075]
 [4.774]
 [4.774]
 [4.774]] [[1.377]
 [1.911]
 [1.377]
 [1.377]
 [1.377]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.543]
 [2.811]
 [3.784]
 [2.543]
 [2.567]] [[0.525]
 [0.632]
 [1.021]
 [0.525]
 [0.535]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 4.7210050545546745
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72739464
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
451 2259
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.588]
 [5.786]
 [5.086]
 [5.191]
 [6.373]] [[1.394]
 [1.484]
 [1.165]
 [1.213]
 [1.75 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.73491406
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.651]
 [5.634]
 [5.651]
 [5.175]
 [5.348]] [[1.83 ]
 [1.819]
 [1.83 ]
 [1.524]
 [1.636]]
first move QE:  0.00921612228235405
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 2.214196256328557e-11
0.0 0.0
0.0 0.0
0.0 1.9332477714631467e-20
0.0 1.2888318476420977e-20
0.0 0.0
0.0 3.2220796191052443e-21
0.0 0.0
0.0 1.0310654781136782e-19
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72593933
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.043]
 [3.939]
 [4.084]
 [3.416]
 [3.713]] [[0.39 ]
 [0.689]
 [0.738]
 [0.515]
 [0.614]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.716178
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7197231
473 2285
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.32]
 [-0.32]
 [-0.32]
 [-0.32]
 [-0.32]] [[0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]]
siam score:  -0.75733966
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.264]
 [2.264]
 [2.264]
 [2.264]
 [2.264]] [[1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.614]
 [2.595]
 [2.806]
 [2.803]
 [2.822]] [[0.731]
 [0.725]
 [0.795]
 [0.795]
 [0.801]]
484 2297
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76423526
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.01411711765529084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.004]
 [1.728]
 [1.004]
 [1.004]
 [1.004]] [[1.056]
 [1.535]
 [1.056]
 [1.056]
 [1.056]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.3273479115957403
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.4994827397937254
siam score:  -0.7520604
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7434178
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.662]
 [3.703]
 [3.662]
 [3.662]
 [3.662]] [[0.566]
 [0.58 ]
 [0.566]
 [0.566]
 [0.566]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.011641050359449623
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.667]
 [5.667]
 [5.667]
 [5.667]
 [5.667]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
line 256 mcts: sample exp_bonus 1.0149339800231723
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.76]
 [2.76]
 [2.76]
 [2.76]
 [2.76]] [[1.693]
 [1.693]
 [1.693]
 [1.693]
 [1.693]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7458818
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.083]
 [3.734]
 [3.807]
 [3.083]
 [4.318]] [[0.541]
 [0.758]
 [0.782]
 [0.541]
 [0.953]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.188762413462065
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.677]
 [3.103]
 [3.103]
 [3.103]
 [1.841]] [[0.88 ]
 [2.306]
 [2.306]
 [2.306]
 [1.045]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.8392730128659363
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.696]
 [0.465]
 [1.067]
 [0.924]
 [0.982]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
515 2338
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.934]
 [3.02 ]
 [2.934]
 [2.934]
 [2.934]] [[1.413]
 [1.494]
 [1.413]
 [1.413]
 [1.413]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
520 2344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.992532854359743
rdn probs:  [0.25, 0.25, 0.25, 0.25]
524 2356
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.503]
 [-0.332]
 [-0.287]
 [-0.509]
 [-0.473]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.249]
 [0.677]
 [0.812]
 [0.722]
 [0.973]] [[0.507]
 [0.127]
 [0.216]
 [0.157]
 [0.323]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
536 2380
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.058]
 [2.132]
 [1.342]
 [1.058]
 [1.058]] [[0.524]
 [1.223]
 [0.709]
 [0.524]
 [0.524]]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.745448
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.75022733
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7544104
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.254]
 [-0.316]
 [-0.254]
 [-0.254]
 [-0.525]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.506]
 [2.506]
 [2.573]
 [2.506]
 [2.506]] [[1.652]
 [1.652]
 [1.697]
 [1.652]
 [1.652]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.214]
 [3.214]
 [3.214]
 [3.214]
 [3.214]] [[1.826]
 [1.826]
 [1.826]
 [1.826]
 [1.826]]
siam score:  -0.7638593
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.08069226735065894
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7547193
start point for exploration sampling:  20019
siam score:  -0.7541044
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.664]
 [4.201]
 [6.108]
 [4.664]
 [4.664]] [[1.258]
 [1.041]
 [1.936]
 [1.258]
 [1.258]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.8197443807197518
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.911]
 [2.911]
 [2.545]
 [2.911]
 [2.87 ]] [[1.869]
 [1.869]
 [1.503]
 [1.869]
 [1.828]]
line 256 mcts: sample exp_bonus 3.550133490883696
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7697249
siam score:  -0.76717645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 1.1070981329973978e-12
0.0 0.0
0.0 2.5776636952841954e-20
0.0 2.0621309562273563e-19
0.0 7.732991085852587e-20
0.0 0.0
0.0 0.0
0.0 6.4441592382104886e-21
0.0 6.4441592382104886e-21
0.0 5.155327390568391e-20
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.01630424467822733
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.75540465
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.091]
 [1.981]
 [1.579]
 [2.091]
 [2.091]] [[1.341]
 [1.231]
 [0.829]
 [1.341]
 [1.341]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75444275
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.015163004117967286
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
583 2471
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
siam score:  -0.752352
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75151354
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.089]
 [3.22 ]
 [2.893]
 [2.935]
 [3.195]] [[1.338]
 [1.412]
 [1.226]
 [1.25 ]
 [1.398]]
586 2474
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.09]
 [1.09]
 [1.09]
 [1.09]
 [1.09]] [[1.534]
 [1.534]
 [1.534]
 [1.534]
 [1.534]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.014617212169131812
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.267]
 [1.529]
 [1.375]
 [1.351]
 [1.424]] [[0.918]
 [1.209]
 [1.039]
 [1.012]
 [1.093]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
592 2487
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.181]
 [2.558]
 [2.296]
 [2.181]
 [2.181]] [[1.108]
 [1.543]
 [1.241]
 [1.108]
 [1.108]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.667 0.125 0.125]
595 2491
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.372]
 [-0.014]
 [-0.367]
 [ 0.01 ]
 [-0.03 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.389]
 [4.313]
 [3.109]
 [4.313]
 [4.766]] [[1.793]
 [1.145]
 [0.42 ]
 [1.145]
 [1.418]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.728]
 [5.728]
 [5.728]
 [5.728]
 [5.105]] [[2.   ]
 [2.   ]
 [2.   ]
 [2.   ]
 [1.741]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7726304
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.78 ]
 [2.267]
 [2.699]
 [1.78 ]
 [1.78 ]] [[0.97 ]
 [1.286]
 [1.566]
 [0.97 ]
 [0.97 ]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.011015192704071721
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7647992
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75932807
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7588404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.7295938
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
line 256 mcts: sample exp_bonus 2.584358642164109
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.73942786
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.7427148
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.18 ]
 [1.631]
 [1.104]
 [1.18 ]
 [1.18 ]] [[1.217]
 [1.669]
 [1.141]
 [1.217]
 [1.217]]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.001]] [[4.732]
 [4.732]
 [4.732]
 [4.732]
 [5.174]] [[1.729]
 [1.729]
 [1.729]
 [1.729]
 [2.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.382342486912547
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
using another actor
from probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.07149591875356882, 0.07149591875356882, 0.7855122437392935, 0.07149591875356882]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.07149591875356882, 0.07149591875356882, 0.7855122437392935, 0.07149591875356882]
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.70484024
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.07149591875356882, 0.07149591875356882, 0.7855122437392935, 0.07149591875356882]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.07149591875356882, 0.07149591875356882, 0.7855122437392935, 0.07149591875356882]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.001]
 [0.   ]] [[0.536]
 [0.404]
 [0.599]
 [0.495]
 [0.219]] [[0.412]
 [0.281]
 [0.478]
 [0.374]
 [0.096]]
siam score:  -0.69507265
first move QE:  0.022702367610418515
siam score:  -0.67722875
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
siam score:  -0.68403876
siam score:  -0.6851744
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.68730634
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
660 2574
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.005]
 [0.004]
 [0.004]
 [0.004]] [[3.672]
 [5.503]
 [3.672]
 [3.672]
 [3.672]] [[1.011]
 [1.881]
 [1.011]
 [1.011]
 [1.011]]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[ 0.122]
 [-0.286]
 [-0.347]
 [-0.188]
 [ 0.184]] [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]]
663 2581
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
siam score:  -0.70009226
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.70721316
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
using another actor
from probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.83]
 [0.83]
 [0.83]
 [0.83]
 [0.83]] [[0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.003]
 [0.003]] [[ 0.117]
 [-0.253]
 [ 0.313]
 [ 0.165]
 [ 0.58 ]] [[0.373]
 [0.001]
 [0.569]
 [0.422]
 [0.837]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
siam score:  -0.75786704
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.629]
 [-0.601]
 [-0.268]
 [-0.204]
 [-0.274]] [[0.063]
 [0.081]
 [0.303]
 [0.346]
 [0.3  ]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.403276299903466
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
from probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.001]] [[ 1.274]
 [-0.025]
 [ 0.352]
 [ 0.008]
 [ 1.297]] [[1.3  ]
 [0.001]
 [0.38 ]
 [0.036]
 [1.324]]
using explorer policy with actor:  1
first move QE:  0.02377089908752442
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.638]
 [0.638]
 [1.088]
 [0.638]
 [0.638]] [[0.663]
 [0.663]
 [0.962]
 [0.663]
 [0.663]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
Starting evaluation
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]] [[3.458]
 [3.458]
 [3.458]
 [3.458]
 [3.458]] [[3.507]
 [3.507]
 [3.507]
 [3.507]
 [3.507]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
rdn probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.057766849958409014, 0.25000000000000006, 0.634466300083182, 0.057766849958409014]
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.034]
 [0.033]
 [0.033]
 [0.034]] [[4.449]
 [4.023]
 [3.791]
 [4.449]
 [4.229]] [[1.6  ]
 [1.318]
 [1.161]
 [1.6  ]
 [1.454]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.081270148801582
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0606776704712722, 0.21213553409425448, 0.6665091249632011, 0.0606776704712722]
siam score:  -0.7736929
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0606776704712722, 0.21213553409425448, 0.6665091249632011, 0.0606776704712722]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0606776704712722, 0.21213553409425448, 0.6665091249632011, 0.0606776704712722]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0606776704712722, 0.21213553409425448, 0.6665091249632011, 0.0606776704712722]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0606776704712722, 0.21213553409425448, 0.6665091249632011, 0.0606776704712722]
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
694 2633
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
siam score:  -0.7646135
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
using explorer policy with actor:  1
in main func line 156:  696
start point for exploration sampling:  20019
siam score:  -0.7675956
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
siam score:  -0.75987744
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
first move QE:  0.021947866292186216
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.491]
 [0.08 ]
 [0.731]
 [0.491]
 [0.412]] [[0.413]
 [0.001]
 [0.653]
 [0.413]
 [0.332]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
siam score:  -0.7442279
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
from probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06256973353545021, 0.18752324451181668, 0.6873372884172828, 0.06256973353545021]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06389821734405589, 0.1702420931474525, 0.7019614721644356, 0.06389821734405589]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06389821734405589, 0.1702420931474525, 0.7019614721644356, 0.06389821734405589]
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.06389821734405589, 0.1702420931474525, 0.7019614721644356, 0.06389821734405589]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06389821734405589, 0.1702420931474525, 0.7019614721644356, 0.06389821734405589]
siam score:  -0.7392672
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06488228686982561, 0.1574411434349128, 0.712794282825436, 0.06488228686982561]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06488228686982561, 0.1574411434349128, 0.712794282825436, 0.06488228686982561]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06488228686982561, 0.1574411434349128, 0.712794282825436, 0.06488228686982561]
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.09 ]
 [0.08 ]
 [0.051]
 [0.051]] [[2.346]
 [2.766]
 [2.9  ]
 [2.346]
 [2.346]] [[0.676]
 [1.032]
 [1.103]
 [0.676]
 [0.676]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.005]
 [0.007]
 [0.005]
 [0.005]] [[ 0.247]
 [ 0.085]
 [ 0.023]
 [-0.075]
 [-0.075]] [[0.572]
 [0.469]
 [0.431]
 [0.362]
 [0.362]]
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.77474
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.003]
 [0.001]
 [0.   ]] [[-0.155]
 [ 0.25 ]
 [-0.165]
 [ 0.026]
 [-0.022]] [[0.007]
 [0.274]
 [0.002]
 [0.126]
 [0.093]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.005]
 [0.005]
 [0.004]] [[ 0.   ]
 [ 0.   ]
 [-0.5  ]
 [-0.684]
 [-0.692]] [[0.47 ]
 [0.47 ]
 [0.135]
 [0.014]
 [0.006]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
713 2712
using another actor
from probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
from probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
siam score:  -0.7868174
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625487, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625484, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
718 2741
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625484, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625484, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625484, 0.7211409222232276, 0.0656405086952588]
719 2744
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.0656405086952588, 0.14757806038625484, 0.7211409222232276, 0.0656405086952588]
from probs:  [0.0656405086952588, 0.14757806038625484, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625484, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625484, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625484, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625484, 0.7211409222232276, 0.0656405086952588]
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.995960220455313
first move QE:  0.008938408392045703
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0656405086952588, 0.14757806038625484, 0.7211409222232276, 0.0656405086952588]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06624262870697532, 0.13974557722418518, 0.7277691653618642, 0.06624262870697532]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06624262870697532, 0.13974557722418518, 0.7277691653618642, 0.06624262870697532]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06624262870697532, 0.13974557722418518, 0.7277691653618642, 0.06624262870697532]
siam score:  -0.8285662
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06624262870697532, 0.13974557722418518, 0.7277691653618642, 0.06624262870697532]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
from probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
siam score:  -0.84362954
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.012]
 [0.014]
 [0.016]
 [0.014]] [[-0.412]
 [-0.321]
 [-0.146]
 [-0.213]
 [-0.207]] [[0.684]
 [0.757]
 [0.938]
 [0.874]
 [0.877]]
from probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
from probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.023]
 [0.026]
 [0.023]
 [0.023]] [[0.246]
 [0.246]
 [0.666]
 [0.246]
 [0.246]] [[0.412]
 [0.412]
 [0.558]
 [0.412]
 [0.412]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
siam score:  -0.8525728
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06673235473252603, 0.1333751348297893, 0.7331601557051586, 0.06673235473252603]
maxi score, test score, baseline:  0.0021 0.0 0.0021
line 256 mcts: sample exp_bonus 1.9165209903733964
actor:  1 policy actor:  1  step number:  97 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 1.6208362595543888
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
732 2785
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
siam score:  -0.8589395
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.8426283
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.061]
 [0.106]
 [0.061]
 [0.061]] [[1.227]
 [1.227]
 [1.963]
 [1.227]
 [1.227]] [[1.176]
 [1.176]
 [1.737]
 [1.176]
 [1.176]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.005]
 [0.005]
 [0.005]
 [0.006]] [[0.409]
 [0.596]
 [0.255]
 [0.157]
 [0.131]] [[0.31 ]
 [0.497]
 [0.157]
 [0.06 ]
 [0.035]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.005]
 [0.009]
 [0.009]
 [0.007]] [[0.548]
 [0.175]
 [2.116]
 [2.116]
 [0.569]] [[0.401]
 [0.024]
 [1.974]
 [1.974]
 [0.422]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.004]
 [0.003]
 [0.003]
 [0.003]] [[0.635]
 [0.564]
 [0.635]
 [0.635]
 [0.635]] [[0.666]
 [0.596]
 [0.666]
 [0.666]
 [0.666]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
from probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.06328638465445782, 0.17818707102094533, 0.6952401596701391, 0.06328638465445782]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.026]
 [0.021]
 [0.018]
 [0.013]] [[4.043]
 [3.036]
 [3.537]
 [3.591]
 [3.308]] [[1.723]
 [1.08 ]
 [1.405]
 [1.435]
 [1.237]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.003]
 [0.003]
 [0.004]
 [0.004]] [[ 0.049]
 [-0.288]
 [-0.263]
 [-0.211]
 [-0.356]] [[0.274]
 [0.047]
 [0.065]
 [0.1  ]
 [0.003]]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.012]
 [0.011]
 [0.011]
 [0.011]] [[1.785]
 [3.13 ]
 [1.785]
 [1.785]
 [1.785]] [[1.073]
 [2.006]
 [1.073]
 [1.073]
 [1.073]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
first move QE:  0.008964087060850879
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
siam score:  -0.8057873
745 2838
using another actor
747 2841
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
from probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.013]
 [0.018]
 [0.018]
 [0.018]] [[1.882]
 [1.947]
 [1.882]
 [1.882]
 [2.46 ]] [[1.112]
 [1.168]
 [1.112]
 [1.112]
 [1.691]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
maxi score, test score, baseline:  0.0021 0.0 0.0021
749 2848
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.016]
 [0.015]
 [0.012]] [[2.029]
 [2.045]
 [0.809]
 [2.029]
 [2.232]] [[1.445]
 [1.457]
 [0.535]
 [1.445]
 [1.593]]
using another actor
siam score:  -0.85117793
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]] [[2.884]
 [2.884]
 [2.884]
 [2.884]
 [2.884]] [[1.825]
 [1.825]
 [1.825]
 [1.825]
 [1.825]]
using explorer policy with actor:  1
753 2866
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]] [[3.446]
 [3.446]
 [3.446]
 [3.446]
 [3.446]] [[2.07]
 [2.07]
 [2.07]
 [2.07]
 [2.07]]
using another actor
from probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
753 2870
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0161],
        [0.1979],
        [0.0000],
        [0.0247],
        [0.0120],
        [0.0000],
        [0.0436],
        [0.1293],
        [0.0939],
        [0.0924]], dtype=torch.float64)
0.0 0.016092391934827113
0.0 0.19788096136238548
0.0 0.0
0.0 0.024745831414702175
0.0 0.012015928469315613
0.99 0.99
0.0 0.043601615214326
0.0 0.12934013659573135
0.0 0.09386010535174853
0.0 0.09237736012718062
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.7594904538677256
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
UNIT TEST: sample policy line 217 mcts : [0.042 0.208 0.542 0.042 0.167]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05985304991962923, 0.22283614998851847, 0.657457750172223, 0.05985304991962923]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.163]
 [0.156]
 [0.142]
 [0.162]] [[2.521]
 [2.343]
 [2.78 ]
 [3.034]
 [2.805]] [[0.852]
 [0.811]
 [1.087]
 [1.229]
 [1.117]]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
760 2898
siam score:  -0.90387774
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
siam score:  -0.9059465
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.007]
 [0.017]
 [0.009]
 [0.008]] [[ 0.993]
 [ 0.133]
 [-0.008]
 [ 0.353]
 [ 0.044]] [[1.065]
 [0.215]
 [0.096]
 [0.439]
 [0.129]]
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
using explorer policy with actor:  1
from probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
siam score:  -0.9106058
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
siam score:  -0.9117392
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
from probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.043]
 [0.061]
 [0.043]
 [0.043]] [[1.147]
 [1.147]
 [1.386]
 [1.147]
 [1.147]] [[0.326]
 [0.326]
 [0.522]
 [0.326]
 [0.326]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[-0.155]
 [-0.161]
 [-0.155]
 [-0.155]
 [-0.155]] [[0.006]
 [0.002]
 [0.006]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
siam score:  -0.91495395
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
line 256 mcts: sample exp_bonus -0.31190217016936467
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.022]
 [0.023]
 [0.024]
 [0.022]] [[1.49 ]
 [1.847]
 [1.302]
 [1.716]
 [1.847]] [[0.806]
 [1.172]
 [0.629]
 [1.044]
 [1.172]]
768 3013
siam score:  -0.9149265
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
from probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
siam score:  -0.9158339
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
from probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
from probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
774 3040
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
from probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
from probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
using another actor
from probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.031]
 [0.023]
 [0.023]
 [0.023]] [[0.33 ]
 [0.106]
 [0.33 ]
 [0.33 ]
 [0.33 ]] [[0.532]
 [0.324]
 [0.532]
 [0.532]
 [0.532]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]] [[0.23]
 [0.23]
 [0.23]
 [0.23]
 [0.23]] [[1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.023]
 [0.004]
 [0.019]
 [0.022]] [[-0.676]
 [-0.697]
 [-0.424]
 [-0.653]
 [-0.59 ]] [[0.12 ]
 [0.103]
 [0.338]
 [0.14 ]
 [0.207]]
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
siam score:  -0.91453815
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.039]] [[1.59 ]
 [1.544]
 [1.59 ]
 [1.59 ]
 [2.222]] [[0.95 ]
 [0.904]
 [0.95 ]
 [0.95 ]
 [1.59 ]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.051]
 [0.001]
 [0.029]
 [0.029]] [[-0.682]
 [-0.604]
 [-0.162]
 [-0.604]
 [-0.604]] [[0.449]
 [0.579]
 [0.92 ]
 [0.534]
 [0.534]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.29852103568412997
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.9296948
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.93120575
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Printing some Q and Qe and total Qs values:  [[0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]] [[4.034]
 [4.034]
 [4.034]
 [4.034]
 [4.034]] [[2.065]
 [2.065]
 [2.065]
 [2.065]
 [2.065]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.042 0.833]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.006]
 [0.025]
 [0.025]
 [0.021]] [[-1.265]
 [ 0.023]
 [-0.596]
 [-0.596]
 [-0.314]] [[0.016]
 [0.006]
 [0.025]
 [0.025]
 [0.021]]
790 3116
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.9295888
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.018]] [[ 0.424]
 [ 0.424]
 [-0.097]
 [ 0.424]
 [-0.032]] [[0.682]
 [0.682]
 [0.161]
 [0.682]
 [0.233]]
siam score:  -0.9189563
from probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.015]
 [0.019]
 [0.011]
 [0.015]] [[2.586]
 [1.773]
 [1.493]
 [1.478]
 [1.773]] [[2.006]
 [1.392]
 [1.186]
 [1.162]
 [1.392]]
first move QE:  -0.061741726422108505
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
from probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.031]
 [0.037]
 [0.037]
 [0.037]] [[0.363]
 [0.865]
 [0.363]
 [0.363]
 [0.363]] [[0.534]
 [0.974]
 [0.534]
 [0.534]
 [0.534]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.070643449372739
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.031]] [[2.083]
 [2.083]
 [2.083]
 [2.083]
 [2.163]] [[1.964]
 [1.964]
 [1.964]
 [1.964]
 [2.014]]
siam score:  -0.92649305
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
siam score:  -0.92558306
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.168]
 [0.139]
 [0.139]
 [0.139]] [[3.298]
 [3.063]
 [2.745]
 [2.745]
 [2.745]] [[1.844]
 [1.684]
 [1.336]
 [1.336]
 [1.336]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
siam score:  -0.921005
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05677370397993334, 0.2628817530680044, 0.6235708389721288, 0.05677370397993334]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.031]
 [0.035]
 [0.037]
 [0.024]] [[2.308]
 [1.802]
 [1.955]
 [1.696]
 [1.104]] [[1.253]
 [0.76 ]
 [0.921]
 [0.665]
 [0.048]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.039]
 [0.02 ]
 [0.027]
 [0.026]] [[0.178]
 [1.045]
 [0.178]
 [0.161]
 [0.369]] [[0.02 ]
 [0.039]
 [0.02 ]
 [0.027]
 [0.026]]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.05677370783896633, 0.26288175281073556, 0.6235708315113317, 0.05677370783896633]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.005]
 [0.046]
 [0.037]
 [0.023]] [[1.175]
 [1.381]
 [1.484]
 [0.802]
 [0.13 ]] [[0.007]
 [0.005]
 [0.046]
 [0.037]
 [0.023]]
actor:  0 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.05677372351018635, 0.2628817517659876, 0.6235708012136397, 0.05677372351018635]
line 256 mcts: sample exp_bonus 2.490485354434107
maxi score, test score, baseline:  0.0121 0.0 0.0121
815 3148
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.05677372351018635, 0.2628817517659876, 0.6235708012136397, 0.05677372351018635]
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.035]
 [0.045]
 [0.029]
 [0.037]] [[2.675]
 [2.596]
 [2.509]
 [1.634]
 [2.734]] [[0.029]
 [0.035]
 [0.045]
 [0.029]
 [0.037]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
actor:  0 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.05677372748771908, 0.26288175150081866, 0.6235707935237431, 0.05677372748771908]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.05677372748771908, 0.26288175150081866, 0.6235707935237431, 0.05677372748771908]
rdn probs:  [0.05677372748771908, 0.26288175150081866, 0.6235707935237431, 0.05677372748771908]
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.173]
 [0.154]
 [0.164]
 [0.138]] [[3.553]
 [3.343]
 [3.592]
 [3.553]
 [3.47 ]] [[2.047]
 [1.875]
 [2.064]
 [2.047]
 [1.927]]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0141 0.3 0.3
820 3159
maxi score, test score, baseline:  0.0141 0.3 0.3
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.106]
 [0.133]
 [0.135]
 [0.12 ]] [[2.603]
 [2.718]
 [2.579]
 [2.603]
 [3.155]] [[1.305]
 [1.325]
 [1.286]
 [1.305]
 [1.643]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.734229879156249
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]] [[2.895]
 [2.725]
 [2.895]
 [2.895]
 [2.895]] [[1.598]
 [1.428]
 [1.598]
 [1.598]
 [1.598]]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
using another actor
from probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.031]
 [0.03 ]
 [0.027]
 [0.028]] [[ 0.04 ]
 [-0.774]
 [-0.764]
 [-0.818]
 [-1.169]] [[0.93 ]
 [0.326]
 [0.332]
 [0.288]
 [0.026]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.013]
 [0.016]
 [0.009]
 [0.009]] [[-0.346]
 [ 0.517]
 [ 1.438]
 [-0.346]
 [-0.346]] [[0.009]
 [0.013]
 [0.016]
 [0.009]
 [0.009]]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.03 ]
 [0.03 ]
 [0.027]
 [0.027]] [[-0.598]
 [-0.855]
 [-0.874]
 [-1.022]
 [-1.351]] [[0.572]
 [0.387]
 [0.372]
 [0.26 ]
 [0.019]]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.081]
 [0.068]
 [0.069]
 [0.065]] [[2.901]
 [1.509]
 [1.88 ]
 [2.901]
 [2.455]] [[1.795]
 [0.893]
 [1.114]
 [1.795]
 [1.491]]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.045]
 [0.068]
 [0.054]
 [0.056]] [[0.158]
 [1.242]
 [1.661]
 [0.951]
 [1.437]] [[0.061]
 [0.755]
 [1.075]
 [0.583]
 [0.903]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
siam score:  -0.92857635
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.051]] [[3.553]
 [3.553]
 [3.553]
 [3.553]
 [3.805]] [[1.752]
 [1.752]
 [1.752]
 [1.752]
 [1.927]]
using explorer policy with actor:  1
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
start point for exploration sampling:  20019
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.048]] [[1.972]
 [1.972]
 [1.972]
 [1.972]
 [2.174]] [[1.387]
 [1.387]
 [1.387]
 [1.387]
 [1.494]]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
siam score:  -0.9276491
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.182]
 [0.154]
 [0.119]
 [0.147]] [[1.937]
 [0.338]
 [2.292]
 [2.712]
 [2.115]] [[1.265]
 [0.108]
 [1.548]
 [1.814]
 [1.403]]
maxi score, test score, baseline:  0.0141 0.3 0.3
siam score:  -0.92669797
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]] [[0.915]
 [0.915]
 [0.915]
 [0.915]
 [1.936]] [[1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.837]]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.132]
 [0.169]
 [0.028]
 [0.077]] [[ 1.077]
 [-0.437]
 [ 1.377]
 [-0.241]
 [ 1.696]] [[0.998]
 [0.06 ]
 [1.309]
 [0.052]
 [1.399]]
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.263]
 [0.241]
 [0.185]
 [0.211]] [[2.097]
 [2.239]
 [2.421]
 [1.938]
 [3.197]] [[1.013]
 [1.18 ]
 [1.292]
 [0.804]
 [1.88 ]]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151755, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
first move QE:  -0.06334289908867184
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
using another actor
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.019]
 [0.014]
 [0.02 ]
 [0.022]] [[-0.418]
 [-0.252]
 [-0.094]
 [-0.358]
 [ 0.21 ]] [[0.022]
 [0.019]
 [0.014]
 [0.02 ]
 [0.022]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
siam score:  -0.9268795
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
from probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
siam score:  -0.9278556
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
first move QE:  -0.0667441738506385
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
siam score:  -0.93643326
maxi score, test score, baseline:  0.0141 0.3 0.3
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
from probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
from probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
siam score:  -0.92883897
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
869 3227
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
siam score:  -0.92497754
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
using explorer policy with actor:  1
siam score:  -0.9233738
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
873 3230
using another actor
siam score:  -0.93333894
using another actor
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
from probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.114]
 [0.121]
 [0.071]
 [0.106]] [[1.851]
 [1.146]
 [1.464]
 [1.359]
 [1.398]] [[1.031]
 [0.397]
 [0.73 ]
 [0.524]
 [0.635]]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
from probs:  [0.05677469477236955, 0.2628816870151754, 0.6235689234400855, 0.05677469477236955]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
siam score:  -0.9343369
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
line 256 mcts: sample exp_bonus 1.4833777050956782
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
from probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
siam score:  -0.9266375
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
first move QE:  -0.0697496644223693
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.055630235856771945, 0.27776710916331826, 0.6109724191231378, 0.055630235856771945]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
887 3257
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.022]
 [0.075]
 [0.053]
 [0.066]] [[-0.543]
 [-0.276]
 [ 0.287]
 [ 0.129]
 [ 0.987]] [[0.026]
 [0.134]
 [0.506]
 [0.393]
 [0.884]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0942],
        [0.0663],
        [0.0851],
        [0.1751],
        [0.0881],
        [0.1964],
        [0.1347],
        [0.1019],
        [0.0515]], dtype=torch.float64)
0.0 0.0
0.0 0.09416532184608958
0.0 0.06633825873763544
0.0 0.08514018271433614
0.0 0.17510041244855393
0.0 0.0881320596713171
0.0 0.19635429699824383
0.0 0.13474337951877122
0.0 0.10193542805775377
0.0 0.05147985629839938
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[4.025]
 [4.025]
 [4.025]
 [4.025]
 [4.112]] [[1.119]
 [1.119]
 [1.119]
 [1.119]
 [1.166]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.145]
 [0.211]
 [0.145]
 [0.145]] [[2.756]
 [2.756]
 [3.367]
 [2.756]
 [2.756]] [[1.327]
 [1.327]
 [1.964]
 [1.327]
 [1.327]]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
siam score:  -0.93063796
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.131]
 [0.141]
 [0.133]
 [0.168]] [[1.53 ]
 [3.755]
 [1.985]
 [1.102]
 [1.916]] [[0.266]
 [1.254]
 [0.683]
 [0.371]
 [0.714]]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.138]
 [0.202]
 [0.158]
 [0.17 ]] [[1.979]
 [2.4  ]
 [3.134]
 [1.912]
 [2.595]] [[0.827]
 [1.045]
 [1.583]
 [0.764]
 [1.207]]
siam score:  -0.9269802
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
siam score:  -0.9265027
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
siam score:  -0.9293019
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
using explorer policy with actor:  1
from probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.056]
 [0.056]
 [0.054]
 [0.06 ]] [[0.393]
 [1.157]
 [1.157]
 [1.294]
 [1.493]] [[0.056]
 [0.554]
 [0.554]
 [0.641]
 [0.784]]
line 256 mcts: sample exp_bonus 0.8739475230820895
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
912 3284
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
siam score:  -0.9304245
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
from probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
first move QE:  -0.059691084666847985
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
first move QE:  -0.06065420134619818
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
from probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
siam score:  -0.934425
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
using another actor
from probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.054292744332612816, 0.29516321284632013, 0.5962512984884542, 0.054292744332612816]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.051214476389426195, 0.3351937958331031, 0.5623772513880445, 0.051214476389426195]
926 3297
siam score:  -0.9328476
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.051214476389426195, 0.3351937958331031, 0.5623772513880445, 0.051214476389426195]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.051214476389426195, 0.3351937958331031, 0.5623772513880445, 0.051214476389426195]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.051214476389426195, 0.3351937958331031, 0.5623772513880445, 0.051214476389426195]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.051214476389426195, 0.3351937958331031, 0.5623772513880445, 0.051214476389426195]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.051214476389426195, 0.3351937958331031, 0.5623772513880445, 0.051214476389426195]
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.21 ]
 [0.091]
 [0.157]
 [0.143]] [[ 0.   ]
 [ 0.118]
 [ 0.328]
 [ 0.168]
 [-0.048]] [[0.247]
 [0.463]
 [0.364]
 [0.39 ]
 [0.217]]
maxi score, test score, baseline:  0.0141 0.3 0.3
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.051214476389426195, 0.3351937958331031, 0.5623772513880445, 0.051214476389426195]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.051214476389426195, 0.3351937958331031, 0.5623772513880445, 0.051214476389426195]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.051214476389426195, 0.3351937958331031, 0.5623772513880445, 0.051214476389426195]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.051214476389426195, 0.3351937958331031, 0.5623772513880445, 0.051214476389426195]
siam score:  -0.9349951
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.051214476389426195, 0.3351937958331031, 0.5623772513880445, 0.051214476389426195]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04846723225261318, 0.37091966064843224, 0.5321458748463416, 0.04846723225261318]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04846723225261318, 0.37091966064843224, 0.5321458748463416, 0.04846723225261318]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04846723225261318, 0.37091966064843224, 0.5321458748463416, 0.04846723225261318]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04846723225261318, 0.37091966064843224, 0.5321458748463416, 0.04846723225261318]
line 256 mcts: sample exp_bonus 1.6084761995024672
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.282]
 [0.316]
 [0.282]
 [0.282]] [[1.596]
 [1.596]
 [2.345]
 [1.596]
 [1.596]] [[0.849]
 [0.849]
 [1.575]
 [0.849]
 [0.849]]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04659341531017998, 0.39529041763558564, 0.5115227517440544, 0.04659341531017998]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04659341531017998, 0.39529041763558564, 0.5115227517440544, 0.04659341531017998]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04659341531017998, 0.39529041763558564, 0.5115227517440544, 0.04659341531017998]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04659341531017998, 0.39529041763558564, 0.5115227517440544, 0.04659341531017998]
940 3317
first move QE:  -0.07249348990982574
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04659341531017998, 0.39529041763558564, 0.5115227517440544, 0.04659341531017998]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04659341531017998, 0.39529041763558564, 0.5115227517440544, 0.04659341531017998]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
siam score:  -0.9474541
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.463]
 [0.434]
 [0.315]
 [0.443]] [[1.738]
 [1.244]
 [1.787]
 [2.333]
 [1.691]] [[1.495]
 [0.991]
 [1.458]
 [1.753]
 [1.383]]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
siam score:  -0.9369336
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
line 256 mcts: sample exp_bonus 1.1246462092464535
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.388]
 [0.604]
 [0.468]
 [0.398]] [[2.102]
 [2.841]
 [1.395]
 [2.347]
 [3.23 ]] [[1.108]
 [1.43 ]
 [0.843]
 [1.237]
 [1.667]]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
using another actor
from probs:  [0.04438755094540669, 0.4239797645846558, 0.48724513352453075, 0.04438755094540669]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.25669995716218336
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04175239425247294, 0.45824760574752704, 0.45824760574752693, 0.04175239425247294]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0141 0.3 0.3
line 256 mcts: sample exp_bonus 3.5090683166530776
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.422]
 [0.479]
 [0.513]
 [0.379]] [[2.391]
 [1.611]
 [2.172]
 [1.884]
 [2.424]] [[1.705]
 [0.954]
 [1.443]
 [1.317]
 [1.409]]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04175239425247294, 0.45824760574752704, 0.45824760574752693, 0.04175239425247294]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04175239425247294, 0.45824760574752704, 0.45824760574752693, 0.04175239425247294]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04175239425247294, 0.45824760574752704, 0.45824760574752693, 0.04175239425247294]
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04175239425247294, 0.45824760574752704, 0.45824760574752693, 0.04175239425247294]
maxi score, test score, baseline:  0.0141 0.3 0.3
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04175239425247294, 0.45824760574752704, 0.45824760574752693, 0.04175239425247294]
siam score:  -0.9429959
maxi score, test score, baseline:  0.0141 0.3 0.3
probs:  [0.04175239425247294, 0.45824760574752704, 0.45824760574752693, 0.04175239425247294]
start point for exploration sampling:  20019
from probs:  [0.04175239425247294, 0.45824760574752704, 0.45824760574752693, 0.04175239425247294]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.04175239425247294, 0.45824760574752704, 0.45824760574752693, 0.04175239425247294]
from probs:  [0.04175239425247294, 0.45824760574752704, 0.45824760574752693, 0.04175239425247294]
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.256]
 [0.081]
 [0.2  ]
 [0.201]] [[-0.175]
 [ 0.038]
 [-0.508]
 [-0.32 ]
 [-0.175]] [[0.856]
 [1.108]
 [0.393]
 [0.758]
 [0.856]]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04175239425247294, 0.45824760574752704, 0.45824760574752693, 0.04175239425247294]
siam score:  -0.94494647
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.186]
 [0.176]
 [0.205]
 [0.171]] [[0.815]
 [0.877]
 [0.523]
 [0.815]
 [0.436]] [[1.188]
 [1.192]
 [0.936]
 [1.188]
 [0.868]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.002]
 [0.002]
 [0.003]] [[-0.358]
 [-0.095]
 [-0.358]
 [-0.358]
 [-0.096]] [[0.142]
 [0.315]
 [0.142]
 [0.142]
 [0.318]]
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.15 ]
 [0.159]
 [0.171]
 [0.185]] [[1.194]
 [1.299]
 [0.93 ]
 [1.194]
 [1.668]] [[0.409]
 [0.402]
 [0.296]
 [0.409]
 [0.595]]
line 256 mcts: sample exp_bonus 2.682168008252387
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
from probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
from probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
first move QE:  -0.08554788782334846
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.329]
 [0.303]
 [0.219]
 [0.329]] [[1.286]
 [1.286]
 [1.231]
 [0.434]
 [1.286]] [[1.303]
 [1.303]
 [1.196]
 [0.23 ]
 [1.303]]
maxi score, test score, baseline:  0.0161 0.3 0.3
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
972 3360
maxi score, test score, baseline:  0.0161 0.3 0.3
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
siam score:  -0.93757075
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.091]
 [0.091]
 [0.101]
 [0.098]] [[0.311]
 [0.882]
 [0.882]
 [1.311]
 [0.873]] [[0.317]
 [0.833]
 [0.833]
 [1.282]
 [0.836]]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04403973985005421, 0.42849889212995307, 0.4834216281699386, 0.04403973985005421]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04175209080900828, 0.45824790919099173, 0.45824790919099173, 0.04175209080900828]
maxi score, test score, baseline:  0.0161 0.3 0.3
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04175209080900828, 0.45824790919099173, 0.45824790919099173, 0.04175209080900828]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04175209080900828, 0.45824790919099173, 0.45824790919099173, 0.04175209080900828]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04175209080900828, 0.45824790919099173, 0.45824790919099173, 0.04175209080900828]
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04175209080900828, 0.45824790919099173, 0.45824790919099173, 0.04175209080900828]
977 3366
maxi score, test score, baseline:  0.0161 0.3 0.3
maxi score, test score, baseline:  0.0161 0.3 0.3
probs:  [0.04175209080900828, 0.45824790919099173, 0.45824790919099173, 0.04175209080900828]
Starting evaluation
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0161 0.3 0.3
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0201 0.3 0.3
probs:  [0.04377297166087099, 0.4804890316731442, 0.43196502500511386, 0.04377297166087099]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn probs:  [0.04377297166087099, 0.4804890316731442, 0.43196502500511386, 0.04377297166087099]
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[-0.463]
 [-0.463]
 [-0.463]
 [-0.463]
 [-0.463]] [[0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.04600007833596991, 0.5049999020800375, 0.4029999412480225, 0.04600007833596991]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.04600007833596991, 0.5049999020800375, 0.4029999412480225, 0.04600007833596991]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.04600007833596991, 0.5049999020800375, 0.4029999412480225, 0.04600007833596991]
first move QE:  -0.0885148940731196
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.04600007833596991, 0.5049999020800375, 0.4029999412480225, 0.04600007833596991]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.04600007833596991, 0.5049999020800375, 0.4029999412480225, 0.04600007833596991]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.734]
 [0.748]
 [0.734]
 [0.734]] [[3.511]
 [2.587]
 [3.296]
 [2.587]
 [2.587]] [[2.038]
 [2.05 ]
 [2.543]
 [2.05 ]
 [2.05 ]]
siam score:  -0.9422537
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.0476992999856311, 0.5237009470782639, 0.380900452950474, 0.0476992999856311]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.0476992999856311, 0.5237009470782639, 0.380900452950474, 0.0476992999856311]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.0476992999856311, 0.5237009470782639, 0.380900452950474, 0.0476992999856311]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.0476992999856311, 0.5237009470782639, 0.380900452950474, 0.0476992999856311]
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.318]
 [0.489]
 [0.081]
 [0.403]] [[ 1.342]
 [ 0.962]
 [ 0.531]
 [-0.077]
 [ 1.019]] [[1.451]
 [1.238]
 [1.293]
 [0.073]
 [1.446]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.0476992999856311, 0.5237009470782639, 0.380900452950474, 0.0476992999856311]
line 256 mcts: sample exp_bonus 0.41054704583304225
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.0476992999856311, 0.5237009470782639, 0.380900452950474, 0.0476992999856311]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.0476992999856311, 0.5237009470782639, 0.380900452950474, 0.0476992999856311]
actor:  1 policy actor:  1  step number:  97 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05147915004800624, 0.5652978205119901, 0.33174387939199745, 0.05147915004800624]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05147915004800624, 0.5652978205119901, 0.33174387939199745, 0.05147915004800624]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05147915004800624, 0.5652978205119901, 0.33174387939199745, 0.05147915004800624]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05147915004800624, 0.5652978205119901, 0.33174387939199745, 0.05147915004800624]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05147915004800624, 0.5652978205119901, 0.33174387939199745, 0.05147915004800624]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05147915004800624, 0.5652978205119901, 0.33174387939199745, 0.05147915004800624]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05147915004800624, 0.5652978205119901, 0.33174387939199745, 0.05147915004800624]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05147915004800624, 0.5652978205119901, 0.33174387939199745, 0.05147915004800624]
first move QE:  -0.09054537098592753
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.379]
 [0.412]
 [0.413]
 [0.413]] [[2.811]
 [2.142]
 [1.73 ]
 [2.205]
 [2.205]] [[1.744]
 [1.346]
 [1.138]
 [1.456]
 [1.456]]
from probs:  [0.05147915004800624, 0.5652978205119901, 0.33174387939199745, 0.05147915004800624]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
line 256 mcts: sample exp_bonus 1.8295813428710184
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
siam score:  -0.9437215
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
from probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[1.777]
 [1.777]
 [1.777]
 [1.777]
 [1.777]] [[1.845]
 [1.845]
 [1.845]
 [1.845]
 [1.845]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
from probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.716]
 [0.393]
 [0.254]
 [0.668]] [[1.95 ]
 [1.36 ]
 [0.782]
 [1.025]
 [2.6  ]] [[1.196]
 [1.25 ]
 [0.276]
 [0.167]
 [1.941]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05399699775668031, 0.5930052539258094, 0.29900075056083, 0.05399699775668031]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.545]
 [0.74 ]
 [0.545]
 [0.545]] [[2.469]
 [1.378]
 [1.288]
 [1.378]
 [1.378]] [[2.102]
 [1.327]
 [1.531]
 [1.327]
 [1.327]]
using another actor
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.502]
 [0.498]
 [0.399]
 [0.399]] [[1.292]
 [1.338]
 [1.31 ]
 [1.292]
 [1.292]] [[1.377]
 [1.614]
 [1.588]
 [1.377]
 [1.377]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
Printing some Q and Qe and total Qs values:  [[1.204]
 [1.204]
 [1.204]
 [1.204]
 [1.204]] [[2.084]
 [2.084]
 [2.084]
 [2.084]
 [2.084]] [[2.409]
 [2.409]
 [2.409]
 [2.409]
 [2.409]]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.253]] [[1.147]
 [1.147]
 [1.147]
 [1.147]
 [5.661]] [[0.619]
 [0.619]
 [0.619]
 [0.619]
 [1.947]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
siam score:  -0.9461002
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
line 256 mcts: sample exp_bonus 1.9957655200034545
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]] [[1.265]
 [1.896]
 [1.896]
 [1.896]
 [1.896]] [[0.935]
 [1.785]
 [1.785]
 [1.785]
 [1.785]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.061]
 [0.361]
 [0.314]
 [0.286]] [[ 0.097]
 [-0.405]
 [ 0.168]
 [ 0.311]
 [-0.287]] [[0.844]
 [0.121]
 [1.104]
 [1.105]
 [0.651]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]] [[1.145]
 [1.145]
 [1.145]
 [1.145]
 [1.145]] [[0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
siam score:  -0.93958795
UNIT TEST: sample policy line 217 mcts : [0.042 0.125 0.042 0.083 0.708]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.017]
 [0.059]
 [0.058]
 [0.058]] [[ 0.487]
 [ 0.573]
 [ 0.152]
 [-0.093]
 [ 0.131]] [[0.644]
 [0.702]
 [0.365]
 [0.119]
 [0.343]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
siam score:  -0.9380727
Printing some Q and Qe and total Qs values:  [[0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]] [[2.088]
 [2.088]
 [2.088]
 [2.088]
 [2.088]] [[2.123]
 [2.123]
 [2.123]
 [2.123]
 [2.123]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.792 0.083 0.042 0.042]
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.296]
 [0.336]
 [0.352]
 [0.32 ]] [[2.174]
 [1.931]
 [2.095]
 [2.579]
 [2.67 ]] [[0.945]
 [0.788]
 [0.979]
 [1.331]
 [1.329]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.222]] [[0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.241]] [[0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.401]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
siam score:  -0.9468167
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
siam score:  -0.9435759
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
siam score:  -0.9427427
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.187]
 [0.253]
 [0.206]
 [0.193]] [[1.981]
 [1.759]
 [1.784]
 [1.981]
 [2.406]] [[1.668]
 [1.494]
 [1.633]
 [1.668]
 [1.907]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
start point for exploration sampling:  20019
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.204]
 [0.212]
 [0.2  ]
 [0.225]] [[2.183]
 [0.738]
 [2.425]
 [3.218]
 [4.728]] [[0.807]
 [0.151]
 [0.924]
 [1.266]
 [1.983]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
from probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
using another actor
using another actor
from probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[2.277]
 [2.277]
 [2.277]
 [2.277]
 [2.277]] [[2.262]
 [2.262]
 [2.262]
 [2.262]
 [2.262]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[2.52]
 [2.52]
 [2.52]
 [2.52]
 [2.52]] [[1.89]
 [1.89]
 [1.89]
 [1.89]
 [1.89]]
siam score:  -0.93308794
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
from probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
siam score:  -0.9228554
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
using another actor
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.444]
 [0.45 ]
 [0.45 ]
 [0.433]] [[2.954]
 [2.108]
 [2.954]
 [2.954]
 [3.072]] [[2.095]
 [1.348]
 [2.095]
 [2.095]
 [2.169]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
siam score:  -0.93284744
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
siam score:  -0.932935
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.313]
 [0.408]
 [0.48 ]
 [0.322]] [[1.652]
 [2.424]
 [2.456]
 [1.966]
 [2.644]] [[0.25 ]
 [1.04 ]
 [1.201]
 [0.95 ]
 [1.211]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.492]
 [0.517]
 [0.492]
 [0.478]] [[3.44 ]
 [2.996]
 [2.242]
 [2.996]
 [2.602]] [[2.114]
 [2.118]
 [1.708]
 [2.118]
 [1.872]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.4686096038264473
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
using explorer policy with actor:  1
siam score:  -0.9259134
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
first move QE:  -0.08712147341682254
siam score:  -0.92410123
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
first move QE:  -0.08486694194635923
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
siam score:  -0.9260701
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
using another actor
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.05270781910709593, 0.5788203014881735, 0.3157640602976347, 0.05270781910709593]
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.122]
 [0.1  ]
 [0.104]
 [0.11 ]] [[0.053]
 [0.382]
 [0.538]
 [0.283]
 [0.253]] [[0.148]
 [0.535]
 [0.701]
 [0.368]
 [0.339]]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
siam score:  -0.92483383
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
siam score:  -0.93338513
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
first move QE:  -0.08193654349819089
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
siam score:  -0.932005
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[4.046]
 [4.046]
 [4.046]
 [4.046]
 [4.046]] [[1.975]
 [1.975]
 [1.975]
 [1.975]
 [1.975]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.114]
 [0.29 ]
 [0.17 ]
 [0.23 ]] [[ 2.661]
 [-0.134]
 [ 2.067]
 [ 1.875]
 [ 2.514]] [[1.887]
 [0.164]
 [1.691]
 [1.381]
 [1.839]]
siam score:  -0.93035895
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
1084 3442
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
siam score:  -0.9267159
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
first move QE:  -0.08082839848000793
1085 3443
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.338]
 [0.319]
 [0.319]
 [0.318]] [[3.133]
 [3.553]
 [3.139]
 [3.076]
 [3.58 ]] [[0.871]
 [1.128]
 [0.911]
 [0.881]
 [1.122]]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[3.722]
 [3.722]
 [3.722]
 [3.722]
 [3.722]] [[1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]]
in main func line 156:  1086
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
line 256 mcts: sample exp_bonus 3.1087774058966664
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
siam score:  -0.94448656
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.276]] [[2.603]
 [2.603]
 [2.603]
 [2.603]
 [2.076]] [[1.025]
 [1.025]
 [1.025]
 [1.025]
 [0.865]]
1090 3446
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.493309445266601
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
siam score:  -0.9419141
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
line 256 mcts: sample exp_bonus 1.693510934455612
1103 3462
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.08784970033098499, 0.5456858405729097, 0.31676777045194726, 0.04969668864415795]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.08784970033098499, 0.5456858405729097, 0.31676777045194726, 0.04969668864415795]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.08784970033098499, 0.5456858405729097, 0.31676777045194726, 0.04969668864415795]
siam score:  -0.9457114
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.08784970033098499, 0.5456858405729097, 0.31676777045194726, 0.04969668864415795]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09133136104689143, 0.567337277906217, 0.2896671597382771, 0.05166420130861431]
using another actor
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
siam score:  -0.9442853
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.336]
 [0.441]
 [0.174]
 [0.312]] [[2.605]
 [3.124]
 [0.926]
 [1.014]
 [2.188]] [[1.47 ]
 [1.887]
 [0.633]
 [0.157]
 [1.214]]
using another actor
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.377]
 [0.252]
 [0.377]
 [0.377]] [[1.64 ]
 [2.604]
 [0.662]
 [2.604]
 [2.604]] [[1.317]
 [1.903]
 [0.359]
 [1.903]
 [1.903]]
1110 3470
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
1111 3470
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
1112 3473
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.616]] [[3.042]
 [3.042]
 [3.042]
 [3.042]
 [3.18 ]] [[2.124]
 [2.124]
 [2.124]
 [2.124]
 [2.261]]
first move QE:  -0.06642693711939583
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
using explorer policy with actor:  1
1123 3476
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
Starting evaluation
line 256 mcts: sample exp_bonus 0.9356661051245218
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.026099999999999998 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.193]
 [0.177]
 [0.247]
 [0.242]] [[-0.077]
 [ 0.591]
 [ 0.531]
 [ 0.349]
 [-0.082]] [[0.244]
 [0.193]
 [0.177]
 [0.247]
 [0.242]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0281 0.25 0.25
maxi score, test score, baseline:  0.0281 0.25 0.25
probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.056100000000000004 0.25 0.25
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.376]
 [0.338]
 [0.22 ]
 [0.376]] [[1.845]
 [1.845]
 [1.672]
 [1.11 ]
 [1.845]] [[0.376]
 [0.376]
 [0.338]
 [0.22 ]
 [0.376]]
actor:  0 policy actor:  0  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.716]
 [0.664]
 [0.659]
 [0.663]] [[2.55 ]
 [2.431]
 [2.257]
 [2.549]
 [2.396]] [[0.711]
 [0.716]
 [0.664]
 [0.659]
 [0.663]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn probs:  [0.09510078995567982, 0.5907782620975044, 0.26032661400295465, 0.05379433394386111]
siam score:  -0.95259947
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0621 0.9 0.9
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.33 ]
 [0.245]
 [0.245]
 [0.245]] [[-0.531]
 [ 0.926]
 [ 0.877]
 [ 0.877]
 [ 0.877]] [[0.259]
 [0.33 ]
 [0.245]
 [0.245]
 [0.245]]
maxi score, test score, baseline:  0.0621 0.9 0.9
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.125 0.    0.833]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
Printing some Q and Qe and total Qs values:  [[1.074]
 [1.008]
 [1.036]
 [1.089]
 [1.032]] [[2.664]
 [2.252]
 [2.215]
 [2.174]
 [2.355]] [[2.723]
 [2.357]
 [2.385]
 [2.456]
 [2.462]]
maxi score, test score, baseline:  0.0641 0.9 0.9
maxi score, test score, baseline:  0.0641 0.9 0.9
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
siam score:  -0.95569366
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
first move QE:  -0.061069803207963534
from probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0661 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0661 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0661 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0661 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
siam score:  -0.95563966
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.506]
 [0.571]
 [0.506]
 [0.444]] [[1.124]
 [1.124]
 [2.035]
 [1.124]
 [1.899]] [[0.506]
 [0.506]
 [0.571]
 [0.506]
 [0.444]]
maxi score, test score, baseline:  0.0661 0.9 0.9
maxi score, test score, baseline:  0.0661 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
maxi score, test score, baseline:  0.0661 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.09516109852213492, 0.5906455832513032, 0.26032259343185765, 0.05387072479470425]
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
siam score:  -0.9555124
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
1156 3488
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
line 256 mcts: sample exp_bonus 1.9662797560000789
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
using another actor
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.421]] [[2.641]
 [2.641]
 [2.641]
 [2.641]
 [2.422]] [[0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.421]]
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
Printing some Q and Qe and total Qs values:  [[0.879]
 [0.816]
 [0.975]
 [0.879]
 [0.838]] [[1.57 ]
 [2.264]
 [1.911]
 [1.57 ]
 [1.952]] [[1.403]
 [1.739]
 [1.822]
 [1.403]
 [1.576]]
actor:  0 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13104301484583172, 0.567218627077782, 0.24999999999999997, 0.05173835807638626]
siam score:  -0.9230668
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.611]] [[2.738]
 [2.738]
 [2.738]
 [2.738]
 [1.988]] [[0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.611]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.7938],
        [0.8121],
        [0.7450],
        [0.4403],
        [0.8116],
        [0.7454],
        [0.8035],
        [0.2268],
        [0.2441],
        [0.0920]], dtype=torch.float64)
0.0 0.7938043956568619
0.0 0.8121333154140414
0.0 0.7450299319316974
0.0 0.44033445987333053
0.0 0.811552929602151
0.0 0.7454130661177435
0.0 0.8035446058216867
0.0 0.22679579182778795
0.0 0.24406631384216984
0.0 0.09202584318497761
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
1173 3499
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
Printing some Q and Qe and total Qs values:  [[1.103]
 [1.018]
 [1.095]
 [1.074]
 [1.018]] [[2.003]
 [1.158]
 [1.591]
 [1.605]
 [1.158]] [[2.506]
 [1.774]
 [2.215]
 [2.183]
 [1.774]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
in main func line 156:  1176
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
siam score:  -0.9289953
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.529]
 [0.648]
 [0.733]
 [0.733]] [[-1.119]
 [-0.764]
 [-1.031]
 [ 0.   ]
 [ 0.   ]] [[0.691]
 [0.98 ]
 [1.04 ]
 [1.896]
 [1.896]]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
in main func line 156:  1184
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.709]
 [0.758]
 [0.776]
 [0.851]] [[1.074]
 [1.27 ]
 [1.066]
 [0.865]
 [0.733]] [[1.352]
 [1.351]
 [1.314]
 [1.217]
 [1.279]]
1187 3502
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
from probs:  [0.13645147224956558, 0.5906455832513031, 0.219032219704427, 0.05387072479470424]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.9490976
Printing some Q and Qe and total Qs values:  [[0.76 ]
 [0.618]
 [0.833]
 [0.76 ]
 [0.76 ]] [[1.539]
 [1.427]
 [1.208]
 [1.539]
 [1.539]] [[1.654]
 [1.286]
 [1.483]
 [1.654]
 [1.654]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1607785251761589, 0.5890416043305962, 0.19646711510569537, 0.053712755387549535]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1607785251761589, 0.5890416043305962, 0.19646711510569537, 0.053712755387549535]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1607785251761589, 0.5890416043305962, 0.19646711510569537, 0.053712755387549535]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1607785251761589, 0.5890416043305962, 0.19646711510569537, 0.053712755387549535]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1607785251761589, 0.5890416043305962, 0.19646711510569537, 0.053712755387549535]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1607785251761589, 0.5890416043305962, 0.19646711510569537, 0.053712755387549535]
line 256 mcts: sample exp_bonus 3.1396625574867327
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.1607785251761589, 0.5890416043305962, 0.19646711510569537, 0.053712755387549535]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
siam score:  -0.940243
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.059]
 [0.527]
 [0.503]
 [0.549]] [[-1.071]
 [-0.584]
 [-0.16 ]
 [-0.137]
 [ 4.525]] [[0.225]
 [0.025]
 [0.412]
 [0.402]
 [1.417]]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
1201 3510
using another actor
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1552398568742798, 0.5687386632410587, 0.22415632460207635, 0.051865155282585076]
actor:  1 policy actor:  1  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.15138430143268894, 0.5787189952243704, 0.21712810047756298, 0.052768602865377746]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.15138430143268894, 0.5787189952243704, 0.21712810047756298, 0.052768602865377746]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.15138430143268894, 0.5787189952243704, 0.21712810047756298, 0.052768602865377746]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.15138430143268894, 0.5787189952243704, 0.21712810047756298, 0.052768602865377746]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.15138430143268894, 0.5787189952243704, 0.21712810047756298, 0.052768602865377746]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.15138430143268894, 0.5787189952243704, 0.21712810047756298, 0.052768602865377746]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.15138430143268894, 0.5787189952243704, 0.21712810047756298, 0.052768602865377746]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.15138430143268894, 0.5787189952243704, 0.21712810047756298, 0.052768602865377746]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.15138430143268894, 0.5787189952243704, 0.21712810047756298, 0.052768602865377746]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.15138430143268894, 0.5787189952243704, 0.21712810047756298, 0.052768602865377746]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.15138430143268894, 0.5787189952243704, 0.21712810047756298, 0.052768602865377746]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1446483784278408, 0.5961553280228089, 0.2048493050405032, 0.05434698850884713]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1446483784278408, 0.5961553280228089, 0.2048493050405032, 0.05434698850884713]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.14168911830898698, 0.6038155468573093, 0.19945492187752725, 0.05504041295617655]
siam score:  -0.9005182
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.14168911830898698, 0.6038155468573093, 0.19945492187752725, 0.05504041295617655]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.14168911830898698, 0.6038155468573093, 0.19945492187752725, 0.05504041295617655]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.14168911830898698, 0.6038155468573093, 0.19945492187752725, 0.05504041295617655]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.14168911830898698, 0.6038155468573093, 0.19945492187752725, 0.05504041295617655]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.14168911830898698, 0.6038155468573093, 0.19945492187752725, 0.05504041295617655]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.1389599545179974, 0.6108801478165086, 0.19447997725899868, 0.05567992040649541]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.20003118054334096, 0.5498129167399544, 0.20003118054334096, 0.050124722173363834]
siam score:  -0.91932875
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.20003118054334096, 0.5498129167399544, 0.20003118054334096, 0.050124722173363834]
using another actor
from probs:  [0.20003118054334096, 0.5498129167399544, 0.20003118054334096, 0.050124722173363834]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.20003118054334096, 0.5498129167399544, 0.20003118054334096, 0.050124722173363834]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.361]
 [0.399]
 [0.433]
 [0.377]] [[1.323]
 [1.586]
 [1.323]
 [1.256]
 [1.602]] [[0.399]
 [0.361]
 [0.399]
 [0.433]
 [0.377]]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.20003118054334096, 0.5498129167399544, 0.20003118054334096, 0.050124722173363834]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.20003118054334096, 0.5498129167399544, 0.20003118054334096, 0.050124722173363834]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.20003118054334096, 0.5498129167399544, 0.20003118054334096, 0.050124722173363834]
siam score:  -0.93338317
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.20003118054334096, 0.5498129167399544, 0.20003118054334096, 0.050124722173363834]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.20003118054334096, 0.5498129167399544, 0.20003118054334096, 0.050124722173363834]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.20003118054334096, 0.5498129167399544, 0.20003118054334096, 0.050124722173363834]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.116]
 [0.464]
 [0.389]
 [0.39 ]] [[ 0.   ]
 [-0.208]
 [ 0.147]
 [-0.208]
 [-0.489]] [[0.818]
 [0.232]
 [1.047]
 [0.778]
 [0.687]]
using another actor
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.20003118054334096, 0.5498129167399544, 0.20003118054334096, 0.050124722173363834]
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
probs:  [0.20003118054334096, 0.5498129167399544, 0.20003118054334096, 0.050124722173363834]
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.20003118054334096, 0.5498129167399544, 0.20003118054334096, 0.050124722173363834]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.20003118054334096, 0.5498129167399544, 0.20003118054334096, 0.050124722173363834]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1910737586833709, 0.5446312065831455, 0.21464425521002253, 0.049650779523461135]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1910737586833709, 0.5446312065831455, 0.21464425521002253, 0.049650779523461135]
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1910737586833709, 0.5446312065831455, 0.21464425521002253, 0.049650779523461135]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1910737586833709, 0.5446312065831455, 0.21464425521002253, 0.049650779523461135]
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1910737586833709, 0.5446312065831455, 0.21464425521002253, 0.049650779523461135]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1910737586833709, 0.5446312065831455, 0.21464425521002253, 0.049650779523461135]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1910737586833709, 0.5446312065831455, 0.21464425521002253, 0.049650779523461135]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1910737586833709, 0.5446312065831455, 0.21464425521002253, 0.049650779523461135]
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.827]
 [0.923]
 [0.827]
 [0.827]] [[1.339]
 [1.339]
 [2.298]
 [1.339]
 [1.339]] [[1.384]
 [1.384]
 [2.468]
 [1.384]
 [1.384]]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.599]
 [0.535]
 [0.535]
 [0.535]] [[1.702]
 [1.461]
 [1.702]
 [1.702]
 [1.702]] [[1.682]
 [1.57 ]
 [1.682]
 [1.682]
 [1.682]]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1866744653022125, 0.5320864727446899, 0.2327293996278761, 0.048509662325221525]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1866744653022125, 0.5320864727446899, 0.2327293996278761, 0.048509662325221525]
siam score:  -0.9351218
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1866744653022125, 0.5320864727446899, 0.2327293996278761, 0.048509662325221525]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1866744653022125, 0.5320864727446899, 0.2327293996278761, 0.048509662325221525]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1866744653022125, 0.5320864727446899, 0.2327293996278761, 0.048509662325221525]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1866744653022125, 0.5320864727446899, 0.2327293996278761, 0.048509662325221525]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1866744653022125, 0.5320864727446899, 0.2327293996278761, 0.048509662325221525]
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1866744653022125, 0.5320864727446899, 0.2327293996278761, 0.048509662325221525]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1866744653022125, 0.5320864727446899, 0.2327293996278761, 0.048509662325221525]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0721 0.9 0.9
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.20498220896898448, 0.5201067461860928, 0.22749110448449228, 0.04741994036043045]
siam score:  -0.9354486
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.20498220896898448, 0.5201067461860928, 0.22749110448449228, 0.04741994036043045]
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.20498220896898448, 0.5201067461860928, 0.22749110448449228, 0.04741994036043045]
siam score:  -0.9331335
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.20498220896898448, 0.5201067461860928, 0.22749110448449228, 0.04741994036043045]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.20498220896898448, 0.5201067461860928, 0.22749110448449228, 0.04741994036043045]
maxi score, test score, baseline:  0.0721 0.9 0.9
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2009026921908659, 0.5282180775850933, 0.2227237178838144, 0.048155512340226456]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2009026921908659, 0.5282180775850933, 0.2227237178838144, 0.048155512340226456]
maxi score, test score, baseline:  0.0721 0.9 0.9
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.638]
 [0.55 ]
 [0.434]
 [0.398]] [[0.102]
 [0.012]
 [0.102]
 [0.343]
 [0.382]] [[0.859]
 [1.004]
 [0.859]
 [0.707]
 [0.648]]
maxi score, test score, baseline:  0.0721 0.9 0.9
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.93519264
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.21796777301707704, 0.5169352248576915, 0.21796777301707704, 0.04712922910815447]
using explorer policy with actor:  1
from probs:  [0.21796777301707704, 0.5169352248576915, 0.21796777301707704, 0.04712922910815447]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.21796777301707704, 0.5169352248576915, 0.21796777301707704, 0.04712922910815447]
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.774]
 [0.762]
 [0.774]
 [0.774]] [[1.063]
 [1.063]
 [1.103]
 [1.063]
 [1.063]] [[0.774]
 [0.774]
 [0.762]
 [0.774]
 [0.774]]
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [1.   ]
 [1.   ]
 [0.85 ]
 [1.001]] [[0.02 ]
 [0.668]
 [0.825]
 [0.02 ]
 [0.062]] [[0.85 ]
 [1.   ]
 [1.   ]
 [0.85 ]
 [1.001]]
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2137142658089687, 0.5247348445892372, 0.2137142658089687, 0.04783662379282546]
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2137142658089687, 0.5247348445892372, 0.2137142658089687, 0.04783662379282546]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2137142658089687, 0.5247348445892372, 0.2137142658089687, 0.04783662379282546]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2137142658089687, 0.5247348445892372, 0.2137142658089687, 0.04783662379282546]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2137142658089687, 0.5247348445892372, 0.2137142658089687, 0.04783662379282546]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.2137142658089687, 0.5247348445892372, 0.2137142658089687, 0.04783662379282546]
using explorer policy with actor:  1
start point for exploration sampling:  20019
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.838]
 [0.942]
 [0.542]
 [0.797]] [[1.281]
 [1.834]
 [1.902]
 [1.173]
 [1.444]] [[0.829]
 [1.667]
 [1.911]
 [0.67 ]
 [1.337]]
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]]
Printing some Q and Qe and total Qs values:  [[1.178]
 [1.034]
 [1.228]
 [1.034]
 [1.141]] [[1.481]
 [1.646]
 [1.747]
 [1.646]
 [1.632]] [[2.096]
 [1.919]
 [2.374]
 [1.919]
 [2.124]]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2097008188793604, 0.5320942678444773, 0.2097008188793604, 0.04850409439680186]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2097008188793604, 0.5320942678444773, 0.2097008188793604, 0.04850409439680186]
maxi score, test score, baseline:  0.0721 0.9 0.9
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2097008188793604, 0.5320942678444773, 0.2097008188793604, 0.04850409439680186]
siam score:  -0.9272466
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2097008188793604, 0.5320942678444773, 0.2097008188793604, 0.04850409439680186]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2097008188793604, 0.5320942678444773, 0.2097008188793604, 0.04850409439680186]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2097008188793604, 0.5320942678444773, 0.2097008188793604, 0.04850409439680186]
siam score:  -0.9247444
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2059076671275785, 0.5390497377192075, 0.2059076671275785, 0.04913492802563548]
Printing some Q and Qe and total Qs values:  [[1.197]
 [1.197]
 [1.197]
 [1.197]
 [1.197]] [[1.478]
 [1.478]
 [1.478]
 [1.478]
 [1.478]] [[2.148]
 [2.148]
 [2.148]
 [2.148]
 [2.148]]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2059076671275785, 0.5390497377192075, 0.2059076671275785, 0.04913492802563548]
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
siam score:  -0.92241454
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2059076671275785, 0.5390497377192075, 0.2059076671275785, 0.04913492802563548]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2059076671275785, 0.5390497377192075, 0.2059076671275785, 0.04913492802563548]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.917499
first move QE:  -0.049540752081434894
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2019505759780556, 0.5286866593272773, 0.22117034558683335, 0.048192419107833666]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2019505759780556, 0.5286866593272773, 0.22117034558683335, 0.048192419107833666]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2019505759780556, 0.5286866593272773, 0.22117034558683335, 0.048192419107833666]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2019505759780556, 0.5286866593272773, 0.22117034558683335, 0.048192419107833666]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.514]
 [0.802]
 [0.726]
 [0.719]] [[2.516]
 [2.237]
 [2.19 ]
 [1.408]
 [1.964]] [[2.457]
 [1.929]
 [2.458]
 [1.524]
 [2.066]]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2019505759780556, 0.5286866593272773, 0.22117034558683335, 0.048192419107833666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2019505759780556, 0.5286866593272773, 0.22117034558683335, 0.048192419107833666]
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2019505759780556, 0.5286866593272773, 0.22117034558683335, 0.048192419107833666]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2019505759780556, 0.5286866593272773, 0.22117034558683335, 0.048192419107833666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2019505759780556, 0.5286866593272773, 0.22117034558683335, 0.048192419107833666]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2019505759780556, 0.5286866593272773, 0.22117034558683335, 0.048192419107833666]
siam score:  -0.9266343
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2019505759780556, 0.5286866593272773, 0.22117034558683335, 0.048192419107833666]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2019505759780556, 0.5286866593272773, 0.22117034558683335, 0.048192419107833666]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2019505759780556, 0.5286866593272773, 0.22117034558683335, 0.048192419107833666]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2019505759780556, 0.5286866593272773, 0.22117034558683335, 0.048192419107833666]
using another actor
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[2.048]
 [1.763]
 [1.763]
 [1.763]
 [1.763]] [[1.796]
 [1.501]
 [1.501]
 [1.501]
 [1.501]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.1985310191382258, 0.5354188938698385, 0.21724701217887096, 0.048803074813064666]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.21325604364594106, 0.5255796726554416, 0.21325604364594106, 0.04790824005267613]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.21325604364594106, 0.5255796726554416, 0.21325604364594106, 0.04790824005267613]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.21325604364594106, 0.5255796726554416, 0.21325604364594106, 0.04790824005267613]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2274495230123107, 0.5160956284547331, 0.20940914142215933, 0.047045707110796815]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2274495230123107, 0.5160956284547331, 0.20940914142215933, 0.047045707110796815]
siam score:  -0.9240914
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2274495230123107, 0.5160956284547331, 0.20940914142215933, 0.047045707110796815]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2274495230123107, 0.5160956284547331, 0.20940914142215933, 0.047045707110796815]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2274495230123107, 0.5160956284547331, 0.20940914142215933, 0.047045707110796815]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2274495230123107, 0.5160956284547331, 0.20940914142215933, 0.047045707110796815]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2274495230123107, 0.5160956284547331, 0.20940914142215933, 0.047045707110796815]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2274495230123107, 0.5160956284547331, 0.20940914142215933, 0.047045707110796815]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2274495230123107, 0.5160956284547331, 0.20940914142215933, 0.047045707110796815]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0721 0.9 0.9
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.241]
 [0.189]
 [0.224]
 [0.216]] [[ 0.175]
 [ 0.552]
 [ 0.118]
 [ 0.009]
 [-0.025]] [[0.189]
 [0.241]
 [0.189]
 [0.224]
 [0.216]]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2274495230123107, 0.5160956284547331, 0.20940914142215933, 0.047045707110796815]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.22360627030557603, 0.5227352068423808, 0.20601045050929342, 0.04764807234274968]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2331174163434214, 0.5201213385052565, 0.19935224903026433, 0.04740899612105758]
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2331174163434214, 0.5201213385052565, 0.19935224903026433, 0.04740899612105758]
siam score:  -0.9206355
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2331174163434214, 0.5201213385052565, 0.19935224903026433, 0.04740899612105758]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2331174163434214, 0.5201213385052565, 0.19935224903026433, 0.04740899612105758]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2331174163434214, 0.5201213385052565, 0.19935224903026433, 0.04740899612105758]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.2331174163434214, 0.5201213385052565, 0.19935224903026433, 0.04740899612105758]
Printing some Q and Qe and total Qs values:  [[1.008]
 [0.627]
 [1.06 ]
 [0.864]
 [0.884]] [[1.47 ]
 [0.501]
 [1.128]
 [1.279]
 [1.385]] [[2.312]
 [0.905]
 [2.188]
 [1.896]
 [2.008]]
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
start point for exploration sampling:  20019
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.756]
 [0.727]
 [0.678]
 [0.588]] [[1.336]
 [1.449]
 [1.206]
 [1.336]
 [1.556]] [[2.051]
 [2.275]
 [2.024]
 [2.051]
 [2.084]]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.23311741634342145, 0.5201213385052565, 0.19935224903026444, 0.04740899612105758]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.23311741634342145, 0.5201213385052565, 0.19935224903026444, 0.04740899612105758]
from probs:  [0.23311741634342145, 0.5201213385052565, 0.19935224903026444, 0.04740899612105758]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.22205993925351825, 0.5094434212173309, 0.22205993925351825, 0.046436700275632695]
Printing some Q and Qe and total Qs values:  [[0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]] [[1.705]
 [1.705]
 [1.705]
 [1.705]
 [1.705]] [[2.246]
 [2.246]
 [2.246]
 [2.246]
 [2.246]]
from probs:  [0.22205993925351825, 0.5094434212173309, 0.22205993925351825, 0.046436700275632695]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.421]
 [0.395]
 [0.385]
 [0.386]] [[1.026]
 [1.647]
 [0.942]
 [1.161]
 [1.479]] [[0.372]
 [0.421]
 [0.395]
 [0.385]
 [0.386]]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.22205993925351825, 0.5094434212173309, 0.22205993925351825, 0.046436700275632695]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.22205993925351825, 0.5094434212173309, 0.22205993925351825, 0.046436700275632695]
maxi score, test score, baseline:  0.0721 0.9 0.9
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.22205993925351825, 0.5094434212173309, 0.22205993925351825, 0.046436700275632695]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.22205993925351825, 0.5094434212173309, 0.22205993925351825, 0.046436700275632695]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.22205993925351825, 0.5094434212173309, 0.22205993925351825, 0.046436700275632695]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.22205993925351825, 0.5094434212173309, 0.22205993925351825, 0.046436700275632695]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.22205993925351825, 0.5094434212173309, 0.22205993925351825, 0.046436700275632695]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.22205993925351825, 0.5094434212173309, 0.22205993925351825, 0.046436700275632695]
using another actor
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.208 0.542 0.208]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.22205993925351825, 0.5094434212173309, 0.22205993925351825, 0.046436700275632695]
Printing some Q and Qe and total Qs values:  [[0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]] [[-0.774]
 [-0.774]
 [-0.774]
 [-0.774]
 [-0.774]] [[0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
in main func line 156:  1289
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.22205993925351825, 0.5094434212173309, 0.22205993925351825, 0.046436700275632695]
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.22205993925351825, 0.5094434212173309, 0.22205993925351825, 0.046436700275632695]
maxi score, test score, baseline:  0.0741 0.9 0.9
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]] [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.618]] [[1.753]
 [1.753]
 [1.753]
 [1.753]
 [1.551]] [[0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.618]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]] [[0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]] [[0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]]
rdn beta is 0 so we're just using the maxi policy
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.09136986231558616
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn probs:  [0.22205993925351825, 0.5094434212173309, 0.22205993925351825, 0.046436700275632695]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.493]
 [0.443]
 [0.21 ]
 [0.407]] [[2.12 ]
 [1.843]
 [1.96 ]
 [1.01 ]
 [2.997]] [[0.371]
 [0.493]
 [0.443]
 [0.21 ]
 [0.407]]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
actor:  0 policy actor:  0  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9125976
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9058423
siam score:  -0.9058614
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.91038775
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.6381023633462855
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9085661
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.5302],
        [0.7232],
        [0.1557],
        [0.8905],
        [0.6110],
        [0.3757],
        [0.1494],
        [0.3757],
        [0.3469],
        [0.8085]], dtype=torch.float64)
0.0 0.530154191859739
0.0 0.7231590209709289
0.0 0.155659578498948
0.0 0.8904953198621235
0.0 0.6110056061211245
0.0 0.3756939263469727
0.0 0.14941009277413364
0.0 0.3756939263469727
0.0 0.34694250839825297
0.0 0.8085160467501203
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.291]] [[3.547]
 [3.547]
 [3.547]
 [3.547]
 [2.363]] [[2.083]
 [2.083]
 [2.083]
 [2.083]
 [1.449]]
siam score:  -0.92033553
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]] [[2.92]
 [2.92]
 [2.92]
 [2.92]
 [2.92]] [[2.042]
 [2.042]
 [2.042]
 [2.042]
 [2.042]]
siam score:  -0.9115406
siam score:  -0.9117991
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9136059
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.343]
 [0.346]
 [0.342]
 [0.343]] [[3.331]
 [2.366]
 [2.607]
 [2.652]
 [2.697]] [[2.242]
 [1.352]
 [1.592]
 [1.628]
 [1.674]]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[2.306]
 [2.306]
 [2.306]
 [2.306]
 [2.306]] [[1.72]
 [1.72]
 [1.72]
 [1.72]
 [1.72]]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.91677564
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.247]
 [0.247]
 [0.247]
 [0.247]] [[2.334]
 [2.38 ]
 [2.38 ]
 [2.38 ]
 [2.38 ]] [[1.594]
 [1.648]
 [1.648]
 [1.648]
 [1.648]]
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.479]
 [0.483]
 [0.483]
 [0.483]] [[1.304]
 [1.649]
 [1.304]
 [1.304]
 [1.304]] [[1.163]
 [1.42 ]
 [1.163]
 [1.163]
 [1.163]]
1314 3584
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.293]
 [0.462]
 [0.395]
 [0.395]] [[2.215]
 [1.924]
 [1.763]
 [1.727]
 [1.727]] [[1.599]
 [1.468]
 [1.532]
 [1.438]
 [1.438]]
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[1.1]
 [1.1]
 [1.1]
 [1.1]
 [1.1]] [[1.46]
 [1.46]
 [1.46]
 [1.46]
 [1.46]]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.05096649433186969
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.759]
 [0.867]
 [0.759]
 [0.759]] [[1.649]
 [1.336]
 [0.966]
 [1.336]
 [1.336]] [[1.263]
 [1.539]
 [1.508]
 [1.539]
 [1.539]]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9121976
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2719457376986991
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.02 ]
 [1.139]
 [1.195]
 [1.081]
 [1.21 ]] [[1.156]
 [1.106]
 [1.21 ]
 [1.318]
 [1.56 ]] [[1.577]
 [1.781]
 [1.962]
 [1.806]
 [2.226]]
Printing some Q and Qe and total Qs values:  [[1.087]
 [1.087]
 [1.087]
 [0.943]
 [1.097]] [[1.172]
 [1.172]
 [1.172]
 [1.562]
 [2.156]] [[1.805]
 [1.805]
 [1.805]
 [1.777]
 [2.481]]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[1.48]
 [1.39]
 [1.39]
 [1.39]
 [1.39]] [[1.751]
 [1.696]
 [1.696]
 [1.696]
 [1.696]]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.05259638780027915
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.854]
 [0.854]
 [1.032]
 [0.854]
 [0.854]] [[1.493]
 [1.493]
 [1.39 ]
 [1.493]
 [1.493]] [[1.591]
 [1.591]
 [1.913]
 [1.591]
 [1.591]]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]] [[1.15]
 [1.15]
 [1.15]
 [1.15]
 [1.15]] [[1.805]
 [1.805]
 [1.805]
 [1.805]
 [1.805]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 2.1236110694622687
maxi score, test score, baseline:  0.1201 1.0 1.0
first move QE:  -0.05293222746976552
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.63 ]
 [0.797]
 [0.586]
 [0.582]] [[1.51 ]
 [1.475]
 [1.203]
 [1.522]
 [1.37 ]] [[2.051]
 [2.244]
 [2.217]
 [2.22 ]
 [2.018]]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.6407422882935399
first move QE:  -0.05503149724274848
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
siam score:  -0.9170454
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]] [[1.46 ]
 [1.46 ]
 [1.398]
 [1.46 ]
 [1.46 ]] [[2.472]
 [2.472]
 [2.409]
 [2.472]
 [2.472]]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.3381695097080377
first move QE:  -0.05527345203005111
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9229072
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.9069372
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.899426
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
1371 3631
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.498]
 [0.67 ]
 [0.59 ]
 [0.498]] [[0.939]
 [0.939]
 [1.094]
 [1.103]
 [0.939]] [[0.71 ]
 [0.71 ]
 [1.155]
 [1.003]
 [0.71 ]]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1241 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9000718
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.5954982685841739
maxi score, test score, baseline:  0.1241 1.0 1.0
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90063447
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  1380
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
siam score:  -0.9017776
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8960254
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
siam score:  -0.89806235
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.009]
 [0.333]
 [0.333]
 [0.333]] [[ 0.04 ]
 [-0.615]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[1.331]
 [0.017]
 [1.282]
 [1.282]
 [1.282]]
maxi score, test score, baseline:  0.1241 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.413]
 [0.488]
 [0.413]
 [0.413]] [[0.463]
 [0.463]
 [0.928]
 [0.463]
 [0.463]] [[1.066]
 [1.066]
 [1.371]
 [1.066]
 [1.066]]
1399 3646
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.208]
 [0.256]
 [0.234]
 [0.233]] [[-0.099]
 [ 0.014]
 [ 0.059]
 [-0.209]
 [-0.402]] [[0.233]
 [0.208]
 [0.256]
 [0.234]
 [0.233]]
maxi score, test score, baseline:  0.1241 1.0 1.0
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1281 1.0 1.0
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]] [[1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.291]] [[1.491]
 [1.491]
 [1.491]
 [1.491]
 [1.491]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]] [[1.955]
 [1.955]
 [1.955]
 [1.955]
 [1.955]] [[1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.779]] [[2.862]
 [2.862]
 [2.862]
 [2.862]
 [3.965]] [[1.537]
 [1.537]
 [1.537]
 [1.537]
 [2.056]]
Printing some Q and Qe and total Qs values:  [[0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.788]] [[2.888]
 [2.888]
 [2.888]
 [2.888]
 [3.641]] [[1.386]
 [1.386]
 [1.386]
 [1.386]
 [1.699]]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8892928
maxi score, test score, baseline:  0.1301 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.04931757872332866
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.51 ]
 [0.25 ]
 [0.393]
 [0.393]] [[0.069]
 [0.818]
 [0.948]
 [0.448]
 [0.448]] [[1.084]
 [1.798]
 [1.528]
 [1.382]
 [1.382]]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.579]
 [0.669]
 [0.064]
 [0.591]] [[1.409]
 [1.555]
 [1.535]
 [1.259]
 [1.679]] [[1.496]
 [1.798]
 [1.957]
 [0.473]
 [1.945]]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  1420
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1301 1.0 1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88314056
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1301 1.0 1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87476057
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8763277
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1321 1.0 1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1448 3678
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]] [[1.638]
 [1.638]
 [1.638]
 [1.638]
 [1.638]] [[1.663]
 [1.663]
 [1.663]
 [1.663]
 [1.663]]
maxi score, test score, baseline:  0.1321 1.0 1.0
1448 3691
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86544824
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.242]
 [0.226]
 [0.237]
 [0.216]] [[2.116]
 [2.201]
 [2.116]
 [1.996]
 [3.954]] [[0.905]
 [0.976]
 [0.905]
 [0.858]
 [1.876]]
maxi score, test score, baseline:  0.1341 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]] [[2.361]
 [2.361]
 [2.361]
 [2.361]
 [2.361]] [[1.929]
 [1.929]
 [1.929]
 [1.929]
 [1.929]]
first move QE:  -0.05112911999993101
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8836089
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.257]
 [0.257]
 [0.257]
 [0.301]] [[0.187]
 [5.534]
 [5.534]
 [5.534]
 [1.908]] [[0.433]
 [5.805]
 [5.805]
 [5.805]
 [2.203]]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1462 3698
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1341 1.0 1.0
maxi score, test score, baseline:  0.1341 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.    0.667 0.25 ]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1341 1.0 1.0
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.283]
 [0.283]
 [0.283]
 [0.283]] [[1.626]
 [0.9  ]
 [0.9  ]
 [0.9  ]
 [0.9  ]] [[1.406]
 [0.701]
 [0.701]
 [0.701]
 [0.701]]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1341 1.0 1.0
maxi score, test score, baseline:  0.1341 1.0 1.0
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.359]
 [0.472]
 [0.384]
 [0.413]] [[0.589]
 [0.51 ]
 [2.007]
 [1.823]
 [2.282]] [[0.508]
 [0.471]
 [1.512]
 [1.315]
 [1.629]]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
in main func line 156:  1477
maxi score, test score, baseline:  0.1341 1.0 1.0
Starting evaluation
maxi score, test score, baseline:  0.1341 1.0 1.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.839]
 [0.942]
 [0.839]
 [0.839]] [[2.3  ]
 [2.023]
 [2.057]
 [2.023]
 [2.023]] [[1.436]
 [1.568]
 [1.797]
 [1.568]
 [1.568]]
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87911165
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.191]
 [0.172]
 [0.187]
 [0.187]] [[3.131]
 [3.131]
 [2.699]
 [3.275]
 [3.148]] [[1.804]
 [1.804]
 [1.374]
 [1.928]
 [1.812]]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.882]
 [0.806]
 [0.996]
 [0.882]
 [0.836]] [[2.68 ]
 [2.639]
 [2.633]
 [2.68 ]
 [3.019]] [[2.172]
 [2.012]
 [2.32 ]
 [2.172]
 [2.375]]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
siam score:  -0.8726358
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8736031
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.728]
 [0.925]
 [0.728]
 [0.728]] [[2.611]
 [2.611]
 [0.729]
 [2.611]
 [2.611]] [[2.037]
 [2.037]
 [1.018]
 [2.037]
 [2.037]]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8633538
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]] [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.04433032675887697
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.489]
 [0.719]
 [0.419]
 [0.419]] [[1.542]
 [1.69 ]
 [1.433]
 [1.542]
 [1.542]] [[1.674]
 [1.916]
 [2.088]
 [1.674]
 [1.674]]
siam score:  -0.8455127
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.983]
 [0.983]
 [0.983]
 [0.983]
 [0.983]] [[0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]] [[2.259]
 [2.259]
 [2.259]
 [2.259]
 [2.259]]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85058427
siam score:  -0.8524399
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.328]
 [0.478]
 [0.343]
 [0.352]] [[0.754]
 [0.754]
 [0.966]
 [0.759]
 [0.521]] [[1.178]
 [1.178]
 [1.62 ]
 [1.212]
 [1.071]]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.462]
 [0.464]
 [0.436]
 [0.464]] [[-0.032]
 [ 0.585]
 [ 0.418]
 [ 0.609]
 [ 0.418]] [[0.681]
 [1.056]
 [1.004]
 [1.012]
 [1.004]]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1517 3726
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.188]
 [0.394]
 [0.237]
 [0.213]] [[3.106]
 [2.12 ]
 [2.355]
 [2.268]
 [2.536]] [[0.242]
 [0.188]
 [0.394]
 [0.237]
 [0.213]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1519 3728
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.319]] [[3.332]
 [3.332]
 [3.332]
 [3.332]
 [3.343]] [[1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.027]]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.697]
 [1.004]
 [0.636]
 [0.636]
 [0.636]] [[2.319]
 [1.266]
 [1.741]
 [1.741]
 [1.741]] [[2.368]
 [1.778]
 [1.701]
 [1.701]
 [1.701]]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.188]
 [1.188]
 [1.136]
 [1.188]
 [1.188]] [[1.681]
 [1.681]
 [0.886]
 [1.681]
 [1.681]] [[2.373]
 [2.373]
 [2.009]
 [2.373]
 [2.373]]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.573]
 [1.011]
 [1.011]
 [0.59 ]] [[1.76 ]
 [1.922]
 [1.889]
 [1.889]
 [1.82 ]] [[1.45 ]
 [1.674]
 [2.529]
 [2.529]
 [1.641]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[1.411]
 [1.411]
 [1.411]
 [1.411]
 [1.411]] [[1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.766]]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8568714
first move QE:  -0.04477572742131342
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.651]
 [0.727]
 [0.651]
 [0.599]] [[1.413]
 [1.413]
 [1.424]
 [1.413]
 [1.698]] [[1.972]
 [1.972]
 [2.094]
 [1.972]
 [2.175]]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.04585500941002577
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.04585500941002577
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1542 3749
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
siam score:  -0.86573577
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8671827
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.451]
 [0.281]
 [0.281]
 [0.281]] [[0.643]
 [0.756]
 [0.643]
 [0.643]
 [0.643]] [[1.139]
 [1.592]
 [1.139]
 [1.139]
 [1.139]]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.319]
 [0.314]
 [0.359]
 [0.324]] [[1.017]
 [1.884]
 [2.421]
 [1.375]
 [1.7  ]] [[0.333]
 [1.131]
 [1.63 ]
 [0.724]
 [0.965]]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
1552 3760
using explorer policy with actor:  1
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.333]
 [0.412]
 [0.405]
 [0.373]] [[0.896]
 [0.573]
 [0.469]
 [0.817]
 [0.989]] [[1.303]
 [0.925]
 [0.976]
 [1.281]
 [1.38 ]]
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.792]] [[4.019]
 [4.019]
 [4.019]
 [4.019]
 [4.02 ]] [[1.62]
 [1.62]
 [1.62]
 [1.62]
 [1.62]]
Printing some Q and Qe and total Qs values:  [[0.2  ]
 [0.2  ]
 [0.2  ]
 [0.193]
 [0.2  ]] [[3.169]
 [3.169]
 [3.169]
 [3.375]
 [3.169]] [[1.782]
 [1.782]
 [1.782]
 [1.948]
 [1.782]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.419]
 [0.371]
 [0.371]
 [0.391]] [[2.652]
 [3.941]
 [2.652]
 [2.652]
 [3.169]] [[1.128]
 [2.082]
 [1.128]
 [1.128]
 [1.512]]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
siam score:  -0.85879403
maxi score, test score, baseline:  0.1901 1.0 1.0
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8580376
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[3.392]
 [3.392]
 [3.392]
 [3.392]
 [3.392]] [[2.165]
 [2.165]
 [2.165]
 [2.165]
 [2.165]]
line 256 mcts: sample exp_bonus 3.3752059864630817
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
siam score:  -0.86998767
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.761]
 [0.813]
 [0.721]
 [0.651]] [[1.737]
 [1.727]
 [1.447]
 [1.423]
 [1.787]] [[1.546]
 [1.663]
 [1.58 ]
 [1.38 ]
 [1.482]]
using explorer policy with actor:  1
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.1901 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.542 0.208 0.208 0.042 0.   ]
maxi score, test score, baseline:  0.1901 1.0 1.0
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8657658
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1921 1.0 1.0
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.801]
 [1.096]
 [0.801]
 [0.801]] [[0.892]
 [0.892]
 [0.716]
 [0.892]
 [0.892]] [[1.154]
 [1.154]
 [1.628]
 [1.154]
 [1.154]]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.   ]
 [1.065]
 [1.102]
 [0.964]
 [0.884]] [[1.259]
 [0.786]
 [0.779]
 [1.187]
 [1.495]] [[1.81 ]
 [1.626]
 [1.695]
 [1.69 ]
 [1.736]]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.9440838764563264
maxi score, test score, baseline:  0.1921 1.0 1.0
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.651]
 [0.806]
 [0.595]
 [0.595]] [[ 1.072]
 [-0.086]
 [ 0.879]
 [ 1.428]
 [ 1.428]] [[2.119]
 [0.443]
 [2.04 ]
 [2.349]
 [2.349]]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1941 1.0 1.0
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.049696281434427274
maxi score, test score, baseline:  0.1941 1.0 1.0
maxi score, test score, baseline:  0.1941 1.0 1.0
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.82 ]
 [0.822]
 [0.833]
 [0.781]] [[0.925]
 [0.452]
 [0.753]
 [0.925]
 [0.88 ]] [[0.833]
 [0.82 ]
 [0.822]
 [0.833]
 [0.781]]
maxi score, test score, baseline:  0.1941 1.0 1.0
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1961 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.074]
 [1.195]
 [1.196]
 [0.984]
 [0.984]] [[0.914]
 [0.515]
 [0.47 ]
 [0.871]
 [0.871]] [[1.875]
 [1.718]
 [1.674]
 [1.652]
 [1.652]]
Printing some Q and Qe and total Qs values:  [[0.944]
 [1.029]
 [1.182]
 [1.029]
 [1.029]] [[1.256]
 [0.805]
 [0.783]
 [0.805]
 [0.805]] [[1.765]
 [1.502]
 [1.768]
 [1.502]
 [1.502]]
siam score:  -0.86096704
maxi score, test score, baseline:  0.1961 1.0 1.0
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2021 1.0 1.0
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  1606
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.259]
 [0.326]
 [0.401]
 [0.34 ]] [[2.356]
 [1.705]
 [2.419]
 [1.833]
 [1.476]] [[1.407]
 [0.968]
 [1.594]
 [1.272]
 [0.919]]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2021 1.0 1.0
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8607883
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.521]
 [0.632]
 [0.6  ]
 [0.575]] [[1.178]
 [1.235]
 [0.928]
 [1.178]
 [0.963]] [[1.327]
 [1.205]
 [1.223]
 [1.327]
 [1.134]]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.813]
 [0.317]
 [0.545]
 [0.595]] [[1.045]
 [1.206]
 [1.283]
 [2.716]
 [0.895]] [[0.986]
 [1.398]
 [0.859]
 [2.296]
 [0.883]]
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.68 ]
 [0.68 ]
 [0.589]
 [0.68 ]] [[1.568]
 [1.568]
 [1.568]
 [1.098]
 [1.568]] [[2.463]
 [2.463]
 [2.463]
 [1.653]
 [2.463]]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.381]
 [0.583]
 [0.658]
 [0.54 ]] [[2.042]
 [2.547]
 [2.622]
 [2.72 ]
 [2.332]] [[1.447]
 [1.326]
 [1.78 ]
 [1.996]
 [1.501]]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.2138004276641636
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.05452393968652154
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.054249983736652484
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]] [[3.317]
 [3.317]
 [3.317]
 [3.317]
 [3.317]] [[1.294]
 [1.294]
 [1.294]
 [1.294]
 [1.294]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.399]] [[1.583]
 [1.583]
 [1.583]
 [1.583]
 [1.864]] [[1.423]
 [1.423]
 [1.423]
 [1.423]
 [1.552]]
maxi score, test score, baseline:  0.2021 1.0 1.0
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.511]
 [0.982]
 [0.487]
 [0.739]] [[2.396]
 [2.364]
 [2.151]
 [5.135]
 [2.784]] [[2.266]
 [1.62 ]
 [2.349]
 [4.342]
 [2.496]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.47 ]
 [0.26 ]
 [0.871]
 [0.607]] [[1.983]
 [1.943]
 [1.049]
 [3.415]
 [1.531]] [[1.395]
 [1.143]
 [0.468]
 [2.31 ]
 [1.072]]
maxi score, test score, baseline:  0.2021 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.708]
 [0.598]
 [0.649]
 [0.614]] [[0.317]
 [0.974]
 [0.745]
 [0.411]
 [0.911]] [[0.531]
 [1.558]
 [1.113]
 [0.823]
 [1.328]]
Printing some Q and Qe and total Qs values:  [[0.8  ]
 [0.652]
 [0.9  ]
 [0.851]
 [0.735]] [[0.845]
 [1.029]
 [0.553]
 [0.664]
 [1.273]] [[0.929]
 [0.898]
 [0.812]
 [0.843]
 [1.205]]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.528]] [[2.227]
 [2.227]
 [2.227]
 [2.227]
 [3.732]] [[1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.947]]
using explorer policy with actor:  1
siam score:  -0.84963346
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.3478],
        [0.7038],
        [0.2253],
        [0.7990],
        [0.2428],
        [0.4737],
        [0.7109],
        [0.7481],
        [0.0000],
        [0.0000]], dtype=torch.float64)
0.0 0.3478110048028988
0.0 0.7037670856175869
0.0 0.22527299593676767
0.0 0.7989868405992504
0.0 0.2428399228913254
0.0 0.47369439205295455
0.0 0.710907791340751
0.0 0.7480747712059708
0.0 0.0
0.0 0.0
siam score:  -0.84890336
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2021 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2021 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.539]
 [0.787]
 [0.7  ]
 [0.555]] [[0.765]
 [1.091]
 [0.591]
 [0.373]
 [1.346]] [[1.105]
 [1.019]
 [1.349]
 [1.101]
 [1.137]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]] [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[1.949]
 [1.949]
 [1.949]
 [1.949]
 [1.949]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.236]
 [0.006]
 [0.229]
 [0.225]] [[-0.133]
 [ 0.434]
 [ 0.044]
 [-0.322]
 [-0.016]] [[0.23 ]
 [0.236]
 [0.006]
 [0.229]
 [0.225]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.2021 1.0 1.0
siam score:  -0.84622496
maxi score, test score, baseline:  0.2021 1.0 1.0
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.05293511089955352
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.672]
 [0.786]
 [0.738]
 [0.738]] [[1.14 ]
 [1.366]
 [1.181]
 [1.14 ]
 [1.14 ]] [[0.738]
 [0.672]
 [0.786]
 [0.738]
 [0.738]]
UNIT TEST: sample policy line 217 mcts : [0.458 0.042 0.125 0.167 0.208]
maxi score, test score, baseline:  0.2021 1.0 1.0
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8388734
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.867]
 [1.029]
 [1.098]
 [0.988]
 [1.029]] [[1.481]
 [1.55 ]
 [1.457]
 [1.483]
 [1.55 ]] [[2.043]
 [2.42 ]
 [2.464]
 [2.277]
 [2.42 ]]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.4652198744725613
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8495952
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.763]
 [0.749]
 [0.612]
 [0.706]] [[0.529]
 [1.274]
 [1.133]
 [1.071]
 [1.144]] [[0.112]
 [0.763]
 [0.749]
 [0.612]
 [0.706]]
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]] [[1.454]
 [1.454]
 [1.454]
 [1.454]
 [1.454]] [[0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.443]
 [0.443]
 [0.424]
 [0.443]] [[1.108]
 [1.108]
 [1.108]
 [1.088]
 [1.108]] [[1.917]
 [1.917]
 [1.917]
 [1.852]
 [1.917]]
first move QE:  -0.05182566502596133
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.428]
 [0.557]
 [0.557]
 [0.557]] [[1.384]
 [2.003]
 [1.384]
 [1.384]
 [1.384]] [[1.473]
 [1.954]
 [1.473]
 [1.473]
 [1.473]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.612]
 [0.478]
 [0.478]
 [0.478]] [[0.204]
 [0.984]
 [0.204]
 [0.204]
 [0.204]] [[1.178]
 [1.832]
 [1.178]
 [1.178]
 [1.178]]
start point for exploration sampling:  20019
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.85196686
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.121144192731959
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.671]
 [0.907]
 [0.671]
 [0.671]] [[0.52 ]
 [0.52 ]
 [0.105]
 [0.52 ]
 [0.52 ]] [[1.594]
 [1.594]
 [1.927]
 [1.594]
 [1.594]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.5398341619711964
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2481 1.0 1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8363417
maxi score, test score, baseline:  0.2481 1.0 1.0
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.05412780256636499
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1697 3830
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.602]
 [0.822]
 [0.602]
 [0.602]] [[1.444]
 [1.444]
 [1.478]
 [1.444]
 [1.444]] [[1.339]
 [1.339]
 [1.8  ]
 [1.339]
 [1.339]]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  1699
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.244]
 [0.244]
 [0.2  ]
 [0.244]] [[1.037]
 [1.037]
 [1.037]
 [1.933]
 [1.037]] [[0.989]
 [0.989]
 [0.989]
 [1.953]
 [0.989]]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.227]
 [0.191]
 [0.21 ]
 [0.199]] [[2.029]
 [0.963]
 [1.743]
 [4.651]
 [1.816]] [[0.723]
 [0.203]
 [0.558]
 [1.994]
 [0.6  ]]
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.225]
 [0.225]
 [0.224]
 [0.225]] [[2.308]
 [2.308]
 [2.308]
 [2.466]
 [2.308]] [[0.736]
 [0.736]
 [0.736]
 [0.811]
 [0.736]]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]] [[1.708]
 [1.708]
 [1.708]
 [1.708]
 [1.708]] [[1.834]
 [1.834]
 [1.834]
 [1.834]
 [1.834]]
maxi score, test score, baseline:  0.2561 1.0 1.0
siam score:  -0.8362899
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.83266187
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.464]
 [0.464]
 [0.458]
 [0.462]] [[ 0.414]
 [ 0.414]
 [ 0.414]
 [-0.831]
 [-0.284]] [[1.843]
 [1.843]
 [1.843]
 [0.416]
 [1.046]]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.752]
 [0.676]
 [0.676]
 [0.676]] [[0.8  ]
 [0.993]
 [0.8  ]
 [0.8  ]
 [0.8  ]] [[1.553]
 [1.964]
 [1.553]
 [1.553]
 [1.553]]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[1.791]
 [1.791]
 [1.791]
 [1.791]
 [1.791]] [[1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
maxi score, test score, baseline:  0.2601 1.0 1.0
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8467143
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.92541503274916
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84257346
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [0.485]
 [0.414]
 [0.405]] [[3.475]
 [3.475]
 [4.443]
 [3.475]
 [4.937]] [[0.414]
 [0.414]
 [0.485]
 [0.414]
 [0.405]]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2641 1.0 1.0
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2641 1.0 1.0
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8294495
siam score:  -0.8303874
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.04987890504391632
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[5.325]
 [5.325]
 [5.325]
 [5.325]
 [5.325]] [[2.078]
 [2.078]
 [2.078]
 [2.078]
 [2.078]]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.424]
 [0.423]
 [0.423]
 [0.416]] [[6.113]
 [5.58 ]
 [6.497]
 [6.497]
 [6.299]] [[2.02 ]
 [1.718]
 [2.172]
 [2.172]
 [2.064]]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.401]
 [0.401]
 [0.401]
 [0.401]] [[6.376]
 [5.509]
 [5.509]
 [5.509]
 [5.509]] [[2.016]
 [1.626]
 [1.626]
 [1.626]
 [1.626]]
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.488]
 [0.598]
 [0.511]
 [0.422]] [[3.872]
 [4.398]
 [3.624]
 [3.657]
 [3.75 ]] [[1.797]
 [2.05 ]
 [1.686]
 [1.593]
 [1.537]]
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.34 ]
 [0.34 ]
 [0.339]
 [0.34 ]] [[4.428]
 [4.428]
 [4.428]
 [4.07 ]
 [4.428]] [[1.922]
 [1.922]
 [1.922]
 [1.739]
 [1.922]]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.327]
 [0.348]
 [0.327]
 [0.327]] [[3.451]
 [3.42 ]
 [4.657]
 [3.42 ]
 [3.42 ]] [[1.22 ]
 [1.245]
 [2.031]
 [1.245]
 [1.245]]
maxi score, test score, baseline:  0.2661 1.0 1.0
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.2681 1.0 1.0
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  1755
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2681 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.485]
 [0.603]
 [0.527]
 [0.56 ]] [[2.938]
 [2.247]
 [2.415]
 [3.177]
 [2.673]] [[1.161]
 [1.036]
 [1.329]
 [1.43 ]
 [1.328]]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1761 3852
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2681 1.0 1.0
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2681 1.0 1.0
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7900926775415333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2681 1.0 1.0
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8101223
siam score:  -0.8112146
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1770 3858
maxi score, test score, baseline:  0.2701 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.9805997618639064
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.9167099700857606
maxi score, test score, baseline:  0.2701 1.0 1.0
maxi score, test score, baseline:  0.2701 1.0 1.0
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.611]] [[3.619]
 [3.619]
 [3.619]
 [3.619]
 [3.865]] [[2.141]
 [2.141]
 [2.141]
 [2.141]
 [2.277]]
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.244]
 [0.244]
 [0.262]
 [0.278]] [[1.403]
 [1.403]
 [1.403]
 [1.691]
 [1.921]] [[1.025]
 [1.025]
 [1.025]
 [1.4  ]
 [1.703]]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.625 0.25  0.042 0.042]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.562]
 [0.622]
 [0.599]
 [0.574]] [[2.189]
 [1.837]
 [1.522]
 [2.189]
 [1.557]] [[1.974]
 [1.665]
 [1.576]
 [1.974]
 [1.503]]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80799234
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.255]
 [0.615]
 [0.557]
 [0.491]] [[2.813]
 [1.813]
 [2.467]
 [2.71 ]
 [2.694]] [[1.475]
 [0.54 ]
 [1.697]
 [1.742]
 [1.6  ]]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3495651510476423
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8053679
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2761 1.0 1.0
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.2781 1.0 1.0
siam score:  -0.81676024
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.027236196937481374
start point for exploration sampling:  20019
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.571]
 [0.732]
 [0.643]
 [0.558]] [[2.602]
 [2.855]
 [2.274]
 [3.184]
 [2.831]] [[1.647]
 [1.809]
 [1.746]
 [2.174]
 [1.769]]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8204417
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.583 0.292]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.398]
 [0.398]
 [0.379]
 [0.374]] [[3.605]
 [3.605]
 [3.605]
 [3.13 ]
 [3.118]] [[1.974]
 [1.974]
 [1.974]
 [1.684]
 [1.672]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1821 3879
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.814324
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.477]
 [0.682]
 [0.477]
 [0.477]] [[1.507]
 [1.507]
 [1.17 ]
 [1.507]
 [1.507]] [[0.477]
 [0.477]
 [0.682]
 [0.477]
 [0.477]]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 7.820287789807276
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.343]
 [0.264]
 [0.264]
 [0.264]] [[1.4  ]
 [0.808]
 [1.274]
 [1.274]
 [1.274]] [[1.591]
 [1.267]
 [1.575]
 [1.575]
 [1.575]]
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.576]
 [0.584]
 [0.584]
 [0.583]] [[1.382]
 [1.377]
 [1.382]
 [1.382]
 [1.627]] [[1.54 ]
 [1.519]
 [1.54 ]
 [1.54 ]
 [1.783]]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.535]
 [0.678]
 [0.631]
 [0.571]] [[1.657]
 [1.923]
 [1.127]
 [1.261]
 [1.513]] [[1.663]
 [1.675]
 [1.43 ]
 [1.426]
 [1.473]]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.188]
 [0.249]
 [0.188]
 [0.188]] [[2.439]
 [2.439]
 [2.562]
 [2.439]
 [2.439]] [[1.104]
 [1.104]
 [1.31 ]
 [1.104]
 [1.104]]
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.542 0.25  0.208]
1848 3888
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.543]
 [0.646]
 [0.543]
 [0.534]] [[1.018]
 [0.673]
 [0.961]
 [0.673]
 [1.458]] [[0.425]
 [0.543]
 [0.646]
 [0.543]
 [0.534]]
first move QE:  -0.013980659139495925
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
1850 3889
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.622]
 [0.798]
 [0.676]
 [0.676]] [[0.688]
 [1.078]
 [0.504]
 [0.688]
 [0.688]] [[1.48 ]
 [1.501]
 [1.662]
 [1.48 ]
 [1.48 ]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3341 1.0 1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.97 ]
 [1.201]
 [1.157]
 [1.201]
 [1.201]] [[0.91 ]
 [1.012]
 [1.019]
 [1.012]
 [1.012]] [[2.034]
 [2.597]
 [2.516]
 [2.597]
 [2.597]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.292 0.25  0.333 0.083]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1861 3899
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  1862
Printing some Q and Qe and total Qs values:  [[1.015]
 [1.015]
 [1.288]
 [1.252]
 [1.015]] [[0.765]
 [0.765]
 [0.633]
 [0.866]
 [0.765]] [[1.538]
 [1.538]
 [1.908]
 [2.147]
 [1.538]]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[3.109]
 [3.109]
 [3.109]
 [3.109]
 [3.109]] [[2.007]
 [2.007]
 [2.007]
 [2.007]
 [2.007]]
in main func line 156:  1864
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.046]
 [1.046]
 [1.258]
 [1.046]
 [1.046]] [[0.782]
 [0.782]
 [0.569]
 [0.782]
 [0.782]] [[1.699]
 [1.699]
 [1.913]
 [1.699]
 [1.699]]
maxi score, test score, baseline:  0.3361 1.0 1.0
siam score:  -0.8115283
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.653]
 [0.768]
 [0.681]
 [0.681]] [[1.211]
 [1.395]
 [1.128]
 [1.211]
 [1.211]] [[1.609]
 [1.676]
 [1.728]
 [1.609]
 [1.609]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.8765379752135503
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
line 256 mcts: sample exp_bonus 2.7857446065719156
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.257]
 [0.307]
 [0.288]
 [0.248]] [[2.285]
 [1.921]
 [1.859]
 [1.172]
 [1.8  ]] [[1.624]
 [1.22 ]
 [1.257]
 [0.532]
 [1.08 ]]
in main func line 156:  1882
siam score:  -0.81947356
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.876]
 [0.814]
 [1.169]
 [1.028]
 [0.979]] [[1.352]
 [1.204]
 [0.573]
 [0.808]
 [1.147]] [[1.469]
 [1.295]
 [1.794]
 [1.591]
 [1.607]]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]] [[1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]] [[1.645]
 [1.645]
 [1.645]
 [1.645]
 [1.645]]
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.936]
 [1.085]
 [0.936]
 [0.936]] [[1.111]
 [0.866]
 [0.92 ]
 [0.866]
 [0.866]] [[1.51 ]
 [1.656]
 [1.99 ]
 [1.656]
 [1.656]]
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.805]
 [0.771]
 [0.816]
 [0.761]] [[0.933]
 [1.068]
 [1.27 ]
 [1.048]
 [0.933]] [[1.603]
 [1.827]
 [1.96 ]
 [1.828]
 [1.603]]
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.767]
 [0.791]
 [0.791]
 [0.674]] [[1.844]
 [1.441]
 [1.844]
 [1.844]
 [1.661]] [[2.243]
 [1.835]
 [2.243]
 [2.243]
 [1.867]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.82147604
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7036304606102972
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 1.8346051269303323
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.253]
 [0.28 ]
 [0.283]
 [0.301]] [[2.662]
 [2.602]
 [1.633]
 [2.744]
 [3.749]] [[1.425]
 [1.356]
 [0.763]
 [1.475]
 [2.134]]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.291]
 [0.313]
 [0.291]
 [0.306]] [[2.498]
 [2.395]
 [1.944]
 [2.395]
 [2.862]] [[1.375]
 [1.264]
 [0.874]
 [1.264]
 [1.722]]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.84 ]
 [0.978]
 [0.84 ]
 [0.84 ]
 [0.607]] [[0.   ]
 [0.311]
 [0.   ]
 [0.   ]
 [0.185]] [[1.204]
 [1.585]
 [1.204]
 [1.204]
 [0.802]]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3381 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80785125
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3401 1.0 1.0
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.085]
 [1.207]
 [1.083]
 [0.747]
 [1.06 ]] [[0.837]
 [0.685]
 [1.002]
 [2.623]
 [1.042]] [[1.321]
 [1.343]
 [1.437]
 [2.234]
 [1.441]]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.104]
 [0.657]
 [0.46 ]
 [0.509]] [[0.685]
 [0.586]
 [0.893]
 [0.743]
 [0.877]] [[0.959]
 [0.32 ]
 [1.6  ]
 [1.098]
 [1.332]]
maxi score, test score, baseline:  0.3421 1.0 1.0
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3421 1.0 1.0
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81304306
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 5.022899399483149
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.6868948023647259
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8233998
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.273500338525329
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.8133061
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.077]
 [1.362]
 [1.248]
 [1.152]
 [1.238]] [[1.017]
 [0.172]
 [0.331]
 [0.537]
 [0.565]] [[2.206]
 [2.458]
 [2.304]
 [2.196]
 [2.356]]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81141126
siam score:  -0.81050247
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8160743
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.51 ]
 [0.625]
 [0.637]
 [0.613]] [[1.062]
 [1.404]
 [1.068]
 [1.062]
 [1.18 ]] [[2.073]
 [2.252]
 [2.058]
 [2.073]
 [2.17 ]]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8184607
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.502]
 [0.55 ]
 [0.489]
 [0.496]] [[0.125]
 [0.86 ]
 [0.552]
 [0.007]
 [0.22 ]] [[0.931]
 [1.491]
 [1.382]
 [0.898]
 [1.052]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81486875
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81813073
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.724]
 [0.854]
 [0.694]
 [0.724]] [[1.664]
 [1.664]
 [1.598]
 [2.754]
 [1.664]] [[1.656]
 [1.656]
 [1.799]
 [2.411]
 [1.656]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.017]
 [ 1.269]
 [-0.017]
 [ 1.269]
 [ 1.269]] [[0.   ]
 [1.286]
 [0.   ]
 [1.286]
 [1.286]]
siam score:  -0.8164988
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.792 0.083 0.042 0.042]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.82296556
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.016104586355892143
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81101644
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.223]
 [1.355]
 [1.17 ]
 [1.157]
 [1.086]] [[0.877]
 [0.192]
 [0.888]
 [0.908]
 [1.107]] [[2.108]
 [1.686]
 [2.012]
 [2.007]
 [2.064]]
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.327]
 [0.436]
 [0.331]
 [0.331]] [[ 0.282]
 [ 0.762]
 [-0.119]
 [ 0.282]
 [ 0.282]] [[0.331]
 [0.327]
 [0.436]
 [0.331]
 [0.331]]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.333 0.333 0.083 0.208]
first move QE:  -0.017984770529874936
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.894]
 [0.894]
 [0.926]
 [0.894]
 [0.894]] [[0.914]
 [0.914]
 [0.722]
 [0.914]
 [0.914]] [[1.624]
 [1.624]
 [1.561]
 [1.624]
 [1.624]]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.81763613
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8184305
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
siam score:  -0.80717963
first move QE:  -0.017626054283831603
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.567]
 [0.567]
 [0.551]
 [0.475]] [[1.226]
 [0.885]
 [0.885]
 [0.894]
 [1.798]] [[1.231]
 [1.129]
 [1.129]
 [1.102]
 [1.554]]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.714]
 [0.853]
 [0.714]
 [0.647]] [[1.741]
 [1.741]
 [1.255]
 [1.741]
 [1.4  ]] [[1.934]
 [1.934]
 [1.888]
 [1.934]
 [1.572]]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.049]
 [1.089]
 [1.054]
 [1.049]
 [1.049]] [[0.623]
 [0.379]
 [0.871]
 [0.623]
 [0.623]] [[1.739]
 [1.737]
 [1.83 ]
 [1.739]
 [1.739]]
first move QE:  -0.01732929064424159
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.586]
 [0.469]
 [0.469]
 [0.469]] [[0.583]
 [1.208]
 [0.583]
 [0.583]
 [0.583]] [[1.289]
 [1.831]
 [1.289]
 [1.289]
 [1.289]]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.502]
 [0.403]
 [0.403]
 [0.403]] [[0.574]
 [0.99 ]
 [0.574]
 [0.574]
 [0.574]] [[1.482]
 [2.117]
 [1.482]
 [1.482]
 [1.482]]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[1.657]
 [1.657]
 [1.657]
 [1.657]
 [1.657]] [[1.782]
 [1.782]
 [1.782]
 [1.782]
 [1.782]]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.276]
 [0.276]
 [0.268]
 [0.283]] [[1.375]
 [1.375]
 [1.375]
 [1.578]
 [1.731]] [[1.19 ]
 [1.19 ]
 [1.19 ]
 [1.376]
 [1.559]]
maxi score, test score, baseline:  0.3581 1.0 1.0
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.317]
 [0.055]
 [0.257]
 [0.241]] [[-0.199]
 [ 0.052]
 [-0.433]
 [-0.594]
 [-0.263]] [[0.274]
 [0.317]
 [0.055]
 [0.257]
 [0.241]]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
maxi score, test score, baseline:  0.3601 1.0 1.0
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
using explorer policy with actor:  1
siam score:  -0.80237275
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.451]
 [0.594]
 [0.469]
 [0.453]] [[-0.954]
 [-0.777]
 [-0.402]
 [-1.018]
 [-1.013]] [[0.609]
 [0.758]
 [1.295]
 [0.633]
 [0.605]]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.394]] [[3.144]
 [3.144]
 [3.144]
 [3.144]
 [3.65 ]] [[1.666]
 [1.666]
 [1.666]
 [1.666]
 [1.898]]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80950403
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8073733
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.546]
 [0.498]
 [0.498]
 [0.498]] [[1.26 ]
 [1.119]
 [1.26 ]
 [1.26 ]
 [1.26 ]] [[0.498]
 [0.546]
 [0.498]
 [0.498]
 [0.498]]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.754]
 [0.249]
 [0.588]
 [0.704]] [[ 0.733]
 [ 0.575]
 [-0.016]
 [ 0.397]
 [ 0.205]] [[0.6  ]
 [1.685]
 [0.281]
 [1.235]
 [1.338]]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2028 3992
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80450785
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.51]
 [1.5 ]
 [1.5 ]
 [1.5 ]
 [1.5 ]] [[-0.845]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.725]
 [3.267]
 [3.267]
 [3.267]
 [3.267]]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0751971110114116
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.16490831699862613
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80900085
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.545]
 [0.776]
 [0.602]
 [0.602]] [[1.653]
 [1.87 ]
 [1.093]
 [1.653]
 [1.653]] [[1.751]
 [1.856]
 [1.539]
 [1.751]
 [1.751]]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]] [[1.065]
 [1.065]
 [1.065]
 [1.065]
 [1.065]] [[1.494]
 [1.494]
 [1.494]
 [1.494]
 [1.494]]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8028169
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8053479
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.328]
 [1.328]
 [1.328]
 [1.328]
 [1.328]] [[0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]] [[2.863]
 [2.863]
 [2.863]
 [2.863]
 [2.863]]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8035382
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.018802089180811013
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.43 ]
 [0.408]
 [0.408]
 [0.408]] [[1.696]
 [1.195]
 [1.194]
 [1.194]
 [1.194]] [[2.094]
 [1.715]
 [1.676]
 [1.676]
 [1.676]]
Printing some Q and Qe and total Qs values:  [[0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]] [[1.1]
 [1.1]
 [1.1]
 [1.1]
 [1.1]] [[2.146]
 [2.146]
 [2.146]
 [2.146]
 [2.146]]
siam score:  -0.80084974
maxi score, test score, baseline:  0.4201 1.0 1.0
siam score:  -0.7991125
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.593]
 [0.934]
 [0.951]
 [0.978]] [[1.122]
 [1.164]
 [0.828]
 [0.637]
 [0.658]] [[1.535]
 [1.305]
 [1.652]
 [1.495]
 [1.57 ]]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
siam score:  -0.8083962
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.256]
 [0.256]
 [0.286]
 [0.343]] [[1.475]
 [1.475]
 [1.475]
 [1.722]
 [1.756]] [[1.138]
 [1.138]
 [1.138]
 [1.528]
 [1.686]]
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.281]
 [0.281]
 [0.317]
 [0.344]] [[1.466]
 [1.466]
 [1.466]
 [1.684]
 [1.734]] [[1.203]
 [1.203]
 [1.203]
 [1.567]
 [1.686]]
Printing some Q and Qe and total Qs values:  [[0.763]
 [1.079]
 [0.763]
 [0.763]
 [0.763]] [[1.906]
 [0.547]
 [1.906]
 [1.906]
 [1.906]] [[1.458]
 [1.636]
 [1.458]
 [1.458]
 [1.458]]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.246]
 [1.246]
 [1.247]
 [1.246]
 [1.246]] [[0.398]
 [0.398]
 [0.42 ]
 [0.398]
 [0.398]] [[1.736]
 [1.736]
 [1.745]
 [1.736]
 [1.736]]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2071 4017
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.869713756117508
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.4221 1.0 1.0
siam score:  -0.8105143
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80718595
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.834]
 [0.834]
 [0.828]
 [0.834]
 [0.834]] [[0.529]
 [0.529]
 [0.917]
 [0.529]
 [0.529]] [[0.834]
 [0.834]
 [0.828]
 [0.834]
 [0.834]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.827]
 [0.789]
 [0.789]
 [0.789]] [[0.392]
 [0.062]
 [0.392]
 [0.392]
 [0.392]] [[0.789]
 [0.827]
 [0.789]
 [0.789]
 [0.789]]
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.515]
 [1.515]
 [0.194]
 [1.515]
 [1.515]] [[-0.126]
 [-0.126]
 [-0.041]
 [-0.126]
 [-0.126]] [[2.749]
 [2.749]
 [0.134]
 [2.749]
 [2.749]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.02 ]
 [-0.019]
 [-0.005]] [[0.035]
 [0.035]
 [0.016]
 [0.016]
 [0.035]]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.516]
 [0.675]
 [0.594]
 [0.544]] [[0.551]
 [0.551]
 [0.394]
 [0.793]
 [0.759]] [[1.134]
 [1.134]
 [1.219]
 [1.504]
 [1.391]]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.54 ]
 [0.54 ]
 [0.534]
 [0.556]] [[-0.077]
 [-0.077]
 [-0.077]
 [-0.201]
 [ 0.377]] [[1.027]
 [1.027]
 [1.027]
 [0.848]
 [1.663]]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8085066
UNIT TEST: sample policy line 217 mcts : [0.125 0.458 0.333 0.042 0.042]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4281 1.0 1.0
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.984]
 [0.801]
 [0.785]
 [0.794]] [[ 0.085]
 [-0.039]
 [ 0.53 ]
 [ 0.742]
 [ 0.085]] [[0.794]
 [0.984]
 [0.801]
 [0.785]
 [0.794]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.523]
 [0.643]
 [0.638]
 [0.61 ]] [[1.854]
 [2.059]
 [1.924]
 [1.678]
 [1.625]] [[1.253]
 [1.482]
 [1.632]
 [1.458]
 [1.366]]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.586]] [[1.854]
 [1.854]
 [1.854]
 [1.854]
 [1.778]] [[2.464]
 [2.464]
 [2.464]
 [2.464]
 [2.37 ]]
siam score:  -0.80028915
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4321 1.0 1.0
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.549434269878641
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]]
in main func line 156:  2100
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]] [[0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]] [[0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.41]
 [0.41]
 [0.41]
 [0.41]
 [0.41]] [[-0.258]
 [-0.258]
 [-0.258]
 [ 0.774]
 [-0.258]] [[0.774]
 [0.774]
 [0.774]
 [1.119]
 [0.774]]
Printing some Q and Qe and total Qs values:  [[1.072]
 [1.072]
 [1.165]
 [1.072]
 [1.002]] [[0.298]
 [0.298]
 [0.27 ]
 [0.298]
 [0.793]] [[1.758]
 [1.758]
 [1.935]
 [1.758]
 [1.782]]
line 256 mcts: sample exp_bonus 2.3045611187543833
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20019
siam score:  -0.79853684
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4361 1.0 1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4361 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.879]
 [0.879]
 [0.879]
 [0.879]
 [0.879]] [[1.265]
 [1.265]
 [1.265]
 [1.265]
 [1.265]] [[1.348]
 [1.348]
 [1.348]
 [1.348]
 [1.348]]
maxi score, test score, baseline:  0.4381 1.0 1.0
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.021229505073566045
siam score:  -0.80270004
first move QE:  -0.021229505073566045
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80915666
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[0.669]
 [0.918]
 [0.918]
 [0.918]
 [0.918]] [[1.315]
 [1.514]
 [1.514]
 [1.514]
 [1.514]]
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4441 1.0 1.0
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.4461 1.0 1.0
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4461 1.0 1.0
maxi score, test score, baseline:  0.4461 1.0 1.0
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.476]
 [0.476]
 [0.476]
 [0.61 ]] [[1.829]
 [1.302]
 [1.302]
 [1.302]
 [1.562]] [[1.588]
 [1.497]
 [1.497]
 [1.497]
 [2.012]]
2132 4046
using explorer policy with actor:  1
