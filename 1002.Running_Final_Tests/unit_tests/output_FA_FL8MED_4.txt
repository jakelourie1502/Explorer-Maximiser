dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:255000
exp_gamma:0.95
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]]
from probs:  [0.25, 0.25, 0.25, 0.25]
7 50
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
siam score:  -0.0028851145709102802
12 80
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  1104 train batches done:  31 episodes:  96
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.007518468507166205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.68174726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.007518468507166205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.25, 0.25, 0.25, 0.25]
17 112
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
from probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  1824 train batches done:  142 episodes:  163
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.48920607563135465
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.48920607563135465
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.51519376
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5497346
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.64284194
from probs:  [0.25, 0.25, 0.25, 0.25]
26 169
siam score:  -0.6451496
siam score:  -0.64361674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.4662816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.4689039
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
29 201
from probs:  [0.25, 0.25, 0.25, 0.25]
33 216
Printing some Q and Qe and total Qs values:  [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]] [[-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5192447
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.40079162
in main func line 156:  41
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
42 274
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.25027719312637303
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
49 311
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
52 312
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.57003015
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5865336
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6007826
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6381672
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
78 429
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.59722054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.07108695654617185
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
82 459
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6760845
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.108]
 [-0.108]
 [-0.198]
 [-0.198]
 [-0.107]] [[0.153]
 [0.153]
 [0.094]
 [0.094]
 [0.154]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6567499
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.227]
 [0.165]
 [0.173]
 [1.094]
 [0.903]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
92 516
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.0003162278528323542
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.24961960349858428
98 552
99 560
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.20954868387438588
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
siam score:  -0.6224048
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]] [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6009639
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.51901454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.46994916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.41317874
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.43796718
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.0204030597747821
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.3440400520002586
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.889]
 [0.702]
 [0.601]
 [0.603]
 [0.601]] [[0.603]
 [0.416]
 [0.315]
 [0.317]
 [0.315]]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.225]] [[0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.208]]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.50894964
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5454853
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.337]
 [-0.332]
 [-0.244]
 [-0.293]
 [-0.312]] [[0.285]
 [0.291]
 [0.409]
 [0.344]
 [0.319]]
Printing some Q and Qe and total Qs values:  [[1.5]
 [0. ]
 [0. ]
 [0. ]
 [0. ]] [[ 0.   ]
 [-0.2  ]
 [-0.14 ]
 [-0.1  ]
 [-0.057]] [[3.2  ]
 [0.   ]
 [0.059]
 [0.1  ]
 [0.142]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69118315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6964022
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
line 256 mcts: sample exp_bonus 0.004490438611795511
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
siam score:  -0.66823214
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.007]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.002]] [[0.015]
 [0.004]
 [0.004]
 [0.004]
 [0.005]]
first move QE:  -0.16987096528056736
124 803
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.16530169400360445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
132 829
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.1596401522577768
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.005993907310627007
UNIT TEST: sample policy line 217 mcts : [0.25  0.292 0.167 0.125 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.15609327355064026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.007]
 [-0.001]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.002]
 [0.   ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.011]
 [ 0.005]
 [ 0.008]
 [ 0.004]
 [-0.   ]] [[0.004]
 [0.002]
 [0.003]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6146117
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.   ]
 [-0.094]
 [ 0.001]
 [ 0.001]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.015]
 [-0.063]
 [ 0.014]
 [ 0.015]
 [ 0.015]] [[0.162]
 [0.085]
 [0.162]
 [0.162]
 [0.163]]
first move QE:  -0.14698798545147773
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.167 0.083 0.167 0.5   0.083]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.6990256
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69892424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.007]
 [ 0.   ]
 [-0.012]
 [-0.015]] [[0.01 ]
 [0.01 ]
 [0.016]
 [0.004]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.194]
 [0.193]
 [0.193]
 [0.193]
 [0.193]]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.068]
 [-0.002]
 [ 0.   ]
 [-0.001]
 [ 0.   ]] [[0.073]
 [0.139]
 [0.141]
 [0.14 ]
 [0.141]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.001]
 [-0.   ]
 [-0.   ]
 [ 0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70840645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.008]
 [ 0.   ]
 [ 0.   ]
 [-0.003]
 [ 0.004]] [[0.   ]
 [0.011]
 [0.011]
 [0.007]
 [0.016]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -2.8355171782850207e-06
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70365876
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70233625
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.006524007629423631
siam score:  -0.6967915
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6983439
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6918435
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6933011
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.002]
 [0.002]
 [0.004]
 [0.004]] [[0.001]
 [0.001]
 [0.001]
 [0.002]
 [0.002]]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.13077352270186146
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.045]
 [-0.071]
 [-0.075]
 [-0.087]
 [-0.061]] [[0.086]
 [0.051]
 [0.047]
 [0.031]
 [0.065]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7109364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7197104
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.719337
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.001]
 [0.   ]]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.005]] [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.007]
 [-0.05 ]
 [-0.012]
 [ 0.007]
 [ 0.007]] [[0.078]
 [0.041]
 [0.066]
 [0.078]
 [0.078]]
siam score:  -0.6595187
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.66313887
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
193 1128
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69778883
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.006]
 [-0.007]
 [ 0.   ]
 [-0.006]
 [-0.006]] [[0.001]
 [0.   ]
 [0.009]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.7153156
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.11351924225273032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
206 1182
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.006]
 [-0.005]
 [-0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.008344635458521769
210 1213
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.67049545
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
221 1280
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.10306932499129126
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
229 1300
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.009]
 [0.008]
 [0.008]
 [0.007]] [[0.004]
 [0.005]
 [0.004]
 [0.004]
 [0.003]]
siam score:  -0.72930837
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.008]
 [0.009]
 [0.009]
 [0.008]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
siam score:  -0.7285721
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
234 1306
UNIT TEST: sample policy line 217 mcts : [0.125 0.458 0.083 0.25  0.083]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7206999
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.71438825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
246 1327
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.007888774111843463
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0072732287452454076
250 1345
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73482573
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
252 1352
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [ 0.   ]
 [-0.048]
 [-0.004]
 [-0.004]] [[0.124]
 [0.13 ]
 [0.066]
 [0.124]
 [0.124]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.011406349046893098
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
STARTED EXPV TRAINING ON FRAME NO.  20063
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.043]
 [0.308]
 [0.129]
 [0.032]
 [1.629]] [[0.151]
 [0.46 ]
 [0.251]
 [0.139]
 [2.   ]]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.09361034029871776
siam score:  -0.7208592
264 1408
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.038]
 [ 0.024]
 [-0.02 ]
 [ 0.006]
 [ 0.041]] [[0.067]
 [0.108]
 [0.079]
 [0.096]
 [0.119]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.482]
 [-0.466]
 [-0.433]
 [-0.432]
 [-0.483]] [[0.001]
 [0.017]
 [0.05 ]
 [0.052]
 [0.   ]]
267 1438
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.72663164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.145]
 [ 0.061]
 [-0.064]
 [-0.182]
 [-0.165]] [[0.037]
 [0.243]
 [0.119]
 [0.   ]
 [0.017]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.241]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.067]] [[0.001]
 [0.082]
 [0.082]
 [0.082]
 [0.059]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.67944723
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70742506
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.10837674793299808
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7241154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.049]
 [-0.03 ]
 [-0.152]
 [-0.026]
 [ 0.   ]] [[0.083]
 [0.089]
 [0.048]
 [0.09 ]
 [0.099]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73286104
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.10646888808381447
292 1587
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72236365
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7078446
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
298 1607
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.204]
 [2.917]
 [0.96 ]
 [1.49 ]
 [2.459]] [[0.   ]
 [0.906]
 [0.252]
 [0.429]
 [0.753]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.364]
 [3.755]
 [3.456]
 [2.909]
 [3.698]] [[1.483]
 [1.741]
 [1.544]
 [1.183]
 [1.703]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 4.469162641004754
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72577196
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73288757
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149949213435704, 0.785501523596929, 0.07149949213435704, 0.07149949213435704]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.323]
 [3.323]
 [3.323]
 [3.323]
 [3.323]] [[1.12]
 [1.12]
 [1.12]
 [1.12]
 [1.12]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149959003527151, 0.7855012298941854, 0.07149959003527151, 0.07149959003527151]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149959003527151, 0.7855012298941854, 0.07149959003527151, 0.07149959003527151]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149959003527151, 0.7855012298941854, 0.07149959003527151, 0.07149959003527151]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.314]
 [-0.239]
 [-0.244]
 [-0.358]
 [-0.409]] [[0.105]
 [0.155]
 [0.152]
 [0.075]
 [0.042]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.256]
 [-0.279]
 [-0.018]
 [-0.467]
 [-0.209]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.417]
 [3.417]
 [3.417]
 [3.417]
 [3.417]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149968793607865, 0.7855009361917642, 0.07149968793607865, 0.07149968793607865]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149968793607865, 0.7855009361917642, 0.07149968793607865, 0.07149968793607865]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149968793607865, 0.7855009361917642, 0.07149968793607865, 0.07149968793607865]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149968793607865, 0.7855009361917642, 0.07149968793607865, 0.07149968793607865]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149968793607865, 0.7855009361917642, 0.07149968793607865, 0.07149968793607865]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149968793607865, 0.7855009361917642, 0.07149968793607865, 0.07149968793607865]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149968793607865, 0.7855009361917642, 0.07149968793607865, 0.07149968793607865]
from probs:  [0.07149968793607865, 0.7855009361917642, 0.07149968793607865, 0.07149968793607865]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149973688644193, 0.7855007893406744, 0.07149973688644193, 0.07149973688644193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149978583677834, 0.7855006424896648, 0.07149978583677834, 0.07149978583677834]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149983478708795, 0.7855004956387364, 0.07149983478708795, 0.07149983478708795]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149983478708795, 0.7855004956387364, 0.07149983478708795, 0.07149983478708795]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149983478708795, 0.7855004956387364, 0.07149983478708795, 0.07149983478708795]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149983478708795, 0.7855004956387364, 0.07149983478708795, 0.07149983478708795]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149990821250199, 0.7855002753624941, 0.07149990821250199, 0.07149990821250199]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.94 ]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.969]] [[1.035]
 [0.465]
 [0.465]
 [0.465]
 [1.052]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149998163785563, 0.7855000550864332, 0.07149998163785563, 0.07149998163785563]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150000611296006, 0.7854999816611196, 0.07150000611296006, 0.07150000611296006]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150000611296006, 0.7854999816611196, 0.07150000611296006, 0.07150000611296006]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.223]
 [ 0.223]
 [-0.269]
 [ 0.223]
 [ 0.223]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150015296344588, 0.7854995411096625, 0.07150015296344588, 0.07150015296344588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150015296344588, 0.7854995411096625, 0.07150015296344588, 0.07150015296344588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150015296344588, 0.7854995411096625, 0.07150015296344588, 0.07150015296344588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150017743850333, 0.78549946768449, 0.07150017743850333, 0.07150017743850333]
344 1677
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150025086363547, 0.7854992474090935, 0.07150025086363547, 0.07150025086363547]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150025086363547, 0.7854992474090935, 0.07150025086363547, 0.07150025086363547]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150025086363547, 0.7854992474090935, 0.07150025086363547, 0.07150025086363547]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150025086363547, 0.7854992474090935, 0.07150025086363547, 0.07150025086363547]
first move QE:  -0.08318596972020918
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150025086363547, 0.7854992474090935, 0.07150025086363547, 0.07150025086363547]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150025086363547, 0.7854992474090935, 0.07150025086363547, 0.07150025086363547]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150025086363547, 0.7854992474090935, 0.07150025086363547, 0.07150025086363547]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150025086363547, 0.7854992474090935, 0.07150025086363547, 0.07150025086363547]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150025086363547, 0.7854992474090935, 0.07150025086363547, 0.07150025086363547]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0715003732387215, 0.7854988802838356, 0.0715003732387215, 0.0715003732387215]
from probs:  [0.0715003732387215, 0.7854988802838356, 0.0715003732387215, 0.0715003732387215]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150042218870892, 0.7854987334338732, 0.07150042218870892, 0.07150042218870892]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150042218870892, 0.7854987334338732, 0.07150042218870892, 0.07150042218870892]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150042218870892, 0.7854987334338732, 0.07150042218870892, 0.07150042218870892]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150042218870892, 0.7854987334338732, 0.07150042218870892, 0.07150042218870892]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150042218870892, 0.7854987334338732, 0.07150042218870892, 0.07150042218870892]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150042218870892, 0.7854987334338732, 0.07150042218870892, 0.07150042218870892]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150061798839014, 0.7854981460348297, 0.07150061798839014, 0.07150061798839014]
siam score:  -0.7027902
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150061798839014, 0.7854981460348297, 0.07150061798839014, 0.07150061798839014]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150061798839014, 0.7854981460348297, 0.07150061798839014, 0.07150061798839014]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150061798839014, 0.7854981460348297, 0.07150061798839014, 0.07150061798839014]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150064246332008, 0.7854980726100399, 0.07150064246332008, 0.07150064246332008]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.552]
 [-0.164]
 [-0.425]
 [-0.396]
 [-0.425]] [[0.   ]
 [0.259]
 [0.085]
 [0.104]
 [0.085]]
siam score:  -0.6991711
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150071588806964, 0.7854978523357911, 0.07150071588806964, 0.07150071588806964]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150071588806964, 0.7854978523357911, 0.07150071588806964, 0.07150071588806964]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.876]
 [2.852]
 [2.966]
 [2.248]
 [2.773]] [[1.246]
 [1.225]
 [1.322]
 [0.708]
 [1.158]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150074036297274, 0.7854977789110816, 0.07150074036297274, 0.07150074036297274]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150074036297274, 0.7854977789110816, 0.07150074036297274, 0.07150074036297274]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150076483786913, 0.7854977054863925, 0.07150076483786913, 0.07150076483786913]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150076483786913, 0.7854977054863925, 0.07150076483786913, 0.07150076483786913]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0715008137876418, 0.7854975586370748, 0.0715008137876418, 0.0715008137876418]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0715008137876418, 0.7854975586370748, 0.0715008137876418, 0.0715008137876418]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150088721225041, 0.7854973383632488, 0.07150088721225041, 0.07150088721225041]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.55 ]
 [-0.163]
 [-0.47 ]
 [-0.426]
 [-0.48 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.6975577
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150088721225041, 0.7854973383632488, 0.07150088721225041, 0.07150088721225041]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.913]] [[0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.477]]
first move QE:  -0.09203532642921171
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150098511163461, 0.785497044665096, 0.07150098511163461, 0.07150098511163461]
line 256 mcts: sample exp_bonus 4.018675266883733
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150103406128644, 0.7854968978161405, 0.07150103406128644, 0.07150103406128644]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150103406128644, 0.7854968978161405, 0.07150103406128644, 0.07150103406128644]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.396]
 [1.729]
 [1.384]
 [2.005]
 [3.459]] [[0.45 ]
 [0.652]
 [0.443]
 [0.821]
 [1.706]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.042 0.833]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150115643529859, 0.7854965306941042, 0.07150115643529859, 0.07150115643529859]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150115643529859, 0.7854965306941042, 0.07150115643529859, 0.07150115643529859]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150115643529859, 0.7854965306941042, 0.07150115643529859, 0.07150115643529859]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150115643529859, 0.7854965306941042, 0.07150115643529859, 0.07150115643529859]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]] [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.107]
 [1.286]
 [1.752]
 [1.31 ]
 [1.534]] [[0.672]
 [0.791]
 [1.101]
 [0.807]
 [0.956]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150130328389166, 0.785496090148325, 0.07150130328389166, 0.07150130328389166]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150130328389166, 0.785496090148325, 0.07150130328389166, 0.07150130328389166]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150132775863369, 0.7854960167240991, 0.07150132775863369, 0.07150132775863369]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.001]] [[1.698]
 [2.408]
 [1.88 ]
 [2.163]
 [3.467]] [[0.334]
 [0.807]
 [0.455]
 [0.644]
 [1.512]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150135223336898, 0.785495943299893, 0.07150135223336898, 0.07150135223336898]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150135223336898, 0.785495943299893, 0.07150135223336898, 0.07150135223336898]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150135223336898, 0.785495943299893, 0.07150135223336898, 0.07150135223336898]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.195]
 [0.547]
 [0.767]
 [0.547]
 [0.411]] [[0.693]
 [0.927]
 [1.073]
 [0.927]
 [0.836]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150142565753465, 0.7854957230273961, 0.07150142565753465, 0.07150142565753465]
line 256 mcts: sample exp_bonus -0.5102498200172976
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.08008892123965634
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.135]
 [1.135]
 [1.328]
 [1.135]
 [1.283]] [[0.557]
 [0.557]
 [0.685]
 [0.557]
 [0.655]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150164592966916, 0.7854950622109923, 0.07150164592966916, 0.07150164592966916]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150164592966916, 0.7854950622109923, 0.07150164592966916, 0.07150164592966916]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150167040431726, 0.7854949887870485, 0.07150167040431726, 0.07150167040431726]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150167040431726, 0.7854949887870485, 0.07150167040431726, 0.07150167040431726]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150167040431726, 0.7854949887870485, 0.07150167040431726, 0.07150167040431726]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150167040431726, 0.7854949887870485, 0.07150167040431726, 0.07150167040431726]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150167040431726, 0.7854949887870485, 0.07150167040431726, 0.07150167040431726]
first move QE:  -0.09793399447691166
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150167040431726, 0.7854949887870485, 0.07150167040431726, 0.07150167040431726]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150167040431726, 0.7854949887870485, 0.07150167040431726, 0.07150167040431726]
first move QE:  -0.09793399447691166
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.584]
 [4.382]
 [4.584]
 [4.584]
 [4.342]] [[1.603]
 [1.468]
 [1.603]
 [1.603]
 [1.442]]
start point for exploration sampling:  20063
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150169487895858, 0.7854949153631241, 0.07150169487895858, 0.07150169487895858]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.558]
 [5.558]
 [5.558]
 [5.558]
 [5.558]] [[1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150176830284236, 0.785494695091473, 0.07150176830284236, 0.07150176830284236]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150189067584774, 0.7854943279724569, 0.07150189067584774, 0.07150189067584774]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150189067584774, 0.7854943279724569, 0.07150189067584774, 0.07150189067584774]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.122]
 [4.041]
 [3.817]
 [3.044]
 [4.671]] [[0.219]
 [1.063]
 [0.964]
 [0.624]
 [1.34 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150198857413123, 0.7854940342776063, 0.07150198857413123, 0.07150198857413123]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150198857413123, 0.7854940342776063, 0.07150198857413123, 0.07150198857413123]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150198857413123, 0.7854940342776063, 0.07150198857413123, 0.07150198857413123]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150198857413123, 0.7854940342776063, 0.07150198857413123, 0.07150198857413123]
first move QE:  -0.10002364513540492
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150198857413123, 0.7854940342776063, 0.07150198857413123, 0.07150198857413123]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.7170637
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
in main func line 156:  393
first move QE:  -0.10285878460750413
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.266]
 [5.266]
 [5.266]
 [5.266]
 [5.266]] [[1.539]
 [1.539]
 [1.539]
 [1.539]
 [1.539]]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
in main func line 156:  400
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.907]
 [3.907]
 [3.907]
 [3.907]
 [3.907]] [[1.981]
 [1.981]
 [1.981]
 [1.981]
 [1.981]]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using another actor
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.953]
 [0.953]
 [1.188]
 [0.953]
 [0.953]] [[0.596]
 [0.596]
 [0.753]
 [0.596]
 [0.596]]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.57 ]
 [1.799]
 [2.522]
 [2.942]
 [3.07 ]] [[1.068]
 [0.555]
 [1.036]
 [1.316]
 [1.401]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.15]
 [2.15]
 [2.15]
 [2.15]
 [2.34]] [[1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.228]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
line 256 mcts: sample exp_bonus 2.1622468519518225
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.7235925
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
first move QE:  -0.10387484476720023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.7179645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
433 1789
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
using explorer policy with actor:  1
rdn probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
UNIT TEST: sample policy line 217 mcts : [0.125 0.292 0.    0.25  0.333]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.391]
 [-0.083]
 [-0.26 ]
 [-0.307]
 [-0.359]] [[0.   ]
 [0.205]
 [0.087]
 [0.056]
 [0.021]]
using another actor
from probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.7139105
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.7187231
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.7854939608539441, 0.07150201304868534, 0.07150201304868534]
siam score:  -0.7205194
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.7065962016898863
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04485095127552091, 0.4180119254003095, 0.04485095127552091, 0.4922861720486487]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04485095127552091, 0.4180119254003095, 0.04485095127552091, 0.4922861720486487]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04485095127552091, 0.4180119254003095, 0.04485095127552091, 0.4922861720486487]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.884]
 [1.793]
 [2.018]
 [1.882]
 [2.181]] [[1.405]
 [1.328]
 [1.517]
 [1.403]
 [1.654]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.044810957041875786, 0.4185321768958886, 0.044810957041875786, 0.49184590902035996]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.044810957041875786, 0.4185321768958886, 0.044810957041875786, 0.49184590902035996]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04477103430074997, 0.4190514984051494, 0.04477103430074997, 0.4914064329933508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04473118286061776, 0.4195698924194897, 0.04473118286061776, 0.4909677418592749]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04473118286061776, 0.4195698924194897, 0.04473118286061776, 0.4909677418592749]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04473118286061776, 0.4195698924194897, 0.04473118286061776, 0.4909677418592749]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04473118286061776, 0.4195698924194897, 0.04473118286061776, 0.4909677418592749]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04473118286061776, 0.4195698924194897, 0.04473118286061776, 0.4909677418592749]
siam score:  -0.7036141
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04473118286061776, 0.4195698924194897, 0.04473118286061776, 0.4909677418592749]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6944522
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04473118286061776, 0.4195698924194897, 0.04473118286061776, 0.4909677418592749]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04473118286061776, 0.4195698924194897, 0.04473118286061776, 0.4909677418592749]
siam score:  -0.68761396
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04473118286061776, 0.4195698924194897, 0.04473118286061776, 0.4909677418592749]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04473118286061776, 0.4195698924194897, 0.04473118286061776, 0.4909677418592749]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04473118286061776, 0.4195698924194897, 0.04473118286061776, 0.4909677418592749]
siam score:  -0.6896236
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04473118286061776, 0.4195698924194897, 0.04473118286061776, 0.4909677418592749]
UNIT TEST: sample policy line 217 mcts : [0.125 0.333 0.125 0.125 0.292]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.251]
 [0.01 ]
 [0.01 ]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.01 ]] [[3.177]
 [3.177]
 [3.177]
 [3.177]
 [3.509]] [[1.43]
 [1.43]
 [1.43]
 [1.43]
 [1.67]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04465169312064577, 0.42060390788458285, 0.04465169312064577, 0.4900927058741257]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.044532988519136237, 0.42214803664612915, 0.044532988519136237, 0.48878598631559844]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04449356090098983, 0.4226609175151511, 0.04449356090098983, 0.4883519606828693]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04449356090098983, 0.4226609175151511, 0.04449356090098983, 0.4883519606828693]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.050412150583743355, 0.5535152502957277, 0.050412150583743355, 0.34566044853678574]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.050412150583743355, 0.5535152502957277, 0.050412150583743355, 0.34566044853678574]
first move QE:  -0.09058887926601061
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.050412150583743355, 0.5535152502957277, 0.050412150583743355, 0.34566044853678574]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.050412150583743355, 0.5535152502957277, 0.050412150583743355, 0.34566044853678574]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.050412150583743355, 0.5535152502957277, 0.050412150583743355, 0.34566044853678574]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[2.741]
 [2.741]
 [2.741]
 [2.741]
 [2.741]] [[1.95]
 [1.95]
 [1.95]
 [1.95]
 [1.95]]
line 256 mcts: sample exp_bonus 3.024978001910031
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05048166624009833, 0.5542801727252777, 0.05048166624009833, 0.3447564947945255]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05048166624009833, 0.5542801727252777, 0.05048166624009833, 0.3447564947945255]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.05048166624009833, 0.5542801727252777, 0.05048166624009833, 0.3447564947945255]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05058508444878281, 0.555418145256857, 0.05058508444878281, 0.3434116858455775]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.032]
 [0.037]
 [0.025]
 [0.031]] [[3.292]
 [4.013]
 [4.566]
 [3.292]
 [4.503]] [[0.743]
 [1.213]
 [1.572]
 [0.743]
 [1.521]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05061933135263106, 0.5557949844650263, 0.05061933135263106, 0.34296635282971155]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05061933135263106, 0.5557949844650263, 0.05061933135263106, 0.34296635282971155]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05061933135263106, 0.5557949844650263, 0.05061933135263106, 0.34296635282971155]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05061933135263106, 0.5557949844650263, 0.05061933135263106, 0.34296635282971155]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05061933135263106, 0.5557949844650263, 0.05061933135263106, 0.34296635282971155]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
in main func line 156:  471
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05610087871134757, 0.6161341384199847, 0.05610087871134757, 0.27166410415732023]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.002]
 [0.002]
 [0.003]] [[2.151]
 [1.92 ]
 [1.761]
 [2.021]
 [1.95 ]] [[1.208]
 [1.058]
 [0.958]
 [1.124]
 [1.08 ]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059290292587176434, 0.6512422876391665, 0.059290292587176434, 0.23017712718648073]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059290292587176434, 0.6512422876391665, 0.059290292587176434, 0.23017712718648073]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059290292587176434, 0.6512422876391665, 0.059290292587176434, 0.23017712718648073]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059290292587176434, 0.6512422876391665, 0.059290292587176434, 0.23017712718648073]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059290292587176434, 0.6512422876391665, 0.059290292587176434, 0.23017712718648073]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059290292587176434, 0.6512422876391665, 0.059290292587176434, 0.23017712718648073]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059290292587176434, 0.6512422876391665, 0.059290292587176434, 0.23017712718648073]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059290292587176434, 0.6512422876391665, 0.059290292587176434, 0.23017712718648073]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059290292587176434, 0.6512422876391665, 0.059290292587176434, 0.23017712718648073]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059290292587176434, 0.6512422876391665, 0.059290292587176434, 0.23017712718648073]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.034]
 [0.02 ]
 [0.012]
 [0.023]] [[ 0.659]
 [-0.126]
 [-0.146]
 [-0.145]
 [ 0.695]] [[0.603]
 [0.072]
 [0.033]
 [0.017]
 [0.598]]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06140462494125073, 0.6745161813120163, 0.06140462494125073, 0.20267456880548243]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06140462494125073, 0.6745161813120163, 0.06140462494125073, 0.20267456880548243]
siam score:  -0.70641226
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06140462494125073, 0.6745161813120163, 0.06140462494125073, 0.20267456880548243]
siam score:  -0.7049587
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.304]
 [ 0.379]
 [-0.016]
 [-0.327]
 [-0.346]] [[0.056]
 [0.968]
 [0.441]
 [0.027]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06144438392653628, 0.6749537018969108, 0.06144438392653628, 0.20215753025001665]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.001]
 [0.001]
 [0.001]] [[2.245]
 [2.521]
 [2.245]
 [0.966]
 [2.552]] [[1.311]
 [1.594]
 [1.311]
 [0.001]
 [1.625]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.061464146121592844, 0.6751711714092198, 0.061464146121592844, 0.20190053634759442]
siam score:  -0.70459306
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.061464146121592844, 0.6751711714092198, 0.061464146121592844, 0.20190053634759442]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.061464146121592844, 0.6751711714092198, 0.061464146121592844, 0.20190053634759442]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.061464146121592844, 0.6751711714092198, 0.061464146121592844, 0.20190053634759442]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.061464146121592844, 0.6751711714092198, 0.061464146121592844, 0.20190053634759442]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.124]
 [ 0.328]
 [ 0.277]
 [-0.043]
 [-0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06148383073225667, 0.6753877871580677, 0.06148383073225667, 0.201644551377419]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.061503438214515455, 0.6756035541612887, 0.061503438214515455, 0.20138956940968022]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.023]
 [0.014]
 [0.004]
 [0.009]] [[1.399]
 [2.248]
 [2.833]
 [1.842]
 [2.093]] [[0.16 ]
 [0.457]
 [0.635]
 [0.283]
 [0.378]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.061503438214515455, 0.6756035541612887, 0.061503438214515455, 0.20138956940968022]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06152296902079063, 0.675818477397473, 0.06152296902079063, 0.20113558456094568]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06152296902079063, 0.675818477397473, 0.06152296902079063, 0.20113558456094568]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.667 0.042 0.208]
using another actor
from probs:  [0.061542423599972015, 0.6760325618063479, 0.061542423599972015, 0.200882590993708]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.061542423599972015, 0.6760325618063479, 0.061542423599972015, 0.200882590993708]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.061542423599972015, 0.6760325618063479, 0.061542423599972015, 0.200882590993708]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.061542423599972015, 0.6760325618063479, 0.061542423599972015, 0.200882590993708]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.002]
 [0.002]] [[-0.275]
 [ 0.056]
 [-0.117]
 [-0.364]
 [-0.404]] [[0.397]
 [0.618]
 [0.503]
 [0.339]
 [0.313]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.061542423599972015, 0.6760325618063479, 0.061542423599972015, 0.200882590993708]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06156180239745238, 0.6762458122891575, 0.06156180239745238, 0.20063058291593766]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06156180239745238, 0.6762458122891575, 0.06156180239745238, 0.20063058291593766]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06156180239745238, 0.6762458122891575, 0.06156180239745238, 0.20063058291593766]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06156180239745238, 0.6762458122891575, 0.06156180239745238, 0.20063058291593766]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06156180239745238, 0.6762458122891575, 0.06156180239745238, 0.20063058291593766]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06156180239745238, 0.6762458122891575, 0.06156180239745238, 0.20063058291593766]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06156180239745238, 0.6762458122891575, 0.06156180239745238, 0.20063058291593766]
from probs:  [0.06156180239745238, 0.6762458122891575, 0.06156180239745238, 0.20063058291593766]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06156180239745238, 0.6762458122891575, 0.06156180239745238, 0.20063058291593766]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[1.824]
 [2.225]
 [1.824]
 [1.824]
 [1.824]] [[1.366]
 [1.633]
 [1.366]
 [1.366]
 [1.366]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06156180239745238, 0.6762458122891575, 0.06156180239745238, 0.20063058291593766]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.75  0.042 0.125]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06410476468929173, 0.7042381626317864, 0.06410476468929173, 0.16755230798963022]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
499 1880
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.004]
 [0.007]
 [0.003]] [[1.514]
 [1.329]
 [1.366]
 [1.402]
 [1.444]] [[1.095]
 [0.851]
 [0.901]
 [0.955]
 [1.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0649566175680415, 0.7136151213651344, 0.0649566175680415, 0.1564716434987826]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0649566175680415, 0.7136151213651344, 0.0649566175680415, 0.1564716434987826]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0649566175680415, 0.7136151213651344, 0.0649566175680415, 0.1564716434987826]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0649566175680415, 0.7136151213651344, 0.0649566175680415, 0.1564716434987826]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06496999054668355, 0.7137622843059925, 0.06496999054668355, 0.1562977346006403]
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.189]] [[4.219]
 [4.219]
 [4.219]
 [4.219]
 [4.48 ]] [[1.701]
 [1.701]
 [1.701]
 [1.701]
 [1.848]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06498330890256639, 0.7139088461507079, 0.06498330890256639, 0.1561245360441593]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06498330890256639, 0.7139088461507079, 0.06498330890256639, 0.1561245360441593]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06498330890256639, 0.7139088461507079, 0.06498330890256639, 0.1561245360441593]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06499657296967302, 0.714054810574596, 0.06499657296967302, 0.15595204348605796]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06499657296967302, 0.714054810574596, 0.06499657296967302, 0.15595204348605796]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06499657296967302, 0.714054810574596, 0.06499657296967302, 0.15595204348605796]
first move QE:  -0.09115135693249632
from probs:  [0.06499657296967302, 0.714054810574596, 0.06499657296967302, 0.15595204348605796]
from probs:  [0.06499657296967302, 0.714054810574596, 0.06499657296967302, 0.15595204348605796]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06499657296967302, 0.714054810574596, 0.06499657296967302, 0.15595204348605796]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06499657296967302, 0.714054810574596, 0.06499657296967302, 0.15595204348605796]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06499657296967302, 0.714054810574596, 0.06499657296967302, 0.15595204348605796]
siam score:  -0.7892976
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06499657296967302, 0.714054810574596, 0.06499657296967302, 0.15595204348605796]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06499657296967302, 0.714054810574596, 0.06499657296967302, 0.15595204348605796]
using another actor
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06499657296967302, 0.714054810574596, 0.06499657296967302, 0.15595204348605796]
siam score:  -0.78934693
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06499657296967302, 0.714054810574596, 0.06499657296967302, 0.15595204348605796]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.06499657296967302, 0.714054810574596, 0.06499657296967302, 0.15595204348605796]
siam score:  -0.79942
from probs:  [0.06499657296967302, 0.714054810574596, 0.06499657296967302, 0.15595204348605796]
siam score:  -0.8010739
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06500978307926918, 0.7142001812230704, 0.06500978307926918, 0.15578025261839124]
Printing some Q and Qe and total Qs values:  [[0.02]
 [0.02]
 [0.02]
 [0.02]
 [0.02]] [[1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.178]] [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0650229395599309, 0.7143449617119464, 0.0650229395599309, 0.15560915916819199]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0650229395599309, 0.7143449617119464, 0.0650229395599309, 0.15560915916819199]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0650229395599309, 0.7143449617119464, 0.0650229395599309, 0.15560915916819199]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0650229395599309, 0.7143449617119464, 0.0650229395599309, 0.15560915916819199]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06503604273757178, 0.71448915562774, 0.06503604273757178, 0.15543875889711634]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.02 ]
 [0.021]
 [0.02 ]
 [0.02 ]] [[1.33 ]
 [1.33 ]
 [1.878]
 [1.33 ]
 [1.33 ]] [[0.661]
 [0.661]
 [1.395]
 [0.661]
 [0.661]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06506209047429468, 0.714775797941427, 0.06506209047429468, 0.15510002110998375]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06506209047429468, 0.714775797941427, 0.06506209047429468, 0.15510002110998375]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06506209047429468, 0.714775797941427, 0.06506209047429468, 0.15510002110998375]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06506209047429468, 0.714775797941427, 0.06506209047429468, 0.15510002110998375]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06506209047429468, 0.714775797941427, 0.06506209047429468, 0.15510002110998375]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06506209047429468, 0.714775797941427, 0.06506209047429468, 0.15510002110998375]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06506209047429468, 0.714775797941427, 0.06506209047429468, 0.15510002110998375]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06506209047429468, 0.714775797941427, 0.06506209047429468, 0.15510002110998375]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06507503567213234, 0.7149182533685069, 0.06507503567213234, 0.15493167528722834]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06507503567213234, 0.7149182533685069, 0.06507503567213234, 0.15493167528722834]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06507503567213234, 0.7149182533685069, 0.06507503567213234, 0.15493167528722834]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06507503567213234, 0.7149182533685069, 0.06507503567213234, 0.15493167528722834]
from probs:  [0.06511356036239126, 0.7153421983149476, 0.06511356036239126, 0.15443068096026988]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06511356036239126, 0.7153421983149476, 0.06511356036239126, 0.15443068096026988]
using explorer policy with actor:  0
524 1961
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06511356036239126, 0.7153421983149476, 0.06511356036239126, 0.15443068096026988]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06511356036239126, 0.7153421983149476, 0.06511356036239126, 0.15443068096026988]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06511356036239126, 0.7153421983149476, 0.06511356036239126, 0.15443068096026988]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06511356036239126, 0.7153421983149476, 0.06511356036239126, 0.15443068096026988]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06512629932639445, 0.7154823842418362, 0.06512629932639445, 0.15426501710537496]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.042 0.292 0.583 0.042 0.042]
525 1973
from probs:  [0.06515162519233439, 0.7157610827286329, 0.06515162519233439, 0.15393566688669827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06515162519233439, 0.7157610827286329, 0.06515162519233439, 0.15393566688669827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06515162519233439, 0.7157610827286329, 0.06515162519233439, 0.15393566688669827]
using another actor
line 256 mcts: sample exp_bonus 5.447425086577814
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.065]
 [0.065]
 [0.065]
 [0.048]] [[3.426]
 [4.715]
 [4.715]
 [4.715]
 [4.68 ]] [[0.712]
 [1.33 ]
 [1.33 ]
 [1.33 ]
 [1.303]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06517675031767028, 0.7160375721654917, 0.06517675031767028, 0.15360892719916774]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06517675031767028, 0.7160375721654917, 0.06517675031767028, 0.15360892719916774]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06517675031767028, 0.7160375721654917, 0.06517675031767028, 0.15360892719916774]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06520167707967829, 0.7163118787131474, 0.06520167707967829, 0.15328476712749609]
531 1993
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06520167707967829, 0.7163118787131474, 0.06520167707967829, 0.15328476712749609]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06520167707967829, 0.7163118787131474, 0.06520167707967829, 0.15328476712749609]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06521406680708924, 0.7164482214660249, 0.06521406680708924, 0.15312364491979652]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.06522640781824487, 0.7165840281208801, 0.06522640781824487, 0.15296315624263002]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06522640781824487, 0.7165840281208801, 0.06522640781824487, 0.15296315624263002]
from probs:  [0.06522640781824487, 0.7165840281208801, 0.06522640781824487, 0.15296315624263002]
from probs:  [0.06522640781824487, 0.7165840281208801, 0.06522640781824487, 0.15296315624263002]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06523870039990896, 0.7167193018334053, 0.06523870039990896, 0.15280329736677675]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06523870039990896, 0.7167193018334053, 0.06523870039990896, 0.15280329736677675]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06523870039990896, 0.7167193018334053, 0.06523870039990896, 0.15280329736677675]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06525094483659905, 0.7168540457345731, 0.06525094483659905, 0.1526440645922286]
539 2022
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06525094483659905, 0.7168540457345731, 0.06525094483659905, 0.1526440645922286]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06526314141060834, 0.7169882629308796, 0.06526314141060834, 0.15248545424790377]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06526314141060834, 0.7169882629308796, 0.06526314141060834, 0.15248545424790377]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06526314141060834, 0.7169882629308796, 0.06526314141060834, 0.15248545424790377]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06526314141060834, 0.7169882629308796, 0.06526314141060834, 0.15248545424790377]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06526314141060834, 0.7169882629308796, 0.06526314141060834, 0.15248545424790377]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06526314141060834, 0.7169882629308796, 0.06526314141060834, 0.15248545424790377]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[-0.078]
 [-0.078]
 [-0.078]
 [-0.078]
 [-0.078]] [[0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06526314141060834, 0.7169882629308796, 0.06526314141060834, 0.15248545424790377]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0652873920887656, 0.7172551295139317, 0.0652873920887656, 0.1521700863085372]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0652873920887656, 0.7172551295139317, 0.0652873920887656, 0.1521700863085372]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0652873920887656, 0.7172551295139317, 0.0652873920887656, 0.1521700863085372]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0653114546490581, 0.7175199259539776, 0.0653114546490581, 0.15185716474790623]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0653114546490581, 0.7175199259539776, 0.0653114546490581, 0.15185716474790623]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0653114546490581, 0.7175199259539776, 0.0653114546490581, 0.15185716474790623]
siam score:  -0.83482605
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0653114546490581, 0.7175199259539776, 0.0653114546490581, 0.15185716474790623]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06532341606771473, 0.7176515553832518, 0.06532341606771473, 0.15170161248131867]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06533533127193646, 0.7177826762457856, 0.06533533127193646, 0.15154666121034147]
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06596621778607863, 0.7247274630117783, 0.06596621778607863, 0.14334010141606449]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06597697318719473, 0.7248458211799997, 0.06597697318719473, 0.14320023244561067]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06597697318719473, 0.7248458211799997, 0.06597697318719473, 0.14320023244561067]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06597697318719473, 0.7248458211799997, 0.06597697318719473, 0.14320023244561067]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06597697318719473, 0.7248458211799997, 0.06597697318719473, 0.14320023244561067]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
using another actor
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4004915118431753
maxi score, test score, baseline:  0.0001 0.0 0.0001
551 2069
from probs:  [0.06597697318719473, 0.7248458211799997, 0.06597697318719473, 0.14320023244561067]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06597697318719473, 0.7248458211799997, 0.06597697318719473, 0.14320023244561067]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06597697318719473, 0.7248458211799997, 0.06597697318719473, 0.14320023244561067]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06597697318719473, 0.7248458211799997, 0.06597697318719473, 0.14320023244561067]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06597697318719473, 0.7248458211799997, 0.06597697318719473, 0.14320023244561067]
siam score:  -0.8112172
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.201]
 [0.226]
 [0.205]
 [0.193]] [[1.039]
 [2.179]
 [2.008]
 [0.764]
 [1.649]] [[0.726]
 [1.435]
 [1.364]
 [0.609]
 [1.115]]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05663339931800848, 0.6220241527121928, 0.1984419655066677, 0.12290048246313087]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0566412883760584, 0.6221109681130971, 0.1984696504871792, 0.12277809302366524]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0566412883760584, 0.6221109681130971, 0.1984696504871792, 0.12277809302366524]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0566412883760584, 0.6221109681130971, 0.1984696504871792, 0.12277809302366524]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0566412883760584, 0.6221109681130971, 0.1984696504871792, 0.12277809302366524]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0566412883760584, 0.6221109681130971, 0.1984696504871792, 0.12277809302366524]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0566412883760584, 0.6221109681130971, 0.1984696504871792, 0.12277809302366524]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.823059
siam score:  -0.82328105
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057905388746604, 0.6360233444723078, 0.1878195361006947, 0.11825173068039352]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057905388746604, 0.6360233444723078, 0.1878195361006947, 0.11825173068039352]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057905388746604, 0.6360233444723078, 0.1878195361006947, 0.11825173068039352]
siam score:  -0.8243988
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057905388746604, 0.6360233444723078, 0.1878195361006947, 0.11825173068039352]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057905388746604, 0.6360233444723078, 0.1878195361006947, 0.11825173068039352]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057905388746604, 0.6360233444723078, 0.1878195361006947, 0.11825173068039352]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.005]
 [0.017]
 [0.009]
 [0.01 ]] [[-0.269]
 [-0.36 ]
 [-0.045]
 [ 0.085]
 [-0.297]] [[0.072]
 [0.004]
 [0.236]
 [0.307]
 [0.054]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057905388746604, 0.6360233444723078, 0.1878195361006947, 0.11825173068039352]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.003]
 [0.004]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-1.183]
 [ 0.   ]] [[1.187]
 [1.187]
 [1.187]
 [0.002]
 [1.187]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05795572776429284, 0.636577303204879, 0.187112329425138, 0.11835463960569015]
siam score:  -0.8274043
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05795572776429284, 0.636577303204879, 0.187112329425138, 0.11835463960569015]
siam score:  -0.82827073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05795572776429284, 0.636577303204879, 0.187112329425138, 0.11835463960569015]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05795572776429284, 0.636577303204879, 0.187112329425138, 0.11835463960569015]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05795572776429284, 0.636577303204879, 0.187112329425138, 0.11835463960569015]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05897709946226913, 0.6478183341473844, 0.17846598637077468, 0.11473858001957173]
562 2121
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.058990727254095335, 0.6479683022955797, 0.17850728063101995, 0.1145336898193051]
siam score:  -0.83095354
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.058990727254095335, 0.6479683022955797, 0.17850728063101995, 0.1145336898193051]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.058990727254095335, 0.6479683022955797, 0.17850728063101995, 0.1145336898193051]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.058990727254095335, 0.6479683022955797, 0.17850728063101995, 0.1145336898193051]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0025],
        [0.2527],
        [0.2859],
        [0.0034],
        [0.0110],
        [0.0000],
        [0.2342],
        [0.0000],
        [0.0022],
        [0.2037]], dtype=torch.float64)
0.0 0.0025425010478350297
0.0 0.25270318792624624
0.0 0.28594832185547114
0.0 0.0034115427190665355
0.0 0.010972090219460025
0.0 0.0
0.0 0.23418262032566423
0.0 0.0
0.0 0.002182526394616236
0.0 0.2037499770535009
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.058990727254095335, 0.6479683022955797, 0.17850728063101995, 0.1145336898193051]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.058990727254095335, 0.6479683022955797, 0.17850728063101995, 0.1145336898193051]
from probs:  [0.058990727254095335, 0.6479683022955797, 0.17850728063101995, 0.1145336898193051]
using another actor
from probs:  [0.058990727254095335, 0.6479683022955797, 0.17850728063101995, 0.1145336898193051]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.058990727254095335, 0.6479683022955797, 0.17850728063101992, 0.11453368981930509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.058990727254095335, 0.6479683022955797, 0.17850728063101992, 0.11453368981930509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.058990727254095335, 0.6479683022955797, 0.17850728063101995, 0.1145336898193051]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 2.1017606657903873
from probs:  [0.05901318693544176, 0.648215461693341, 0.17830905143390283, 0.11446229993731442]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05905113281328548, 0.6486330402052849, 0.17789449249722009, 0.11442133448420953]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.333]
 [0.323]
 [0.324]
 [0.256]] [[2.238]
 [2.78 ]
 [2.5  ]
 [2.238]
 [2.554]] [[1.322]
 [1.7  ]
 [1.493]
 [1.322]
 [1.395]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05905113281328548, 0.6486330402052849, 0.17789449249722009, 0.11442133448420953]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05905113281328548, 0.6486330402052849, 0.17789449249722009, 0.11442133448420953]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05905113281328548, 0.6486330402052849, 0.17789449249722009, 0.11442133448420953]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05905113281328548, 0.6486330402052849, 0.17789449249722009, 0.11442133448420953]
from probs:  [0.05905113281328548, 0.6486330402052849, 0.17789449249722009, 0.11442133448420953]
rdn beta is 0 so we're just using the maxi policy
using another actor
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
siam score:  -0.83931017
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0590733744195773, 0.6488777997782964, 0.17769856053355942, 0.1143502652685669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059088823762218835, 0.6490478133312134, 0.17748316242747963, 0.11438020047908806]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9804980891612047
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0591042130920663, 0.6492171664684366, 0.17726860103286932, 0.11441001940662772]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0591042130920663, 0.6492171664684366, 0.17726860103286932, 0.11441001940662772]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0591042130920663, 0.6492171664684366, 0.17726860103286932, 0.11441001940662772]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0591042130920663, 0.6492171664684366, 0.17726860103286932, 0.11441001940662772]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0591042130920663, 0.6492171664684366, 0.17726860103286932, 0.11441001940662772]
from probs:  [0.0591042130920663, 0.6492171664684366, 0.17726860103286932, 0.11441001940662772]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0591042130920663, 0.6492171664684366, 0.17726860103286932, 0.11441001940662772]
569 2168
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0591042130920663, 0.6492171664684366, 0.17726860103286932, 0.11441001940662772]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05911092936803173, 0.6492910762758469, 0.17728877216094627, 0.11430922219517493]
using another actor
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.019]
 [0.02 ]
 [0.015]
 [0.016]] [[0.477]
 [1.008]
 [1.   ]
 [0.477]
 [0.341]] [[0.015]
 [0.019]
 [0.02 ]
 [0.015]
 [0.016]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05911761958202312, 0.6493646992821355, 0.177308865016567, 0.11420881611927458]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05911761958202311, 0.6493646992821354, 0.17730886501656704, 0.1142088161192746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05911761958202311, 0.6493646992821354, 0.17730886501656704, 0.1142088161192746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8463984
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.844984
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05911761958202311, 0.6493646992821354, 0.17730886501656704, 0.1142088161192746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.03 ]
 [0.033]
 [0.033]
 [0.033]] [[1.445]
 [1.095]
 [1.445]
 [1.445]
 [1.445]] [[0.677]
 [0.437]
 [0.677]
 [0.677]
 [0.677]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059124283885443794, 0.6494380371534323, 0.1773288800544443, 0.11410879890667967]
from probs:  [0.059130922428526675, 0.6495110915429879, 0.17734881772577563, 0.11400916830270973]
from probs:  [0.059130922428526675, 0.6495110915429879, 0.17734881772577563, 0.11400916830270973]
start point for exploration sampling:  20063
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.036]
 [0.019]
 [0.017]
 [0.031]] [[0.918]
 [1.288]
 [1.298]
 [0.309]
 [1.489]] [[1.16 ]
 [1.505]
 [1.493]
 [0.625]
 [1.675]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0591462659827296, 0.649679940938392, 0.17713501222761716, 0.11403878085126115]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05915288235406957, 0.6497527513371918, 0.1771548541788692, 0.1139395121298694]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05915288235406957, 0.6497527513371918, 0.1771548541788692, 0.1139395121298694]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05915288235406957, 0.6497527513371918, 0.1771548541788692, 0.1139395121298694]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05915288235406957, 0.6497527513371918, 0.1771548541788692, 0.1139395121298694]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05915288235406957, 0.6497527513371918, 0.1771548541788692, 0.1139395121298694]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05915288235406957, 0.6497527513371918, 0.1771548541788692, 0.1139395121298694]
siam score:  -0.84591705
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05915288235406957, 0.6497527513371918, 0.1771548541788692, 0.1139395121298694]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05916816996773183, 0.6499209851303472, 0.17694185720110814, 0.11396898770081301]
using explorer policy with actor:  1
siam score:  -0.8465649
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05916816996773183, 0.6499209851303472, 0.17694185720110814, 0.11396898770081301]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05916816996773183, 0.6499209851303472, 0.17694185720110814, 0.11396898770081301]
using another actor
first move QE:  -0.14877896616135208
siam score:  -0.8503039
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059181333244509145, 0.650065841484748, 0.17698127507868758, 0.11377155019205523]
siam score:  -0.84975654
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059181333244509145, 0.650065841484748, 0.17698127507868758, 0.11377155019205523]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059181333244509145, 0.650065841484748, 0.17698127507868758, 0.11377155019205523]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059181333244509145, 0.650065841484748, 0.17698127507868758, 0.11377155019205523]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.552469371115707
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05924848096223604, 0.6508047740287334, 0.1761568392757731, 0.11378990573325737]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]] [[0.394]
 [0.28 ]
 [0.439]
 [0.394]
 [0.302]] [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]]
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05924848491442267, 0.6508047657244469, 0.17615684080573182, 0.11378990855539854]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05924848491442267, 0.6508047657244469, 0.17615684080573182, 0.11378990855539854]
rdn probs:  [0.05924848491442267, 0.6508047657244469, 0.17615684080573182, 0.11378990855539854]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.05924858715500072, 0.6508045508977867, 0.17615688038480032, 0.11378998156241231]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.05924858715500072, 0.6508045508977867, 0.17615688038480032, 0.11378998156241231]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.02 ]
 [0.021]
 [0.016]
 [0.015]] [[-1.614]
 [-0.898]
 [-0.473]
 [-1.704]
 [-1.473]] [[0.068]
 [0.555]
 [0.841]
 [0.011]
 [0.163]]
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.14 ]
 [0.128]
 [0.13 ]
 [0.132]] [[2.581]
 [2.261]
 [3.332]
 [2.762]
 [3.04 ]] [[0.515]
 [0.358]
 [1.047]
 [0.673]
 [0.86 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.059263593016496766, 0.6509696837730173, 0.1759478940925213, 0.11381882911796463]
first move QE:  -0.1593853075741177
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.05927854148655332, 0.6511341850808152, 0.1757397070895171, 0.11384756634311444]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0592999749182213, 0.6513700505256831, 0.17555170780796445, 0.11377826674813128]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0592999749182213, 0.6513700505256831, 0.17555170780796445, 0.11377826674813128]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0592999749182213, 0.6513700505256831, 0.17555170780796445, 0.11377826674813128]
from probs:  [0.0592999749182213, 0.6513700505256831, 0.17555170780796445, 0.11377826674813128]
using explorer policy with actor:  1
using another actor
from probs:  [0.0592999749182213, 0.6513700505256831, 0.17555170780796445, 0.11377826674813128]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0592999749182213, 0.6513700505256831, 0.17555170780796445, 0.11377826674813128]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0592999749182213, 0.6513700505256831, 0.17555170780796445, 0.11377826674813128]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0592999749182213, 0.6513700505256831, 0.17555170780796445, 0.11377826674813128]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]] [[0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.059314812869492986, 0.6515333356230877, 0.17534508770366122, 0.11380676380375822]
588 2238
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.059314812869492986, 0.6515333356230877, 0.17534508770366122, 0.11380676380375822]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.001]
 [0.012]
 [0.008]
 [0.007]] [[-1.335]
 [-0.319]
 [ 0.139]
 [-0.591]
 [-0.813]] [[0.007]
 [0.666]
 [0.993]
 [0.498]
 [0.348]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.05932959439604047, 0.6516959997913345, 0.17513925331985164, 0.11383515249277344]
from probs:  [0.05932959439604047, 0.6516959997913345, 0.17513925331985164, 0.11383515249277344]
UNIT TEST: sample policy line 217 mcts : [0.333 0.125 0.25  0.167 0.125]
maxi score, test score, baseline:  0.0021 0.05 0.05
589 2243
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.05938013688482399, 0.6522521975475465, 0.17454564983407078, 0.11382201573355884]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.05938013688482399, 0.6522521975475465, 0.17454564983407078, 0.11382201573355884]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.05938013688482399, 0.6522521975475465, 0.17454564983407078, 0.11382201573355884]
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.059409206484761015, 0.6525720956500758, 0.1741409065167368, 0.11387779134842627]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.059409206484761015, 0.6525720956500758, 0.1741409065167368, 0.11387779134842627]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.059409206484761015, 0.6525720956500758, 0.1741409065167368, 0.11387779134842627]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.059409206484761015, 0.6525720956500758, 0.1741409065167368, 0.11387779134842627]
from probs:  [0.059409206484761015, 0.6525720956500758, 0.1741409065167368, 0.11387779134842627]
maxi score, test score, baseline:  0.0021 0.05 0.05
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06029819879775462, 0.6623562761222763, 0.1663690369451334, 0.11097648813483557]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.060304351029394306, 0.6624239788370252, 0.16638603316604525, 0.11088563696753533]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.060304351029394306, 0.6624239788370252, 0.16638603316604525, 0.11088563696753533]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.060304351029394306, 0.6624239788370252, 0.16638603316604525, 0.11088563696753533]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.060304351029394306, 0.6624239788370252, 0.16638603316604525, 0.11088563696753533]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.060304351029394306, 0.6624239788370252, 0.16638603316604525, 0.11088563696753533]
siam score:  -0.8411637
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.060304351029394306, 0.6624239788370252, 0.16638603316604525, 0.11088563696753533]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.015]
 [0.003]
 [0.012]
 [0.007]] [[-0.684]
 [ 0.298]
 [ 0.523]
 [ 0.257]
 [ 0.2  ]] [[0.003]
 [1.006]
 [1.207]
 [0.958]
 [0.892]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.060337392113597274, 0.6627875820349857, 0.16603040788927942, 0.11084461796213756]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06107000561023703, 0.6708507240661993, 0.15981991953023092, 0.1082593507933327]
siam score:  -0.8472775
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06107000561023703, 0.6708507240661993, 0.15981991953023092, 0.1082593507933327]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06107000561023703, 0.6708507240661993, 0.15981991953023092, 0.1082593507933327]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06107000561023703, 0.6708507240661993, 0.15981991953023092, 0.1082593507933327]
from probs:  [0.06107000561023703, 0.6708507240661993, 0.15981991953023092, 0.1082593507933327]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06107000561023703, 0.6708507240661993, 0.15981991953023092, 0.1082593507933327]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06107000561023703, 0.6708507240661993, 0.15981991953023092, 0.1082593507933327]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06108265887206783, 0.6709899680924544, 0.159645572450023, 0.10828180058545481]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06108265887206783, 0.6709899680924544, 0.159645572450023, 0.10828180058545481]
using explorer policy with actor:  1
602 2278
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06108265887206783, 0.6709899680924544, 0.159645572450023, 0.10828180058545481]
from probs:  [0.06109526430212001, 0.671128685749341, 0.15947188443552127, 0.10830416551301777]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06109526430212001, 0.671128685749341, 0.15947188443552127, 0.10830416551301777]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06110782217110196, 0.6712668800158956, 0.1592988517566819, 0.10832644605632048]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06110782217110196, 0.6712668800158956, 0.1592988517566819, 0.10832644605632048]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06110782217110196, 0.6712668800158956, 0.1592988517566819, 0.10832644605632048]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06110782217110196, 0.6712668800158956, 0.1592988517566819, 0.10832644605632048]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.061120332747683144, 0.6714045538487169, 0.1591264707115556, 0.10834864269204421]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.005]
 [0.008]
 [0.009]
 [0.009]] [[0.666]
 [0.285]
 [0.698]
 [0.404]
 [0.404]] [[0.367]
 [0.106]
 [0.388]
 [0.193]
 [0.193]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06113279629851324, 0.6715417101821767, 0.1589547376260234, 0.10837075589328675]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
605 2296
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06043001013930775, 0.6638068043501573, 0.16474826639982806, 0.11101491911070693]
first move QE:  -0.18434432325649938
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06043001013930775, 0.6638068043501573, 0.16474826639982806, 0.11101491911070693]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06043001013930775, 0.6638068043501573, 0.16474826639982806, 0.11101491911070693]
using another actor
606 2306
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.060443044966250754, 0.6639502471237728, 0.16456782104587842, 0.11103888686409799]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06044917724252095, 0.6640177302380956, 0.16458453835286857, 0.11094855416651483]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06046137191152032, 0.6641519274278429, 0.16461778245608946, 0.11076891820454722]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06051306267114948, 0.6647207624391497, 0.16390247057121793, 0.11086370431848283]
siam score:  -0.8517148
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.060538623612117044, 0.6650020498152417, 0.16354875077693037, 0.11091057579571079]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.060538623612117044, 0.6650020498152417, 0.16354875077693037, 0.11091057579571079]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]] [[0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]]
using another actor
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06057661531232194, 0.6654201324545886, 0.16302301049224516, 0.11098024174084417]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06057661531232194, 0.6654201324545886, 0.16302301049224516, 0.11098024174084417]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06057661531232194, 0.6654201324545886, 0.16302301049224516, 0.11098024174084417]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06057661531232194, 0.6654201324545886, 0.16302301049224516, 0.11098024174084417]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06057661531232194, 0.6654201324545886, 0.16302301049224516, 0.11098024174084417]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06057661531232194, 0.6654201324545886, 0.16302301049224516, 0.11098024174084417]
maxi score, test score, baseline:  0.0021 0.05 0.05
612 2350
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06058918697490626, 0.665558478299757, 0.1628490401400264, 0.11100329458531048]
using another actor
from probs:  [0.06058918697490626, 0.665558478299757, 0.1628490401400264, 0.11100329458531048]
from probs:  [0.06058918697490626, 0.665558478299757, 0.1628490401400264, 0.11100329458531048]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06062662875805537, 0.6659705093338095, 0.16233090976876674, 0.11107195213936821]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06062662875805537, 0.6659705093338095, 0.16233090976876674, 0.11107195213936821]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06062662875805537, 0.6659705093338095, 0.16233090976876674, 0.11107195213936821]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06062662875805537, 0.6659705093338095, 0.16233090976876674, 0.11107195213936821]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06062662875805537, 0.6659705093338095, 0.16233090976876674, 0.11107195213936821]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06062662875805537, 0.6659705093338095, 0.16233090976876674, 0.11107195213936821]
from probs:  [0.06062662875805537, 0.6659705093338095, 0.16233090976876674, 0.11107195213936821]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06063901910736071, 0.666106859902804, 0.1621594484828542, 0.11109467250698099]
from probs:  [0.06063901910736071, 0.666106859902804, 0.1621594484828542, 0.11109467250698099]
from probs:  [0.06063901910736071, 0.666106859902804, 0.1621594484828542, 0.11109467250698099]
siam score:  -0.8339038
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06063901910736071, 0.666106859902804, 0.1621594484828542, 0.11109467250698099]
from probs:  [0.06065136474123313, 0.6662427183973161, 0.16198860598221085, 0.11111731087924004]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06065136474123313, 0.6662427183973161, 0.16198860598221085, 0.11111731087924004]
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06066366590129636, 0.6663780874763131, 0.16181837892317624, 0.11113986769921443]
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.151]
 [0.139]
 [0.161]
 [0.113]] [[1.784]
 [1.544]
 [2.412]
 [2.629]
 [2.285]] [[0.49 ]
 [0.392]
 [0.947]
 [1.135]
 [0.81 ]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06066366590129636, 0.6663780874763131, 0.16181837892317624, 0.11113986769921443]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06066366590129636, 0.6663780874763131, 0.16181837892317624, 0.11113986769921443]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06066366590129636, 0.6663780874763131, 0.16181837892317624, 0.11113986769921443]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06066366590129636, 0.6663780874763131, 0.16181837892317624, 0.11113986769921443]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06067592282743645, 0.6665129697796397, 0.161648763986137, 0.11116234340678674]
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.   ]
 [0.   ]] [[ 0.237]
 [-0.202]
 [-0.092]
 [-0.2  ]
 [ 0.7  ]] [[0.306]
 [0.015]
 [0.088]
 [0.015]
 [0.614]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7669096197078904
siam score:  -0.8435396
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06067592282743645, 0.6665129697796397, 0.161648763986137, 0.11116234340678674]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06067592282743645, 0.6665129697796397, 0.161648763986137, 0.11116234340678674]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06067592282743645, 0.6665129697796397, 0.161648763986137, 0.11116234340678674]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06067592282743645, 0.6665129697796397, 0.161648763986137, 0.11116234340678674]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06067592282743645, 0.6665129697796397, 0.161648763986137, 0.11116234340678674]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06067592282743645, 0.6665129697796397, 0.161648763986137, 0.11116234340678674]
siam score:  -0.8414648
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06067592282743645, 0.6665129697796397, 0.161648763986137, 0.11116234340678674]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06067592282743645, 0.6665129697796397, 0.161648763986137, 0.11116234340678674]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06067592282743645, 0.6665129697796397, 0.161648763986137, 0.11116234340678674]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06067592282743645, 0.6665129697796397, 0.161648763986137, 0.11116234340678674]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06067592282743645, 0.6665129697796397, 0.161648763986137, 0.11116234340678674]
using another actor
from probs:  [0.06067592282743645, 0.6665129697796397, 0.161648763986137, 0.11116234340678674]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06067592282743645, 0.6665129697796397, 0.161648763986137, 0.11116234340678674]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.018]
 [0.021]
 [0.036]
 [0.036]] [[0.53 ]
 [0.802]
 [0.601]
 [0.53 ]
 [0.53 ]] [[0.036]
 [0.018]
 [0.021]
 [0.036]
 [0.036]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.132]] [[3.421]
 [3.421]
 [3.421]
 [3.421]
 [3.259]] [[1.42 ]
 [1.42 ]
 [1.42 ]
 [1.42 ]
 [1.303]]
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
siam score:  -0.8478634
line 256 mcts: sample exp_bonus -0.38173038000258586
maxi score, test score, baseline:  0.0041 0.05 0.05
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
from probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
in main func line 156:  632
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
633 2492
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[-0.13]
 [-0.13]
 [-0.13]
 [-0.13]
 [-0.13]] [[0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
siam score:  -0.85078603
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
first move QE:  -0.20061686473587403
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06139002770069209, 0.6743724376734429, 0.15569501385034606, 0.10854252077551907]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.037]
 [0.087]
 [0.035]
 [0.032]] [[0.991]
 [1.637]
 [1.377]
 [1.12 ]
 [1.675]] [[0.359]
 [1.067]
 [0.907]
 [0.546]
 [1.094]]
first move QE:  -0.1995647879032553
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06201566143012119, 0.6812581884838397, 0.15047887958065234, 0.10624727050538679]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06201566143012119, 0.6812581884838397, 0.15047887958065234, 0.10624727050538679]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06256830613198137, 0.6873406190253768, 0.14587128118443407, 0.10421979365820774]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06256830613198137, 0.6873406190253768, 0.14587128118443407, 0.10421979365820774]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06256830613198137, 0.6873406190253768, 0.14587128118443407, 0.10421979365820774]
from probs:  [0.06256830613198137, 0.6873406190253768, 0.14587128118443407, 0.10421979365820774]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06256830613198137, 0.6873406190253768, 0.14587128118443407, 0.10421979365820774]
640 2511
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06256830613198137, 0.6873406190253768, 0.14587128118443407, 0.10421979365820774]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06256830613198137, 0.6873406190253768, 0.14587128118443407, 0.10421979365820774]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06256830613198137, 0.6873406190253768, 0.14587128118443407, 0.10421979365820774]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
using another actor
from probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
from probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
first move QE:  -0.20085313642162292
line 256 mcts: sample exp_bonus 2.906744538611741
650 2531
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
line 256 mcts: sample exp_bonus 0.7232396778378396
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.005]
 [0.011]
 [0.012]
 [0.012]] [[1.403]
 [1.123]
 [1.189]
 [1.851]
 [1.346]] [[1.214]
 [0.996]
 [1.052]
 [1.555]
 [1.172]]
siam score:  -0.8737701
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
using another actor
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
664 2554
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
666 2563
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
from probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
siam score:  -0.86197037
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
using another actor
from probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
from probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
using another actor
siam score:  -0.87169385
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
first move QE:  -0.20980910539262804
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.024]
 [0.026]
 [0.028]
 [0.028]] [[-0.861]
 [-0.7  ]
 [-0.136]
 [-0.812]
 [-0.223]] [[0.041]
 [0.192]
 [0.76 ]
 [0.089]
 [0.677]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
from probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
676 2600
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0606756686452895, 0.6665135289803632, 0.1742702674581158, 0.0985405349162316]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.021]
 [0.002]
 [0.013]
 [0.017]] [[ 0.063]
 [ 0.5  ]
 [-0.061]
 [-0.244]
 [ 0.063]] [[0.017]
 [0.021]
 [0.002]
 [0.013]
 [0.017]]
using explorer policy with actor:  1
siam score:  -0.8718583
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
using explorer policy with actor:  1
from probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
first move QE:  -0.20482894417581152
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
using another actor
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05939267467877157, 0.6523932423448155, 0.18646422489292383, 0.10174985808348899]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05939267467877157, 0.6523932423448155, 0.18646422489292383, 0.10174985808348899]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05939267467877157, 0.6523932423448155, 0.18646422489292383, 0.10174985808348899]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05939267467877157, 0.6523932423448155, 0.18646422489292383, 0.10174985808348899]
siam score:  -0.8613193
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[-0.273]
 [-0.273]
 [-0.273]
 [-0.273]
 [-0.273]] [[0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.008]
 [0.015]
 [0.015]] [[-0.19 ]
 [-0.19 ]
 [ 0.232]
 [-0.19 ]
 [-0.19 ]] [[1.355]
 [1.355]
 [1.75 ]
 [1.355]
 [1.355]]
using another actor
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05776427013216105, 0.6344714597356779, 0.20194106753304028, 0.10582320259912081]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05776427013216105, 0.6344714597356779, 0.20194106753304028, 0.10582320259912081]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05776427013216105, 0.6344714597356779, 0.20194106753304028, 0.10582320259912081]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05776427013216105, 0.6344714597356779, 0.20194106753304028, 0.10582320259912081]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05776427013216105, 0.6344714597356779, 0.20194106753304028, 0.10582320259912081]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05776427013216105, 0.6344714597356779, 0.20194106753304028, 0.10582320259912081]
line 256 mcts: sample exp_bonus 0.3111353892468738
siam score:  -0.85943896
697 2645
using another actor
line 256 mcts: sample exp_bonus 2.5304623782434437
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05776427013216105, 0.6344714597356779, 0.20194106753304028, 0.10582320259912081]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05776427013216105, 0.6344714597356779, 0.20194106753304028, 0.10582320259912081]
first move QE:  -0.20719407520290645
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05776427013216105, 0.6344714597356779, 0.20194106753304028, 0.10582320259912081]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.058629816980529485, 0.6439974356283217, 0.19371465205309688, 0.10365809533805195]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.127]
 [0.192]
 [0.143]
 [0.118]] [[1.988]
 [1.988]
 [2.148]
 [2.616]
 [2.252]] [[1.   ]
 [1.   ]
 [1.251]
 [1.639]
 [1.245]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
using explorer policy with actor:  1
using explorer policy with actor:  1
707 2657
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
first move QE:  -0.20443914595338544
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.055629269264260796, 0.6109742142235155, 0.22223275275203724, 0.11116376376018626]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.055629269264260796, 0.6109742142235155, 0.22223275275203724, 0.11116376376018626]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.055629269264260796, 0.6109742142235155, 0.22223275275203724, 0.11116376376018626]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.055629269264260796, 0.6109742142235155, 0.22223275275203724, 0.11116376376018626]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.055629269264260796, 0.6109742142235155, 0.22223275275203724, 0.11116376376018626]
from probs:  [0.055629269264260796, 0.6109742142235155, 0.22223275275203724, 0.11116376376018626]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]] [[-0.322]
 [-0.322]
 [ 0.244]
 [-0.322]
 [-0.322]] [[0.631]
 [0.631]
 [1.008]
 [0.631]
 [0.631]]
siam score:  -0.863912
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.055629269264260796, 0.6109742142235155, 0.22223275275203724, 0.11116376376018626]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.055629269264260796, 0.6109742142235155, 0.22223275275203724, 0.11116376376018626]
actor:  1 policy actor:  1  step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
from probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
siam score:  -0.86258215
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]] [[0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
from probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
using explorer policy with actor:  0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05677380321224654, 0.62357064712299, 0.21135476064244935, 0.10830078902231413]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.058629816980529485, 0.6439974356283217, 0.19371465205309688, 0.10365809533805195]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.058629816980529485, 0.6439974356283217, 0.19371465205309688, 0.10365809533805195]
maxi score, test score, baseline:  0.0041 0.05 0.05
from probs:  [0.058629816980529485, 0.6439974356283217, 0.19371465205309688, 0.10365809533805195]
using another actor
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.058629816980529485, 0.6439974356283217, 0.19371465205309688, 0.10365809533805195]
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.145]] [[3.122]
 [3.122]
 [3.122]
 [3.122]
 [3.11 ]] [[1.665]
 [1.665]
 [1.665]
 [1.665]
 [1.656]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.058629816980529485, 0.6439974356283217, 0.19371465205309688, 0.10365809533805195]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05939267467877157, 0.6523932423448155, 0.18646422489292383, 0.10174985808348899]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.05939267467877157, 0.6523932423448155, 0.18646422489292383, 0.10174985808348899]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
rdn probs:  [0.06007009455231831, 0.6598487433344711, 0.18002582430874883, 0.10005533780446181]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06007002180465934, 0.6598489003162613, 0.18002579750697975, 0.10005528037209947]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06007002180465934, 0.6598489003162613, 0.18002579750697975, 0.10005528037209947]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06007002180465934, 0.6598489003162613, 0.18002579750697975, 0.10005528037209947]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06007002180465934, 0.6598489003162613, 0.18002579750697975, 0.10005528037209947]
from probs:  [0.06007002180465934, 0.6598489003162613, 0.18002579750697975, 0.10005528037209947]
siam score:  -0.86933607
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.066]
 [0.041]
 [0.041]
 [0.041]] [[0.35 ]
 [0.838]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[0.862]
 [1.236]
 [0.862]
 [0.862]
 [0.862]]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067559997501586, 0.6665136800549651, 0.17427023999000632, 0.09854047998001268]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06122018607600425, 0.6725072025918001, 0.16909436546114467, 0.09717824587105106]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06122018607600425, 0.6725072025918001, 0.16909436546114467, 0.09717824587105106]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06122018607600425, 0.6725072025918001, 0.16909436546114467, 0.09717824587105106]
maxi score, test score, baseline:  0.0041 0.0 0.0041
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
from probs:  [0.06122018607600425, 0.6725072025918001, 0.16909436546114467, 0.09717824587105106]
using explorer policy with actor:  1
744 2719
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06122018607600425, 0.6725072025918001, 0.16909436546114467, 0.09717824587105106]
maxi score, test score, baseline:  0.0041 0.0 0.0041
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06122018607600425, 0.6725072025918001, 0.16909436546114467, 0.09717824587105106]
siam score:  -0.8808104
first move QE:  -0.20073904646040974
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06122018607600425, 0.6725072025918001, 0.16909436546114467, 0.09717824587105106]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06122018607600425, 0.6725072025918001, 0.16909436546114467, 0.09717824587105106]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06122018607600425, 0.6725072025918001, 0.16909436546114467, 0.09717824587105106]
first move QE:  -0.20073904646040974
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06122018607600425, 0.6725072025918001, 0.16909436546114467, 0.09717824587105106]
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]] [[1.541]
 [1.541]
 [1.541]
 [1.541]
 [1.541]] [[1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
749 2728
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061712552745698, 0.67792601648705, 0.16441479670259002, 0.09594663406466199]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061712552745698, 0.67792601648705, 0.16441479670259002, 0.09594663406466199]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061712552745698, 0.67792601648705, 0.16441479670259002, 0.09594663406466199]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061712552745698, 0.67792601648705, 0.16441479670259002, 0.09594663406466199]
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.076]
 [0.092]
 [0.076]
 [0.078]] [[0.537]
 [0.676]
 [0.511]
 [0.676]
 [0.853]] [[0.639]
 [0.733]
 [0.655]
 [0.733]
 [0.853]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061712552745698, 0.67792601648705, 0.16441479670259002, 0.09594663406466199]
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.074]
 [0.136]
 [0.074]
 [0.074]] [[1.293]
 [1.293]
 [1.782]
 [1.293]
 [1.293]] [[1.171]
 [1.171]
 [1.782]
 [1.171]
 [1.171]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061712552745698, 0.67792601648705, 0.16441479670259002, 0.09594663406466199]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.027]
 [0.025]
 [0.027]
 [0.027]] [[0.792]
 [0.792]
 [0.887]
 [0.792]
 [0.792]] [[0.313]
 [0.313]
 [0.372]
 [0.313]
 [0.313]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061712552745698, 0.67792601648705, 0.16441479670259002, 0.09594663406466199]
from probs:  [0.061712552745698, 0.67792601648705, 0.16441479670259002, 0.09594663406466199]
758 2738
from probs:  [0.061712552745698, 0.67792601648705, 0.16441479670259002, 0.09594663406466199]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061712552745698, 0.67792601648705, 0.16441479670259002, 0.09594663406466199]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061712552745698, 0.67792601648705, 0.16441479670259002, 0.09594663406466199]
siam score:  -0.8673181
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061712552745698, 0.67792601648705, 0.16441479670259002, 0.09594663406466199]
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.05967283556474122, 0.65547961118816, 0.158973964835311, 0.12587358841178775]
maxi score, test score, baseline:  0.0041 0.0 0.0041
siam score:  -0.8783152
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[1.21]
 [1.21]
 [1.21]
 [1.21]
 [1.21]] [[0.997]
 [0.997]
 [0.997]
 [0.997]
 [0.997]] [[2.328]
 [2.328]
 [2.328]
 [2.328]
 [2.328]]
Printing some Q and Qe and total Qs values:  [[0.909]
 [0.909]
 [0.613]
 [0.909]
 [0.687]] [[2.527]
 [2.527]
 [1.776]
 [2.527]
 [2.156]] [[2.102]
 [2.102]
 [1.009]
 [2.102]
 [1.411]]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.044]] [[2.652]
 [2.652]
 [2.652]
 [2.652]
 [2.836]] [[1.526]
 [1.526]
 [1.526]
 [1.526]
 [1.655]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
siam score:  -0.88278055
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
line 256 mcts: sample exp_bonus 2.086222319849027
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
first move QE:  -0.19718254503189794
using another actor
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586654]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
from probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
first move QE:  -0.19466994147382838
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.091]
 [0.096]
 [0.099]
 [0.099]] [[1.393]
 [1.426]
 [1.371]
 [1.393]
 [1.393]] [[1.108]
 [1.137]
 [1.074]
 [1.108]
 [1.108]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
from probs:  [0.06019635789379983, 0.6612412245634338, 0.15509817894689992, 0.12346423859586655]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
maxi score, test score, baseline:  0.0041 0.0 0.0041
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
siam score:  -0.8843898
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
siam score:  -0.8851229
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367252, 0.12125931361557177]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367252, 0.12125931361557177]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367252, 0.12125931361557177]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367252, 0.12125931361557177]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367252, 0.12125931361557177]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367252, 0.12125931361557177]
using explorer policy with actor:  1
789 2788
line 256 mcts: sample exp_bonus 0.23537965497652594
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061115568611001155, 0.6713575777139205, 0.14829299848284677, 0.11923385519223158]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061115568611001155, 0.6713575777139205, 0.14829299848284677, 0.11923385519223158]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061115568611001155, 0.6713575777139205, 0.14829299848284677, 0.11923385519223158]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061115568611001155, 0.6713575777139205, 0.14829299848284677, 0.11923385519223158]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061115568611001155, 0.6713575777139205, 0.14829299848284677, 0.11923385519223158]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061115568611001155, 0.6713575777139205, 0.14829299848284677, 0.11923385519223158]
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[1.963]
 [1.963]
 [1.963]
 [1.963]
 [1.963]] [[1.61]
 [1.61]
 [1.61]
 [1.61]
 [1.61]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061115568611001155, 0.6713575777139205, 0.14829299848284677, 0.11923385519223158]
794 2800
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.061115568611001155, 0.6713575777139205, 0.14829299848284677, 0.11923385519223158]
from probs:  [0.061115568611001155, 0.6713575777139205, 0.14829299848284677, 0.11923385519223158]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
using another actor
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
using another actor
first move QE:  -0.19128958579517685
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.009]
 [0.013]
 [0.01 ]
 [0.01 ]] [[-1.123]
 [-0.217]
 [-0.458]
 [-0.617]
 [-0.583]] [[0.011]
 [0.009]
 [0.013]
 [0.01 ]
 [0.01 ]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06067546119937027, 0.6665139853613854, 0.15155123982367255, 0.12125931361557178]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
using another actor
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06455343390479171, 0.7090144242857809, 0.13023888100131562, 0.09619326080811176]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06455343390479171, 0.7090144242857809, 0.13023888100131562, 0.09619326080811176]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06455343390479171, 0.7090144242857809, 0.13023888100131562, 0.09619326080811176]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06455343390479171, 0.7090144242857809, 0.13023888100131562, 0.09619326080811176]
maxi score, test score, baseline:  0.0041 0.0 0.0041
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06455343390479171, 0.7090144242857809, 0.13023888100131562, 0.09619326080811176]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06455343390479171, 0.7090144242857809, 0.13023888100131562, 0.09619326080811176]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06454412329175331, 0.7089123851094261, 0.13029004977969647, 0.09625344181912412]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.88165087
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0648601419120027, 0.712398423218654, 0.12761470948288742, 0.09512672538645596]
siam score:  -0.88098085
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0648601419120027, 0.712398423218654, 0.12761470948288742, 0.09512672538645596]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0648601419120027, 0.712398423218654, 0.12761470948288742, 0.09512672538645596]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0648601419120027, 0.712398423218654, 0.12761470948288742, 0.09512672538645596]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06514865507834684, 0.715581045470773, 0.12517222397532043, 0.09409807547555978]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06514865507834684, 0.715581045470773, 0.12517222397532043, 0.09409807547555978]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06514865507834684, 0.715581045470773, 0.12517222397532043, 0.09409807547555978]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0651400499222751, 0.7154867272896864, 0.12521994769903638, 0.09415327508900215]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8190526227214154
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06564835029235465, 0.7210937991826722, 0.12091880605132807, 0.09233904447364501]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06564835029235465, 0.7210937991826722, 0.12091880605132807, 0.09233904447364501]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06564835029235465, 0.7210937991826722, 0.12091880605132807, 0.09233904447364501]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06564835029235465, 0.7210937991826722, 0.12091880605132807, 0.09233904447364501]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06564835029235465, 0.7210937991826722, 0.12091880605132807, 0.09233904447364501]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06564835029235465, 0.7210937991826722, 0.12091880605132807, 0.09233904447364501]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06564835029235465, 0.7210937991826722, 0.12091880605132807, 0.09233904447364501]
line 256 mcts: sample exp_bonus -0.24760950352526873
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06564835029235465, 0.7210937991826722, 0.12091880605132807, 0.09233904447364501]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06587315756222956, 0.7235736528252282, 0.11901652942891464, 0.09153666018362763]
UNIT TEST: sample policy line 217 mcts : [0.292 0.    0.625 0.042 0.042]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06608130265719153, 0.7258697056717793, 0.11725524498096707, 0.09079374669006218]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]] [[1.847]
 [1.847]
 [1.847]
 [1.847]
 [1.847]] [[1.277]
 [1.277]
 [1.277]
 [1.277]
 [1.277]]
from probs:  [0.06607381550108048, 0.7257876279960573, 0.11729732824599763, 0.0908412282568646]
from probs:  [0.06607381550108048, 0.7257876279960573, 0.11729732824599763, 0.0908412282568646]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06607381550108048, 0.7257876279960573, 0.11729732824599763, 0.0908412282568646]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06626731414355143, 0.7279220971834651, 0.11566073929255236, 0.09014984938043101]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06644746328803779, 0.729909309047764, 0.11413705883632774, 0.0895061688278703]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06644746328803779, 0.729909309047764, 0.11413705883632774, 0.0895061688278703]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06644746328803779, 0.729909309047764, 0.11413705883632774, 0.0895061688278703]
siam score:  -0.8794131
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.066615598345019, 0.7317639944030025, 0.1127149921312159, 0.08890541512076254]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.066615598345019, 0.7317639944030025, 0.1127149921312159, 0.08890541512076254]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.066615598345019, 0.7317639944030025, 0.1127149921312159, 0.08890541512076254]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.066615598345019, 0.7317639944030025, 0.1127149921312159, 0.08890541512076254]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.066615598345019, 0.7317639944030025, 0.1127149921312159, 0.08890541512076254]
siam score:  -0.8730581
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.066615598345019, 0.7317639944030025, 0.1127149921312159, 0.08890541512076254]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.066615598345019, 0.7317639944030025, 0.1127149921312159, 0.08890541512076254]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.066615598345019, 0.7317639944030025, 0.1127149921312159, 0.08890541512076254]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06644746328803779, 0.729909309047764, 0.11413705883632774, 0.0895061688278703]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.191]] [[2.163]
 [2.163]
 [2.163]
 [2.163]
 [2.062]] [[1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.117]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06660879234017673, 0.7316893770433914, 0.1127535277937853, 0.08894830282264657]
siam score:  -0.873243
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06660879234017673, 0.7316893770433914, 0.1127535277937853, 0.08894830282264657]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
using another actor
using explorer policy with actor:  1
start point for exploration sampling:  20063
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.012]
 [0.013]
 [0.017]
 [0.014]] [[-0.791]
 [-0.541]
 [-0.295]
 [-0.558]
 [-0.574]] [[0.269]
 [0.507]
 [0.748]
 [0.497]
 [0.478]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06660201764617094, 0.731615102958873, 0.1127918861741081, 0.0889909932208479]
Printing some Q and Qe and total Qs values:  [[0.09 ]
 [0.09 ]
 [0.105]
 [0.09 ]
 [0.09 ]] [[0.748]
 [0.748]
 [1.03 ]
 [0.748]
 [0.748]] [[0.569]
 [0.569]
 [0.785]
 [0.569]
 [0.569]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2206531554506952
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06660201764617094, 0.731615102958873, 0.1127918861741081, 0.0889909932208479]
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06675968422058577, 0.7333542817030886, 0.11145958548661598, 0.08842644858970958]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06675968422058577, 0.7333542817030886, 0.11145958548661598, 0.08842644858970958]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06675968422058577, 0.7333542817030886, 0.11145958548661598, 0.08842644858970958]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06675968422058577, 0.7333542817030886, 0.11145958548661598, 0.08842644858970958]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06675968422058577, 0.7333542817030886, 0.11145958548661598, 0.08842644858970958]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06675968422058577, 0.7333542817030886, 0.11145958548661598, 0.08842644858970958]
siam score:  -0.88228226
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06675968422058577, 0.7333542817030886, 0.11145958548661598, 0.08842644858970958]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06675968422058577, 0.7333542817030886, 0.11145958548661598, 0.08842644858970958]
first move QE:  -0.1977376009939158
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.06690749681667693, 0.7349847637897524, 0.1102105520492935, 0.0878971873442771]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.88273454
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.07230844512769778, 0.7651841970551135, 0.09284572158904379, 0.06966163622814484]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.07230844512769778, 0.7651841970551135, 0.09284572158904379, 0.06966163622814484]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.07228269977178055, 0.7658240485186907, 0.09217412458695991, 0.06971912712256889]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.07228269977178055, 0.7658240485186907, 0.09217412458695991, 0.06971912712256889]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.07228269977178055, 0.7658240485186907, 0.09217412458695991, 0.06971912712256889]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.07228269977178055, 0.7658240485186907, 0.09217412458695991, 0.06971912712256889]
from probs:  [0.07228269977178055, 0.7658240485186907, 0.09217412458695991, 0.06971912712256889]
maxi score, test score, baseline:  0.0041 0.0 0.0041
847 2884
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.07228269977178055, 0.7658240485186907, 0.09217412458695991, 0.06971912712256889]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.07228269977178055, 0.7658240485186907, 0.09217412458695991, 0.06971912712256889]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.07228269977178055, 0.7658240485186907, 0.09217412458695991, 0.06971912712256889]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.09366186049374842, 0.748171620380489, 0.09005192121562891, 0.06811459791013358]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.09366186049374842, 0.748171620380489, 0.09005192121562891, 0.06811459791013358]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.09366186049374842, 0.748171620380489, 0.09005192121562891, 0.06811459791013358]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.09366186049374842, 0.748171620380489, 0.09005192121562891, 0.06811459791013358]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.09366186049374842, 0.748171620380489, 0.09005192121562891, 0.06811459791013358]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.006]
 [0.006]
 [0.014]
 [0.015]] [[0.708]
 [1.001]
 [1.001]
 [0.791]
 [0.997]] [[0.086]
 [0.166]
 [0.166]
 [0.113]
 [0.182]]
siam score:  -0.8740872
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.09352936757600752, 0.7482810176454079, 0.09006507313119588, 0.06812454164738879]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.09352936757600752, 0.7482810176454079, 0.09006507313119588, 0.06812454164738879]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.09352936757600752, 0.7482810176454079, 0.09006507313119588, 0.06812454164738879]
in main func line 156:  855
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.09352936757600752, 0.7482810176454079, 0.09006507313119588, 0.06812454164738879]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.09352936757600752, 0.7482810176454079, 0.09006507313119588, 0.06812454164738879]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.09352936757600752, 0.7482810176454079, 0.09006507313119588, 0.06812454164738879]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.09352936757600752, 0.7482810176454079, 0.09006507313119588, 0.06812454164738879]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.09352936757600752, 0.7482810176454079, 0.09006507313119588, 0.06812454164738879]
line 256 mcts: sample exp_bonus 0.6942535469762169
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.031]
 [0.028]
 [0.023]
 [0.025]] [[-1.076]
 [-0.26 ]
 [ 0.448]
 [-0.465]
 [-0.689]] [[0.017]
 [0.573]
 [1.037]
 [0.42 ]
 [0.275]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12935223447377717, 0.7197605964743155, 0.08535615394054394, 0.06553101511136358]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12935223447377717, 0.7197605964743155, 0.08535615394054394, 0.06553101511136358]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12935223447377717, 0.7197605964743155, 0.08535615394054394, 0.06553101511136358]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12935223447377717, 0.7197605964743155, 0.08535615394054394, 0.06553101511136358]
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.132]
 [0.135]
 [0.096]
 [0.108]] [[0.061]
 [0.702]
 [0.757]
 [0.806]
 [0.271]] [[0.023]
 [0.646]
 [0.69 ]
 [0.644]
 [0.311]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12935223447377717, 0.7197605964743155, 0.08535615394054394, 0.06553101511136358]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.417 0.083 0.333 0.042 0.125]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12935223447377717, 0.7197605964743155, 0.08535615394054394, 0.06553101511136358]
maxi score, test score, baseline:  0.0041 0.0 0.0041
using another actor
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12935223447377717, 0.7197605964743155, 0.08535615394054394, 0.06553101511136358]
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]] [[0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]] [[0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12935223447377717, 0.7197605964743155, 0.08535615394054394, 0.06553101511136358]
actor:  0 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.14678914113323563, 0.7053420244121956, 0.0836483804147445, 0.06422045403982415]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.14678914113323563, 0.7053420244121956, 0.0836483804147445, 0.06422045403982415]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.14678914113323563, 0.7053420244121956, 0.0836483804147445, 0.06422045403982415]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.14678914113323563, 0.7053420244121956, 0.0836483804147445, 0.06422045403982415]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.14678914113323563, 0.7053420244121956, 0.0836483804147445, 0.06422045403982415]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0061 0.0 0.0061
864 2910
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.117]
 [0.147]
 [0.147]
 [0.147]] [[1.533]
 [1.405]
 [1.08 ]
 [1.08 ]
 [1.08 ]] [[0.447]
 [0.356]
 [0.306]
 [0.306]
 [0.306]]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.0 0.0061
in main func line 156:  867
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.1629923546474759, 0.6919435954413922, 0.08206143373227098, 0.06300261617886095]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.1629923546474759, 0.6919435954413922, 0.08206143373227098, 0.06300261617886095]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.1629923546474759, 0.6919435954413922, 0.08206143373227098, 0.06300261617886095]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.1629923546474759, 0.6919435954413922, 0.08206143373227098, 0.06300261617886095]
using another actor
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.1629923546474759, 0.6919435954413922, 0.08206143373227098, 0.06300261617886095]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.16299235464747594, 0.6919435954413922, 0.08206143373227098, 0.06300261617886095]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.111]
 [0.183]
 [0.111]
 [0.117]] [[1.739]
 [1.181]
 [1.457]
 [1.181]
 [1.101]] [[0.92 ]
 [0.564]
 [0.893]
 [0.564]
 [0.524]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.16299235464747594, 0.6919435954413922, 0.08206143373227098, 0.06300261617886095]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.095]
 [0.02 ]
 [0.041]
 [0.049]] [[ 0.   ]
 [-0.096]
 [ 0.246]
 [-0.308]
 [ 0.   ]] [[0.249]
 [0.276]
 [0.355]
 [0.028]
 [0.249]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.15835521795245003, 0.6966863697224646, 0.08152568679384589, 0.06343272553123946]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.15835521795245003, 0.6966863697224646, 0.08152568679384589, 0.06343272553123946]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
871 2935
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.15789990989436148, 0.6970633553152118, 0.08156974403173506, 0.06346699075869161]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.15789990989436148, 0.6970633553152118, 0.08156974403173506, 0.06346699075869161]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.15789990989436148, 0.6970633553152118, 0.08156974403173506, 0.06346699075869161]
siam score:  -0.8774638
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.15767347779510477, 0.6972508363791614, 0.08159165441237837, 0.06348403141335535]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.15767347779510477, 0.6972508363791614, 0.08159165441237837, 0.06348403141335535]
874 2945
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.17283056184024884, 0.6847010863548226, 0.08012500069162885, 0.06234335111329962]
using another actor
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.17233613986335122, 0.6851104574580061, 0.08017284273234575, 0.06238055994629692]
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.141]
 [0.134]
 [0.161]
 [0.138]] [[1.911]
 [2.203]
 [1.911]
 [2.048]
 [2.174]] [[0.767]
 [0.975]
 [0.767]
 [0.912]
 [0.95 ]]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.16988436931738116, 0.6875518675400232, 0.07996174052860375, 0.06260202261399175]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.16754898381132383, 0.6898773843912053, 0.07976065930274183, 0.06281297249472896]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.16754898381132383, 0.6898773843912053, 0.07976065930274183, 0.06281297249472896]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.16754898381132383, 0.6898773843912053, 0.07976065930274183, 0.06281297249472896]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.16754898381132383, 0.6898773843912053, 0.07976065930274183, 0.06281297249472896]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.16754898381132383, 0.6898773843912053, 0.07976065930274183, 0.06281297249472896]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.17871482431275434, 0.6809871035006072, 0.07829354365874977, 0.06200452852788875]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]] [[-0.135]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.135]] [[1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
line 256 mcts: sample exp_bonus -0.10086882501968629
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.18121799268671382, 0.6785465342130357, 0.0784523776140007, 0.06178309548624974]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.19750558171584576, 0.6646683840678773, 0.07730393776954331, 0.060522096446733765]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.19750558171584576, 0.6646683840678773, 0.07730393776954331, 0.060522096446733765]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.19695107944125878, 0.6651277753303869, 0.07735729377919423, 0.06056385144916015]
887 2966
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.3473994684796225
using another actor
891 2972
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.19352811645403634, 0.6683421244161127, 0.07727415689165404, 0.060855602238196846]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.19352811645403634, 0.6683421244161127, 0.07727415689165404, 0.060855602238196846]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.19352811645403634, 0.6683421244161127, 0.07727415689165404, 0.060855602238196846]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.19352811645403634, 0.6683421244161127, 0.07727415689165404, 0.060855602238196846]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.19352811645403634, 0.6683421244161127, 0.07727415689165404, 0.060855602238196846]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.19352811645403634, 0.6683421244161127, 0.07727415689165404, 0.060855602238196846]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.18816259985189843, 0.6734944159579811, 0.07701984175561763, 0.06132314243450281]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.18816259985189837, 0.6734944159579811, 0.07701984175561763, 0.06132314243450281]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.18790403852458057, 0.6737089734559083, 0.07704434439217314, 0.061342643627338]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.18790403852458057, 0.6737089734559083, 0.07704434439217314, 0.061342643627338]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.18790403852458057, 0.6737089734559083, 0.07704434439217314, 0.061342643627338]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.18790403852458057, 0.6737089734559083, 0.07704434439217314, 0.061342643627338]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.18790403852458057, 0.6737089734559083, 0.07704434439217314, 0.061342643627338]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.18790403852458057, 0.6737089734559083, 0.07704434439217314, 0.061342643627338]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.18514626155122607, 0.6763249915777242, 0.07694868505757801, 0.06158006181347171]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0061 0.0 0.0061
from probs:  [0.18225741772365203, 0.6790363544168704, 0.0768800675159997, 0.06182616034347794]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.182257417723652, 0.6790363544168703, 0.07688006751599968, 0.06182616034347792]
siam score:  -0.8834079
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.182257417723652, 0.6790363544168703, 0.07688006751599968, 0.06182616034347792]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.182257417723652, 0.6790363544168703, 0.07688006751599968, 0.06182616034347792]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.182257417723652, 0.6790363544168703, 0.07688006751599968, 0.06182616034347792]
first move QE:  -0.21084504683566482
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.182257417723652, 0.6790363544168703, 0.07688006751599968, 0.06182616034347792]
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.08 ]
 [0.086]
 [0.08 ]
 [0.075]] [[0.619]
 [0.619]
 [0.461]
 [0.619]
 [0.761]] [[0.602]
 [0.602]
 [0.508]
 [0.602]
 [0.686]]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8834634
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.17997035464863786, 0.6812351845320722, 0.07676877202557789, 0.06202568879371217]
siam score:  -0.88272303
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.17997035464863786, 0.6812351845320722, 0.07676877202557789, 0.06202568879371217]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.17997035464863786, 0.6812351845320722, 0.07676877202557789, 0.06202568879371217]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.17997035464863786, 0.6812351845320722, 0.07676877202557789, 0.06202568879371217]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.17997035464863786, 0.6812351845320722, 0.07676877202557789, 0.06202568879371217]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.17997035464863786, 0.6812351845320722, 0.07676877202557789, 0.06202568879371217]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.17997035464863786, 0.6812351845320722, 0.07676877202557789, 0.06202568879371217]
from probs:  [0.17997035464863786, 0.6812351845320722, 0.07676877202557789, 0.06202568879371217]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.17997035464863786, 0.6812351845320722, 0.07676877202557789, 0.06202568879371217]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.028]
 [0.024]
 [0.024]
 [0.025]] [[-0.789]
 [-0.322]
 [-0.471]
 [-0.902]
 [-0.756]] [[0.025]
 [0.028]
 [0.024]
 [0.024]
 [0.025]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.17997035464863784, 0.6812351845320722, 0.07676877202557787, 0.06202568879371217]
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 1.2956345867617807
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [0.1799703583840633, 0.6812351615297153, 0.07676878126584086, 0.06202569882038051]
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.038]
 [0.038]
 [0.038]
 [0.038]] [[2.043]
 [1.422]
 [1.422]
 [1.422]
 [1.573]] [[0.029]
 [0.038]
 [0.038]
 [0.038]
 [0.038]]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [0.1799703583840633, 0.6812351615297153, 0.07676878126584086, 0.06202569882038051]
actor:  0 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.888943
line 256 mcts: sample exp_bonus 1.5049626080519256
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.17997035881194345, 0.6812351588948745, 0.0767687823242812, 0.06202569996890087]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.17777582899211658, 0.6833450260473006, 0.0766619895810798, 0.062217155379503106]
rdn probs:  [0.17777582899211658, 0.6833450260473006, 0.0766619895810798, 0.062217155379503106]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.17566862557919008, 0.6853694787504582, 0.0765601263514435, 0.06240176931890828]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1716947958933319, 0.6891900578156601, 0.0763667213286925, 0.06274842496231545]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1716947958933319, 0.6891900578156601, 0.0763667213286925, 0.06274842496231545]
using another actor
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1716947958933319, 0.6891900578156601, 0.0763667213286925, 0.06274842496231545]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.256]] [[1.22]
 [1.22]
 [1.22]
 [1.22]
 [1.22]]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
from probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
from probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
line 256 mcts: sample exp_bonus 0.013674699079625818
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
siam score:  -0.8959663
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
first move QE:  -0.20740945222009569
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
from probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
siam score:  -0.9000392
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
using another actor
siam score:  -0.9000233
siam score:  -0.9004024
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16981943764988425, 0.6909930929256369, 0.07627544824141577, 0.06291202118306313]
from probs:  [0.1680129852739253, 0.6927298795208032, 0.07618752878072164, 0.06306960642454969]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1680129852739253, 0.6927298795208032, 0.07618752878072164, 0.06306960642454969]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1680129852739253, 0.6927298795208032, 0.07618752878072164, 0.06306960642454969]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1680129852739253, 0.6927298795208032, 0.07618752878072164, 0.06306960642454969]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1680129852739253, 0.6927298795208032, 0.07618752878072164, 0.06306960642454969]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1680129852739253, 0.6927298795208032, 0.07618752878072164, 0.06306960642454969]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1680129852739253, 0.6927298795208032, 0.07618752878072164, 0.06306960642454969]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1680129852739253, 0.6927298795208032, 0.07618752878072164, 0.06306960642454969]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1680129852739253, 0.6927298795208032, 0.07618752878072164, 0.06306960642454969]
Printing some Q and Qe and total Qs values:  [[1.059]
 [1.059]
 [1.059]
 [1.059]
 [1.059]] [[1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.252]] [[2.179]
 [2.179]
 [2.179]
 [2.179]
 [2.179]]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16627170958638943, 0.6944040029645482, 0.07610278144865502, 0.06322150600040724]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16627170958638943, 0.6944040029645482, 0.07610278144865502, 0.06322150600040724]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16627170958638943, 0.6944040029645482, 0.07610278144865502, 0.06322150600040724]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16627170958638943, 0.6944040029645482, 0.07610278144865502, 0.06322150600040724]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16627170958638943, 0.6944040029645482, 0.07610278144865502, 0.06322150600040724]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16627170958638943, 0.6944040029645482, 0.07610278144865502, 0.06322150600040724]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16627170958638943, 0.6944040029645482, 0.07610278144865502, 0.06322150600040724]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16627170958638943, 0.6944040029645482, 0.07610278144865502, 0.06322150600040724]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16627170958638943, 0.6944040029645482, 0.07610278144865502, 0.06322150600040724]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.16627170958638943, 0.6944040029645482, 0.07610278144865502, 0.06322150600040724]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.17687506078907989, 0.6855702900824371, 0.07513601493040846, 0.06241863419807453]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.17687506078907989, 0.6855702900824371, 0.07513601493040846, 0.06241863419807453]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1872119787891014, 0.6769585442341107, 0.07419354060948383, 0.061635936367304106]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1872119787891014, 0.6769585442341107, 0.07419354060948383, 0.061635936367304106]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1872119787891014, 0.6769585442341107, 0.07419354060948383, 0.061635936367304106]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1872119787891014, 0.6769585442341107, 0.07419354060948383, 0.061635936367304106]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1872119787891014, 0.6769585442341107, 0.07419354060948383, 0.061635936367304106]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1872119787891014, 0.6769585442341107, 0.07419354060948383, 0.061635936367304106]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.1872119787891014, 0.6769585442341107, 0.07419354060948383, 0.061635936367304106]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.19514468540257662, 0.6705574119135798, 0.07324398629719119, 0.06105391638665264]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
siam score:  -0.90165293
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.19514468540257662, 0.6705574119135798, 0.07324398629719119, 0.06105391638665264]
using explorer policy with actor:  1
953 3072
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20483802966440537, 0.6624793290650975, 0.07236291667999446, 0.06031972459050256]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.3132],
        [0.1481],
        [0.0000],
        [0.0759],
        [0.3177],
        [0.6842],
        [0.0895],
        [0.1958],
        [0.6693],
        [0.0720]], dtype=torch.float64)
0.0 0.3131693676543903
0.0 0.14813809561369048
0.99 0.99
0.0 0.07590052415104652
0.0 0.3177358592201736
0.0 0.684234450878301
0.0 0.08945036375649221
0.0 0.1958332064398299
0.0 0.6692531596800955
0.0 0.07195018767980915
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20483802966440537, 0.6624793290650975, 0.07236291667999446, 0.06031972459050256]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20483802966440537, 0.6624793290650975, 0.07236291667999446, 0.06031972459050256]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20483802966440537, 0.6624793290650975, 0.07236291667999446, 0.06031972459050256]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
from probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
from probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
first move QE:  -0.2061700052564744
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.205]
 [0.22 ]
 [0.226]
 [0.206]] [[1.099]
 [0.86 ]
 [1.19 ]
 [1.408]
 [0.964]] [[0.66 ]
 [0.385]
 [0.855]
 [1.158]
 [0.524]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
first move QE:  -0.20707849772059925
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
siam score:  -0.92162204
maxi score, test score, baseline:  0.026099999999999998 0.5 0.5
probs:  [0.20712569902260314, 0.6603683093550843, 0.07237789595078442, 0.06012809567152818]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.21430056525057006, 0.6545935938268728, 0.07150282625285023, 0.05960301466970691]
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.   ]
 [0.17 ]
 [0.087]
 [0.123]] [[-1.556]
 [-0.838]
 [-0.547]
 [-1.152]
 [-0.98 ]] [[0.058]
 [0.128]
 [0.564]
 [0.198]
 [0.327]]
siam score:  -0.9223127
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0281 0.5 0.5
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.21430056525057006, 0.6545935938268728, 0.07150282625285023, 0.05960301466970691]
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
970 3099
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2119595731157137, 0.6567399489935228, 0.071502612312195, 0.05979786557856845]
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2119595731157137, 0.6567399489935228, 0.071502612312195, 0.05979786557856845]
maxi score, test score, baseline:  0.0281 0.5 0.5
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2119595731157137, 0.6567399489935228, 0.071502612312195, 0.05979786557856845]
siam score:  -0.9207718
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2119595731157137, 0.6567399489935228, 0.071502612312195, 0.05979786557856845]
maxi score, test score, baseline:  0.0281 0.5 0.5
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2119595731157137, 0.6567399489935228, 0.071502612312195, 0.05979786557856845]
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2119595731157137, 0.6567399489935228, 0.071502612312195, 0.05979786557856845]
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2119595731157137, 0.6567399489935228, 0.071502612312195, 0.05979786557856845]
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2119595731157137, 0.6567399489935228, 0.071502612312195, 0.05979786557856845]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2210768339971316, 0.6491396908395844, 0.0706763707822157, 0.059107104381068316]
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.195]
 [0.128]
 [0.136]
 [0.181]] [[1.409]
 [0.779]
 [0.942]
 [1.036]
 [0.639]] [[0.161]
 [0.195]
 [0.128]
 [0.136]
 [0.181]]
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2210768339971316, 0.6491396908395844, 0.0706763707822157, 0.059107104381068316]
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2210768339971316, 0.6491396908395844, 0.0706763707822157, 0.059107104381068316]
first move QE:  -0.21042292565720055
maxi score, test score, baseline:  0.0281 0.5 0.5
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2210768339971316, 0.6491396908395844, 0.0706763707822157, 0.059107104381068316]
maxi score, test score, baseline:  0.0281 0.5 0.5
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2210768339971316, 0.6491396908395844, 0.0706763707822157, 0.059107104381068316]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.559]
 [0.612]
 [0.559]
 [0.559]] [[1.171]
 [1.171]
 [1.101]
 [1.171]
 [1.171]] [[1.642]
 [1.642]
 [1.724]
 [1.642]
 [1.642]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2274868234880511, 0.643980588959106, 0.0698945879044087, 0.05863799964843424]
maxi score, test score, baseline:  0.0281 0.5 0.5
probs:  [0.2274868234880511, 0.643980588959106, 0.0698945879044087, 0.05863799964843424]
siam score:  -0.9230354
from probs:  [0.2274868234880511, 0.643980588959106, 0.0698945879044087, 0.05863799964843424]
actor:  0 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0301 0.5 0.5
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
from probs:  [0.2250657554814557, 0.6461774406835379, 0.06991934514384644, 0.05883745869116006]
using another actor
maxi score, test score, baseline:  0.0301 0.5 0.5
probs:  [0.2250657554814557, 0.6461774406835379, 0.06991934514384644, 0.05883745869116006]
maxi score, test score, baseline:  0.0301 0.5 0.5
probs:  [0.2250657554814557, 0.6461774406835379, 0.06991934514384644, 0.05883745869116006]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.2250657554814557, 0.6461774406835379, 0.06991934514384644, 0.05883745869116006]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.2250657554814557, 0.6461774406835379, 0.06991934514384644, 0.05883745869116006]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.2250657554814557, 0.6461774406835379, 0.06991934514384644, 0.05883745869116006]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.2250657554814557, 0.6461774406835379, 0.06991934514384644, 0.05883745869116006]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.2250657554814557, 0.6461774406835379, 0.06991934514384644, 0.05883745869116006]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.2335594500807034, 0.6390930147566869, 0.06915395088773708, 0.05819358427487267]
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.619]
 [0.862]
 [0.602]
 [0.588]] [[0.879]
 [0.622]
 [0.931]
 [0.992]
 [0.832]] [[0.591]
 [0.806]
 [1.497]
 [1.018]
 [0.884]]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.6  ]
 [0.645]
 [0.645]
 [0.544]] [[1.363]
 [1.267]
 [1.081]
 [1.081]
 [1.917]] [[1.029]
 [1.344]
 [1.31 ]
 [1.31 ]
 [1.664]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.2311093260083863, 0.6413068183977115, 0.06918926322312637, 0.0583945923707757]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.22873217926765194, 0.6434546835484389, 0.0692235237750416, 0.058589613408867564]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.22873217926765194, 0.6434546835484389, 0.0692235237750416, 0.058589613408867564]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.2240770775998742, 0.6388438360018874, 0.07890871215916956, 0.05817037423906891]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.2240770775998742, 0.6388438360018874, 0.07890871215916956, 0.05817037423906891]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.2240770775998742, 0.6388438360018874, 0.07890871215916956, 0.05817037423906891]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.2240770775998742, 0.6388438360018874, 0.07890871215916956, 0.05817037423906891]
siam score:  -0.9276747
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.2240770775998742, 0.6388438360018874, 0.07890871215916956, 0.05817037423906891]
using explorer policy with actor:  1
first move QE:  -0.21121228422675112
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.9292183
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.21977046837555686, 0.6429839111177608, 0.07869932079482225, 0.05854629971186017]
siam score:  -0.9266174
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.21977046837555686, 0.6429839111177608, 0.07869932079482225, 0.05854629971186017]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.21977046837555686, 0.6429839111177608, 0.07869932079482225, 0.05854629971186017]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.21977046837555686, 0.6429839111177608, 0.07869932079482225, 0.05854629971186017]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.21977046837555686, 0.6429839111177608, 0.07869932079482225, 0.05854629971186017]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
using explorer policy with actor:  1
in main func line 156:  993
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.21770705718247507, 0.6449675313835735, 0.07859899581467561, 0.0587264156192757]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.21770705718247507, 0.6449675313835735, 0.07859899581467561, 0.0587264156192757]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.21770705718247507, 0.6449675313835735, 0.07859899581467561, 0.0587264156192757]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.21770705718247507, 0.6449675313835735, 0.07859899581467561, 0.0587264156192757]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.21770705718247507, 0.6449675313835735, 0.07859899581467561, 0.0587264156192757]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.21770705718247507, 0.6449675313835735, 0.07859899581467561, 0.0587264156192757]
first move QE:  -0.21140787390782873
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.21770705718247507, 0.6449675313835735, 0.07859899581467561, 0.0587264156192757]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.001]
 [1.001]
 [1.123]
 [1.001]
 [1.001]] [[0.956]
 [0.956]
 [0.587]
 [0.956]
 [0.956]] [[1.675]
 [1.675]
 [1.797]
 [1.675]
 [1.675]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.22540378717137097, 0.6386201626923385, 0.07782651019959688, 0.05814953993669368]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.0449776132208421
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.24999999999999994, 0.6180650348445231, 0.07565340454733115, 0.05628156060814572]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.24528495755103866, 0.6224883534679477, 0.07554342938842958, 0.05668325959258413]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.24528495755103866, 0.6224883534679477, 0.07554342938842958, 0.05668325959258413]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.24528495755103866, 0.6224883534679477, 0.07554342938842958, 0.05668325959258413]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.24528495755103866, 0.6224883534679477, 0.07554342938842958, 0.05668325959258413]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.24528495755103866, 0.6224883534679477, 0.07554342938842958, 0.05668325959258413]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.24528495755103866, 0.6224883534679477, 0.07554342938842958, 0.05668325959258413]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.24528495755103866, 0.6224883534679477, 0.07554342938842958, 0.05668325959258413]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.24528495755103866, 0.6224883534679477, 0.07554342938842958, 0.05668325959258413]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.24528495755103866, 0.6224883534679477, 0.07554342938842958, 0.05668325959258413]
Printing some Q and Qe and total Qs values:  [[0.972]
 [0.972]
 [1.02 ]
 [0.972]
 [0.972]] [[0.588]
 [0.588]
 [1.055]
 [0.588]
 [0.588]] [[1.568]
 [1.568]
 [1.82 ]
 [1.568]
 [1.568]]
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.076]] [[2.01 ]
 [2.01 ]
 [2.01 ]
 [2.01 ]
 [2.044]] [[1.594]
 [1.594]
 [1.594]
 [1.594]
 [1.626]]
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.24528495755103866, 0.6224883534679477, 0.07554342938842958, 0.05668325959258413]
line 256 mcts: sample exp_bonus 0.8355188636580514
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.93809783
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.2660484168447705, 0.6053578015627746, 0.07346741470752484, 0.05512636688493003]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.263581358640004, 0.6076424441867729, 0.07344233767994757, 0.05533385949327553]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.5 0.5
maxi score, test score, baseline:  0.0361 0.5 0.5
probs:  [0.2701891528006967, 0.6021885544121537, 0.07278410319388448, 0.05483818959326521]
maxi score, test score, baseline:  0.0361 0.5 0.5
probs:  [0.2701891528006967, 0.6021885544121537, 0.07278410319388448, 0.05483818959326521]
siam score:  -0.9409381
maxi score, test score, baseline:  0.0361 0.5 0.5
maxi score, test score, baseline:  0.0361 0.5 0.5
probs:  [0.2701891528006967, 0.6021885544121537, 0.07278410319388448, 0.05483818959326521]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0361 0.5 0.5
probs:  [0.2701891528006967, 0.6021885544121537, 0.07278410319388448, 0.05483818959326521]
maxi score, test score, baseline:  0.0361 0.5 0.5
probs:  [0.2701891528006967, 0.6021885544121537, 0.07278410319388448, 0.05483818959326521]
maxi score, test score, baseline:  0.0361 0.5 0.5
probs:  [0.2701891528006967, 0.6021885544121537, 0.07278410319388448, 0.05483818959326521]
using another actor
maxi score, test score, baseline:  0.0361 0.5 0.5
probs:  [0.2701891528006967, 0.6021885544121537, 0.07278410319388448, 0.05483818959326521]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.587]
 [0.481]
 [0.481]
 [0.481]] [[0.729]
 [1.   ]
 [0.729]
 [0.729]
 [0.729]] [[0.481]
 [0.587]
 [0.481]
 [0.481]
 [0.481]]
start point for exploration sampling:  20063
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.414]
 [0.476]
 [0.402]
 [0.456]] [[0.401]
 [1.508]
 [0.207]
 [0.158]
 [0.401]] [[0.892]
 [1.544]
 [0.801]
 [0.622]
 [0.892]]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2701891528006967, 0.6021885544121537, 0.07278410319388448, 0.05483818959326521]
maxi score, test score, baseline:  0.0381 0.5 0.5
siam score:  -0.92026955
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2701891528006967, 0.6021885544121537, 0.07278410319388448, 0.05483818959326521]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2701891528006967, 0.6021885544121537, 0.07278410319388448, 0.05483818959326521]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2701891528006967, 0.6021885544121537, 0.07278410319388448, 0.05483818959326521]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2701891528006967, 0.6021885544121537, 0.07278410319388448, 0.05483818959326521]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.18 ]
 [1.203]
 [1.18 ]
 [1.18 ]
 [1.18 ]] [[-0.414]
 [ 1.707]
 [-0.414]
 [-0.414]
 [-0.414]] [[1.542]
 [2.612]
 [1.542]
 [1.542]
 [1.542]]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.27169245882667187, 0.6014178329920848, 0.07212183762129033, 0.05476787055995281]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2692888163682735, 0.6036283000850148, 0.07211424904814405, 0.054968634498567585]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2692888163682735, 0.6036283000850148, 0.07211424904814405, 0.054968634498567585]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2692888163682735, 0.6036283000850148, 0.07211424904814405, 0.054968634498567585]
siam score:  -0.9339899
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2692888163682735, 0.6036283000850148, 0.07211424904814405, 0.054968634498567585]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2692888163682735, 0.6036283000850148, 0.07211424904814405, 0.054968634498567585]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.8945939975818886
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.28160586518552755, 0.5934504016827333, 0.07090009728201022, 0.05404363584972883]
siam score:  -0.93871385
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.28160586518552755, 0.5934504016827333, 0.07090009728201022, 0.05404363584972883]
maxi score, test score, baseline:  0.0381 0.5 0.5
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.28160586518552755, 0.5934504016827333, 0.07090009728201022, 0.05404363584972883]
maxi score, test score, baseline:  0.0381 0.5 0.5
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.28160586518552755, 0.5934504016827333, 0.07090009728201022, 0.05404363584972883]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.28160586518552755, 0.5934504016827333, 0.07090009728201022, 0.05404363584972883]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.28160586518552755, 0.5934504016827333, 0.07090009728201022, 0.05404363584972883]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.28160586518552755, 0.5934504016827333, 0.07090009728201022, 0.05404363584972883]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.28160586518552755, 0.5934504016827333, 0.07090009728201022, 0.05404363584972883]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.28160586518552755, 0.5934504016827333, 0.07090009728201022, 0.05404363584972883]
using another actor
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.28160586518552755, 0.5934504016827333, 0.07090009728201022, 0.05404363584972883]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
in main func line 156:  1027
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.3433790847234133
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
actor:  0 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
using another actor
actor:  0 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.0441 0.5 0.5
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
siam score:  -0.94888693
maxi score, test score, baseline:  0.0441 0.5 0.5
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.0441 0.5 0.5
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
from probs:  [0.2841156087916743, 0.5911560879167427, 0.07089305384371013, 0.05383524944787299]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.92421293
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.2961218470099089, 0.5812387194348004, 0.06970550714308332, 0.05293392641220736]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.301975128046757, 0.5764038041336343, 0.06912655439728543, 0.05249451342232318]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0441 0.5 0.5
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.30773181401673017, 0.5716486780932113, 0.06855715594741932, 0.05206235194263926]
maxi score, test score, baseline:  0.0441 0.5 0.5
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.30773181401673017, 0.5716486780932113, 0.06855715594741932, 0.05206235194263926]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.30773181401673017, 0.5716486780932113, 0.06855715594741932, 0.05206235194263926]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.30773181401673017, 0.5716486780932113, 0.06855715594741932, 0.05206235194263926]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.30773181401673017, 0.5716486780932113, 0.06855715594741932, 0.05206235194263926]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.30773181401673017, 0.5716486780932113, 0.06855715594741932, 0.05206235194263926]
first move QE:  -0.21795087386427017
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.30773181401673017, 0.5716486780932113, 0.06855715594741932, 0.05206235194263926]
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.38 ]
 [0.198]
 [0.237]
 [0.24 ]] [[-0.594]
 [ 1.114]
 [ 1.183]
 [ 0.48 ]
 [ 0.371]] [[0.455]
 [1.256]
 [0.916]
 [0.76 ]
 [0.729]]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.647]
 [0.663]
 [0.588]
 [0.619]] [[1.611]
 [0.929]
 [1.473]
 [1.94 ]
 [1.622]] [[1.382]
 [1.271]
 [1.484]
 [1.49 ]
 [1.447]]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.396]] [[-0.308]
 [-0.308]
 [-0.308]
 [-0.308]
 [ 4.858]] [[0.321]
 [0.321]
 [0.321]
 [0.321]
 [2.146]]
siam score:  -0.94301724
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.302]] [[2.68 ]
 [2.68 ]
 [2.68 ]
 [2.68 ]
 [4.749]] [[0.883]
 [0.883]
 [0.883]
 [0.883]
 [1.424]]
maxi score, test score, baseline:  0.0441 0.5 0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
siam score:  -0.94807374
maxi score, test score, baseline:  0.0441 0.5 0.5
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
UNIT TEST: sample policy line 217 mcts : [0.    0.333 0.5   0.    0.167]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.6388],
        [0.2826],
        [0.1129],
        [0.1828],
        [0.4856],
        [0.3161],
        [0.6511],
        [0.3216],
        [0.4918],
        [0.6376]], dtype=torch.float64)
0.0 0.6388014008444646
0.0 0.28258442347007634
0.0 0.11289351533597025
0.0 0.18284571176081105
0.0 0.4856247875138269
0.0 0.31609495056944337
0.0 0.6511167761442885
0.0 0.321627256135589
0.0 0.4918292683770678
0.0 0.6375606182100572
siam score:  -0.9559863
first move QE:  -0.21332704899805877
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
siam score:  -0.95389986
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
maxi score, test score, baseline:  0.0441 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.705]
 [0.588]
 [0.622]
 [0.588]] [[1.055]
 [1.221]
 [1.386]
 [1.861]
 [1.386]] [[1.   ]
 [1.136]
 [1.014]
 [1.397]
 [1.014]]
line 256 mcts: sample exp_bonus 2.499523903060798
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.31339427647351525, 0.5669713823675764, 0.06799707722119802, 0.051637263937710204]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0461 0.5 0.5
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.31896480996738497, 0.5623700216169791, 0.0674460912628044, 0.05121907715283148]
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.31896480996738497, 0.5623700216169791, 0.0674460912628044, 0.05121907715283148]
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.31896480996738497, 0.5623700216169791, 0.0674460912628044, 0.05121907715283148]
siam score:  -0.9486097
maxi score, test score, baseline:  0.0461 0.5 0.5
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.31896480996738497, 0.5623700216169791, 0.0674460912628044, 0.05121907715283148]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.337]
 [0.565]
 [0.337]
 [0.337]] [[-0.726]
 [-0.726]
 [-0.098]
 [-0.726]
 [-0.726]] [[0.69 ]
 [0.69 ]
 [1.355]
 [0.69 ]
 [0.69 ]]
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.708]
 [0.71 ]
 [0.708]
 [0.719]] [[1.625]
 [1.625]
 [1.929]
 [1.625]
 [1.711]] [[1.332]
 [1.332]
 [1.437]
 [1.332]
 [1.384]]
line 256 mcts: sample exp_bonus 0.968940977351969
maxi score, test score, baseline:  0.0461 0.5 0.5
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.3244456351453706, 0.5578427615470734, 0.06690397842625051, 0.050807624881305496]
maxi score, test score, baseline:  0.0461 0.5 0.5
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.33514669119482776, 0.5490034969864882, 0.06584552834607012, 0.05000428347261378]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.3322541175314514, 0.5515984309486553, 0.06590745123913244, 0.05024000028076072]
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.3322541175314514, 0.5515984309486553, 0.06590745123913244, 0.05024000028076072]
maxi score, test score, baseline:  0.0461 0.5 0.5
maxi score, test score, baseline:  0.0461 0.5 0.5
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.3322541175314514, 0.5515984309486553, 0.06590745123913244, 0.05024000028076072]
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.175]
 [0.135]
 [0.103]
 [0.189]] [[ 0.445]
 [ 0.159]
 [ 0.821]
 [ 0.316]
 [-0.099]] [[0.701]
 [0.508]
 [0.869]
 [0.469]
 [0.365]]
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.3322541175314514, 0.5515984309486553, 0.06590745123913244, 0.05024000028076072]
maxi score, test score, baseline:  0.0461 0.5 0.5
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.3322541175314514, 0.5515984309486553, 0.06590745123913244, 0.05024000028076072]
line 256 mcts: sample exp_bonus 0.5092508793152509
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.3322541175314514, 0.5515984309486553, 0.06590745123913244, 0.05024000028076072]
Printing some Q and Qe and total Qs values:  [[0.947]
 [1.053]
 [0.947]
 [0.947]
 [0.947]] [[2.022]
 [2.09 ]
 [2.022]
 [2.022]
 [2.022]] [[2.042]
 [2.298]
 [2.042]
 [2.042]
 [2.042]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
Printing some Q and Qe and total Qs values:  [[0.909]
 [0.82 ]
 [0.903]
 [0.909]
 [0.909]] [[3.085]
 [1.755]
 [1.605]
 [3.085]
 [3.085]] [[2.546]
 [1.641]
 [1.697]
 [2.546]
 [2.546]]
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.3322541175314514, 0.5515984309486553, 0.06590745123913244, 0.05024000028076072]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.048100000000000004 0.5 0.5
probs:  [0.3322541175314514, 0.5515984309486553, 0.06590745123913244, 0.05024000028076072]
maxi score, test score, baseline:  0.048100000000000004 0.5 0.5
maxi score, test score, baseline:  0.048100000000000004 0.5 0.5
probs:  [0.3322541175314514, 0.5515984309486553, 0.06590745123913244, 0.05024000028076072]
maxi score, test score, baseline:  0.048100000000000004 0.5 0.5
probs:  [0.3322541175314514, 0.5515984309486553, 0.06590745123913244, 0.05024000028076072]
maxi score, test score, baseline:  0.048100000000000004 0.5 0.5
probs:  [0.3322541175314514, 0.5515984309486553, 0.06590745123913244, 0.05024000028076072]
maxi score, test score, baseline:  0.048100000000000004 0.5 0.5
start point for exploration sampling:  20063
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.96027756
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]] [[1.985]
 [1.985]
 [1.985]
 [1.985]
 [1.985]] [[2.288]
 [2.288]
 [2.288]
 [2.288]
 [2.288]]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.3322541175314514, 0.5515984309486553, 0.06590745123913244, 0.05024000028076072]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.3374440699955502, 0.5473098379848708, 0.06539585223161619, 0.049850239787962825]
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.3374440699955502, 0.5473098379848708, 0.06539585223161619, 0.049850239787962825]
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.3374440699955502, 0.5473098379848708, 0.06539585223161619, 0.049850239787962825]
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.3374440699955502, 0.5473098379848708, 0.06539585223161619, 0.049850239787962825]
Starting evaluation
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.3374440699955502, 0.5473098379848708, 0.06539585223161619, 0.049850239787962825]
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.3374440699955502, 0.5473098379848708, 0.06539585223161619, 0.049850239787962825]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn probs:  [0.3374440699955502, 0.5473098379848708, 0.06539585223161619, 0.049850239787962825]
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0901 1.0 1.0
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.958]
 [0.958]
 [0.918]
 [0.958]
 [0.958]] [[1.606]
 [1.606]
 [2.004]
 [1.606]
 [1.606]] [[1.585]
 [1.585]
 [1.638]
 [1.585]
 [1.585]]
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.96111435
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.905]
 [0.913]
 [0.833]
 [0.833]] [[1.59 ]
 [1.85 ]
 [1.956]
 [1.739]
 [1.739]] [[1.587]
 [1.97 ]
 [2.095]
 [1.738]
 [1.738]]
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.95889264
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
maxi score, test score, baseline:  0.0901 1.0 1.0
maxi score, test score, baseline:  0.0901 1.0 1.0
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0901 1.0 1.0
1102 3196
line 256 mcts: sample exp_bonus 1.0778468392428284
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.917]
 [0.917]
 [0.828]
 [0.917]
 [0.888]] [[1.505]
 [1.244]
 [1.699]
 [1.505]
 [0.784]] [[2.014]
 [1.84 ]
 [1.965]
 [2.014]
 [1.475]]
maxi score, test score, baseline:  0.0901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.873]
 [0.826]
 [0.788]
 [0.739]
 [0.748]] [[1.473]
 [0.233]
 [0.89 ]
 [1.525]
 [1.319]] [[0.873]
 [0.826]
 [0.788]
 [0.739]
 [0.748]]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9549399
maxi score, test score, baseline:  0.0941 1.0 1.0
maxi score, test score, baseline:  0.0941 1.0 1.0
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.701]
 [1.008]
 [0.701]
 [0.701]
 [0.761]] [[1.194]
 [1.458]
 [1.194]
 [1.194]
 [1.238]] [[1.31 ]
 [2.115]
 [1.31 ]
 [1.31 ]
 [1.461]]
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0941 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.93 ]
 [0.886]
 [1.173]
 [1.23 ]
 [1.002]] [[1.456]
 [1.969]
 [1.109]
 [1.842]
 [1.798]] [[1.617]
 [2.041]
 [1.755]
 [2.601]
 [2.102]]
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]] [[2.194]
 [2.194]
 [2.194]
 [2.194]
 [2.194]] [[0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.915]]
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.834]
 [0.908]
 [0.908]
 [0.908]] [[2.732]
 [4.016]
 [2.732]
 [2.732]
 [2.732]] [[1.469]
 [2.098]
 [1.469]
 [1.469]
 [1.469]]
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.575]
 [0.753]
 [0.575]
 [0.575]] [[0.626]
 [0.626]
 [0.49 ]
 [0.626]
 [0.626]] [[0.914]
 [0.914]
 [1.226]
 [0.914]
 [0.914]]
Printing some Q and Qe and total Qs values:  [[1.247]
 [1.088]
 [1.284]
 [1.152]
 [1.306]] [[1.087]
 [1.115]
 [0.513]
 [1.216]
 [1.087]] [[2.034]
 [1.734]
 [1.724]
 [1.929]
 [2.151]]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.454]
 [0.364]
 [0.398]
 [0.451]] [[ 0.426]
 [-0.071]
 [ 0.   ]
 [-0.92 ]
 [-0.187]] [[1.38 ]
 [0.972]
 [0.839]
 [0.294]
 [0.887]]
start point for exploration sampling:  20063
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9530086
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.49 ]
 [0.515]
 [0.49 ]
 [0.49 ]] [[0.467]
 [0.467]
 [0.749]
 [0.467]
 [0.467]] [[1.553]
 [1.553]
 [1.842]
 [1.553]
 [1.553]]
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.504]
 [0.577]
 [0.504]
 [0.572]] [[0.649]
 [0.699]
 [0.753]
 [0.699]
 [0.656]] [[0.993]
 [0.948]
 [1.149]
 [0.948]
 [1.041]]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.93977904
Printing some Q and Qe and total Qs values:  [[1.138]
 [1.138]
 [1.138]
 [1.043]
 [1.035]] [[2.782]
 [2.782]
 [2.782]
 [3.247]
 [2.8  ]] [[1.965]
 [1.965]
 [1.965]
 [2.144]
 [1.842]]
siam score:  -0.9368528
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0961 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0981 1.0 1.0
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9485933
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.94670546
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.079]
 [1.079]
 [1.126]
 [1.079]
 [1.079]] [[0.956]
 [0.956]
 [1.579]
 [0.956]
 [0.956]] [[1.54 ]
 [1.54 ]
 [2.144]
 [1.54 ]
 [1.54 ]]
Printing some Q and Qe and total Qs values:  [[1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]] [[1.361]
 [1.361]
 [1.361]
 [1.361]
 [1.361]] [[2.143]
 [2.143]
 [2.143]
 [2.143]
 [2.143]]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.072]
 [1.241]
 [1.225]
 [1.098]
 [1.194]] [[1.196]
 [0.801]
 [0.591]
 [1.439]
 [1.074]] [[2.142]
 [2.085]
 [1.843]
 [2.436]
 [2.265]]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.34938106022757875
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.418]
 [0.744]
 [0.418]
 [0.418]] [[1.414]
 [1.414]
 [0.327]
 [1.414]
 [1.414]] [[1.076]
 [1.076]
 [1.365]
 [1.076]
 [1.076]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.94095963
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.94134
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.17021581906570377
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.679]
 [0.446]
 [0.446]
 [0.446]] [[0.405]
 [0.454]
 [0.405]
 [0.405]
 [0.405]] [[0.77 ]
 [1.268]
 [0.77 ]
 [0.77 ]
 [0.77 ]]
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.696]
 [1.162]
 [0.722]
 [0.728]] [[1.659]
 [1.239]
 [0.665]
 [1.353]
 [1.146]] [[2.056]
 [2.012]
 [2.158]
 [2.191]
 [1.957]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9420137
maxi score, test score, baseline:  0.1021 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
maxi score, test score, baseline:  0.1021 1.0 1.0
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  1169
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.21326229078792916
maxi score, test score, baseline:  0.1021 1.0 1.0
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1021 1.0 1.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
1170 3228
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.454]
 [0.777]
 [0.454]
 [0.454]] [[0.771]
 [0.492]
 [0.794]
 [0.492]
 [0.492]] [[1.22 ]
 [0.835]
 [1.782]
 [0.835]
 [0.835]]
maxi score, test score, baseline:  0.1061 1.0 1.0
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.18 ]
 [1.259]
 [1.18 ]
 [0.918]
 [1.111]] [[1.067]
 [1.285]
 [1.246]
 [2.318]
 [1.808]] [[0.987]
 [1.227]
 [1.115]
 [1.598]
 [1.441]]
in main func line 156:  1173
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.436]
 [1.462]
 [1.37 ]
 [1.268]
 [1.425]] [[0.975]
 [0.875]
 [1.091]
 [1.271]
 [1.069]] [[2.518]
 [2.469]
 [2.503]
 [2.478]
 [2.59 ]]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.21270162709368046
siam score:  -0.94614875
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.    0.708 0.167 0.    0.125]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1101 1.0 1.0
siam score:  -0.94294435
line 256 mcts: sample exp_bonus 0.6562300841113704
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9418085
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1101 1.0 1.0
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
1193 3232
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
siam score:  -0.9435263
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.21153842835392836
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.411]
 [1.411]
 [1.378]
 [1.411]
 [1.411]] [[0.146]
 [0.146]
 [0.01 ]
 [0.146]
 [0.146]] [[2.091]
 [2.091]
 [1.979]
 [2.091]
 [2.091]]
Printing some Q and Qe and total Qs values:  [[0.892]
 [0.888]
 [0.799]
 [0.864]
 [0.857]] [[0.651]
 [0.963]
 [0.605]
 [0.691]
 [0.684]] [[1.794]
 [2.074]
 [1.623]
 [1.791]
 [1.776]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.166]
 [0.16 ]
 [0.159]
 [0.166]] [[0.735]
 [0.735]
 [1.005]
 [0.765]
 [0.735]] [[0.873]
 [0.873]
 [1.221]
 [0.898]
 [0.873]]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1121 1.0 1.0
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.669]
 [0.834]
 [0.663]
 [0.799]] [[1.209]
 [1.26 ]
 [0.431]
 [1.081]
 [1.195]] [[0.681]
 [0.669]
 [0.834]
 [0.663]
 [0.799]]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
siam score:  -0.93409103
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.93523806
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9390973
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9411882
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1227 3252
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.464]
 [0.013]
 [0.358]
 [0.386]] [[ 0.419]
 [ 0.368]
 [-0.265]
 [-0.173]
 [ 0.335]] [[0.992]
 [1.006]
 [0.005]
 [0.471]
 [0.891]]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
siam score:  -0.94651854
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.94013643
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.157]
 [1.225]
 [1.276]
 [1.228]
 [1.251]] [[0.539]
 [0.318]
 [0.043]
 [0.642]
 [0.064]] [[1.555]
 [1.616]
 [1.626]
 [1.73 ]
 [1.584]]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.206911000133929
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9413427
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.212]
 [1.28 ]
 [1.212]
 [1.212]
 [1.212]] [[1.516]
 [0.88 ]
 [1.516]
 [1.516]
 [1.516]] [[1.826]
 [1.748]
 [1.826]
 [1.826]
 [1.826]]
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.309]
 [0.674]
 [0.643]
 [0.643]] [[1.795]
 [1.103]
 [0.813]
 [0.383]
 [0.383]] [[1.532]
 [0.478]
 [1.111]
 [0.907]
 [0.907]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9261227
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
siam score:  -0.9308933
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.04 ]
 [1.295]
 [1.049]
 [1.056]
 [0.954]] [[1.915]
 [1.435]
 [1.917]
 [1.998]
 [2.165]] [[1.924]
 [1.831]
 [1.938]
 [2.026]
 [2.039]]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9332335
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.385]
 [0.385]
 [0.317]
 [0.385]] [[1.702]
 [2.867]
 [2.867]
 [2.192]
 [2.867]] [[1.087]
 [2.258]
 [2.258]
 [1.501]
 [2.258]]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.192]
 [1.241]
 [1.108]
 [1.241]
 [1.152]] [[1.977]
 [1.662]
 [1.692]
 [1.662]
 [2.048]] [[2.755]
 [2.54 ]
 [2.304]
 [2.54 ]
 [2.746]]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9336886
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.2559951227483304
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.859]
 [0.597]
 [0.597]
 [0.532]] [[0.524]
 [0.598]
 [0.524]
 [0.524]
 [1.158]] [[0.851]
 [1.424]
 [0.851]
 [0.851]
 [1.143]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.302]
 [1.302]
 [1.273]
 [1.302]
 [1.277]] [[2.942]
 [2.942]
 [2.266]
 [2.942]
 [3.13 ]] [[2.677]
 [2.677]
 [2.263]
 [2.677]
 [2.739]]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9233683
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]] [[4.139]
 [4.139]
 [4.139]
 [4.139]
 [4.139]] [[1.871]
 [1.871]
 [1.871]
 [1.871]
 [1.871]]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1241 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.527]
 [0.138]
 [0.372]
 [0.373]] [[0.608]
 [0.545]
 [0.659]
 [0.608]
 [0.77 ]] [[0.372]
 [0.527]
 [0.138]
 [0.372]
 [0.373]]
Starting evaluation
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
first move QE:  -0.1968053302438158
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.706]
 [0.672]
 [0.484]
 [0.691]] [[0.96 ]
 [1.03 ]
 [0.877]
 [1.132]
 [1.108]] [[0.553]
 [0.706]
 [0.672]
 [0.484]
 [0.691]]
siam score:  -0.9304134
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9299243
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.19295962637223835
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.19295962637223835
maxi score, test score, baseline:  0.1661 1.0 1.0
siam score:  -0.91774076
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1661 1.0 1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1661 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.827]
 [0.581]
 [0.581]
 [0.581]] [[ 0.017]
 [-0.161]
 [ 0.017]
 [ 0.017]
 [ 0.017]] [[1.259]
 [1.693]
 [1.259]
 [1.259]
 [1.259]]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.067]
 [0.013]
 [1.067]
 [1.067]
 [1.067]] [[ 0.156]
 [-0.194]
 [ 0.156]
 [ 0.156]
 [ 0.156]] [[2.244]
 [0.019]
 [2.244]
 [2.244]
 [2.244]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.2939176088723143
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.3622579808388549
maxi score, test score, baseline:  0.1701 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.836]
 [0.179]
 [0.523]
 [0.643]] [[-0.209]
 [ 0.298]
 [-0.344]
 [ 0.608]
 [ 0.562]] [[0.011]
 [1.56 ]
 [0.16 ]
 [1.334]
 [1.473]]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.92454165
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.087]
 [1.368]
 [1.147]
 [1.103]
 [1.23 ]] [[0.836]
 [0.575]
 [1.151]
 [1.229]
 [0.772]] [[1.836]
 [2.137]
 [2.27 ]
 [2.261]
 [2.058]]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.9  ]
 [0.9  ]
 [0.9  ]
 [0.821]
 [0.902]] [[0.896]
 [0.896]
 [0.896]
 [1.07 ]
 [1.077]] [[1.541]
 [1.541]
 [1.541]
 [1.557]
 [1.725]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.084]
 [1.058]
 [1.272]
 [0.768]
 [1.084]] [[0.743]
 [0.854]
 [0.493]
 [0.62 ]
 [0.743]] [[1.592]
 [1.576]
 [1.884]
 [0.917]
 [1.592]]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
first move QE:  -0.18804288667251218
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9165655
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.532]] [[2.239]
 [2.239]
 [2.239]
 [2.239]
 [3.784]] [[1.173]
 [1.173]
 [1.173]
 [1.173]
 [1.85 ]]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.065]
 [1.065]
 [1.296]
 [1.065]
 [0.852]] [[1.939]
 [1.939]
 [1.867]
 [1.939]
 [2.423]] [[1.795]
 [1.795]
 [2.102]
 [1.795]
 [1.841]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9225699
siam score:  -0.9218998
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1309 3292
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.082]
 [1.082]
 [1.278]
 [1.082]
 [1.082]] [[1.421]
 [1.421]
 [0.898]
 [1.421]
 [1.421]] [[1.708]
 [1.708]
 [1.923]
 [1.708]
 [1.708]]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.628]
 [0.637]
 [0.637]
 [0.637]] [[2.266]
 [2.475]
 [2.266]
 [2.266]
 [2.266]] [[1.757]
 [1.922]
 [1.757]
 [1.757]
 [1.757]]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9207085
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
siam score:  -0.92168015
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.18257174472997642
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.292 0.042 0.042 0.    0.625]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.745493167913797
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[3.072]
 [2.601]
 [2.601]
 [2.601]
 [2.601]] [[1.804]
 [1.55 ]
 [1.55 ]
 [1.55 ]
 [1.55 ]]
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.224]
 [0.303]
 [0.303]
 [0.278]] [[1.12 ]
 [0.473]
 [1.536]
 [1.536]
 [0.489]] [[0.792]
 [0.411]
 [1.035]
 [1.035]
 [0.462]]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
1335 3306
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.17288135539959626
Printing some Q and Qe and total Qs values:  [[1.002]
 [1.002]
 [1.002]
 [1.002]
 [1.002]] [[0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]] [[2.778]
 [2.778]
 [2.778]
 [2.778]
 [2.778]]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.91070646
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.7586581989167832
Printing some Q and Qe and total Qs values:  [[1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.239]] [[0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]] [[2.558]
 [2.558]
 [2.558]
 [2.558]
 [2.558]]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.9245633
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1861 1.0 1.0
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1881 1.0 1.0
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1881 1.0 1.0
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
1370 3321
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.1686362139102469
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.1686362139102469
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1901 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9097974
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.215]
 [0.2  ]
 [0.215]
 [0.215]] [[1.263]
 [1.263]
 [1.576]
 [1.263]
 [1.263]] [[1.552]
 [1.552]
 [1.835]
 [1.552]
 [1.552]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.56 ]
 [0.57 ]
 [0.573]
 [0.542]] [[1.261]
 [1.308]
 [1.447]
 [1.261]
 [1.286]] [[0.573]
 [0.56 ]
 [0.57 ]
 [0.573]
 [0.542]]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.242]
 [0.746]
 [0.616]
 [0.48 ]] [[1.838]
 [0.41 ]
 [0.462]
 [1.724]
 [1.903]] [[0.519]
 [0.242]
 [0.746]
 [0.616]
 [0.48 ]]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9161777
maxi score, test score, baseline:  0.1981 1.0 1.0
maxi score, test score, baseline:  0.1981 1.0 1.0
maxi score, test score, baseline:  0.1981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2369225882358983
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
start point for exploration sampling:  20063
Printing some Q and Qe and total Qs values:  [[1.154]
 [1.183]
 [1.061]
 [1.154]
 [1.154]] [[1.4  ]
 [1.273]
 [1.529]
 [1.4  ]
 [1.4  ]] [[2.037]
 [2.01 ]
 [1.938]
 [2.037]
 [2.037]]
maxi score, test score, baseline:  0.2001 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.458]
 [0.458]
 [0.458]
 [0.44 ]] [[3.006]
 [3.917]
 [3.917]
 [3.917]
 [3.56 ]] [[1.708]
 [2.295]
 [2.295]
 [2.295]
 [2.047]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.288]] [[2.655]
 [2.655]
 [2.655]
 [2.655]
 [2.921]] [[1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.986]]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9118283
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2001 1.0 1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2021 1.0 1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]] [[1.524]
 [1.524]
 [1.524]
 [1.524]
 [1.524]] [[2.07]
 [2.07]
 [2.07]
 [2.07]
 [2.07]]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.667 0.    0.    0.042 0.292]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.278]
 [0.339]
 [0.161]
 [0.278]] [[3.06 ]
 [2.62 ]
 [2.155]
 [0.643]
 [2.62 ]] [[1.556]
 [1.332]
 [1.138]
 [0.212]
 [1.332]]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.9197046
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9184296
Printing some Q and Qe and total Qs values:  [[0.724]
 [1.079]
 [1.244]
 [1.215]
 [1.146]] [[1.85 ]
 [1.115]
 [0.652]
 [0.906]
 [1.104]] [[1.622]
 [1.842]
 [1.864]
 [1.975]
 [1.969]]
siam score:  -0.91875494
Printing some Q and Qe and total Qs values:  [[1.339]
 [1.339]
 [1.339]
 [1.339]
 [1.339]] [[0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]] [[2.396]
 [2.396]
 [2.396]
 [2.396]
 [2.396]]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.153]
 [1.283]
 [1.18 ]
 [1.153]
 [1.153]] [[0.514]
 [0.014]
 [0.122]
 [0.514]
 [0.514]] [[1.829]
 [1.921]
 [1.751]
 [1.829]
 [1.829]]
Printing some Q and Qe and total Qs values:  [[0.973]
 [0.835]
 [1.009]
 [1.053]
 [1.06 ]] [[0.858]
 [0.766]
 [0.905]
 [0.805]
 [0.618]] [[1.582]
 [1.274]
 [1.67 ]
 [1.722]
 [1.674]]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]] [[1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.336]] [[3.085]
 [3.085]
 [3.085]
 [3.085]
 [3.085]]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9093955
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
siam score:  -0.91586554
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9109821
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
1435 3346
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
siam score:  -0.89950156
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.689]
 [0.007]
 [0.554]
 [0.635]] [[ 1.397]
 [ 1.016]
 [-0.125]
 [-0.318]
 [ 0.355]] [[1.889]
 [1.664]
 [0.017]
 [0.494]
 [1.099]]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.914]
 [0.719]
 [0.719]
 [0.719]] [[ 0.256]
 [-0.172]
 [ 0.256]
 [ 0.256]
 [ 0.256]] [[1.178]
 [1.426]
 [1.178]
 [1.178]
 [1.178]]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1445 3359
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]] [[0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]] [[0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9065016
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9093023
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.489]
 [0.468]] [[0.913]
 [0.913]
 [0.913]
 [0.925]
 [0.913]] [[0.468]
 [0.468]
 [0.468]
 [0.489]
 [0.468]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.261]
 [0.292]
 [0.261]
 [0.261]] [[2.825]
 [2.195]
 [2.882]
 [2.195]
 [2.195]] [[1.301]
 [0.924]
 [1.34 ]
 [0.924]
 [0.924]]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.419]] [[2.095]
 [2.569]
 [2.569]
 [2.569]
 [2.532]] [[1.494]
 [1.798]
 [1.798]
 [1.798]
 [1.783]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90929246
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2521 1.0 1.0
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.14677012563783204
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2521 1.0 1.0
siam score:  -0.905738
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.    0.042 0.833 0.083 0.042]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2521 1.0 1.0
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.1460642402109475
maxi score, test score, baseline:  0.2521 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2521 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2521 1.0 1.0
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.90244764
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
1474 3373
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8961299
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
in main func line 156:  1478
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.523]
 [0.523]
 [0.523]
 [0.571]] [[0.94 ]
 [0.741]
 [0.741]
 [0.741]
 [1.218]] [[1.36 ]
 [0.965]
 [0.965]
 [0.965]
 [1.697]]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89486957
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
siam score:  -0.8967037
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
Printing some Q and Qe and total Qs values:  [[0.833]
 [1.236]
 [1.181]
 [1.181]
 [1.097]] [[2.483]
 [0.667]
 [1.855]
 [1.855]
 [1.783]] [[2.495]
 [1.485]
 [2.563]
 [2.563]
 [2.323]]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89593536
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89681685
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.921]
 [0.67 ]
 [0.883]
 [0.813]
 [0.686]] [[1.096]
 [1.664]
 [1.232]
 [1.226]
 [1.426]] [[1.544]
 [1.422]
 [1.559]
 [1.414]
 [1.295]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
in main func line 156:  1497
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2621 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.22 ]
 [1.425]
 [1.312]
 [1.23 ]
 [1.286]] [[-0.25 ]
 [-0.096]
 [-0.443]
 [-0.42 ]
 [-0.45 ]] [[2.242]
 [2.673]
 [2.354]
 [2.208]
 [2.303]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.13894847953129516
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2621 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.714]
 [0.486]
 [0.486]
 [0.468]] [[0.86 ]
 [1.34 ]
 [0.888]
 [0.888]
 [1.152]] [[0.948]
 [1.434]
 [0.984]
 [0.984]
 [1.097]]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]] [[1.643]
 [1.643]
 [1.643]
 [1.643]
 [1.643]] [[2.106]
 [2.106]
 [2.106]
 [2.106]
 [2.106]]
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.832]
 [1.008]
 [0.832]
 [0.832]] [[0.8  ]
 [0.8  ]
 [1.246]
 [0.8  ]
 [0.8  ]] [[1.411]
 [1.411]
 [2.062]
 [1.411]
 [1.411]]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2681 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.107]
 [1.19 ]
 [1.168]
 [0.913]
 [1.17 ]] [[0.982]
 [0.474]
 [0.576]
 [1.394]
 [0.693]] [[1.802]
 [1.798]
 [1.789]
 [1.551]
 [1.832]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8232670800603687
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.40876139986308424
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8964045
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1519 3405
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.34724144874841756
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9009163
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8991595
siam score:  -0.9003641
siam score:  -0.8999242
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.13323226916170822
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1545 3420
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
siam score:  -0.89247894
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8897242
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
first move QE:  -0.1312437571085702
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1551 3424
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.504]
 [0.55 ]
 [0.436]
 [0.436]] [[1.946]
 [1.87 ]
 [1.735]
 [1.704]
 [1.704]] [[1.769]
 [1.789]
 [1.704]
 [1.448]
 [1.448]]
siam score:  -0.8889973
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.667 0.083 0.167]
in main func line 156:  1557
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89838517
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8948271
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1173716490755496
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.455]
 [0.455]
 [0.455]
 [0.454]] [[0.885]
 [2.421]
 [2.421]
 [2.421]
 [1.762]] [[0.152]
 [0.933]
 [0.933]
 [0.933]
 [0.589]]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.908]
 [0.908]
 [1.048]
 [0.908]] [[1.551]
 [1.744]
 [1.744]
 [1.577]
 [1.744]] [[1.778]
 [2.12 ]
 [2.12 ]
 [2.287]
 [2.12 ]]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87346953
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.91 ]
 [0.896]
 [0.792]
 [0.792]] [[1.752]
 [1.632]
 [1.733]
 [1.692]
 [1.692]] [[1.966]
 [2.391]
 [2.498]
 [2.237]
 [2.237]]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87910676
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8769268
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.155]] [[2.124]
 [2.124]
 [2.124]
 [2.124]
 [2.124]] [[2.016]
 [2.016]
 [2.016]
 [2.016]
 [2.016]]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1592 3440
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2961 1.0 1.0
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.783]
 [0.727]
 [0.727]
 [0.727]] [[0.601]
 [0.875]
 [0.601]
 [0.601]
 [0.601]] [[1.644]
 [2.029]
 [1.644]
 [1.644]
 [1.644]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8971088
Printing some Q and Qe and total Qs values:  [[1.051]
 [1.051]
 [1.151]
 [1.051]
 [1.051]] [[1.688]
 [1.688]
 [1.841]
 [1.688]
 [1.688]] [[2.338]
 [2.338]
 [2.697]
 [2.338]
 [2.338]]
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]] [[0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]] [[0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1600 3442
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
siam score:  -0.8951265
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1606 3447
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.573]
 [0.6  ]
 [0.602]
 [0.602]] [[2.012]
 [1.95 ]
 [2.104]
 [3.199]
 [3.199]] [[0.937]
 [0.958]
 [1.083]
 [1.789]
 [1.789]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3041 1.0 1.0
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.25]
 [1.25]
 [1.25]
 [1.25]
 [1.25]] [[0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]] [[1.694]
 [1.694]
 [1.694]
 [1.694]
 [1.694]]
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.837]
 [0.686]
 [0.686]
 [0.686]] [[0.343]
 [0.41 ]
 [0.343]
 [0.343]
 [0.343]] [[0.905]
 [1.231]
 [0.905]
 [0.905]
 [0.905]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1613 3453
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.884931
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
siam score:  -0.8862354
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8889585
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3081 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.254]
 [1.254]
 [1.257]
 [1.254]
 [1.254]] [[0.698]
 [0.698]
 [0.292]
 [0.698]
 [0.698]] [[2.049]
 [2.049]
 [1.649]
 [2.049]
 [2.049]]
using explorer policy with actor:  1
first move QE:  -0.12410594943220035
Printing some Q and Qe and total Qs values:  [[1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.147]] [[1.181]
 [1.181]
 [1.181]
 [1.181]
 [1.181]] [[2.332]
 [2.332]
 [2.332]
 [2.332]
 [2.332]]
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.996]
 [0.996]
 [1.187]
 [0.996]
 [0.996]] [[1.38 ]
 [1.38 ]
 [1.579]
 [1.38 ]
 [1.38 ]] [[1.816]
 [1.816]
 [2.306]
 [1.816]
 [1.816]]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
siam score:  -0.8838796
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [0.957]
 [0.901]
 [0.81 ]
 [0.917]] [[-0.317]
 [ 0.101]
 [-0.509]
 [-0.425]
 [-0.436]] [[0.85 ]
 [0.957]
 [0.901]
 [0.81 ]
 [0.917]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8849364
Printing some Q and Qe and total Qs values:  [[0.834]
 [1.195]
 [0.834]
 [0.834]
 [0.834]] [[0.825]
 [0.409]
 [0.825]
 [0.825]
 [0.825]] [[1.332]
 [1.775]
 [1.332]
 [1.332]
 [1.332]]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88815284
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.778]
 [1.116]
 [1.052]
 [1.052]
 [1.099]] [[1.142]
 [0.961]
 [0.825]
 [0.825]
 [0.737]] [[1.339]
 [1.895]
 [1.676]
 [1.676]
 [1.711]]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  1658
maxi score, test score, baseline:  0.3581 1.0 1.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]] [[1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.378]] [[2.063]
 [2.063]
 [2.063]
 [2.063]
 [2.063]]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.049]
 [1.212]
 [1.049]
 [1.049]
 [1.049]] [[0.924]
 [0.838]
 [0.924]
 [0.924]
 [0.924]] [[1.7 ]
 [1.97]
 [1.7 ]
 [1.7 ]
 [1.7 ]]
start point for exploration sampling:  20063
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.454]
 [0.002]
 [0.413]
 [0.438]] [[ 0.639]
 [ 0.59 ]
 [ 0.075]
 [-0.194]
 [ 0.339]] [[0.449]
 [0.454]
 [0.002]
 [0.413]
 [0.438]]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.447]
 [0.455]
 [0.415]
 [0.442]] [[0.799]
 [0.78 ]
 [0.447]
 [0.403]
 [0.852]] [[0.44 ]
 [0.447]
 [0.455]
 [0.415]
 [0.442]]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3621 1.0 1.0
siam score:  -0.879145
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.8928706952006157
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.9921090285666323
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.55 ]
 [0.876]
 [0.55 ]
 [0.601]] [[2.588]
 [1.716]
 [1.252]
 [1.716]
 [2.041]] [[1.293]
 [0.814]
 [0.832]
 [0.814]
 [1.094]]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86464936
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.183]
 [1.237]
 [1.225]
 [1.183]
 [1.183]] [[0.897]
 [1.064]
 [0.913]
 [0.897]
 [0.897]] [[2.178]
 [2.455]
 [2.28 ]
 [2.178]
 [2.178]]
Printing some Q and Qe and total Qs values:  [[1.102]
 [1.197]
 [1.141]
 [0.997]
 [1.19 ]] [[0.86 ]
 [0.677]
 [0.629]
 [0.685]
 [0.685]] [[2.322]
 [2.327]
 [2.203]
 [2.025]
 [2.323]]
maxi score, test score, baseline:  0.3621 1.0 1.0
line 256 mcts: sample exp_bonus 0.42878773887131044
Printing some Q and Qe and total Qs values:  [[1.087]
 [1.204]
 [1.2  ]
 [1.2  ]
 [1.21 ]] [[0.983]
 [0.974]
 [0.67 ]
 [0.67 ]
 [0.798]] [[2.166]
 [2.354]
 [2.092]
 [2.092]
 [2.217]]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87950486
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.328]
 [0.357]
 [0.29 ]
 [0.253]] [[-0.674]
 [ 0.05 ]
 [-0.503]
 [-0.47 ]
 [-0.745]] [[0.264]
 [0.328]
 [0.357]
 [0.29 ]
 [0.253]]
maxi score, test score, baseline:  0.3641 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.851]
 [0.872]
 [0.985]
 [0.815]
 [0.844]] [[1.05 ]
 [0.851]
 [0.636]
 [0.75 ]
 [1.273]] [[1.58 ]
 [1.49 ]
 [1.572]
 [1.308]
 [1.715]]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]] [[0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]] [[1.86]
 [1.86]
 [1.86]
 [1.86]
 [1.86]]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.215]
 [1.215]
 [1.29 ]
 [1.215]
 [1.125]] [[ 0.68 ]
 [ 0.68 ]
 [-0.414]
 [ 0.68 ]
 [ 0.717]] [[1.826]
 [1.826]
 [1.611]
 [1.826]
 [1.658]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.12311513844054502
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
first move QE:  -0.12336070485442033
maxi score, test score, baseline:  0.3681 1.0 1.0
siam score:  -0.874057
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.12339876589545816
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.7946864780350698
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3741 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3761 1.0 1.0
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.044]] [[1.704]
 [1.704]
 [1.704]
 [1.704]
 [2.25 ]] [[1.666]
 [1.666]
 [1.666]
 [1.666]
 [1.826]]
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.7885478898307414
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3781 1.0 1.0
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87491524
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  1703
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
siam score:  -0.8679895
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.273]
 [1.273]
 [1.273]
 [1.273]
 [1.273]] [[1.079]
 [1.079]
 [1.079]
 [1.079]
 [1.079]] [[2.137]
 [2.137]
 [2.137]
 [2.137]
 [2.137]]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3821 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86923814
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8689595
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3861 1.0 1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 1.0 1.0
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8753325
Printing some Q and Qe and total Qs values:  [[1.303]
 [1.303]
 [1.303]
 [1.303]
 [1.303]] [[0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[2.174]
 [2.174]
 [2.174]
 [2.174]
 [2.174]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.926]
 [0.878]
 [0.878]
 [0.878]] [[0.464]
 [0.47 ]
 [0.337]
 [0.337]
 [0.337]] [[2.35 ]
 [2.557]
 [2.283]
 [2.283]
 [2.283]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 1.0 1.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87237185
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.964]
 [0.964]
 [1.072]
 [0.964]
 [0.964]] [[1.054]
 [1.054]
 [1.141]
 [1.054]
 [1.054]] [[2.043]
 [2.043]
 [2.316]
 [2.043]
 [2.043]]
siam score:  -0.87247056
start point for exploration sampling:  20063
Printing some Q and Qe and total Qs values:  [[0.954]
 [1.135]
 [0.954]
 [0.954]
 [0.954]] [[0.371]
 [0.718]
 [0.371]
 [0.371]
 [0.371]] [[2.098]
 [2.689]
 [2.098]
 [2.098]
 [2.098]]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.7144],
        [0.5273],
        [0.6806],
        [0.4733],
        [0.9260],
        [0.0000],
        [0.0000],
        [0.2912],
        [0.8985],
        [0.6961]], dtype=torch.float64)
0.0 0.7143568441168593
0.0 0.5273052168125785
0.0 0.6806117182824157
0.0 0.4732794622399424
0.0 0.9260028141084052
0.0 0.0
0.0 0.0
0.0 0.2911851898616169
0.0 0.8985052673723026
0.0 0.696128236954202
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.093]
 [1.182]
 [1.105]
 [1.093]
 [1.093]] [[0.906]
 [0.928]
 [0.803]
 [0.906]
 [0.906]] [[1.96 ]
 [2.159]
 [1.882]
 [1.96 ]
 [1.96 ]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4061 1.0 1.0
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.002]
 [1.   ]
 [1.001]
 [1.002]
 [1.   ]] [[ 1.237]
 [ 0.054]
 [-0.114]
 [ 0.931]
 [ 0.061]] [[1.002]
 [1.   ]
 [1.001]
 [1.002]
 [1.   ]]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]] [[2.467]
 [2.467]
 [2.467]
 [2.467]
 [2.467]] [[1.52]
 [1.52]
 [1.52]
 [1.52]
 [1.52]]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.005]
 [1.005]
 [1.107]
 [1.005]
 [1.005]] [[0.539]
 [0.539]
 [0.107]
 [0.539]
 [0.539]] [[1.532]
 [1.532]
 [1.592]
 [1.532]
 [1.532]]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.975]
 [1.106]
 [0.893]
 [0.941]] [[2.285]
 [1.545]
 [1.034]
 [1.23 ]
 [1.094]] [[2.402]
 [1.975]
 [1.743]
 [1.531]
 [1.495]]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.19]
 [1.19]
 [1.25]
 [1.19]
 [1.19]] [[1.218]
 [1.218]
 [1.781]
 [1.218]
 [1.218]] [[2.356]
 [2.356]
 [2.851]
 [2.356]
 [2.356]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85869896
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.899]
 [1.133]
 [0.899]
 [0.899]
 [0.899]] [[0.185]
 [0.26 ]
 [0.185]
 [0.185]
 [0.185]] [[1.564]
 [2.048]
 [1.564]
 [1.564]
 [1.564]]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.725]] [[3.471]
 [3.471]
 [3.471]
 [3.471]
 [5.724]] [[1.168]
 [1.168]
 [1.168]
 [1.168]
 [1.992]]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.055]
 [1.055]
 [1.055]
 [1.055]
 [1.049]] [[1.581]
 [1.581]
 [1.581]
 [1.581]
 [1.647]] [[2.407]
 [2.407]
 [2.407]
 [2.407]
 [2.461]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.909]
 [0.759]
 [0.939]
 [0.909]
 [0.892]] [[1.45 ]
 [2.303]
 [1.304]
 [1.45 ]
 [1.466]] [[1.438]
 [1.827]
 [1.375]
 [1.438]
 [1.431]]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8647553
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8605881
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8582747
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.865]
 [0.853]
 [0.78 ]
 [0.78 ]] [[1.105]
 [1.348]
 [1.476]
 [1.105]
 [1.105]] [[1.286]
 [1.596]
 [1.689]
 [1.286]
 [1.286]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.85554916
maxi score, test score, baseline:  0.4201 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.75  0.042 0.    0.167]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.12214549642643753
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.986]] [[0.928]
 [0.928]
 [0.928]
 [0.928]
 [0.928]] [[2.363]
 [2.363]
 [2.363]
 [2.363]
 [2.363]]
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.4201 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.596]
 [0.623]
 [0.596]
 [0.596]] [[2.354]
 [1.79 ]
 [2.468]
 [1.79 ]
 [1.79 ]] [[1.757]
 [1.455]
 [1.961]
 [1.455]
 [1.455]]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  1791
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.808]
 [0.63 ]
 [0.633]
 [0.619]] [[ 0.273]
 [-0.073]
 [-0.266]
 [ 0.048]
 [ 0.148]] [[1.408]
 [1.436]
 [0.887]
 [1.206]
 [1.28 ]]
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.422]
 [0.456]
 [0.422]
 [0.422]] [[2.897]
 [2.748]
 [2.243]
 [2.748]
 [2.748]] [[2.203]
 [2.087]
 [1.648]
 [2.087]
 [2.087]]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85059714
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.947]
 [0.995]
 [0.843]
 [0.86 ]] [[2.326]
 [1.818]
 [1.562]
 [1.661]
 [1.596]] [[1.575]
 [1.61 ]
 [1.459]
 [1.353]
 [1.32 ]]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84723496
start point for exploration sampling:  20063
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.675]
 [0.803]
 [0.93 ]
 [0.747]] [[1.323]
 [2.388]
 [2.173]
 [1.539]
 [2.335]] [[0.475]
 [1.728]
 [1.84 ]
 [1.673]
 [1.837]]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.06 ]
 [1.292]
 [1.183]
 [1.06 ]
 [1.06 ]] [[1.512]
 [0.947]
 [1.21 ]
 [1.512]
 [1.512]] [[1.954]
 [2.043]
 [1.999]
 [1.954]
 [1.954]]
maxi score, test score, baseline:  0.4201 1.0 1.0
siam score:  -0.8458284
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.795]
 [0.779]
 [0.657]
 [0.795]] [[2.773]
 [1.383]
 [1.666]
 [2.001]
 [1.383]] [[2.106]
 [1.415]
 [1.65 ]
 [1.737]
 [1.415]]
Printing some Q and Qe and total Qs values:  [[1.177]
 [1.048]
 [1.288]
 [1.179]
 [1.226]] [[ 0.649]
 [ 0.456]
 [-0.024]
 [ 0.489]
 [ 0.249]] [[1.863]
 [1.476]
 [1.636]
 [1.761]
 [1.694]]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.851044
first move QE:  -0.11986172608514897
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.093]
 [1.182]
 [1.093]
 [1.093]
 [1.093]] [[1.069]
 [1.135]
 [1.069]
 [1.069]
 [1.069]] [[2.36 ]
 [2.583]
 [2.36 ]
 [2.36 ]
 [2.36 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.257]
 [0.263]
 [0.22 ]
 [0.211]] [[ 0.001]
 [ 0.301]
 [ 0.221]
 [-0.069]
 [-0.061]] [[0.222]
 [0.257]
 [0.263]
 [0.22 ]
 [0.211]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8455064
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20063
siam score:  -0.8464053
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84852004
maxi score, test score, baseline:  0.4601 1.0 1.0
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4641 1.0 1.0
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4641 1.0 1.0
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.717]
 [0.717]
 [0.713]
 [0.722]] [[0.217]
 [0.394]
 [0.394]
 [0.159]
 [0.717]] [[0.847]
 [0.969]
 [0.969]
 [0.747]
 [1.273]]
maxi score, test score, baseline:  0.4641 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.725]] [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.517]] [[1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.25 ]]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]] [[3.418]
 [3.418]
 [3.418]
 [3.418]
 [3.418]] [[2.34]
 [2.34]
 [2.34]
 [2.34]
 [2.34]]
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4681 1.0 1.0
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1840 3517
siam score:  -0.8483243
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4681 1.0 1.0
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8342454
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.05 ]
 [1.05 ]
 [1.056]
 [1.05 ]
 [1.05 ]] [[1.466]
 [1.466]
 [1.581]
 [1.466]
 [1.466]] [[2.025]
 [2.025]
 [2.113]
 [2.025]
 [2.025]]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]] [[4.221]
 [4.221]
 [4.221]
 [4.221]
 [4.221]] [[1.855]
 [1.855]
 [1.855]
 [1.855]
 [1.855]]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.653]] [[4.042]
 [4.042]
 [4.042]
 [4.042]
 [4.719]] [[1.648]
 [1.648]
 [1.648]
 [1.648]
 [2.012]]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.69 ]
 [0.75 ]
 [0.69 ]
 [0.634]] [[0.36 ]
 [0.36 ]
 [0.092]
 [0.36 ]
 [1.89 ]] [[0.69 ]
 [0.69 ]
 [0.75 ]
 [0.69 ]
 [0.634]]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8372586
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.3  ]
 [1.382]
 [1.378]
 [1.382]
 [1.184]] [[ 0.468]
 [ 0.234]
 [-0.112]
 [ 0.234]
 [ 0.414]] [[1.859]
 [1.944]
 [1.822]
 [1.944]
 [1.609]]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4841 1.0 1.0
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4881 1.0 1.0
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.83078784
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4881 1.0 1.0
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.166]
 [0.163]
 [0.16 ]
 [0.162]] [[1.371]
 [0.175]
 [0.227]
 [0.271]
 [0.361]] [[0.787]
 [0.415]
 [0.425]
 [0.434]
 [0.468]]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4901 1.0 1.0
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.4008059265057373
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
1875 3531
Printing some Q and Qe and total Qs values:  [[0.963]
 [0.963]
 [1.12 ]
 [0.963]
 [0.901]] [[1.641]
 [1.641]
 [1.794]
 [1.641]
 [2.272]] [[1.617]
 [1.617]
 [2.032]
 [1.617]
 [1.913]]
maxi score, test score, baseline:  0.4901 1.0 1.0
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8384765
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4901 1.0 1.0
maxi score, test score, baseline:  0.4901 1.0 1.0
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4901 1.0 1.0
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8385001
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.949]
 [0.949]
 [0.915]
 [0.949]
 [0.949]] [[1.392]
 [1.392]
 [1.699]
 [1.392]
 [1.392]] [[2.544]
 [2.544]
 [2.747]
 [2.544]
 [2.544]]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.1094398397995621
first move QE:  -0.1094398397995621
maxi score, test score, baseline:  0.4921 1.0 1.0
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4921 1.0 1.0
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.116]
 [1.212]
 [1.112]
 [1.112]
 [1.128]] [[1.07 ]
 [0.938]
 [1.365]
 [1.365]
 [1.389]] [[1.812]
 [1.917]
 [2.   ]
 [2.   ]
 [2.048]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4921 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4941 1.0 1.0
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8443012
maxi score, test score, baseline:  0.4961 1.0 1.0
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20063
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.024]
 [1.032]
 [1.172]
 [1.024]
 [1.032]] [[1.508]
 [1.241]
 [1.12 ]
 [1.64 ]
 [1.241]] [[2.151]
 [1.99 ]
 [2.19 ]
 [2.24 ]
 [1.99 ]]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
maxi score, test score, baseline:  0.4981 1.0 1.0
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4981 1.0 1.0
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4981 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
