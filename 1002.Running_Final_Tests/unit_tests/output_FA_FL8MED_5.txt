dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:255000
exp_gamma:0.95
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 0.6666666666666666, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
4 40
from probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  6
7 68
7 77
using explorer policy with actor:  1
8 78
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
siam score:  0.0023557743248106403
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
deleting a thread, now have 2 threads
Frames:  1080 train batches done:  29 episodes:  100
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
13 95
14 103
siam score:  -0.5733173
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6021409
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
16 142
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
19 166
19 170
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.46928483
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5218656
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5663063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5522365
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
29 231
from probs:  [0.25, 0.25, 0.25, 0.25]
30 242
siam score:  -0.44947022
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.22646200699862606
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[ 0.]
 [ 0.]
 [ 0.]
 [-0.]
 [ 0.]] [[-0.02 ]
 [ 0.   ]
 [ 0.   ]
 [-0.214]
 [-0.368]] [[ 0.115]
 [ 0.121]
 [ 0.121]
 [ 0.05 ]
 [-0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.1626497279466579
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
41 338
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
46 357
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
50 364
siam score:  -0.5199029
siam score:  -0.50746316
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.46208534
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
50 368
from probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  52
first move QE:  -0.13036033181639117
52 384
53 388
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.41056937
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.42745763
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.4623905
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.46390143
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.40429008
siam score:  -0.3826442
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
67 486
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.35698074
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
72 524
siam score:  -0.43827394
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.028]
 [ 0.059]
 [ 0.129]
 [-0.028]
 [ 0.104]] [[0.171]
 [0.287]
 [0.38 ]
 [0.171]
 [0.347]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.361]
 [-0.204]
 [-0.195]
 [-0.154]
 [-0.214]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.054]
 [ 0.057]
 [ 0.311]
 [ 0.057]
 [ 0.163]] [[0.   ]
 [0.074]
 [0.243]
 [0.074]
 [0.145]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
78 583
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.03756339780721087
81 614
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  84
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.048]
 [-0.043]
 [-0.048]
 [ 0.   ]
 [ 0.   ]] [[0.35 ]
 [0.355]
 [0.35 ]
 [0.398]
 [0.398]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.395]
 [-0.241]
 [-0.008]
 [-0.008]
 [-0.133]] [[0.   ]
 [0.206]
 [0.516]
 [0.517]
 [0.35 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6292477
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.34 ]
 [-0.229]
 [-0.092]
 [-0.039]
 [-0.036]] [[0.   ]
 [0.148]
 [0.331]
 [0.402]
 [0.406]]
first move QE:  -0.04473257779227184
90 680
siam score:  -0.5750408
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.008]
 [-0.041]
 [-0.008]
 [-0.008]
 [-0.261]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6184833
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.897]
 [0.951]
 [0.897]
 [0.897]
 [0.897]] [[0.297]
 [0.332]
 [0.297]
 [0.297]
 [0.297]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
95 727
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.04309820280650421
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]] [[0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]]
95 736
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
96 738
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]] [[0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.459]
 [0.459]
 [0.008]
 [0.009]
 [0.002]] [[0.601]
 [0.601]
 [0.15 ]
 [0.15 ]
 [0.143]]
siam score:  -0.6305479
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.542 0.083 0.125 0.125 0.125]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
99 769
99 772
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.670848
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.66139984
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.197]
 [-0.197]
 [-0.197]
 [-0.197]
 [-0.197]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.072]
 [-0.   ]
 [-0.029]
 [-0.041]
 [-0.016]] [[0.193]
 [0.121]
 [0.092]
 [0.08 ]
 [0.104]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.164]
 [0.004]
 [0.164]
 [0.164]
 [0.164]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.64307266
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.64278245
105 844
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.0013605468355338672
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
107 853
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
107 865
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.027]
 [0.027]
 [0.027]
 [0.027]
 [0.027]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.515219
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.003]
 [ 0.002]
 [-0.027]
 [ 0.003]
 [ 0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.60902846
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.037]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.003]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.032]
 [-0.028]
 [-0.027]
 [-0.033]
 [-0.035]] [[0.003]
 [0.006]
 [0.007]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.007]
 [-0.006]
 [-0.026]
 [-0.006]] [[0.009]
 [0.009]
 [0.01 ]
 [0.003]
 [0.009]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.04 ]] [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.04002647121320362
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
123 1002
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.   ]
 [-0.   ]
 [-0.003]
 [-0.003]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.62508345
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.656652
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.015]
 [0.006]
 [0.004]
 [0.002]] [[0.003]
 [0.005]
 [0.002]
 [0.001]
 [0.001]]
siam score:  -0.6516969
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.643629
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6405884
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
line 256 mcts: sample exp_bonus -0.005337515495858128
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6040232
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.036]
 [-0.002]
 [-0.002]
 [-0.036]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
135 1118
line 256 mcts: sample exp_bonus 0.002064528731262255
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.035037922448313726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.64358705
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.004]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.002]]
line 256 mcts: sample exp_bonus 0.004464826841010825
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
145 1181
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.68721044
line 256 mcts: sample exp_bonus 0.0034225711463526367
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.01 ]
 [ 0.   ]
 [-0.005]
 [-0.007]] [[0.013]
 [0.   ]
 [0.013]
 [0.007]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
158 1236
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.208 0.208 0.083 0.208 0.292]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.68159014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.003]
 [-0.006]
 [-0.003]
 [-0.003]] [[0.009]
 [0.009]
 [0.008]
 [0.009]
 [0.009]]
first move QE:  -0.031178028538135567
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.014]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.04 ]
 [0.021]
 [0.039]
 [0.04 ]
 [0.039]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6995575
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
171 1305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
171 1308
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.001]] [[0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.039]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6959354
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.695154
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.006]
 [-0.006]
 [-0.006]
 [-0.005]
 [-0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.638161
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.0030015957239639154
182 1371
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6625642
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.004201822387572723
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.006195283092986086
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69607687
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.00879999877740119
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7043278
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  -0.026809781823287412
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
192 1485
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.026083084770953356
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7021444
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [-0.003]
 [ 0.   ]
 [-0.002]] [[0.003]
 [0.003]
 [0.   ]
 [0.003]
 [0.   ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6699015
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.024935168263304573
siam score:  -0.66653216
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.004300002406494012
siam score:  -0.66836196
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
STARTED EXPV TRAINING ON FRAME NO.  20004
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [1.439]
 [0.194]
 [0.738]
 [1.535]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71849227
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.003]
 [-0.063]
 [ 0.   ]
 [ 0.001]] [[0.087]
 [0.083]
 [0.003]
 [0.087]
 [0.089]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 -4.151617987060203e-12
0.0 0.0
0.0 1.0552029046148576e-11
0.0 0.0
0.0 0.0
0.0 0.0
0.0 -4.082424329884338e-12
0.0 4.386876329951081e-11
0.0 0.0
0.0 0.0
siam score:  -0.7264301
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7293545
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.037]
 [-0.037]
 [-0.215]
 [-0.037]
 [-0.025]] [[0.126]
 [0.126]
 [0.008]
 [0.126]
 [0.134]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7212573
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.184]
 [-0.164]
 [-0.243]
 [-0.131]
 [-0.09 ]] [[0.379]
 [0.406]
 [0.301]
 [0.45 ]
 [0.505]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
227 1736
siam score:  -0.7174985
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 -4.843554069062747e-13
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 1.7990344569781205e-11
0.0 9.756302248003543e-12
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.15502859238617836
first move QE:  -0.028158358120285467
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.476]
 [-0.001]
 [ 0.476]
 [-0.012]
 [ 0.082]] [[0.654]
 [0.177]
 [0.654]
 [0.167]
 [0.26 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.02760535988109095
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.242]
 [1.942]
 [0.429]
 [0.688]
 [1.332]] [[0.227]
 [1.359]
 [0.351]
 [0.524]
 [0.953]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7113941
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.328]
 [ 0.114]
 [ 0.468]
 [ 0.228]
 [ 0.227]] [[0.   ]
 [0.148]
 [0.266]
 [0.186]
 [0.186]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.714]
 [1.714]
 [1.81 ]
 [1.714]
 [1.756]] [[0.301]
 [0.301]
 [0.333]
 [0.301]
 [0.315]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
239 1790
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
245 1799
siam score:  -0.73376024
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.02244036155775121
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.25679999859024216
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.208 0.208 0.125 0.25  0.208]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7192215
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.711344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
263 1829
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7160375
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
269 1846
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.283]
 [2.262]
 [2.608]
 [3.136]
 [3.112]] [[1.62 ]
 [1.045]
 [1.24 ]
 [1.538]
 [1.524]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.277]
 [1.246]
 [2.457]
 [3.253]
 [2.999]] [[0.375]
 [0.358]
 [1.037]
 [1.482]
 [1.34 ]]
start point for exploration sampling:  20004
siam score:  -0.7102405
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
276 1854
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.011246104450679872
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.266]
 [2.266]
 [2.266]
 [0.448]
 [2.258]] [[1.902]
 [1.902]
 [1.902]
 [0.207]
 [1.895]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.71795136
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 3.845836512826127
line 256 mcts: sample exp_bonus 10.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.55 ]
 [7.482]
 [6.42 ]
 [5.77 ]
 [6.851]] [[1.574]
 [2.   ]
 [1.514]
 [1.217]
 [1.711]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  288
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-1.825]
 [-1.574]
 [-1.419]
 [-1.543]
 [-1.502]] [[0.   ]
 [0.084]
 [0.136]
 [0.094]
 [0.108]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.005986558906082639
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71141493
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71845675
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
296 1972
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.679]
 [1.067]
 [0.679]
 [0.679]
 [0.679]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6886908
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.966]
 [1.966]
 [2.315]
 [1.966]
 [1.966]] [[0.289]
 [0.289]
 [0.406]
 [0.289]
 [0.289]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.041]
 [8.934]
 [7.041]
 [7.041]
 [7.041]] [[1.015]
 [1.645]
 [1.015]
 [1.015]
 [1.015]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[8.292]
 [9.189]
 [8.393]
 [8.292]
 [8.737]] [[1.393]
 [1.712]
 [1.429]
 [1.393]
 [1.551]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.145]
 [3.145]
 [3.145]
 [3.145]
 [3.145]] [[1.791]
 [1.791]
 [1.791]
 [1.791]
 [1.791]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7204944
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.014337395884670824
in main func line 156:  320
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.009133488906780785
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
328 2009
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[8.175]
 [8.175]
 [8.175]
 [8.175]
 [8.03 ]] [[1.431]
 [1.431]
 [1.431]
 [1.431]
 [1.374]]
line 256 mcts: sample exp_bonus 0.3915498945918597
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72789156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.307]
 [7.813]
 [6.071]
 [5.307]
 [7.111]] [[0.681]
 [1.544]
 [0.944]
 [0.681]
 [1.302]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 5.310136176892309
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.083 0.167 0.25  0.125 0.375]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.374]
 [3.432]
 [3.374]
 [3.374]
 [3.374]] [[0.962]
 [0.986]
 [0.962]
 [0.962]
 [0.962]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7053446
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.782]
 [6.635]
 [3.512]
 [7.782]
 [7.017]] [[1.515]
 [1.132]
 [0.089]
 [1.515]
 [1.26 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.69 ]
 [6.067]
 [6.067]
 [6.067]
 [6.774]] [[1.407]
 [1.108]
 [1.108]
 [1.108]
 [1.447]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.0028305267100555446
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.726529
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.004]
 [4.004]
 [4.004]
 [4.004]
 [4.004]] [[5.341]
 [5.341]
 [5.341]
 [5.341]
 [5.341]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.554]
 [4.788]
 [3.69 ]
 [4.721]
 [4.955]] [[1.309]
 [1.427]
 [0.871]
 [1.393]
 [1.512]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.077]
 [3.531]
 [3.337]
 [2.981]
 [3.58 ]] [[1.004]
 [1.378]
 [1.219]
 [0.925]
 [1.419]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 1.496]
 [ 2.376]
 [-0.551]
 [ 1.496]
 [ 2.108]] [[0.905]
 [1.294]
 [0.   ]
 [0.905]
 [1.175]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 1.117]
 [ 2.682]
 [-0.566]
 [ 1.575]
 [ 2.144]] [[0.741]
 [1.431]
 [0.   ]
 [0.943]
 [1.193]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.    0.042 0.083 0.    0.875]
in main func line 156:  368
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.139]
 [5.139]
 [5.139]
 [5.139]
 [5.139]] [[1.586]
 [1.586]
 [1.586]
 [1.586]
 [1.586]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.992]
 [4.948]
 [4.992]
 [4.992]
 [5.586]] [[1.504]
 [1.48 ]
 [1.504]
 [1.504]
 [1.832]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.922]
 [5.456]
 [5.922]
 [5.922]
 [5.073]] [[2.   ]
 [1.79 ]
 [2.   ]
 [2.   ]
 [1.617]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.814]
 [6.262]
 [5.814]
 [5.84 ]
 [6.721]] [[1.258]
 [1.495]
 [1.258]
 [1.272]
 [1.737]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7221009
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
rdn probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  378
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.006214749299647117
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7194795
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7130432
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
start point for exploration sampling:  20004
start point for exploration sampling:  20004
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.558]
 [2.558]
 [2.558]
 [2.558]
 [2.966]] [[1.223]
 [1.223]
 [1.223]
 [1.223]
 [1.689]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.767]
 [2.767]
 [2.767]
 [2.767]
 [2.767]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.248]
 [4.791]
 [4.248]
 [4.248]
 [5.06 ]] [[0.835]
 [1.173]
 [0.835]
 [0.835]
 [1.34 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.604]
 [0.196]
 [1.049]
 [0.604]
 [0.604]] [[0.55 ]
 [0.111]
 [1.028]
 [0.55 ]
 [0.55 ]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.222]] [[0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.474]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.71 ]
 [3.654]
 [2.71 ]
 [2.71 ]
 [2.71 ]] [[1.097]
 [1.825]
 [1.097]
 [1.097]
 [1.097]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.235]
 [2.721]
 [1.808]
 [3.235]
 [2.32 ]] [[0.969]
 [0.62 ]
 [0.   ]
 [0.969]
 [0.347]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
394 2073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7111244
siam score:  -0.7097625
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.831]
 [4.831]
 [4.831]
 [4.831]
 [4.831]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  0.021463706883115035
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
siam score:  -0.7238398
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7390079
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.099]
 [3.831]
 [3.167]
 [3.847]
 [3.753]] [[0.751]
 [1.283]
 [0.801]
 [1.295]
 [1.226]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.744]
 [8.068]
 [6.744]
 [5.307]
 [7.811]] [[1.143]
 [1.585]
 [1.143]
 [0.663]
 [1.499]]
line 256 mcts: sample exp_bonus 4.595040008845616
in main func line 156:  404
siam score:  -0.75243574
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.367]
 [4.649]
 [4.823]
 [4.964]
 [4.786]] [[1.059]
 [1.245]
 [1.359]
 [1.451]
 [1.335]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 3.8664955429262935e-20
0.0 1.6110398095526221e-21
0.0 3.8664955429262935e-20
0.0 0.0
0.0 0.0
0.0 5.155327390568391e-20
0.0 0.0
0.0 1.2888318476420977e-20
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.75034213
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.9922091787768696
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.007]
 [6.151]
 [4.007]
 [4.007]
 [4.497]] [[1.167]
 [1.883]
 [1.167]
 [1.167]
 [1.331]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.115]
 [7.115]
 [7.115]
 [7.115]
 [8.253]] [[1.419]
 [1.419]
 [1.419]
 [1.419]
 [1.782]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
420 2083
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.076]
 [3.076]
 [3.076]
 [3.076]
 [3.678]] [[1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.821]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.605]
 [2.709]
 [3.605]
 [3.289]
 [3.544]] [[1.77 ]
 [1.171]
 [1.77 ]
 [1.559]
 [1.729]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7433532
first move QE:  0.078074855611869
first move QE:  0.078074855611869
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-1.039]
 [ 1.122]
 [-0.656]
 [-0.107]
 [-0.623]] [[0.049]
 [0.771]
 [0.177]
 [0.36 ]
 [0.188]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73879135
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.7380688
442 2091
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  445
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74354535
first move QE:  0.08324867819298726
siam score:  -0.7487447
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.462]
 [5.462]
 [5.462]
 [5.462]
 [4.018]] [[1.138]
 [1.138]
 [1.138]
 [1.138]
 [0.656]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7448314
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74398685
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7402429
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.251]
 [2.358]
 [2.251]
 [2.251]
 [2.221]] [[0.787]
 [0.858]
 [0.787]
 [0.787]
 [0.767]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.469]
 [3.469]
 [3.469]
 [3.469]
 [2.767]] [[0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.374]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.    0.875 0.042 0.042 0.042]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.392]
 [2.392]
 [2.392]
 [2.392]
 [2.392]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74227506
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  465
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75525314
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.25  0.167 0.125 0.292 0.167]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.623]
 [1.623]
 [1.377]
 [1.623]
 [1.623]] [[0.324]
 [0.324]
 [0.241]
 [0.324]
 [0.324]]
start point for exploration sampling:  20004
siam score:  -0.73017085
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 0.0
0.0 1.611039809552622e-20
0.0 -3.2220796191052443e-21
0.0 6.4441592382104886e-21
0.0 0.0
0.0 2.5776636952841954e-20
0.0 6.4441592382104886e-21
0.0 1.9332477714631467e-20
0.0 3.222079619105244e-20
0.0 6.4441592382104886e-21
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.971]
 [2.971]
 [2.971]
 [2.354]
 [3.467]] [[1.304]
 [1.304]
 [1.304]
 [0.87 ]
 [1.654]]
siam score:  -0.7315939
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72586226
siam score:  -0.7255691
first move QE:  0.11834389346341406
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.697]
 [2.697]
 [2.697]
 [2.697]
 [2.697]] [[0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7201535
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.869]
 [1.395]
 [1.458]
 [0.852]
 [2.08 ]] [[0.253]
 [0.603]
 [0.645]
 [0.242]
 [1.06 ]]
line 256 mcts: sample exp_bonus 1.7406541494900618
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]] [[2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.732296
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.13508686767000494
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.963]
 [0.376]
 [0.364]
 [0.81 ]
 [1.261]] [[0.696]
 [0.108]
 [0.097]
 [0.542]
 [0.994]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.756]
 [3.615]
 [3.615]
 [3.216]
 [3.997]] [[0.943]
 [0.841]
 [0.841]
 [0.551]
 [1.118]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.0183537020377413
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.47120228268190534
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.333 0.25  0.375]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7670636
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7657715
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.324]
 [2.71 ]
 [2.529]
 [2.273]
 [2.624]] [[0.637]
 [0.993]
 [0.826]
 [0.59 ]
 [0.914]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.593]
 [1.593]
 [1.593]
 [1.593]
 [1.211]] [[0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.152]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7610991
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.848]
 [1.372]
 [0.828]
 [1.509]
 [1.848]] [[1.51 ]
 [0.875]
 [0.148]
 [1.057]
 [1.51 ]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.152]
 [3.022]
 [3.152]
 [3.152]
 [2.912]] [[1.359]
 [1.304]
 [1.359]
 [1.359]
 [1.257]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.1495405107597859
siam score:  -0.7569133
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.284]
 [3.284]
 [3.284]
 [3.284]
 [3.19 ]] [[1.851]
 [1.851]
 [1.851]
 [1.851]
 [1.775]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[     0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 1.0310654781136782e-19
0.0 -1.6110398095526221e-21
0.0 6.4441592382104886e-21
0.0 1.2888318476420977e-20
0.0 6.4441592382104886e-21
0.0 0.0
0.0 2.5776636952841954e-20
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.777]
 [1.777]
 [1.777]
 [1.777]
 [1.987]] [[1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.713]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.988]
 [2.912]
 [2.988]
 [2.988]
 [4.127]] [[0.922]
 [0.874]
 [0.922]
 [0.922]
 [1.641]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7721885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
siam score:  -0.77549684
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7728261
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.796]
 [5.448]
 [4.796]
 [4.174]
 [5.408]] [[1.478]
 [1.884]
 [1.478]
 [1.09 ]
 [1.86 ]]
siam score:  -0.7559338
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.178]
 [1.302]
 [0.065]
 [0.943]
 [1.383]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.269]
 [3.747]
 [3.269]
 [3.269]
 [3.182]] [[0.953]
 [1.271]
 [0.953]
 [0.953]
 [0.895]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.756463
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.77244794
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.253]
 [2.253]
 [2.253]
 [2.253]
 [3.724]] [[0.828]
 [0.828]
 [0.828]
 [0.828]
 [1.624]]
siam score:  -0.7726532
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7730596
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
553 2209
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.753187020697069
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.5556652181332646
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
555 2211
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9388349307342156
siam score:  -0.77352804
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.259]
 [2.259]
 [2.259]
 [2.259]
 [2.259]] [[0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.239]
 [0.902]
 [1.239]
 [1.451]
 [1.729]] [[0.465]
 [0.241]
 [0.465]
 [0.606]
 [0.791]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.673]
 [4.673]
 [4.673]
 [4.673]
 [4.954]] [[1.797]
 [1.797]
 [1.797]
 [1.797]
 [1.955]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.316]
 [5.066]
 [5.901]
 [3.876]
 [5.324]] [[0.977]
 [1.461]
 [2.   ]
 [0.693]
 [1.628]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  567
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.774]
 [3.774]
 [3.774]
 [3.774]
 [3.774]] [[0.94]
 [0.94]
 [0.94]
 [0.94]
 [0.94]]
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.9250637408856845
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.173]
 [4.173]
 [4.173]
 [2.513]
 [4.726]] [[1.6  ]
 [1.6  ]
 [1.6  ]
 [0.709]
 [1.898]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7564862
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.018]
 [4.018]
 [4.018]
 [4.018]
 [7.443]] [[0.819]
 [0.819]
 [0.819]
 [0.819]
 [1.96 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7659886
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
siam score:  -0.77370054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.7781626
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.471]
 [3.471]
 [3.471]
 [3.471]
 [3.471]] [[1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.4792457803348924
using explorer policy with actor:  1
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.418]
 [3.124]
 [3.124]
 [3.124]
 [4.749]] [[1.479]
 [0.739]
 [0.739]
 [0.739]
 [1.669]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.149]
 [4.164]
 [4.149]
 [4.149]
 [4.593]] [[1.608]
 [1.616]
 [1.608]
 [1.608]
 [1.875]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7630152
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7623424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.74 ]
 [5.74 ]
 [5.74 ]
 [5.74 ]
 [6.363]] [[1.362]
 [1.362]
 [1.362]
 [1.362]
 [1.579]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.833]
 [3.904]
 [4.833]
 [4.833]
 [4.409]] [[1.589]
 [1.168]
 [1.589]
 [1.589]
 [1.397]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
siam score:  -0.7699797
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.678]
 [3.76 ]
 [3.678]
 [3.678]
 [4.066]] [[1.173]
 [1.227]
 [1.173]
 [1.173]
 [1.431]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.95 ]
 [2.856]
 [4.433]
 [3.896]
 [4.981]] [[1.853]
 [0.723]
 [1.574]
 [1.284]
 [1.869]]
line 256 mcts: sample exp_bonus 3.533863146698836
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.455]
 [2.455]
 [2.455]
 [2.455]
 [2.455]] [[0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.78121245
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.658]
 [2.658]
 [2.658]
 [2.658]
 [2.658]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7787581
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.388]
 [3.214]
 [2.388]
 [2.391]
 [2.702]] [[1.083]
 [1.869]
 [1.083]
 [1.087]
 [1.382]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.435]
 [1.998]
 [2.865]
 [2.764]
 [3.054]] [[1.654]
 [0.575]
 [1.226]
 [1.15 ]
 [1.368]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
610 2248
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7712874
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 4.823115484137506
first move QE:  0.22762736821320811
UNIT TEST: sample policy line 217 mcts : [0.167 0.292 0.083 0.042 0.417]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.023]
 [3.023]
 [3.023]
 [3.023]
 [3.229]] [[1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.962]]
line 256 mcts: sample exp_bonus 0.9281798728930567
first move QE:  0.22947223764868832
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7859304
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.23277850470047204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
siam score:  -0.7898056
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.996]
 [0.996]
 [0.996]
 [0.996]
 [0.996]] [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.618]
 [3.562]
 [3.915]
 [3.915]
 [4.691]] [[0.77 ]
 [0.417]
 [0.535]
 [0.535]
 [0.794]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.957]
 [3.726]
 [4.165]
 [3.54 ]
 [4.101]] [[0.566]
 [1.071]
 [1.358]
 [0.949]
 [1.316]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7821834
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.267]
 [3.267]
 [3.267]
 [3.267]
 [3.855]] [[1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.673]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.749]
 [1.421]
 [0.483]
 [0.913]
 [1.711]] [[0.304]
 [0.752]
 [0.127]
 [0.414]
 [0.945]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.7921118
first move QE:  0.24276416120443983
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.756]
 [4.756]
 [4.756]
 [4.756]
 [4.756]] [[1.669]
 [1.669]
 [1.669]
 [1.669]
 [1.669]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.125 0.542 0.25 ]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.5   0.292 0.125]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.703]
 [5.703]
 [5.703]
 [5.703]
 [5.703]] [[1.129]
 [1.129]
 [1.129]
 [1.129]
 [1.129]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
655 2287
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.192]
 [4.37 ]
 [4.192]
 [4.192]
 [4.218]] [[1.447]
 [1.551]
 [1.447]
 [1.447]
 [1.462]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.781]
 [4.215]
 [2.063]
 [1.751]
 [2.286]] [[1.51 ]
 [1.753]
 [0.547]
 [0.372]
 [0.672]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.443]
 [0.797]
 [0.427]
 [0.115]
 [0.261]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
663 2307
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.908]
 [2.473]
 [3.269]
 [3.269]
 [2.606]] [[0.231]
 [0.42 ]
 [0.686]
 [0.686]
 [0.464]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7941507
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.511]
 [2.511]
 [2.511]
 [2.511]
 [2.511]] [[0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.915]
 [2.467]
 [2.467]
 [2.467]
 [3.014]] [[1.495]
 [1.059]
 [1.059]
 [1.059]
 [1.592]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.764]
 [2.532]
 [2.606]
 [2.389]
 [2.805]] [[1.369]
 [1.132]
 [1.208]
 [0.986]
 [1.411]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.5937510193163953
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.22]
 [3.22]
 [3.22]
 [3.22]
 [3.22]] [[1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.329]
 [2.329]
 [2.329]
 [2.329]
 [3.666]] [[0.955]
 [0.955]
 [0.955]
 [0.955]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.532]
 [2.333]
 [2.369]
 [1.762]
 [2.778]] [[1.43 ]
 [1.234]
 [1.269]
 [0.67 ]
 [1.673]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.709]
 [1.709]
 [1.709]
 [1.709]
 [1.709]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7910158
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7910442
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7910982
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7911601
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78159666
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.286]
 [3.286]
 [3.286]
 [3.286]
 [3.32 ]] [[1.797]
 [1.797]
 [1.797]
 [1.797]
 [1.828]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Starting evaluation
first move QE:  0.2811551287580729
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.298]
 [1.375]
 [0.546]
 [0.73 ]
 [1.429]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.352]
 [1.352]
 [1.352]
 [1.352]
 [1.352]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]] [[0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.2818894670171308
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78667676
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.014]
 [3.046]
 [1.06 ]
 [1.03 ]
 [3.309]] [[0.   ]
 [1.77 ]
 [0.04 ]
 [0.014]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.29796458232005757
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.161]
 [2.161]
 [1.607]
 [2.161]
 [3.735]] [[0.699]
 [0.699]
 [0.36 ]
 [0.699]
 [1.66 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.536]
 [3.196]
 [2.672]
 [2.611]
 [2.985]] [[0.398]
 [0.953]
 [0.778]
 [0.757]
 [0.882]]
line 256 mcts: sample exp_bonus 2.7834150457832165
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.431]
 [1.573]
 [0.127]
 [1.758]
 [1.452]] [[0.436]
 [0.483]
 [0.   ]
 [0.545]
 [0.443]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.2995392614204479
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.083 0.125 0.5   0.042 0.25 ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.095]
 [1.567]
 [1.293]
 [1.267]
 [2.23 ]] [[0.188]
 [0.503]
 [0.32 ]
 [0.303]
 [0.945]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
724 2360
using explorer policy with actor:  1
729 2362
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.702]
 [3.108]
 [2.416]
 [3.115]
 [3.574]] [[0.183]
 [1.162]
 [0.68 ]
 [1.167]
 [1.487]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7942488
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.486]
 [1.486]
 [1.486]
 [1.486]
 [2.217]] [[0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.744]]
738 2370
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
740 2372
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.899]
 [1.063]
 [1.857]
 [1.06 ]
 [2.181]] [[0.314]
 [0.422]
 [0.95 ]
 [0.421]
 [1.165]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.617]
 [4.119]
 [2.977]
 [3.131]
 [4.401]] [[0.362]
 [0.529]
 [0.148]
 [0.199]
 [0.623]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.612]
 [2.805]
 [2.634]
 [2.706]
 [2.892]] [[0.706]
 [0.834]
 [0.72 ]
 [0.768]
 [0.892]]
using explorer policy with actor:  1
746 2377
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.987]
 [3.33 ]
 [4.572]
 [4.572]
 [3.547]] [[1.042]
 [1.25 ]
 [2.   ]
 [2.   ]
 [1.38 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 4.205662851575579
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4715131837225153
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.937]
 [1.937]
 [1.937]
 [1.937]
 [1.937]] [[1.357]
 [1.357]
 [1.357]
 [1.357]
 [1.357]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.688]
 [2.688]
 [2.688]
 [2.688]
 [2.688]] [[1.137]
 [1.137]
 [1.137]
 [1.137]
 [1.137]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.444]
 [5.444]
 [5.444]
 [5.444]
 [5.189]] [[1.828]
 [1.828]
 [1.828]
 [1.828]
 [1.727]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.93 ]
 [1.93 ]
 [1.93 ]
 [1.93 ]
 [2.549]] [[1.058]
 [1.058]
 [1.058]
 [1.058]
 [1.677]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.39 ]
 [3.39 ]
 [3.39 ]
 [3.39 ]
 [5.576]] [[0.907]
 [0.907]
 [0.907]
 [0.907]
 [1.918]]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.3189697840824634
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
start point for exploration sampling:  20004
from probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
758 2389
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
siam score:  -0.7409829
760 2390
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
763 2392
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.74728674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.742]
 [4.742]
 [4.742]
 [4.742]
 [4.742]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
first move QE:  0.3270097444368533
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.834]
 [3.558]
 [3.746]
 [2.709]
 [3.274]] [[0.781]
 [1.362]
 [1.513]
 [0.68 ]
 [1.134]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
using another actor
from probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.9  ]
 [1.9  ]
 [1.831]
 [1.9  ]
 [2.48 ]] [[1.031]
 [1.031]
 [0.961]
 [1.031]
 [1.61 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.981]
 [2.981]
 [2.981]
 [2.981]
 [2.981]] [[0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
siam score:  -0.76705223
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
siam score:  -0.78849596
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.014]
 [3.014]
 [3.014]
 [3.014]
 [3.872]] [[1.331]
 [1.331]
 [1.331]
 [1.331]
 [1.846]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.987]
 [2.987]
 [2.987]
 [2.987]
 [3.081]] [[1.579]
 [1.579]
 [1.579]
 [1.579]
 [1.673]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.7839017
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[4.647]
 [4.647]
 [4.647]
 [4.647]
 [4.647]] [[1.687]
 [1.687]
 [1.687]
 [1.687]
 [1.687]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.853548838022469
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.77395356
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.7689321
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
first move QE:  0.3394910879556955
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
siam score:  -0.7794427
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[4.264]
 [4.264]
 [4.264]
 [4.264]
 [4.264]] [[1.87]
 [1.87]
 [1.87]
 [1.87]
 [1.87]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.876]
 [3.007]
 [3.15 ]
 [2.876]
 [3.285]] [[0.93 ]
 [1.054]
 [1.19 ]
 [0.93 ]
 [1.319]]
using explorer policy with actor:  1
in main func line 156:  802
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.7896425
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[3.062]
 [3.062]
 [3.062]
 [3.062]
 [4.843]] [[1.15 ]
 [1.15 ]
 [1.15 ]
 [1.15 ]
 [1.997]]
siam score:  -0.7916396
siam score:  -0.7925006
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[3.129]
 [3.129]
 [3.267]
 [3.34 ]
 [3.898]] [[0.595]
 [0.594]
 [0.695]
 [0.75 ]
 [1.16 ]]
siam score:  -0.798809
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.79626197
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
first move QE:  0.3567679262266667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
using explorer policy with actor:  1
siam score:  -0.7851751
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
first move QE:  0.36279508562247686
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.862733692006307
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.76803774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
first move QE:  0.3705440534578044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.75789964
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.002]
 [0.   ]
 [0.001]] [[1.649]
 [0.643]
 [0.482]
 [1.649]
 [1.003]] [[1.348]
 [0.343]
 [0.184]
 [1.348]
 [0.705]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.75282836
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using another actor
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[3.062]
 [3.103]
 [3.062]
 [3.062]
 [3.062]] [[1.6  ]
 [1.633]
 [1.6  ]
 [1.6  ]
 [1.6  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[3.867]
 [3.867]
 [3.867]
 [3.867]
 [3.867]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.193]
 [1.408]
 [1.419]
 [1.581]
 [1.722]] [[0.411]
 [0.554]
 [0.561]
 [0.669]
 [0.763]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.002]] [[6.234]
 [6.234]
 [6.234]
 [6.234]
 [5.96 ]] [[1.245]
 [1.245]
 [1.245]
 [1.245]
 [1.152]]
siam score:  -0.756772
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.7557275
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.033]
 [2.033]
 [2.033]
 [2.033]
 [2.833]] [[1.242]
 [1.242]
 [1.242]
 [1.242]
 [1.861]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
836 2427
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
Starting evaluation
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[2.582]
 [2.582]
 [2.582]
 [2.582]
 [2.582]] [[1.221]
 [1.221]
 [1.221]
 [1.221]
 [1.221]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.002]] [[2.185]
 [2.495]
 [2.615]
 [2.196]
 [2.428]] [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.426]
 [2.136]
 [1.868]
 [2.285]
 [3.041]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.435]
 [1.779]
 [1.621]
 [2.293]
 [3.177]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 2.1937309447174984
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.   ]
 [0.001]] [[1.972]
 [2.138]
 [2.512]
 [2.038]
 [2.5  ]] [[0.001]
 [0.001]
 [0.001]
 [0.   ]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.538]
 [2.538]
 [2.538]
 [2.538]
 [2.056]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.207]
 [1.44 ]
 [1.513]
 [1.686]
 [1.954]] [[1.611]
 [0.846]
 [0.919]
 [1.092]
 [1.358]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.714]
 [0.98 ]
 [0.894]
 [1.071]
 [1.89 ]] [[0.428]
 [0.183]
 [0.155]
 [0.214]
 [0.488]]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.255]
 [1.509]
 [1.648]
 [0.777]
 [2.126]] [[0.886]
 [1.14 ]
 [1.28 ]
 [0.408]
 [1.757]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.927]
 [1.413]
 [0.629]
 [1.367]
 [1.964]] [[1.131]
 [0.748]
 [0.162]
 [0.713]
 [1.158]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
line 256 mcts: sample exp_bonus 1.5015133772075493
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.756]
 [1.756]
 [1.756]
 [1.756]
 [2.248]] [[0.967]
 [0.967]
 [0.967]
 [0.967]
 [1.459]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.79299164
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[1.443]
 [1.614]
 [1.459]
 [1.768]
 [1.596]] [[0.646]
 [0.816]
 [0.661]
 [0.97 ]
 [0.798]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
854 2443
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[1.634]
 [1.679]
 [1.634]
 [1.552]
 [1.563]] [[0.906]
 [0.953]
 [0.906]
 [0.825]
 [0.836]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]] [[1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.958]
 [0.958]
 [0.749]
 [0.826]
 [1.29 ]] [[0.336]
 [0.336]
 [0.127]
 [0.203]
 [0.668]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.496]
 [1.496]
 [1.496]
 [1.496]
 [1.496]] [[1.023]
 [1.023]
 [1.023]
 [1.023]
 [1.023]]
UNIT TEST: sample policy line 217 mcts : [0.    0.167 0.    0.042 0.792]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
first move QE:  0.40017297580882427
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.76754606
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.887]
 [1.249]
 [1.976]
 [1.4  ]
 [2.491]] [[0.086]
 [0.358]
 [0.907]
 [0.473]
 [1.296]]
using explorer policy with actor:  1
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.7628044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.4132186691779839
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0001]], dtype=torch.float64)
0.0 4.7592598548077394e-07
0.0 4.660053276801772e-07
0.0 0.0
0.0 7.516689683132058e-07
0.0 5.873106026125112e-07
0.0 0.0
0.0 1.261061571737516e-07
0.0 0.0
0.0 3.3398022943407925e-07
0.0 7.024531054509076e-05
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.197]
 [1.062]
 [1.114]
 [1.324]
 [2.951]] [[0.472]
 [0.36 ]
 [0.403]
 [0.576]
 [1.927]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
line 256 mcts: sample exp_bonus 1.3975491504793813
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.232]
 [1.232]
 [1.232]
 [1.232]
 [2.18 ]] [[0.794]
 [0.794]
 [0.794]
 [0.794]
 [1.52 ]]
using explorer policy with actor:  1
883 2465
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.305]
 [2.305]
 [2.305]
 [2.305]
 [2.342]] [[1.243]
 [1.243]
 [1.243]
 [1.243]
 [1.266]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
in main func line 156:  890
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.042 0.042 0.792]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.636]
 [1.636]
 [1.636]
 [1.636]
 [2.62 ]] [[1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.98 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.212850504240106
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.875]
 [2.875]
 [2.875]
 [2.875]
 [2.875]] [[1.969]
 [1.969]
 [1.969]
 [1.969]
 [1.969]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.415]
 [1.415]
 [1.415]
 [1.415]
 [2.253]] [[0.743]
 [0.743]
 [0.743]
 [0.743]
 [1.373]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.14]
 [1.14]
 [1.14]
 [1.14]
 [1.14]] [[0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.924]
 [0.937]
 [0.803]
 [1.007]
 [1.152]] [[0.478]
 [0.491]
 [0.357]
 [0.562]
 [0.707]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.180651610507073
901 2471
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.793489
siam score:  -0.795954
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.   ]] [[3.699]
 [3.699]
 [3.699]
 [3.699]
 [6.122]] [[0.887]
 [0.887]
 [0.887]
 [0.887]
 [1.654]]
903 2472
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
line 256 mcts: sample exp_bonus 2.9841785303952504
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.001]
 [0.001]] [[2.436]
 [2.436]
 [2.436]
 [1.591]
 [3.358]] [[1.125]
 [1.125]
 [1.125]
 [0.514]
 [1.793]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.612]
 [0.686]
 [1.126]
 [1.302]
 [2.171]] [[0.268]
 [0.337]
 [0.748]
 [0.913]
 [1.725]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.612]
 [0.686]
 [1.126]
 [1.302]
 [2.435]] [[0.   ]
 [0.074]
 [0.514]
 [0.69 ]
 [1.823]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.481]
 [1.481]
 [1.481]
 [1.505]
 [1.481]] [[0.767]
 [0.767]
 [0.767]
 [0.79 ]
 [0.767]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.03 ]
 [1.473]
 [1.081]
 [1.798]
 [1.409]] [[0.235]
 [0.679]
 [0.287]
 [1.004]
 [0.615]]
first move QE:  0.44247683615097594
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.7864358
siam score:  -0.7846819
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
line 256 mcts: sample exp_bonus 2.3991638401761985
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.573]
 [1.573]
 [1.621]
 [1.684]
 [1.678]] [[1.175]
 [1.175]
 [1.237]
 [1.318]
 [1.31 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.509]
 [1.509]
 [1.509]
 [1.509]
 [2.384]] [[0.761]
 [0.761]
 [0.761]
 [0.761]
 [1.517]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.31 ]
 [3.31 ]
 [3.31 ]
 [3.31 ]
 [3.442]] [[1.903]
 [1.903]
 [1.903]
 [1.903]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.7799432
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.7807187
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
919 2477
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.749]
 [1.701]
 [2.749]
 [1.87 ]
 [2.229]] [[1.179]
 [0.635]
 [1.179]
 [0.722]
 [0.909]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.7719281
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.428]
 [1.475]
 [1.428]
 [1.428]
 [1.428]] [[0.727]
 [0.774]
 [0.727]
 [0.727]
 [0.727]]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.355]
 [1.452]
 [1.933]
 [1.616]
 [1.583]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.033]
 [3.132]
 [2.872]
 [1.626]
 [2.356]] [[0.78 ]
 [1.447]
 [1.289]
 [0.533]
 [0.976]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.681]
 [1.681]
 [1.742]
 [2.366]
 [2.114]] [[0.863]
 [0.863]
 [0.922]
 [1.532]
 [1.286]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.45568143624717655
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.003]
 [0.   ]
 [0.   ]] [[1.71 ]
 [1.769]
 [1.613]
 [1.651]
 [1.938]] [[0.942]
 [1.001]
 [0.851]
 [0.883]
 [1.17 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.936]
 [1.936]
 [1.936]
 [1.936]
 [1.936]] [[1.279]
 [1.279]
 [1.279]
 [1.279]
 [1.279]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7832428
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.457]
 [2.18 ]
 [2.18 ]
 [2.006]
 [2.582]] [[0.68 ]
 [1.323]
 [1.323]
 [1.168]
 [1.68 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.468]
 [0.469]
 [0.466]
 [0.467]
 [0.469]] [[0.001]
 [0.002]
 [0.   ]
 [0.001]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.698]
 [2.698]
 [2.698]
 [2.698]
 [4.381]] [[0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [1.501]]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
962 2509
siam score:  -0.74991435
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.73356444
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
962 2512
siam score:  -0.73414797
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.7501376
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]] [[0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.827]
 [1.827]
 [1.827]
 [1.827]
 [1.827]] [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.167 0.042 0.708]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.787893
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.554]
 [0.6  ]
 [0.455]
 [0.591]
 [1.248]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.79668444
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
first move QE:  0.47991565307005496
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
991 2532
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[1.277]
 [1.277]
 [1.277]
 [1.277]
 [2.311]] [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [1.187]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9814512532103052
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.002]] [[1.313]
 [1.313]
 [1.313]
 [1.313]
 [2.604]] [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [1.019]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.7888645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.878]
 [1.181]
 [1.29 ]
 [1.004]
 [1.634]] [[0.461]
 [0.765]
 [0.874]
 [0.588]
 [1.218]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.477]
 [1.477]
 [1.477]
 [1.477]
 [1.681]] [[0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.918]]
using explorer policy with actor:  1
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.78479767
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.79880315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
line 256 mcts: sample exp_bonus 3.451780805970806
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]] [[0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.033]
 [2.033]
 [2.033]
 [2.033]
 [2.033]] [[1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Starting evaluation
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.8123529
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
rdn probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[0.865]
 [0.865]
 [0.865]
 [0.865]
 [1.684]] [[1.062]
 [1.062]
 [1.062]
 [1.062]
 [1.82 ]]
siam score:  -0.80557764
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.421]
 [1.421]
 [1.421]
 [1.421]
 [1.421]] [[0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.80960363
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
line 256 mcts: sample exp_bonus 1.296436995830606
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
1015 2546
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.776]
 [1.776]
 [1.776]
 [1.776]
 [1.776]] [[1.795]
 [1.795]
 [1.795]
 [1.795]
 [1.795]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.001]
 [0.001]] [[ 0.023]
 [-0.046]
 [ 0.04 ]
 [ 0.148]
 [ 1.959]] [[0.071]
 [0.002]
 [0.09 ]
 [0.195]
 [2.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8377078
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using another actor
start point for exploration sampling:  20004
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.644]
 [1.644]
 [1.644]
 [1.644]
 [1.644]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]] [[1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.346]]
using explorer policy with actor:  1
siam score:  -0.83722466
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9154931265847999
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.34 ]
 [-0.021]
 [-0.235]
 [-0.423]
 [ 1.118]] [[0.108]
 [0.214]
 [0.143]
 [0.08 ]
 [0.595]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
using explorer policy with actor:  1
1034 2550
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.081]
 [0.967]
 [0.252]
 [0.967]
 [0.737]] [[0.001]
 [0.886]
 [0.171]
 [0.886]
 [0.656]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]] [[0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]]
line 256 mcts: sample exp_bonus 1.529162038507051
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.813]
 [1.813]
 [1.423]
 [1.506]
 [1.296]] [[1.772]
 [1.772]
 [1.352]
 [1.441]
 [1.215]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.884]
 [1.026]
 [0.936]
 [1.034]
 [1.021]] [[0.392]
 [0.581]
 [0.461]
 [0.591]
 [0.574]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
in main func line 156:  1044
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.642]
 [1.642]
 [1.642]
 [0.823]
 [1.642]] [[1.252]
 [1.252]
 [1.252]
 [0.433]
 [1.252]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[1.065]
 [1.065]
 [1.304]
 [1.116]
 [1.068]] [[0.82 ]
 [0.82 ]
 [1.06 ]
 [0.871]
 [0.823]]
siam score:  -0.81669617
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.    0.042 0.958]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.658]
 [3.658]
 [3.658]
 [3.658]
 [3.209]] [[1.353]
 [1.353]
 [1.353]
 [1.353]
 [1.131]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
1061 2577
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.801]] [[0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.851]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[-0.097]
 [ 0.599]
 [ 0.826]
 [ 0.464]
 [ 1.315]] [[0.001]
 [0.696]
 [0.925]
 [0.561]
 [1.413]]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.8331891
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.66 ]
 [1.798]
 [2.694]
 [2.038]
 [2.178]] [[0.642]
 [0.758]
 [1.512]
 [0.96 ]
 [1.077]]
start point for exploration sampling:  20004
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[3.032]
 [3.032]
 [3.032]
 [3.032]
 [2.831]] [[1.877]
 [1.877]
 [1.877]
 [1.877]
 [1.676]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.005]
 [2.005]
 [2.005]
 [2.005]
 [1.841]] [[1.596]
 [1.596]
 [1.596]
 [1.596]
 [1.432]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.83351016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
1075 2596
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.082]
 [0.675]
 [1.406]
 [1.488]
 [1.818]] [[0.634]
 [0.226]
 [0.958]
 [1.039]
 [1.37 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.004]
 [2.004]
 [2.004]
 [2.004]
 [2.182]] [[0.968]
 [0.968]
 [0.968]
 [0.968]
 [1.146]]
siam score:  -0.8286084
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.8335162
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.8431318
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.8386064
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
start point for exploration sampling:  20004
siam score:  -0.8370006
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7212356823940039
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.003]
 [0.001]
 [0.   ]] [[2.047]
 [2.047]
 [1.325]
 [2.047]
 [2.494]] [[1.214]
 [1.214]
 [0.497]
 [1.214]
 [1.661]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.001]] [[0.58 ]
 [0.745]
 [0.766]
 [0.796]
 [1.173]] [[0.023]
 [0.189]
 [0.212]
 [0.239]
 [0.617]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
start point for exploration sampling:  20004
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.673]
 [0.352]
 [1.181]
 [0.754]
 [1.264]] [[0.377]
 [0.001]
 [0.975]
 [0.472]
 [1.072]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.82605517
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.8494292
siam score:  -0.8516374
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.85325897
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.156]
 [1.156]
 [1.287]
 [1.156]
 [1.219]] [[1.401]
 [1.401]
 [1.53 ]
 [1.401]
 [1.463]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.16 ]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
1103 2620
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.015]] [[2.949]
 [2.949]
 [2.949]
 [2.949]
 [2.613]] [[0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.647]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.034]] [[3.056]
 [3.056]
 [3.056]
 [3.056]
 [5.174]] [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [1.301]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.002]
 [0.002]
 [0.002]] [[0.658]
 [0.955]
 [0.658]
 [0.951]
 [0.782]] [[0.226]
 [0.526]
 [0.226]
 [0.52 ]
 [0.35 ]]
using explorer policy with actor:  1
1107 2629
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
1107 2631
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8804129
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
rdn beta is 0 so we're just using the maxi policy
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.86526656
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.07 ]
 [1.07 ]
 [1.208]
 [1.036]
 [0.92 ]] [[1.201]
 [1.201]
 [1.33 ]
 [1.17 ]
 [1.062]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.839]
 [0.839]
 [0.839]
 [1.034]
 [1.202]] [[1.051]
 [1.051]
 [1.051]
 [1.245]
 [1.414]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.8629738
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
first move QE:  0.5085881957246354
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using another actor
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.   ]
 [0.001]] [[1.976]
 [1.976]
 [1.976]
 [1.519]
 [2.317]] [[1.255]
 [1.255]
 [1.255]
 [0.798]
 [1.597]]
first move QE:  0.509567106856911
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.788]
 [1.788]
 [1.788]
 [1.905]
 [1.977]] [[1.53 ]
 [1.53 ]
 [1.53 ]
 [1.647]
 [1.72 ]]
siam score:  -0.8536247
line 256 mcts: sample exp_bonus 2.517983015215914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
line 256 mcts: sample exp_bonus 1.7966445677117375
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.555]
 [1.555]
 [1.555]
 [1.768]
 [1.649]] [[1.471]
 [1.471]
 [1.471]
 [1.728]
 [1.584]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.84380263
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.844489
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-1.148]
 [ 0.277]
 [-0.046]
 [-0.315]
 [-0.288]] [[0.   ]
 [0.948]
 [0.733]
 [0.554]
 [0.572]]
line 256 mcts: sample exp_bonus 1.05814830487309
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8494567
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
1150 2663
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
in main func line 156:  1152
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
siam score:  -0.8491873
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.092]
 [0.038]
 [0.023]
 [0.058]] [[1.796]
 [2.751]
 [1.932]
 [2.381]
 [6.864]] [[0.176]
 [0.568]
 [0.237]
 [0.386]
 [2.014]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
1156 2695
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.02 ]] [[2.045]
 [2.045]
 [2.045]
 [2.045]
 [2.033]] [[1.042]
 [1.042]
 [1.042]
 [1.042]
 [1.045]]
1158 2705
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
first move QE:  0.503785869436415
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
line 256 mcts: sample exp_bonus 0.502877855197858
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
1163 2720
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.789]
 [0.622]
 [1.134]
 [1.515]
 [1.049]] [[0.169]
 [0.001]
 [0.513]
 [0.894]
 [0.428]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.21 ]
 [0.21 ]
 [0.808]
 [0.291]
 [0.459]] [[0.003]
 [0.002]
 [0.6  ]
 [0.084]
 [0.251]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.001]
 [0.002]
 [0.001]] [[1.025]
 [0.684]
 [1.019]
 [1.336]
 [0.706]] [[0.693]
 [0.353]
 [0.687]
 [1.004]
 [0.373]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1405922134444555
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.85070413
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.002]
 [0.001]
 [0.002]] [[1.533]
 [1.093]
 [1.486]
 [1.088]
 [1.353]] [[0.714]
 [0.274]
 [0.668]
 [0.27 ]
 [0.534]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.967]
 [1.967]
 [1.967]
 [1.967]
 [1.968]] [[1.295]
 [1.295]
 [1.295]
 [1.295]
 [1.295]]
first move QE:  0.5019192084056804
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
from probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.506]
 [1.506]
 [1.506]
 [1.465]
 [1.454]] [[1.177]
 [1.177]
 [1.177]
 [1.136]
 [1.125]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[2.117]
 [2.117]
 [2.117]
 [2.117]
 [2.166]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[2.119]
 [2.119]
 [2.119]
 [2.119]
 [2.167]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.798]
 [1.798]
 [1.798]
 [1.798]
 [2.437]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.911]
 [1.911]
 [1.911]
 [1.911]
 [1.956]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 2.0692917805934807
siam score:  -0.86975294
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.258]
 [1.216]
 [1.984]
 [1.449]
 [1.287]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.   ]] [[2.245]
 [2.245]
 [2.245]
 [2.245]
 [1.768]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.   ]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.678]
 [1.678]
 [1.678]
 [1.678]
 [2.245]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[2.071]
 [2.071]
 [2.071]
 [2.071]
 [2.102]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[2.307]
 [2.307]
 [2.307]
 [2.307]
 [2.307]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.682]
 [1.759]
 [1.759]
 [1.848]
 [2.112]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
siam score:  -0.8725671
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.53]
 [2.53]
 [2.53]
 [2.53]
 [2.53]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
rdn probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.005]] [[1.155]
 [1.155]
 [1.155]
 [1.155]
 [3.33 ]] [[0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [1.283]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.001]
 [0.001]
 [0.001]] [[0.634]
 [0.282]
 [0.408]
 [2.265]
 [0.85 ]] [[0.289]
 [0.001]
 [0.103]
 [1.625]
 [0.466]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149385449222193, 0.07149385449222193, 0.7855184365233342, 0.07149385449222193]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
siam score:  -0.89166087
line 256 mcts: sample exp_bonus 2.676654415864899
from probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.039]] [[0.919]
 [0.919]
 [0.919]
 [0.919]
 [2.156]] [[0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [1.782]]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
in main func line 156:  1196
1197 2775
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
start point for exploration sampling:  20004
siam score:  -0.88408583
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.009]
 [0.005]
 [0.006]
 [0.006]] [[0.243]
 [0.39 ]
 [0.864]
 [0.581]
 [1.305]] [[0.003]
 [0.158]
 [0.625]
 [0.343]
 [1.069]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
using explorer policy with actor:  1
siam score:  -0.88731414
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[1.686]
 [1.686]
 [1.686]
 [1.686]
 [1.686]] [[1.885]
 [1.885]
 [1.885]
 [1.885]
 [1.885]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
from probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
line 256 mcts: sample exp_bonus 2.477875399986698
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
first move QE:  0.5121147246744491
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.006]
 [0.004]
 [0.004]] [[2.175]
 [2.223]
 [1.69 ]
 [3.049]
 [2.021]] [[0.872]
 [0.91 ]
 [0.494]
 [1.558]
 [0.752]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.006]] [[2.596]
 [2.596]
 [2.596]
 [2.596]
 [1.235]] [[1.121]
 [1.121]
 [1.121]
 [1.121]
 [0.127]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
siam score:  -0.89628756
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07149589420125588, 0.07149589420125588, 0.7855123173962324, 0.07149589420125588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.005]
 [0.003]
 [0.005]
 [0.005]] [[0.63 ]
 [0.976]
 [0.921]
 [1.138]
 [0.699]] [[0.445]
 [0.812]
 [0.752]
 [0.985]
 [0.516]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]] [[0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.003]] [[0.357]
 [0.357]
 [0.461]
 [0.651]
 [0.708]] [[0.913]
 [0.913]
 [1.016]
 [1.206]
 [1.262]]
1217 2802
siam score:  -0.88532287
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.003]] [[1.711]
 [1.194]
 [1.711]
 [1.711]
 [0.926]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8818927
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
1220 2811
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0835626564331777
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.311]
 [0.518]
 [0.93 ]
 [0.608]
 [0.971]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[1.52]
 [1.52]
 [1.52]
 [1.52]
 [1.52]] [[0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8863852
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
first move QE:  0.5122062073944242
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.208 0.667]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.89174545
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.006]] [[0.749]
 [0.749]
 [0.749]
 [0.749]
 [1.269]] [[0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.91 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
line 256 mcts: sample exp_bonus 1.1727916988567726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.9145505
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.007]
 [0.006]] [[1.225]
 [1.225]
 [1.225]
 [1.697]
 [1.225]] [[0.844]
 [0.844]
 [0.844]
 [1.318]
 [0.844]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.006]
 [0.008]
 [0.005]
 [0.006]] [[0.299]
 [1.094]
 [1.307]
 [1.564]
 [1.019]] [[0.006]
 [0.744]
 [0.945]
 [1.18 ]
 [0.675]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.004]] [[0.561]
 [0.253]
 [0.637]
 [1.091]
 [0.651]] [[0.312]
 [0.004]
 [0.388]
 [0.842]
 [0.405]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[1.532]
 [1.532]
 [1.532]
 [1.532]
 [1.532]] [[0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
1248 2848
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.58 ]
 [0.58 ]
 [0.491]
 [0.697]
 [0.128]] [[0.453]
 [0.453]
 [0.365]
 [0.571]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.201]
 [0.201]
 [0.201]
 [0.594]
 [0.201]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.242]
 [1.242]
 [1.406]
 [1.372]
 [1.142]] [[0.919]
 [0.919]
 [1.137]
 [1.093]
 [0.785]]
1259 2868
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.701]
 [1.379]
 [1.319]
 [1.581]
 [1.364]] [[0.896]
 [0.573]
 [0.513]
 [0.775]
 [0.558]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[0.836]
 [1.379]
 [1.319]
 [1.667]
 [1.464]] [[0.029]
 [0.573]
 [0.513]
 [0.861]
 [0.659]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.84515643
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.104]
 [2.104]
 [2.104]
 [2.104]
 [2.104]] [[1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.748]
 [2.748]
 [2.748]
 [1.619]
 [1.97 ]] [[1.814]
 [1.814]
 [1.814]
 [0.685]
 [1.036]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
1270 2874
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8626559
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.004]
 [0.006]
 [0.004]
 [0.003]] [[0.153]
 [0.538]
 [0.643]
 [0.575]
 [0.839]] [[0.002]
 [0.387]
 [0.498]
 [0.425]
 [0.688]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.504]] [[0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.019]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
1286 2880
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8915367
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[1.29 ]
 [1.29 ]
 [1.29 ]
 [1.101]
 [1.29 ]] [[0.929]
 [0.929]
 [0.929]
 [0.74 ]
 [0.929]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
1293 2885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]] [[0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.003]
 [0.007]
 [0.006]
 [0.006]] [[0.22 ]
 [0.226]
 [0.69 ]
 [0.46 ]
 [0.52 ]] [[0.004]
 [0.006]
 [0.477]
 [0.245]
 [0.304]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.9108706
siam score:  -0.9121232
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
1298 2890
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.9018907
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8984859
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
using another actor
first move QE:  0.5193237132597435
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8801906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8777649
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.006]
 [0.006]
 [0.008]
 [0.007]] [[1.395]
 [0.829]
 [1.016]
 [1.106]
 [1.19 ]] [[0.745]
 [0.362]
 [0.488]
 [0.552]
 [0.608]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.005]
 [0.003]
 [0.005]
 [0.006]] [[0.58 ]
 [1.751]
 [1.293]
 [1.482]
 [1.88 ]] [[0.004]
 [1.173]
 [0.71 ]
 [0.904]
 [1.304]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[1.123]
 [1.123]
 [1.123]
 [1.536]
 [1.123]] [[0.988]
 [0.988]
 [0.988]
 [1.363]
 [0.988]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.87921244
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[1.71 ]
 [1.71 ]
 [1.71 ]
 [1.89 ]
 [1.543]] [[1.412]
 [1.412]
 [1.412]
 [1.632]
 [1.208]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.894037
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.89899325
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.89886373
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.5218551431155843
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20004
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8970429
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.894658
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
1341 2923
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
line 256 mcts: sample exp_bonus 0.9853410540628001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.003]] [[1.64 ]
 [1.64 ]
 [1.64 ]
 [1.64 ]
 [1.701]] [[1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.351]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8943697
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
siam score:  -0.8917072
first move QE:  0.5221325544211101
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
1353 2952
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  0
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3235897209334617
using another actor
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[2.962]
 [2.962]
 [2.962]
 [2.962]
 [2.962]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.27 ]
 [0.865]
 [2.589]
 [3.117]
 [1.741]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.004]
 [0.005]
 [0.005]] [[1.516]
 [1.17 ]
 [1.327]
 [2.599]
 [1.733]] [[0.537]
 [0.241]
 [0.375]
 [1.463]
 [0.722]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.002]
 [0.003]
 [0.003]] [[1.23 ]
 [1.228]
 [0.859]
 [1.859]
 [1.565]] [[0.003]
 [0.003]
 [0.002]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
rdn probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
first move QE:  0.5247943414512656
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.7672928469982767
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
from probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.005]
 [0.004]
 [0.004]
 [0.004]] [[1.038]
 [1.251]
 [1.038]
 [1.038]
 [1.623]] [[0.294]
 [0.509]
 [0.294]
 [0.294]
 [0.88 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
line 256 mcts: sample exp_bonus 1.7483005167919057
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.003]
 [0.003]] [[3.031]
 [3.031]
 [1.335]
 [2.954]
 [2.149]] [[1.555]
 [1.555]
 [0.26 ]
 [1.496]
 [0.882]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
line 256 mcts: sample exp_bonus 2.889929096333251
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[6.464]
 [6.464]
 [6.464]
 [5.813]
 [4.538]] [[1.964]
 [1.964]
 [1.964]
 [1.678]
 [1.119]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[2.367]
 [1.932]
 [1.932]
 [2.781]
 [1.932]] [[0.427]
 [0.241]
 [0.241]
 [0.604]
 [0.241]]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.9078889
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.004]
 [0.006]
 [0.006]] [[1.946]
 [1.946]
 [1.463]
 [2.26 ]
 [1.946]] [[0.521]
 [0.521]
 [0.214]
 [0.718]
 [0.521]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07150201304868534, 0.07150201304868534, 0.7854939608539441, 0.07150201304868534]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.007]
 [0.006]] [[6.543]
 [6.543]
 [6.543]
 [7.525]
 [6.543]] [[1.559]
 [1.559]
 [1.559]
 [2.004]
 [1.559]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.007]
 [0.007]] [[3.739]
 [3.739]
 [3.739]
 [4.778]
 [3.035]] [[0.74 ]
 [0.74 ]
 [0.74 ]
 [1.074]
 [0.515]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[7.835]
 [7.835]
 [7.835]
 [7.979]
 [7.835]] [[1.959]
 [1.959]
 [1.959]
 [2.002]
 [1.959]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.006]
 [0.007]
 [0.008]] [[4.226]
 [4.226]
 [1.928]
 [4.421]
 [3.822]] [[1.52 ]
 [1.52 ]
 [0.545]
 [1.601]
 [1.349]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.005]
 [0.004]
 [0.005]
 [0.005]] [[3.196]
 [3.385]
 [1.899]
 [3.24 ]
 [2.848]] [[1.497]
 [1.672]
 [0.279]
 [1.536]
 [1.17 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
UNIT TEST: sample policy line 217 mcts : [0.    0.042 0.042 0.875 0.042]
using another actor
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
1385 2973
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.90181047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[3.46 ]
 [3.46 ]
 [3.46 ]
 [3.483]
 [3.172]] [[1.499]
 [1.499]
 [1.499]
 [1.518]
 [1.264]]
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
in main func line 156:  1390
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.006]
 [0.007]
 [0.007]
 [0.007]] [[1.584]
 [1.077]
 [1.584]
 [2.33 ]
 [1.584]] [[0.329]
 [0.045]
 [0.329]
 [0.746]
 [0.329]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[6.814]
 [6.814]
 [6.814]
 [6.814]
 [6.814]] [[2.004]
 [2.004]
 [2.004]
 [2.004]
 [2.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
in main func line 156:  1396
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[4.575]
 [4.575]
 [4.575]
 [6.143]
 [4.575]] [[0.8  ]
 [0.8  ]
 [0.8  ]
 [1.541]
 [0.8  ]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.91644305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.006]
 [0.007]
 [0.005]
 [0.007]] [[4.02 ]
 [1.85 ]
 [1.416]
 [4.591]
 [3.198]] [[0.956]
 [0.161]
 [0.002]
 [1.164]
 [0.654]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[5.396]
 [5.396]
 [5.396]
 [5.181]
 [5.396]] [[1.945]
 [1.945]
 [1.945]
 [1.844]
 [1.945]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.468707530830449
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[7.259]
 [7.259]
 [7.259]
 [7.439]
 [7.259]] [[1.663]
 [1.663]
 [1.663]
 [1.727]
 [1.663]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.011]
 [0.007]
 [0.006]
 [0.009]] [[3.515]
 [1.715]
 [1.474]
 [7.52 ]
 [2.975]] [[0.556]
 [0.069]
 [0.002]
 [1.639]
 [0.41 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.91571146
UNIT TEST: sample policy line 217 mcts : [0.208 0.042 0.    0.75  0.   ]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[6.726]
 [6.726]
 [6.726]
 [7.296]
 [6.726]] [[1.511]
 [1.511]
 [1.511]
 [1.754]
 [1.511]]
line 256 mcts: sample exp_bonus 7.745261417681792
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
siam score:  -0.9264148
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.027]
 [0.027]
 [0.027]
 [0.026]] [[3.409]
 [3.409]
 [3.409]
 [3.409]
 [5.921]] [[0.599]
 [0.599]
 [0.599]
 [0.599]
 [1.266]]
siam score:  -0.9228908
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
1416 2979
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.9193036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.005]
 [0.006]] [[0.271]
 [0.271]
 [0.271]
 [2.138]
 [0.271]] [[0.146]
 [0.146]
 [0.146]
 [1.388]
 [0.146]]
line 256 mcts: sample exp_bonus 4.039498204794485
UNIT TEST: sample policy line 217 mcts : [0.    0.042 0.042 0.917 0.   ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.9002798
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.005]] [[0.804]
 [0.804]
 [0.804]
 [1.142]
 [0.642]] [[0.274]
 [0.274]
 [0.274]
 [0.488]
 [0.172]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[2.188]
 [2.188]
 [2.188]
 [2.79 ]
 [2.188]] [[1.172]
 [1.172]
 [1.172]
 [1.574]
 [1.172]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[2.98 ]
 [2.98 ]
 [2.98 ]
 [3.622]
 [2.98 ]] [[0.98 ]
 [0.98 ]
 [0.98 ]
 [1.299]
 [0.98 ]]
UNIT TEST: sample policy line 217 mcts : [0.    0.083 0.042 0.833 0.042]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[1.973]
 [1.973]
 [1.973]
 [2.977]
 [1.973]] [[1.03 ]
 [1.03 ]
 [1.03 ]
 [1.699]
 [1.03 ]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[3.245]
 [3.245]
 [3.245]
 [3.781]
 [3.245]] [[1.157]
 [1.157]
 [1.157]
 [1.386]
 [1.157]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.004]
 [0.005]] [[2.764]
 [2.764]
 [2.764]
 [4.031]
 [2.764]] [[0.794]
 [0.794]
 [0.794]
 [1.273]
 [0.794]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[6.566]
 [6.566]
 [6.566]
 [6.566]
 [6.566]] [[2.002]
 [2.002]
 [2.002]
 [2.002]
 [2.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
line 256 mcts: sample exp_bonus 3.257169763442798
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[2.301]
 [2.301]
 [2.301]
 [2.466]
 [2.301]] [[1.275]
 [1.275]
 [1.275]
 [1.385]
 [1.275]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.88979185
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
start point for exploration sampling:  20004
siam score:  -0.8889427
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[1.789]
 [1.789]
 [1.789]
 [1.044]
 [1.789]] [[1.306]
 [1.306]
 [1.306]
 [0.562]
 [1.306]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.007]
 [0.007]
 [0.007]] [[2.843]
 [2.568]
 [1.817]
 [2.927]
 [3.007]] [[0.951]
 [0.703]
 [0.02 ]
 [1.027]
 [1.099]]
in main func line 156:  1440
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[1.732]
 [1.732]
 [1.732]
 [1.878]
 [1.732]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.   ]
 [0.008]
 [0.008]
 [0.009]] [[-0.504]
 [ 1.078]
 [ 0.021]
 [-0.312]
 [-0.203]] [[0.037]
 [1.071]
 [0.383]
 [0.161]
 [0.235]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using another actor
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
start point for exploration sampling:  20004
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.069]
 [2.069]
 [2.069]
 [2.069]
 [2.069]] [[1.531]
 [1.531]
 [1.531]
 [1.531]
 [1.531]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
1449 3035
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.016]
 [0.014]] [[1.363]
 [1.363]
 [1.363]
 [2.06 ]
 [1.363]] [[0.982]
 [0.982]
 [0.982]
 [1.916]
 [0.982]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.9059557
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
in main func line 156:  1455
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.007]
 [0.007]] [[2.705]
 [2.409]
 [3.26 ]
 [3.713]
 [2.448]] [[0.918]
 [0.663]
 [1.396]
 [1.787]
 [0.698]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.9047959
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.90302175
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[3.562]
 [3.562]
 [3.562]
 [3.562]
 [3.562]] [[2.01]
 [2.01]
 [2.01]
 [2.01]
 [2.01]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.018]
 [0.017]
 [0.018]] [[4.413]
 [4.413]
 [4.413]
 [4.107]
 [4.413]] [[1.706]
 [1.706]
 [1.706]
 [1.572]
 [1.706]]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[2.306]
 [2.306]
 [2.306]
 [2.306]
 [2.306]] [[2.338]
 [2.338]
 [2.338]
 [2.338]
 [2.338]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
1466 3071
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.9248218
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using another actor
siam score:  -0.9286955
line 256 mcts: sample exp_bonus 2.1280745130373693
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[2.126]
 [2.126]
 [2.126]
 [2.126]
 [2.126]] [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.005]
 [0.004]] [[0.642]
 [0.642]
 [0.642]
 [0.805]
 [0.642]] [[0.004]
 [0.004]
 [0.004]
 [0.005]
 [0.004]]
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.008]] [[2.13 ]
 [2.13 ]
 [2.13 ]
 [2.13 ]
 [2.287]] [[0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.008]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[3.976]
 [3.976]
 [3.976]
 [3.974]
 [3.976]] [[1.756]
 [1.756]
 [1.756]
 [1.755]
 [1.756]]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.011]
 [0.013]
 [0.015]
 [0.015]] [[2.122]
 [1.373]
 [2.347]
 [2.771]
 [1.895]] [[0.5  ]
 [0.005]
 [0.646]
 [0.925]
 [0.351]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.011]
 [0.014]
 [0.013]
 [0.012]] [[1.797]
 [2.409]
 [1.797]
 [2.203]
 [2.078]] [[0.792]
 [1.194]
 [0.792]
 [1.06 ]
 [0.976]]
line 256 mcts: sample exp_bonus 3.3134758296304847
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.92185163
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.92264783
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.01 ]
 [0.011]
 [0.013]
 [0.012]] [[1.636]
 [1.392]
 [1.37 ]
 [2.72 ]
 [1.54 ]] [[0.387]
 [0.219]
 [0.207]
 [1.11 ]
 [0.321]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.013]] [[6.544]
 [6.544]
 [6.544]
 [5.27 ]
 [5.71 ]] [[1.942]
 [1.942]
 [1.942]
 [1.491]
 [1.648]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.016]
 [0.013]] [[3.279]
 [3.279]
 [3.279]
 [3.574]
 [3.279]] [[1.168]
 [1.168]
 [1.168]
 [1.326]
 [1.168]]
siam score:  -0.92168623
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[3.059]
 [3.059]
 [3.059]
 [3.213]
 [3.059]] [[1.254]
 [1.254]
 [1.254]
 [1.358]
 [1.254]]
first move QE:  0.5909856744011212
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.013]
 [0.011]] [[2.546]
 [2.546]
 [2.546]
 [2.54 ]
 [1.88 ]] [[0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.462]]
Starting evaluation
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.003]
 [0.004]
 [0.005]
 [0.004]] [[1.952]
 [1.731]
 [1.952]
 [2.522]
 [1.952]] [[0.004]
 [0.003]
 [0.004]
 [0.005]
 [0.004]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.005]
 [0.004]] [[2.88 ]
 [2.88 ]
 [2.88 ]
 [3.172]
 [2.88 ]] [[0.004]
 [0.004]
 [0.004]
 [0.005]
 [0.004]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[3.115]
 [3.115]
 [3.115]
 [2.524]
 [3.115]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[3.308]
 [3.308]
 [3.308]
 [3.308]
 [3.308]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[3.305]
 [3.305]
 [3.305]
 [2.712]
 [3.305]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.002]
 [0.003]
 [0.003]
 [0.004]] [[2.648]
 [1.95 ]
 [2.607]
 [3.078]
 [1.424]] [[0.004]
 [0.002]
 [0.003]
 [0.003]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 2.460075881248648
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.006]
 [0.006]
 [0.005]
 [0.006]] [[1.99 ]
 [2.695]
 [2.695]
 [3.542]
 [2.695]] [[0.005]
 [0.006]
 [0.006]
 [0.005]
 [0.006]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[1.854]
 [1.388]
 [1.388]
 [3.239]
 [2.407]] [[0.007]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[2.816]
 [2.816]
 [2.816]
 [2.816]
 [2.816]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.006]
 [0.005]] [[4.074]
 [4.074]
 [4.074]
 [4.023]
 [4.074]] [[0.005]
 [0.005]
 [0.005]
 [0.006]
 [0.005]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.005]
 [0.004]] [[1.968]
 [1.968]
 [1.968]
 [3.331]
 [1.968]] [[0.004]
 [0.004]
 [0.004]
 [0.005]
 [0.004]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.006]
 [0.005]] [[4.185]
 [4.185]
 [4.185]
 [4.134]
 [4.185]] [[0.005]
 [0.005]
 [0.005]
 [0.006]
 [0.005]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[4.261]
 [4.261]
 [4.261]
 [4.261]
 [4.261]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
rdn probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
1491 3090
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
line 256 mcts: sample exp_bonus 8.143026338957522
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
first move QE:  0.5944107476322018
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[6.145]
 [6.552]
 [6.552]
 [5.965]
 [6.552]] [[1.802]
 [1.97 ]
 [1.97 ]
 [1.728]
 [1.97 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.921503
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[6.467]
 [6.467]
 [6.467]
 [7.802]
 [6.467]] [[1.501]
 [1.501]
 [1.501]
 [2.003]
 [1.501]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.90643466
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.012]
 [0.011]] [[4.71 ]
 [4.71 ]
 [4.71 ]
 [5.202]
 [4.71 ]] [[0.956]
 [0.956]
 [0.956]
 [1.17 ]
 [0.956]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 6.783255614871976
siam score:  -0.9065866
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[5.074]
 [5.074]
 [5.074]
 [5.074]
 [5.659]] [[1.685]
 [1.685]
 [1.685]
 [1.685]
 [2.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.9053106
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[3.753]
 [2.101]
 [2.101]
 [2.101]
 [2.101]] [[2.009]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
start point for exploration sampling:  20004
siam score:  -0.9055664
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.9023407
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[1.758]
 [1.758]
 [1.758]
 [2.325]
 [1.758]] [[0.682]
 [0.682]
 [0.682]
 [1.093]
 [0.682]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.008]
 [0.008]] [[1.974]
 [1.974]
 [1.974]
 [2.233]
 [1.635]] [[0.684]
 [0.684]
 [0.684]
 [0.894]
 [0.411]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.006]
 [0.007]
 [0.007]
 [0.008]] [[2.056]
 [1.18 ]
 [1.499]
 [2.056]
 [2.107]] [[1.17 ]
 [0.329]
 [0.637]
 [1.169]
 [1.219]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[2.261]
 [2.261]
 [2.261]
 [2.261]
 [2.261]] [[1.708]
 [1.708]
 [1.708]
 [1.708]
 [1.708]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
1513 3104
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.89833635
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8978433
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[4.125]
 [4.125]
 [4.125]
 [5.047]
 [4.125]] [[0.722]
 [0.722]
 [0.722]
 [1.266]
 [0.722]]
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.008]] [[5.618]
 [5.815]
 [3.018]
 [4.786]
 [3.808]] [[1.727]
 [1.818]
 [0.514]
 [1.339]
 [0.883]]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[3.655]
 [1.979]
 [1.979]
 [0.447]
 [1.979]] [[1.147]
 [0.583]
 [0.583]
 [0.068]
 [0.583]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.006]
 [0.007]
 [0.008]] [[5.912]
 [3.795]
 [2.624]
 [4.053]
 [3.819]] [[1.893]
 [1.037]
 [0.562]
 [1.141]
 [1.047]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
line 256 mcts: sample exp_bonus 3.4003285521144364
siam score:  -0.887009
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
first move QE:  0.6139038861611927
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.877436
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.006]
 [0.006]
 [0.007]
 [0.006]] [[3.1  ]
 [2.531]
 [2.435]
 [2.678]
 [2.75 ]] [[1.71 ]
 [1.145]
 [1.049]
 [1.291]
 [1.361]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.003]
 [0.004]
 [0.004]] [[4.377]
 [3.135]
 [3.132]
 [3.127]
 [2.965]] [[0.004]
 [0.004]
 [0.003]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[3.949]
 [3.949]
 [3.949]
 [3.949]
 [3.949]] [[1.647]
 [1.647]
 [1.647]
 [1.647]
 [1.647]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.8780911
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.88251036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[3.242]
 [3.242]
 [3.242]
 [3.242]
 [3.904]] [[1.144]
 [1.144]
 [1.144]
 [1.144]
 [1.657]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[4.428]
 [4.428]
 [4.428]
 [4.468]
 [4.428]] [[1.824]
 [1.824]
 [1.824]
 [1.849]
 [1.824]]
siam score:  -0.87792885
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[4.133]
 [4.133]
 [4.133]
 [4.133]
 [4.133]] [[1.557]
 [1.557]
 [1.557]
 [1.557]
 [1.557]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.948033849042439
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.86643755
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.8655938
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.86373335
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[5.072]
 [5.072]
 [5.072]
 [4.957]
 [5.072]] [[1.979]
 [1.979]
 [1.979]
 [1.917]
 [1.979]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
start point for exploration sampling:  20004
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.594762735128937
using another actor
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.005]
 [0.012]
 [0.009]
 [0.009]] [[1.407]
 [1.223]
 [1.066]
 [0.638]
 [0.638]] [[0.006]
 [0.005]
 [0.012]
 [0.009]
 [0.009]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.003]
 [0.004]] [[3.648]
 [3.648]
 [3.648]
 [3.877]
 [3.648]] [[1.57 ]
 [1.57 ]
 [1.57 ]
 [1.751]
 [1.57 ]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[3.194]
 [3.194]
 [3.194]
 [3.278]
 [3.558]] [[1.154]
 [1.154]
 [1.154]
 [1.218]
 [1.428]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.002]
 [0.002]
 [0.003]
 [0.003]] [[2.272]
 [0.75 ]
 [0.742]
 [1.43 ]
 [0.818]] [[1.166]
 [0.149]
 [0.142]
 [0.603]
 [0.196]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.8755992
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.8730994
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
first move QE:  0.6319028134129479
using another actor
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[2.354]
 [2.354]
 [2.354]
 [2.354]
 [2.354]] [[1.327]
 [1.327]
 [1.327]
 [1.327]
 [1.327]]
from probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
siam score:  -0.8671303
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[3.385]
 [3.052]
 [3.052]
 [3.556]
 [2.979]] [[1.325]
 [1.029]
 [1.029]
 [1.477]
 [0.965]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
in main func line 156:  1568
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.85577774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04175829468318963, 0.4582417053168104, 0.4582417053168104, 0.04175829468318963]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8629652
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.2289109542175054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05271132610305698, 0.578814456494905, 0.315762891298981, 0.05271132610305698]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05271132610305698, 0.578814456494905, 0.315762891298981, 0.05271132610305698]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05271132610305698, 0.578814456494905, 0.315762891298981, 0.05271132610305698]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
siam score:  -0.8513756
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.003]
 [0.004]
 [0.004]] [[1.76 ]
 [1.76 ]
 [4.625]
 [1.76 ]
 [1.76 ]] [[0.543]
 [0.543]
 [2.002]
 [0.543]
 [0.543]]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 2.13214379264717
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
line 256 mcts: sample exp_bonus 0.7504134383051152
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[2.374]
 [2.374]
 [2.374]
 [2.699]
 [2.374]] [[1.178]
 [1.178]
 [1.178]
 [1.396]
 [1.178]]
line 256 mcts: sample exp_bonus 2.790569154925763
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
first move QE:  0.6437681666723613
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
siam score:  -0.82895845
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057766835721272385, 0.6344663285574552, 0.25, 0.057766835721272385]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.706]
 [0.59 ]
 [0.706]
 [0.706]] [[2.054]
 [2.054]
 [2.776]
 [2.054]
 [2.054]] [[1.664]
 [1.664]
 [1.913]
 [1.664]
 [1.664]]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06067765942388064, 0.6665091492674625, 0.21213553188477607, 0.06067765942388064]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
in main func line 156:  1598
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256972451238153, 0.6873373094711096, 0.18752324150412716, 0.06256972451238153]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256972451238153, 0.6873373094711096, 0.18752324150412716, 0.06256972451238153]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.06256972451238153, 0.6873373094711096, 0.18752324150412716, 0.06256972451238153]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[1.795]
 [1.795]
 [1.795]
 [1.795]
 [2.073]] [[1.151]
 [1.151]
 [1.151]
 [1.151]
 [1.429]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256972451238153, 0.6873373094711096, 0.18752324150412716, 0.06256972451238153]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[1.074]
 [1.074]
 [1.074]
 [1.5  ]
 [1.074]] [[0.57 ]
 [0.57 ]
 [0.57 ]
 [0.995]
 [0.57 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256972451238153, 0.6873373094711096, 0.18752324150412716, 0.06256972451238153]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256972451238153, 0.6873373094711096, 0.18752324150412716, 0.06256972451238153]
siam score:  -0.8350117
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256972451238153, 0.6873373094711096, 0.18752324150412716, 0.06256972451238153]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8331659
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.004]
 [0.003]
 [0.004]] [[0.6  ]
 [0.596]
 [0.927]
 [0.675]
 [0.859]] [[0.185]
 [0.182]
 [0.404]
 [0.235]
 [0.358]]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[1.701]
 [1.701]
 [1.701]
 [1.701]
 [1.701]] [[1.143]
 [1.143]
 [1.143]
 [1.143]
 [1.143]]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.349211782884817
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06564050287537179, 0.7211409370962721, 0.14757805715298433, 0.06564050287537179]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06564050287537179, 0.7211409370962721, 0.14757805715298433, 0.06564050287537179]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06564050287537179, 0.7211409370962721, 0.14757805715298433, 0.06564050287537179]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 4.514296771085744
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
first move QE:  0.6472946022812504
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
siam score:  -0.8266933
1613 3192
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.006]
 [0.032]
 [0.031]
 [0.041]] [[1.037]
 [1.563]
 [1.586]
 [1.423]
 [1.432]] [[0.251]
 [0.583]
 [0.65 ]
 [0.539]
 [0.566]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
from probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
from probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
1618 3202
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.13018110607360384, 0.6779246211657005, 0.13018110607360384, 0.06171316668709177]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.025]
 [0.084]
 [0.025]
 [0.025]] [[1.709]
 [1.709]
 [2.289]
 [1.709]
 [1.709]] [[0.369]
 [0.369]
 [0.681]
 [0.369]
 [0.369]]
siam score:  -0.84001523
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.747]
 [0.719]
 [0.719]
 [0.719]] [[1.559]
 [3.163]
 [1.559]
 [1.559]
 [1.559]] [[1.315]
 [2.26 ]
 [1.315]
 [1.315]
 [1.315]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.125045733411539, 0.6873399330596136, 0.125045733411539, 0.06256860011730848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.125045733411539, 0.6873399330596136, 0.125045733411539, 0.06256860011730848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.125045733411539, 0.6873399330596136, 0.125045733411539, 0.06256860011730848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.125045733411539, 0.6873399330596136, 0.125045733411539, 0.06256860011730848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.125045733411539, 0.6873399330596136, 0.125045733411539, 0.06256860011730848]
from probs:  [0.125045733411539, 0.6873399330596136, 0.125045733411539, 0.06256860011730848]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17649798295243907, 0.6469108920568293, 0.11769636931439029, 0.05889475567634153]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16850844825986594, 0.6574577587006704, 0.11418074709977656, 0.05985304593968719]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16850844825986594, 0.6574577587006704, 0.11418074709977656, 0.05985304593968719]
siam score:  -0.857112
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16850844825986594, 0.6574577587006704, 0.11418074709977656, 0.05985304593968719]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16850844825986594, 0.6574577587006704, 0.11418074709977656, 0.05985304593968719]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16850844825986594, 0.6574577587006704, 0.11418074709977656, 0.05985304593968719]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[1.605]
 [1.605]
 [1.605]
 [1.605]
 [1.605]] [[0.949]
 [0.949]
 [0.949]
 [0.949]
 [0.949]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16850844825986594, 0.6574577587006704, 0.11418074709977656, 0.05985304593968719]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.1616487178106794, 0.6665131874639398, 0.11116227084535336, 0.060675823880027305]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1616487178106794, 0.6665131874639398, 0.11116227084535336, 0.060675823880027305]
from probs:  [0.1616487178106794, 0.6665131874639398, 0.11116227084535336, 0.060675823880027305]
1628 3218
from probs:  [0.1616487178106794, 0.6665131874639398, 0.11116227084535336, 0.060675823880027305]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20194104362344495, 0.6344716510124406, 0.10582313087033478, 0.05776417449377971]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19371462581667118, 0.6439976192833022, 0.10365802712334493, 0.058629727776681816]
siam score:  -0.83155465
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19371462581667118, 0.6439976192833022, 0.10365802712334493, 0.058629727776681816]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]] [[1.58]
 [1.58]
 [1.58]
 [1.58]
 [1.58]] [[0.83]
 [0.83]
 [0.83]
 [0.83]
 [0.83]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18002579534455246, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18002579534455246, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18002579534455246, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18002579534455246, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
from probs:  [0.18002579534455246, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.0461688046141986
from probs:  [0.18002579534455243, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18002579534455243, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18002579534455243, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18002579534455243, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18002579534455243, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18002579534455243, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18002579534455243, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18002579534455243, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.02 ]
 [0.02 ]
 [0.016]
 [0.02 ]] [[1.485]
 [1.485]
 [1.485]
 [1.786]
 [1.485]] [[0.441]
 [0.441]
 [0.441]
 [0.634]
 [0.441]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18002579534455243, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]] [[1.219]
 [1.219]
 [1.219]
 [1.219]
 [1.219]] [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]]
from probs:  [0.18002579534455243, 0.6598489129819072, 0.10005527573832665, 0.06007001593521376]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.015]
 [0.014]
 [0.015]] [[2.185]
 [2.185]
 [2.185]
 [2.422]
 [2.185]] [[1.358]
 [1.358]
 [1.358]
 [1.593]
 [1.358]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.005]
 [0.005]
 [0.001]] [[0.615]
 [0.926]
 [0.623]
 [0.724]
 [0.627]] [[0.001]
 [0.002]
 [0.005]
 [0.005]
 [0.001]]
siam score:  -0.82980806
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
siam score:  -0.8306199
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.006]
 [0.005]
 [0.004]
 [0.006]] [[0.766]
 [0.945]
 [0.738]
 [1.081]
 [0.793]] [[0.257]
 [0.38 ]
 [0.239]
 [0.465]
 [0.277]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.6486964277011475
Starting evaluation
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.008]
 [0.009]
 [0.008]
 [0.008]] [[1.553]
 [1.479]
 [1.692]
 [1.642]
 [1.617]] [[0.009]
 [0.008]
 [0.009]
 [0.008]
 [0.008]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[2.291]
 [2.291]
 [2.291]
 [2.291]
 [2.291]] [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]]
line 256 mcts: sample exp_bonus 1.8503653671235638
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
rdn probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
line 256 mcts: sample exp_bonus 2.4319488018125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20004
from probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8155548
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
line 256 mcts: sample exp_bonus 2.150983880114055
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4238347087660561
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
from probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17427023777381723, 0.6665136922440053, 0.09854047554763444, 0.06067559443454306]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8448696
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16909436321271828, 0.6725072143335822, 0.09717824162402343, 0.06122018082967603]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16909436321271828, 0.6725072143335822, 0.09717824162402343, 0.06122018082967603]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16909436321271828, 0.6725072143335822, 0.09717824162402343, 0.06122018082967603]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[2.442]
 [2.442]
 [2.442]
 [2.442]
 [2.442]] [[0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16909436321271828, 0.6725072143335822, 0.09717824162402343, 0.06122018082967603]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.063]
 [0.018]
 [0.018]] [[0.135]
 [0.135]
 [0.96 ]
 [0.135]
 [0.135]] [[0.021]
 [0.021]
 [0.661]
 [0.021]
 [0.021]]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16016341248247437, 0.6828490125844416, 0.09482771246972849, 0.062159862463355545]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16016341248247437, 0.6828490125844416, 0.09482771246972849, 0.062159862463355545]
Printing some Q and Qe and total Qs values:  [[1.5]
 [0. ]
 [0. ]
 [1.5]
 [0. ]] [[1.875]
 [0.662]
 [0.662]
 [1.875]
 [0.662]] [[2.375]
 [0.   ]
 [0.   ]
 [2.375]
 [0.   ]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.16016341248247437, 0.6828490125844416, 0.09482771246972849, 0.062159862463355545]
1665 3286
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18673211775111165, 0.6612412346177742, 0.09183029437777916, 0.06019635325333501]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.58 ]
 [0.155]
 [0.155]
 [0.155]] [[3.343]
 [6.543]
 [3.343]
 [3.343]
 [3.343]] [[0.697]
 [1.927]
 [0.697]
 [0.697]
 [0.697]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18673211775111165, 0.6612412346177742, 0.09183029437777916, 0.06019635325333501]
1667 3288
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18184316443611473, 0.6665139951126319, 0.09096738368426785, 0.060675456766985554]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18184316443611473, 0.6665139951126319, 0.09096738368426785, 0.060675456766985554]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18184316443611473, 0.6665139951126319, 0.09096738368426785, 0.060675456766985554]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18184316443611473, 0.6665139951126319, 0.09096738368426785, 0.060675456766985554]
1671 3293
using explorer policy with actor:  1
from probs:  [0.18184316443611473, 0.6665139951126319, 0.09096738368426785, 0.060675456766985554]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.026]
 [0.001]
 [0.004]] [[-0.083]
 [-0.028]
 [ 0.535]
 [-0.153]
 [ 0.227]] [[0.003]
 [0.002]
 [0.026]
 [0.001]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18184316443611473, 0.6665139951126319, 0.09096738368426785, 0.060675456766985554]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.021]] [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.912]] [[0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.696]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18184316443611473, 0.6665139951126319, 0.09096738368426785, 0.060675456766985554]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16938417251045573, 0.6799510799442359, 0.08876834502091151, 0.061896402524396786]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16938417251045573, 0.6799510799442359, 0.08876834502091151, 0.061896402524396786]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16583367038580596, 0.6837803141654619, 0.08814167381885753, 0.06224434162987475]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16583367038580596, 0.6837803141654619, 0.08814167381885753, 0.06224434162987475]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16583367038580596, 0.6837803141654619, 0.08814167381885753, 0.06224434162987475]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16583367038580596, 0.6837803141654619, 0.08814167381885753, 0.06224434162987475]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16583367038580596, 0.6837803141654619, 0.08814167381885753, 0.06224434162987475]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16583367038580596, 0.6837803141654619, 0.08814167381885753, 0.06224434162987475]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]] [[1.743]
 [1.191]
 [1.191]
 [1.191]
 [1.191]] [[0.879]
 [0.498]
 [0.498]
 [0.498]
 [0.498]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.01 ]
 [0.01 ]
 [0.013]
 [0.01 ]] [[1.591]
 [1.195]
 [1.195]
 [1.316]
 [1.195]] [[0.799]
 [0.524]
 [0.524]
 [0.612]
 [0.524]]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.788]
 [0.792]
 [0.712]
 [0.679]] [[2.736]
 [2.606]
 [3.369]
 [2.302]
 [3.264]] [[1.263]
 [1.53 ]
 [1.81 ]
 [1.338]
 [1.65 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18689178832950087, 0.6665141970252942, 0.08591864965670229, 0.06067536498850265]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.43227951188715
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1829513200565454, 0.6705780832816698, 0.08542596741152049, 0.06104462925026428]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1829513200565454, 0.6705780832816698, 0.08542596741152049, 0.06104462925026428]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1829513200565454, 0.6705780832816698, 0.08542596741152049, 0.06104462925026428]
from probs:  [0.1829513200565454, 0.6705780832816698, 0.0854259674115205, 0.06104462925026428]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1829513200565454, 0.6705780832816698, 0.0854259674115205, 0.06104462925026428]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1829513200565454, 0.6705780832816698, 0.0854259674115205, 0.06104462925026428]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1829513200565454, 0.6705780832816698, 0.0854259674115205, 0.06104462925026428]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1829513200565454, 0.6705780832816698, 0.0854259674115205, 0.06104462925026428]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1829513200565454, 0.6705780832816698, 0.0854259674115205, 0.06104462925026428]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1829513200565454, 0.6705780832816698, 0.0854259674115205, 0.06104462925026428]
siam score:  -0.8534484
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1829513200565454, 0.6705780832816698, 0.0854259674115205, 0.06104462925026428]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1829513200565454, 0.6705780832816698, 0.0854259674115205, 0.06104462925026428]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20239846976584255, 0.6546130069903386, 0.08339464418044881, 0.05959387906337007]
siam score:  -0.8582655
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20239846976584255, 0.6546130069903386, 0.08339464418044881, 0.05959387906337007]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20239846976584255, 0.6546130069903386, 0.08339464418044881, 0.05959387906337007]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20239846976584255, 0.6546130069903386, 0.08339464418044881, 0.05959387906337007]
using explorer policy with actor:  0
using another actor
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20239846976584255, 0.6546130069903386, 0.08339464418044881, 0.05959387906337007]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20239846976584255, 0.6546130069903386, 0.08339464418044881, 0.05959387906337007]
from probs:  [0.20239846976584255, 0.6546130069903386, 0.08339464418044881, 0.05959387906337007]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20239846976584255, 0.6546130069903386, 0.08339464418044881, 0.05959387906337007]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20239846976584255, 0.6546130069903386, 0.08339464418044881, 0.05959387906337007]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20239846976584255, 0.6546130069903386, 0.08339464418044881, 0.05959387906337007]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20239846976584255, 0.6546130069903386, 0.08339464418044881, 0.05959387906337007]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.945]
 [1.017]
 [0.981]
 [0.909]] [[1.677]
 [1.12 ]
 [1.872]
 [1.697]
 [2.009]] [[0.993]
 [1.445]
 [2.091]
 [1.903]
 [1.967]]
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.272]
 [0.357]
 [0.272]
 [0.272]] [[1.923]
 [1.923]
 [2.943]
 [1.923]
 [1.923]] [[1.306]
 [1.306]
 [2.14 ]
 [1.306]
 [1.306]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.22094100197094438, 0.6393905735893456, 0.08145781143147729, 0.05821061300823279]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.767]
 [0.469]
 [0.469]
 [0.573]] [[4.257]
 [3.542]
 [4.257]
 [4.257]
 [4.672]] [[1.891]
 [1.978]
 [1.891]
 [1.891]
 [2.244]]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.21180448367199434, 0.6483246702777731, 0.08084842769026075, 0.05902241835997183]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.21180448367199434, 0.6483246702777731, 0.08084842769026075, 0.05902241835997183]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.21180448367199434, 0.6483246702777731, 0.08084842769026075, 0.05902241835997183]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.21180448367199434, 0.6483246702777731, 0.08084842769026075, 0.05902241835997183]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.21180448367199434, 0.6483246702777731, 0.08084842769026075, 0.05902241835997183]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.21180448367199434, 0.6483246702777731, 0.08084842769026075, 0.05902241835997183]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.21180448367199434, 0.6483246702777731, 0.08084842769026075, 0.05902241835997183]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.21180448367199434, 0.6483246702777731, 0.08084842769026075, 0.05902241835997183]
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.983]
 [0.837]
 [0.837]
 [0.837]] [[3.457]
 [3.299]
 [3.457]
 [3.457]
 [3.457]] [[2.355]
 [2.578]
 [2.355]
 [2.355]
 [2.355]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.21180448367199434, 0.6483246702777731, 0.08084842769026075, 0.05902241835997183]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.22864040907397642, 0.6344726366684245, 0.07912327259181132, 0.05776368166578774]
from probs:  [0.22864040907397642, 0.6344726366684245, 0.07912327259181132, 0.05776368166578774]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.22864040907397642, 0.6344726366684245, 0.07912327259181132, 0.05776368166578774]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.22407599376666218, 0.638860093500067, 0.07890155885997052, 0.058162353873300286]
from probs:  [0.22407599376666223, 0.638860093500067, 0.07890155885997052, 0.058162353873300286]
using explorer policy with actor:  1
from probs:  [0.22407599376666223, 0.638860093500067, 0.07890155885997052, 0.058162353873300286]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.22407599376666223, 0.638860093500067, 0.07890155885997052, 0.058162353873300286]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.22407599376666223, 0.638860093500067, 0.07890155885997052, 0.058162353873300286]
Printing some Q and Qe and total Qs values:  [[1.124]
 [1.145]
 [1.115]
 [1.124]
 [1.124]] [[3.546]
 [3.099]
 [3.033]
 [3.546]
 [3.546]] [[2.311]
 [2.203]
 [2.121]
 [2.311]
 [2.311]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.22407599376666218, 0.638860093500067, 0.07890155885997052, 0.058162353873300286]
Printing some Q and Qe and total Qs values:  [[1.102]
 [1.153]
 [1.126]
 [1.087]
 [1.03 ]] [[2.689]
 [2.442]
 [2.452]
 [1.802]
 [2.124]] [[2.086]
 [2.104]
 [2.055]
 [1.759]
 [1.753]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23984118439508148, 0.6258761773819842, 0.07730013471638567, 0.0569825035065487]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23984118439508148, 0.6258761773819842, 0.07730013471638567, 0.0569825035065487]
siam score:  -0.90170074
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23984118439508148, 0.6258761773819842, 0.07730013471638567, 0.0569825035065487]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23984118439508148, 0.6258761773819842, 0.07730013471638567, 0.0569825035065487]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23518337529436725, 0.6302933674445738, 0.07713937843428462, 0.0573838788267743]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23518337529436725, 0.6302933674445738, 0.07713937843428462, 0.0573838788267743]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23518337529436725, 0.6302933674445738, 0.07713937843428462, 0.0573838788267743]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23518337529436725, 0.6302933674445738, 0.07713937843428462, 0.0573838788267743]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23518337529436725, 0.6302933674445738, 0.07713937843428462, 0.0573838788267743]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23518337529436725, 0.6302933674445738, 0.07713937843428462, 0.0573838788267743]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  1718
siam score:  -0.8995013
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.975]
 [0.806]
 [0.856]
 [0.75 ]] [[3.698]
 [2.275]
 [1.963]
 [1.959]
 [2.287]] [[2.575]
 [1.914]
 [1.425]
 [1.512]
 [1.518]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23518337529436725, 0.6302933674445738, 0.07713937843428462, 0.0573838788267743]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23518337529436725, 0.6302933674445738, 0.07713937843428462, 0.0573838788267743]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23077636422394388, 0.6344727155211215, 0.07698727801549528, 0.057763642239439215]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23077636422394388, 0.6344727155211215, 0.07698727801549528, 0.057763642239439215]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23077636422394388, 0.6344727155211215, 0.07698727801549528, 0.057763642239439215]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23077636422394388, 0.6344727155211215, 0.07698727801549528, 0.057763642239439215]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23077636422394388, 0.6344727155211215, 0.07698727801549528, 0.057763642239439215]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23077636422394388, 0.6344727155211215, 0.07698727801549528, 0.057763642239439215]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23077636422394388, 0.6344727155211215, 0.07698727801549528, 0.057763642239439215]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23077636422394388, 0.6344727155211215, 0.07698727801549528, 0.057763642239439215]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23077636422394388, 0.6344727155211215, 0.07698727801549528, 0.057763642239439215]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23077636422394388, 0.6344727155211215, 0.07698727801549528, 0.057763642239439215]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23077636422394388, 0.6344727155211215, 0.07698727801549528, 0.057763642239439215]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23077636422394388, 0.6344727155211215, 0.07698727801549528, 0.057763642239439215]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.387]
 [0.611]
 [0.387]
 [0.387]] [[2.556]
 [2.556]
 [1.824]
 [2.556]
 [2.556]] [[1.886]
 [1.886]
 [1.845]
 [1.886]
 [1.886]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23077636422394388, 0.6344727155211215, 0.07698727801549528, 0.057763642239439215]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23077636422394388, 0.6344727155211215, 0.07698727801549528, 0.057763642239439215]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.22660042603790287, 0.6384329277708122, 0.0768431526804813, 0.0581234935108036]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.231]
 [0.638]
 [0.289]
 [0.226]] [[2.593]
 [1.795]
 [2.593]
 [2.601]
 [2.289]] [[2.635]
 [1.022]
 [2.635]
 [1.943]
 [1.506]]
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.22263785124471933, 0.6421907988256896, 0.0767063912165557, 0.05846495871303527]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.22263785124471933, 0.6421907988256896, 0.0767063912165557, 0.05846495871303527]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.22263785124471933, 0.6421907988256896, 0.0767063912165557, 0.05846495871303527]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.22263785124471933, 0.6421907988256896, 0.0767063912165557, 0.05846495871303527]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.21887269495365078, 0.6457614498750112, 0.07657644331319731, 0.05878941185814062]
siam score:  -0.8923288
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20862446266077095, 0.655480265924445, 0.07622274317523792, 0.059672528239546295]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20551792535173813, 0.6584263217704046, 0.07611552637497623, 0.059940226502881]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20551792535173813, 0.6584263217704046, 0.07611552637497623, 0.059940226502881]
siam score:  -0.9032078
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.20254901624796331, 0.6612418591843178, 0.07601305957586554, 0.06019606499185333]
Printing some Q and Qe and total Qs values:  [[0.931]
 [0.931]
 [1.164]
 [0.931]
 [1.091]] [[2.347]
 [2.347]
 [2.288]
 [2.347]
 [2.654]] [[1.732]
 [1.732]
 [2.159]
 [1.732]
 [2.256]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19970878758882415, 0.663935363691986, 0.07591503396131427, 0.060440814757875556]
first move QE:  0.662858153500572
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.1969890508007556, 0.6665146008512063, 0.07582116691676832, 0.06067518143126991]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  0.6622773952028618
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19405037341936177, 0.6565672864859711, 0.08961107046883711, 0.05977126962583007]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.19405037341936177, 0.6565672864859711, 0.08961107046883711, 0.05977126962583007]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.211]
 [1.341]
 [1.254]
 [1.211]
 [1.211]] [[3.095]
 [2.034]
 [3.003]
 [3.095]
 [3.095]] [[2.208]
 [2.114]
 [2.264]
 [2.208]
 [2.208]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2005831303704693, 0.6523945098404641, 0.08763028550297063, 0.059392074286095956]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2005831303704693, 0.6523945098404641, 0.08763028550297063, 0.059392074286095956]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2005831303704693, 0.6523945098404641, 0.08763028550297063, 0.059392074286095956]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2005831303704693, 0.6523945098404641, 0.08763028550297063, 0.059392074286095956]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2005831303704693, 0.6523945098404641, 0.08763028550297063, 0.059392074286095956]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2005831303704693, 0.6523945098404641, 0.08763028550297063, 0.059392074286095956]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2005831303704693, 0.6523945098404641, 0.08763028550297063, 0.059392074286095956]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2005831303704693, 0.6523945098404641, 0.08763028550297063, 0.059392074286095956]
from probs:  [0.2005831303704693, 0.6523945098404641, 0.08763028550297063, 0.059392074286095956]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2005831303704693, 0.6523945098404641, 0.08763028550297063, 0.059392074286095956]
siam score:  -0.9143454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2005831303704693, 0.6523945098404641, 0.08763028550297063, 0.059392074286095956]
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.83 ]
 [0.815]
 [0.05 ]
 [0.83 ]] [[1.471]
 [1.471]
 [2.495]
 [0.968]
 [1.471]] [[1.779]
 [1.779]
 [2.092]
 [0.052]
 [1.779]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1980799954769648, 0.654976035279675, 0.08731731916115627, 0.05962665008220414]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.428]
 [0.574]
 [0.428]
 [0.428]] [[2.08 ]
 [2.08 ]
 [2.469]
 [2.08 ]
 [2.08 ]] [[1.354]
 [1.354]
 [2.035]
 [1.354]
 [1.354]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20903149135622795, 0.6460289168897964, 0.0861259654249119, 0.05881362632906389]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20645034450640096, 0.6486468464414067, 0.08585129852412668, 0.059051510528065725]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20645034450640096, 0.6486468464414067, 0.08585129852412668, 0.059051510528065725]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20645034450640096, 0.6486468464414067, 0.08585129852412668, 0.059051510528065725]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20645034450640096, 0.6486468464414067, 0.08585129852412668, 0.059051510528065725]
first move QE:  0.6578549662089309
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.998]
 [0.998]
 [0.998]
 [0.998]
 [0.998]] [[2.654]
 [2.654]
 [2.654]
 [2.654]
 [2.654]] [[2.303]
 [2.303]
 [2.303]
 [2.303]
 [2.303]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20156816847357473, 0.6535985960535438, 0.08533177281015414, 0.05950146266272734]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19925718610818052, 0.6559425111345557, 0.08508585485158673, 0.059714447905677]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19925718610818052, 0.6559425111345557, 0.08508585485158673, 0.059714447905677]
siam score:  -0.9124556
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19925718610818052, 0.6559425111345557, 0.08508585485158673, 0.059714447905677]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19925718610818052, 0.6559425111345557, 0.08508585485158673, 0.059714447905677]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19925718610818052, 0.6559425111345557, 0.08508585485158673, 0.059714447905677]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.949]
 [0.359]
 [0.359]
 [0.359]] [[2.808]
 [2.654]
 [2.808]
 [2.808]
 [2.808]] [[1.284]
 [2.412]
 [1.284]
 [1.284]
 [1.284]]
1749 3390
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.20928816981836088, 0.6477232640821665, 0.08402100002870214, 0.058967566070770396]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.20691239459328598, 0.6500991930623443, 0.08380495057410316, 0.0591834617702666]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.20691239459328598, 0.6500991930623443, 0.08380495057410316, 0.0591834617702666]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.20691239459328598, 0.6500991930623443, 0.08380495057410316, 0.0591834617702666]
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.21177877866826203, 0.6469126830603557, 0.08241464493007211, 0.05889389334131029]
from probs:  [0.21177877866826203, 0.6469126830603557, 0.08241464493007211, 0.05889389334131029]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.21177877866826203, 0.6469126830603557, 0.08241464493007211, 0.05889389334131029]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.20517340203634654, 0.6534393816728806, 0.0819002576362997, 0.059486958654472996]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.20517340203634654, 0.6534393816728806, 0.0819002576362997, 0.059486958654472996]
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.128]
 [0.453]
 [0.128]
 [0.128]] [[-0.139]
 [-0.139]
 [-0.099]
 [-0.139]
 [-0.139]] [[0.263]
 [0.263]
 [0.71 ]
 [0.263]
 [0.263]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.357]
 [0.593]
 [0.357]
 [0.357]] [[1.165]
 [1.165]
 [2.222]
 [1.165]
 [1.165]] [[0.932]
 [0.932]
 [1.756]
 [0.932]
 [0.932]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.22031333243583312, 0.641324254254927, 0.0799763584961352, 0.058386054813104736]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.22407596805816626, 0.6388604791275061, 0.07890138918389736, 0.05816216363043037]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.22407596805816626, 0.6388604791275061, 0.07890138918389736, 0.05816216363043037]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.22407596805816626, 0.6388604791275061, 0.07890138918389736, 0.05816216363043037]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.22407596805816626, 0.6388604791275061, 0.07890138918389736, 0.05816216363043037]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.22407596805816626, 0.6388604791275061, 0.07890138918389736, 0.05816216363043037]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.22407596805816626, 0.6388604791275061, 0.07890138918389736, 0.05816216363043037]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.22407596805816626, 0.6388604791275061, 0.07890138918389736, 0.05816216363043037]
using explorer policy with actor:  1
1764 3403
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
from probs:  [0.22407596805816626, 0.6388604791275061, 0.07890138918389736, 0.05816216363043037]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.22189176488981163, 0.640959997441711, 0.07879529523794358, 0.05835294243053386]
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
in main func line 156:  1766
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.018519858789896
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.22189176500116714, 0.6409599958928568, 0.07879529591619995, 0.05835294318977607]
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21976921114191644, 0.6430002551550864, 0.07869219647085979, 0.058538337232137416]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21976921114191644, 0.6430002551550864, 0.07869219647085979, 0.058538337232137416]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21976921114191644, 0.6430002551550864, 0.07869219647085979, 0.058538337232137416]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21976921114191644, 0.6430002551550864, 0.07869219647085979, 0.058538337232137416]
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.837]
 [0.808]
 [0.837]
 [0.607]] [[2.075]
 [2.197]
 [2.333]
 [2.197]
 [2.531]] [[1.582]
 [1.702]
 [1.731]
 [1.702]
 [1.493]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21976921114191644, 0.6430002551550864, 0.07869219647085979, 0.058538337232137416]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21976921114191644, 0.6430002551550864, 0.07869219647085979, 0.058538337232137416]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21976921114191644, 0.6430002551550864, 0.07869219647085979, 0.058538337232137416]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21976921114191644, 0.6430002551550864, 0.07869219647085979, 0.058538337232137416]
siam score:  -0.90979886
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21976921114191644, 0.6430002551550864, 0.07869219647085979, 0.058538337232137416]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21976921114191644, 0.6430002551550864, 0.07869219647085979, 0.058538337232137416]
Printing some Q and Qe and total Qs values:  [[1.24]
 [1.24]
 [1.24]
 [1.24]
 [1.24]] [[1.664]
 [1.664]
 [1.664]
 [1.664]
 [1.664]] [[2.244]
 [2.244]
 [2.244]
 [2.244]
 [2.244]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2177057328225028, 0.6449837293247735, 0.07859196651943791, 0.05871857133328579]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2177057328225028, 0.6449837293247735, 0.07859196651943791, 0.05871857133328579]
first move QE:  0.6525415880469949
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2177057328225028, 0.6449837293247735, 0.07859196651943791, 0.05871857133328579]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2177057328225028, 0.6449837293247735, 0.07859196651943791, 0.05871857133328579]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21374640469433698, 0.6487895483622926, 0.07839964888652858, 0.05906439805684166]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.566]
 [0.525]
 [0.525]
 [0.525]] [[3.182]
 [2.493]
 [3.182]
 [3.182]
 [3.182]] [[2.258]
 [1.992]
 [2.258]
 [2.258]
 [2.258]]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]] [[2.429]
 [2.429]
 [2.429]
 [2.429]
 [2.429]] [[1.692]
 [1.692]
 [1.692]
 [1.692]
 [1.692]]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.20999584965583631, 0.6523946887559995, 0.07821747205153243, 0.05939198953663187]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2064379791501719, 0.6558146152852408, 0.0780446545401522, 0.05970275102443511]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2064379791501719, 0.6558146152852408, 0.0780446545401522, 0.05970275102443511]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2064379791501719, 0.6558146152852408, 0.0780446545401522, 0.05970275102443511]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2064379791501719, 0.6558146152852408, 0.0780446545401522, 0.05970275102443511]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2064379791501719, 0.6558146152852408, 0.0780446545401522, 0.05970275102443511]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2064379791501719, 0.6558146152852408, 0.0780446545401522, 0.05970275102443511]
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.20472671920992686, 0.6574595271106584, 0.07796153299772199, 0.059852220681692735]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.447]
 [0.265]
 [0.17 ]
 [0.315]] [[2.549]
 [2.34 ]
 [1.613]
 [1.312]
 [2.206]] [[1.327]
 [1.45 ]
 [0.604]
 [0.212]
 [1.098]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2067632396689604, 0.656425547111772, 0.07705295867584167, 0.05975825454342582]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
siam score:  -0.9129161
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
1792 3417
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.207053365749971, 0.6569704864645604, 0.07616838517845413, 0.05980776260701452]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2055178910863227, 0.6584266363892185, 0.07611539242835252, 0.059940080096106255]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2055178910863227, 0.6584266363892185, 0.07611539242835252, 0.059940080096106255]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2055178910863227, 0.6584266363892185, 0.07611539242835252, 0.059940080096106255]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2055178910863227, 0.6584266363892185, 0.07611539242835252, 0.059940080096106255]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2055178910863228, 0.6584266363892185, 0.07611539242835254, 0.059940080096106255]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2055178910863228, 0.6584266363892185, 0.07611539242835254, 0.059940080096106255]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.21189198566618886, 0.6531426779524231, 0.07550540805044374, 0.05945992833094431]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.235]
 [0.417]
 [0.235]
 [0.235]] [[1.919]
 [1.919]
 [3.366]
 [1.919]
 [1.919]] [[1.047]
 [1.047]
 [2.207]
 [1.047]
 [1.047]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21189198566618886, 0.6531426779524231, 0.07550540805044374, 0.05945992833094431]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21189198566618886, 0.6531426779524231, 0.07550540805044374, 0.05945992833094431]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21189198566618886, 0.6531426779524231, 0.07550540805044374, 0.05945992833094431]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21189198566618886, 0.6531426779524231, 0.07550540805044374, 0.05945992833094431]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21189198566618886, 0.6531426779524231, 0.07550540805044374, 0.05945992833094431]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21189198566618886, 0.6531426779524231, 0.07550540805044374, 0.05945992833094431]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21189198566618886, 0.6531426779524231, 0.07550540805044374, 0.05945992833094431]
from probs:  [0.21189198566618886, 0.6531426779524231, 0.07550540805044374, 0.05945992833094431]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.061]
 [0.005]
 [0.034]
 [0.04 ]] [[2.135]
 [1.512]
 [1.25 ]
 [1.359]
 [1.447]] [[0.037]
 [0.061]
 [0.005]
 [0.034]
 [0.04 ]]
from probs:  [0.21189198566618886, 0.6531426779524231, 0.07550540805044374, 0.05945992833094431]
in main func line 156:  1799
Printing some Q and Qe and total Qs values:  [[0.935]
 [0.935]
 [0.935]
 [0.935]
 [0.935]] [[2.866]
 [2.866]
 [2.866]
 [2.866]
 [2.866]] [[2.326]
 [2.326]
 [2.326]
 [2.326]
 [2.326]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2118919856661889, 0.6531426779524231, 0.07550540805044373, 0.0594599283309443]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2118919856661889, 0.6531426779524231, 0.07550540805044373, 0.0594599283309443]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2118919856661889, 0.6531426779524231, 0.07550540805044373, 0.0594599283309443]
maxi score, test score, baseline:  0.0041 0.0 0.0041
in main func line 156:  1801
first move QE:  0.6546196519875144
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2118919856661889, 0.6531426779524231, 0.07550540805044373, 0.0594599283309443]
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]] [[1.189]
 [1.189]
 [1.189]
 [1.367]
 [1.189]] [[0.834]
 [0.834]
 [0.834]
 [1.072]
 [0.834]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2118919856661889, 0.6531426779524231, 0.07550540805044373, 0.0594599283309443]
line 256 mcts: sample exp_bonus 2.773751395050878
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.6867],
        [0.6013],
        [0.3630],
        [0.0505],
        [0.0000],
        [0.0505],
        [0.0505],
        [0.0505],
        [0.0000],
        [0.0390]], dtype=torch.float64)
0.0 0.6866560378695882
0.0 0.6012517389263309
0.0 0.3629854499815281
0.0 0.050514707366547874
0.0 0.0
0.0 0.050514707366547874
0.0 0.050514707366547874
0.0 0.050514707366547874
0.0 0.0
0.0 0.038969504511761614
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2118919856661889, 0.6531426779524231, 0.07550540805044373, 0.0594599283309443]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2118919856661889, 0.6531426779524231, 0.07550540805044373, 0.0594599283309443]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2118919856661889, 0.6531426779524231, 0.07550540805044373, 0.0594599283309443]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2118919856661889, 0.6531426779524231, 0.07550540805044373, 0.0594599283309443]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2118919856661889, 0.6531426779524231, 0.07550540805044373, 0.0594599283309443]
from probs:  [0.21189198566618886, 0.6531426779524231, 0.07550540805044374, 0.05945992833094431]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21189198566618886, 0.6531426779524231, 0.07550540805044374, 0.05945992833094431]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.059]
 [0.063]
 [0.071]
 [0.069]] [[0.834]
 [0.815]
 [0.965]
 [0.688]
 [0.569]] [[0.642]
 [0.603]
 [0.762]
 [0.5  ]
 [0.378]]
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.074]
 [0.086]
 [0.066]
 [0.068]] [[1.658]
 [1.381]
 [1.658]
 [1.467]
 [1.675]] [[1.267]
 [0.966]
 [1.267]
 [1.035]
 [1.247]]
siam score:  -0.9028671
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.83 ]
 [0.834]
 [0.771]
 [0.771]] [[4.052]
 [3.213]
 [3.522]
 [4.052]
 [4.052]] [[2.267]
 [2.005]
 [2.132]
 [2.267]
 [2.267]]
Printing some Q and Qe and total Qs values:  [[0.802]
 [0.802]
 [0.954]
 [0.802]
 [0.802]] [[3.265]
 [3.265]
 [2.893]
 [3.265]
 [3.265]] [[2.371]
 [2.371]
 [2.427]
 [2.371]
 [2.371]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21189198566618886, 0.6531426779524231, 0.07550540805044374, 0.05945992833094431]
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21033199794510035, 0.654613620959977, 0.07546079095844131, 0.059593590136481425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21033199794510035, 0.654613620959977, 0.07546079095844131, 0.059593590136481425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21033199794510035, 0.654613620959977, 0.07546079095844131, 0.059593590136481425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21033199794510035, 0.654613620959977, 0.07546079095844131, 0.059593590136481425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21033199794510035, 0.654613620959977, 0.07546079095844131, 0.059593590136481425]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.21033199794510035, 0.654613620959977, 0.07546079095844131, 0.059593590136481425]
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.163]
 [0.149]
 [0.163]
 [0.218]] [[1.442]
 [2.799]
 [3.548]
 [2.799]
 [2.392]] [[0.339]
 [1.069]
 [1.458]
 [1.069]
 [0.935]]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.04 ]
 [0.039]
 [0.035]] [[1.119]
 [1.119]
 [1.753]
 [1.597]
 [1.119]] [[0.035]
 [0.035]
 [0.04 ]
 [0.039]
 [0.035]]
1808 3450
from probs:  [0.2088062948676206, 0.6560522363048825, 0.07541715443896359, 0.05972431438853334]
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.142]
 [0.165]
 [0.047]
 [0.142]] [[2.779]
 [2.779]
 [3.652]
 [2.912]
 [2.779]] [[1.183]
 [1.183]
 [1.811]
 [1.081]
 [1.183]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2088062948676206, 0.6560522363048825, 0.07541715443896359, 0.05972431438853334]
siam score:  -0.89273995
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2088062948676206, 0.6560522363048825, 0.07541715443896359, 0.05972431438853334]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2088062948676206, 0.6560522363048825, 0.07541715443896359, 0.05972431438853334]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.2088062948676206, 0.6560522363048825, 0.07541715443896359, 0.05972431438853334]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.221030752879571, 0.6459130439791955, 0.0742532341360645, 0.05880296900516907]
siam score:  -0.89304245
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.22700234749471312, 0.6409600925898763, 0.07368466412613424, 0.05835289578927635]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.22700234749471312, 0.6409600925898763, 0.07368466412613424, 0.05835289578927635]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.22700234749471312, 0.6409600925898763, 0.07368466412613424, 0.05835289578927635]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.22700234749471312, 0.6409600925898763, 0.07368466412613424, 0.05835289578927635]
maxi score, test score, baseline:  0.0041 0.0 0.0041
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.22700234749471312, 0.6409600925898763, 0.07368466412613424, 0.05835289578927635]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.22535051132842013, 0.6424957042320787, 0.07366135027254432, 0.058492434166956736]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
from probs:  [0.22373339756196794, 0.6439990365704806, 0.07363852648749918, 0.05862903938005229]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.22373339756196794, 0.6439990365704806, 0.07363852648749918, 0.05862903938005229]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.22214992284057267, 0.6454710956638682, 0.07361617799029355, 0.05876280350526564]
maxi score, test score, baseline:  0.0061 0.0 0.0061
1821 3464
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.22214992284057267, 0.6454710956638682, 0.07361617799029355, 0.05876280350526564]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.22214992284057267, 0.6454710956638682, 0.07361617799029355, 0.05876280350526564]
siam score:  -0.87361526
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.22628611302530363, 0.6421912076584407, 0.07305792026572674, 0.05846475905052894]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.2318907475265167, 0.6375380029325428, 0.07252932575986352, 0.05804192378107687]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.2318907475265167, 0.6375380029325428, 0.07252932575986352, 0.05804192378107687]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.23741473095309298, 0.6329517581416002, 0.07200833776517171, 0.05762517314013508]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.704]
 [0.689]
 [0.704]
 [0.704]] [[3.099]
 [3.099]
 [3.055]
 [3.099]
 [3.099]] [[2.066]
 [2.066]
 [2.005]
 [2.066]
 [2.066]]
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.23576025336405074, 0.6344731591706294, 0.07200316705063449, 0.05776342041468525]
siam score:  -0.8882643
siam score:  -0.8899536
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.23576025336405074, 0.6344731591706294, 0.07200316705063449, 0.05776342041468525]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.23576025336405074, 0.6344731591706294, 0.07200316705063449, 0.05776342041468525]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.23576025336405074, 0.6344731591706294, 0.07200316705063449, 0.05776342041468525]
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.843]
 [0.73 ]
 [0.73 ]
 [0.73 ]] [[3.337]
 [3.062]
 [3.337]
 [3.337]
 [3.337]] [[2.262]
 [2.395]
 [2.262]
 [2.262]
 [2.262]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.24116310611792763, 0.6299864369291112, 0.07149474358213848, 0.05735571337082271]
using explorer policy with actor:  1
start point for exploration sampling:  20004
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.2394996879336028, 0.631511338412432, 0.07149469487124742, 0.057494278782717796]
from probs:  [0.2394996879336028, 0.631511338412432, 0.07149469487124742, 0.057494278782717796]
siam score:  -0.89246064
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.23626881540458722, 0.6344731686715596, 0.07149460025963303, 0.057763415664220184]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.23626881540458722, 0.6344731686715596, 0.07149460025963303, 0.057763415664220184]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.23626881540458722, 0.6344731686715596, 0.07149460025963303, 0.057763415664220184]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.23626881540458722, 0.6344731686715596, 0.07149460025963303, 0.057763415664220184]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.23626881540458722, 0.6344731686715596, 0.07149460025963303, 0.057763415664220184]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.23164896368888435, 0.6387083145899969, 0.07149446497369244, 0.05814825674742645]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.23164896368888435, 0.6387083145899969, 0.07149446497369244, 0.05814825674742645]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.23164896368888435, 0.6387083145899969, 0.07149446497369244, 0.05814825674742645]
siam score:  -0.90865934
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.23164896368888435, 0.6387083145899969, 0.07149446497369244, 0.05814825674742645]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.926]
 [0.768]
 [0.575]
 [0.768]] [[2.266]
 [2.065]
 [2.266]
 [2.694]
 [2.266]] [[1.798]
 [1.98 ]
 [1.798]
 [1.697]
 [1.798]]
Printing some Q and Qe and total Qs values:  [[0.815]
 [1.067]
 [0.762]
 [0.815]
 [0.815]] [[2.599]
 [0.897]
 [2.956]
 [2.599]
 [2.599]] [[2.267]
 [1.639]
 [2.4  ]
 [2.267]
 [2.267]]
Starting evaluation
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.22871033877029234, 0.6414022333769334, 0.0714943789201434, 0.05839304893263099]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]] [[-0.252]
 [-0.252]
 [-0.252]
 [-0.252]
 [-0.252]] [[0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]]
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.502220190920631
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]] [[0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [0.22871033962706502, 0.6414022176254977, 0.07149438610385252, 0.05839305664358481]
actor:  0 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [0.22871033968705, 0.6414022165226965, 0.07149438660680374, 0.05839305718344988]
actor:  0 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.038339097522096674
from probs:  [0.22871033974741034, 0.6414022154129947, 0.07149438711290197, 0.05839305772669294]
using explorer policy with actor:  0
rdn probs:  [0.22871033974741034, 0.6414022154129947, 0.07149438711290197, 0.05839305772669294]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.22871092023673983, 0.6413915433399373, 0.07149925429266453, 0.05839828213065826]
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [1.152]
 [0.78 ]
 [0.832]
 [0.832]] [[3.121]
 [1.934]
 [2.641]
 [2.565]
 [2.565]] [[1.643]
 [1.707]
 [1.518]
 [1.556]
 [1.556]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23373040967782918, 0.6372162496676665, 0.07103450645612053, 0.05801883419838384]
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23226508830395254, 0.6385557926134029, 0.071038618339885, 0.058140500742759595]
siam score:  -0.9026731
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.117]
 [0.086]
 [0.115]
 [0.11 ]] [[2.194]
 [2.194]
 [1.881]
 [1.864]
 [2.25 ]] [[1.694]
 [1.694]
 [1.215]
 [1.249]
 [1.753]]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23226508830395254, 0.6385557926134029, 0.071038618339885, 0.058140500742759595]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23226508830395254, 0.6385557926134029, 0.071038618339885, 0.058140500742759595]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23226508830395254, 0.6385557926134029, 0.071038618339885, 0.058140500742759595]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23226508830395254, 0.6385557926134029, 0.071038618339885, 0.058140500742759595]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.2322650883039526, 0.6385557926134029, 0.071038618339885, 0.058140500742759595]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.128]] [[2.842]
 [2.842]
 [2.842]
 [2.842]
 [2.642]] [[2.063]
 [2.063]
 [2.063]
 [2.063]
 [1.87 ]]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.2322650883039526, 0.6385557926134029, 0.071038618339885, 0.058140500742759595]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.119]
 [0.09 ]
 [0.119]
 [0.119]] [[1.736]
 [1.736]
 [2.135]
 [1.736]
 [1.736]] [[1.108]
 [1.108]
 [1.583]
 [1.108]
 [1.108]]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.2322650883039526, 0.6385557926134029, 0.071038618339885, 0.058140500742759595]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.2322650883039526, 0.6385557926134029, 0.071038618339885, 0.058140500742759595]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
siam score:  -0.89116704
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.125]
 [0.125]
 [0.125]
 [0.125]] [[4.432]
 [3.47 ]
 [3.47 ]
 [3.47 ]
 [3.47 ]] [[1.683]
 [1.255]
 [1.255]
 [1.255]
 [1.255]]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284722, 0.6398713552354397, 0.0710426566132408, 0.05825998922847229]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284722, 0.6398713552354397, 0.0710426566132408, 0.05825998922847229]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
siam score:  -0.900844
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
siam score:  -0.89785624
siam score:  -0.89832413
siam score:  -0.89661837
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23082599892284725, 0.6398713552354397, 0.0710426566132408, 0.05825998922847228]
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.322]
 [0.361]
 [0.322]
 [0.322]] [[1.608]
 [1.608]
 [2.044]
 [1.608]
 [1.608]] [[1.395]
 [1.395]
 [1.908]
 [1.395]
 [1.395]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.095]
 [1.427]
 [1.039]
 [1.095]
 [1.095]] [[1.958]
 [1.232]
 [2.937]
 [1.958]
 [1.958]] [[1.641]
 [2.061]
 [1.856]
 [1.641]
 [1.641]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.2390531477392186, 0.6331398291273511, 0.07015885571573315, 0.05764816741769719]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23905314773921854, 0.6331398291273512, 0.07015885571573315, 0.057648167417697196]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23905314773921854, 0.6331398291273512, 0.07015885571573315, 0.057648167417697196]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23905314773921854, 0.6331398291273512, 0.07015885571573315, 0.057648167417697196]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23905314773921854, 0.6331398291273512, 0.07015885571573315, 0.057648167417697196]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23905314773921854, 0.6331398291273512, 0.07015885571573315, 0.057648167417697196]
start point for exploration sampling:  20004
first move QE:  0.6635821681017576
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.2390531477392186, 0.6331398291273511, 0.07015885571573315, 0.05764816741769719]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23759795958323857, 0.6344632529196058, 0.07017041395695864, 0.057768373540197177]
line 256 mcts: sample exp_bonus 2.9497892178755825
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9006643913597313
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.2379855134126524, 0.634463570795122, 0.06978270118978659, 0.057768214602439034]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.2379855134126524, 0.634463570795122, 0.06978270118978659, 0.057768214602439034]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.2379855134126524, 0.634463570795122, 0.06978270118978659, 0.057768214602439034]
siam score:  -0.8924023
siam score:  -0.8929832
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.2379855134126524, 0.634463570795122, 0.06978270118978659, 0.057768214602439034]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.2379855134126524, 0.634463570795122, 0.06978270118978659, 0.057768214602439034]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.2379855134126524, 0.634463570795122, 0.06978270118978659, 0.057768214602439034]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23659646683660093, 0.6357238988133723, 0.06979694302541295, 0.0578826913246138]
maxi score, test score, baseline:  0.040100000000000004 0.85 0.85
probs:  [0.23659646683660093, 0.6357238988133723, 0.06979694302541295, 0.0578826913246138]
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.23659646683660093, 0.6357238988133723, 0.06979694302541295, 0.0578826913246138]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.23659646683660093, 0.6357238988133723, 0.06979694302541295, 0.0578826913246138]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[2.203]
 [2.203]
 [2.203]
 [2.203]
 [2.203]] [[1.992]
 [1.992]
 [1.992]
 [1.992]
 [1.992]]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.23972203367320186, 0.6332213158991877, 0.0694014488291184, 0.05765520159849197]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.24416118492757383, 0.6295229797077043, 0.06899673275478722, 0.05731910260993477]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.24416118492757383, 0.6295229797077042, 0.0689967327547872, 0.05731910260993476]
siam score:  -0.8940922
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.24416118492757383, 0.6295229797077042, 0.0689967327547872, 0.05731910260993476]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.24416118492757383, 0.6295229797077042, 0.0689967327547872, 0.05731910260993476]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.24416118492757383, 0.6295229797077042, 0.0689967327547872, 0.05731910260993476]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.24416118492757383, 0.6295229797077042, 0.0689967327547872, 0.05731910260993476]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2413628594048557, 0.6271551393213005, 0.07437814123206624, 0.05710386004177767]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2369994819823221, 0.6356820345244453, 0.06943724975447318, 0.057881233738759474]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.23567093059014055, 0.6368848740662032, 0.06945372543577183, 0.05799046990788433]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.23567093059014055, 0.6368848740662032, 0.06945372543577183, 0.05799046990788433]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.23567093059014055, 0.6368848740662032, 0.06945372543577183, 0.05799046990788433]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.23567093059014055, 0.6368848740662032, 0.06945372543577183, 0.05799046990788433]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.23567093059014055, 0.6368848740662032, 0.06945372543577183, 0.05799046990788433]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.23869235361874835, 0.6344599769625552, 0.06907765789997407, 0.05777001151872245]
line 256 mcts: sample exp_bonus 1.781054607003671
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.23869235361874835, 0.6344599769625552, 0.06907765789997407, 0.05777001151872245]
start point for exploration sampling:  20004
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.24447127065828148, 0.6204248658951412, 0.07860939040672578, 0.05649447303985169]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.24447127065828148, 0.6204248658951412, 0.07860939040672578, 0.05649447303985169]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.24447127065828148, 0.6204248658951412, 0.07860939040672578, 0.05649447303985169]
first move QE:  0.6691749964844844
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.24447127065828148, 0.6204248658951412, 0.07860939040672578, 0.05649447303985169]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
first move QE:  0.6693044583508312
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.04 ]
 [0.09 ]
 [0.067]
 [0.089]] [[2.526]
 [2.825]
 [3.152]
 [2.847]
 [2.478]] [[1.021]
 [1.103]
 [1.421]
 [1.171]
 [0.969]]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25273411776508686, 0.6136376627565623, 0.0777505807995231, 0.05587763867882763]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2567981029413342, 0.6102994558907177, 0.07732818529010896, 0.05557425587783922]
1900 3517
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2567981029413342, 0.6102994558907177, 0.07732818529010896, 0.05557425587783922]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2567981029413342, 0.6102994558907177, 0.07732818529010896, 0.05557425587783922]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2567981029413342, 0.6102994558907177, 0.07732818529010896, 0.05557425587783922]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2567981029413342, 0.6102994558907177, 0.07732818529010896, 0.05557425587783922]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2567981029413342, 0.6102994558907177, 0.07732818529010896, 0.05557425587783922]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2567981029413342, 0.6102994558907177, 0.07732818529010896, 0.05557425587783922]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2567981029413342, 0.6102994558907177, 0.07732818529010896, 0.05557425587783922]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2567981029413342, 0.6102994558907177, 0.07732818529010896, 0.05557425587783922]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2567981029413342, 0.6102994558907177, 0.07732818529010896, 0.05557425587783922]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2567981029413342, 0.6102994558907177, 0.07732818529010896, 0.05557425587783922]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25822033358332885, 0.6089545664720287, 0.07737299475009309, 0.05545210519454936]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5654999820016076
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
from probs:  [0.25822033358332885, 0.6089545664720287, 0.07737299475009309, 0.05545210519454936]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25822033358332885, 0.6089545664720287, 0.07737299475009309, 0.05545210519454936]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25822033358332885, 0.6089545664720287, 0.07737299475009309, 0.05545210519454936]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25822033358332885, 0.6089545664720287, 0.07737299475009309, 0.05545210519454936]
from probs:  [0.25822033358332885, 0.6089545664720287, 0.07737299475009309, 0.05545210519454936]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25822033358332885, 0.6089545664720287, 0.07737299475009309, 0.05545210519454936]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25822033358332885, 0.6089545664720287, 0.07737299475009309, 0.05545210519454936]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25822033358332885, 0.6089545664720287, 0.07737299475009309, 0.05545210519454936]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25822033358332885, 0.6089545664720287, 0.07737299475009309, 0.05545210519454936]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25822033358332885, 0.6089545664720287, 0.07737299475009309, 0.05545210519454936]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25822033358332885, 0.6089545664720287, 0.07737299475009309, 0.05545210519454936]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25679810294133437, 0.6102994558907177, 0.07732818529010896, 0.05557425587783922]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25679810294133426, 0.6102994558907177, 0.07732818529010896, 0.05557425587783922]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8749617
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2553973733358818, 0.6116240135040841, 0.07728405325178068, 0.05569455990825329]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25401766085142696, 0.6129286969122366, 0.07724058338864016, 0.0558130588476963]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25799248635238553, 0.6096618858573487, 0.07682946236498026, 0.055516165425285535]
siam score:  -0.8777263
start point for exploration sampling:  20004
siam score:  -0.87899536
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25799248635238553, 0.6096618858573487, 0.07682946236498026, 0.055516165425285535]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25799248635238553, 0.6096618858573487, 0.07682946236498026, 0.055516165425285535]
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.064]
 [0.073]
 [0.063]
 [0.064]] [[1.554]
 [1.554]
 [1.801]
 [1.81 ]
 [1.554]] [[0.691]
 [0.691]
 [1.031]
 [1.022]
 [0.691]]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25799248635238553, 0.6096618858573487, 0.07682946236498026, 0.055516165425285535]
siam score:  -0.88021
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2566110720754239, 0.6109645353181453, 0.07678991162389369, 0.05563448098253719]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2566110720754239, 0.6109645353181453, 0.07678991162389369, 0.05563448098253719]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2566110720754239, 0.6109645353181453, 0.07678991162389369, 0.05563448098253719]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
start point for exploration sampling:  20004
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.25913944516291115, 0.6090496314000837, 0.07635054190468683, 0.05546038153231834]
UNIT TEST: sample policy line 217 mcts : [0.    0.167 0.    0.625 0.208]
first move QE:  0.6777680500995662
Printing some Q and Qe and total Qs values:  [[0.337]
 [1.042]
 [0.337]
 [0.337]
 [0.337]] [[2.543]
 [2.106]
 [2.543]
 [2.543]
 [2.543]] [[0.932]
 [2.196]
 [0.932]
 [0.932]
 [0.932]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.833 0.042 0.042]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.765]
 [1.175]
 [1.044]
 [0.765]
 [0.765]] [[2.441]
 [2.249]
 [2.486]
 [2.441]
 [2.441]] [[1.798]
 [2.425]
 [2.272]
 [1.798]
 [1.798]]
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.128]
 [0.191]
 [0.113]
 [0.09 ]] [[0.517]
 [0.94 ]
 [1.011]
 [0.961]
 [1.172]] [[0.23 ]
 [0.415]
 [0.564]
 [0.391]
 [0.417]]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2654258184047976, 0.5996518838420791, 0.08031599754722632, 0.05460630020589697]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2654258184047976, 0.5996518838420791, 0.08031599754722632, 0.05460630020589697]
first move QE:  0.678505291699106
using explorer policy with actor:  1
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2654258184047976, 0.5996518838420791, 0.08031599754722632, 0.05460630020589697]
1932 3539
Printing some Q and Qe and total Qs values:  [[0.448]
 [1.289]
 [0.555]
 [0.402]
 [0.653]] [[3.294]
 [1.624]
 [3.003]
 [3.214]
 [3.058]] [[1.413]
 [2.312]
 [1.507]
 [1.318]
 [1.678]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.26918358482643573, 0.5965834325309399, 0.0799055478722693, 0.05432743477035492]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.26918358482643573, 0.5965834325309399, 0.0799055478722693, 0.05432743477035492]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
siam score:  -0.8740706
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.26918358482643573, 0.5965834325309399, 0.0799055478722693, 0.05432743477035492]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2729030820287285, 0.5935462304309281, 0.07949927823057643, 0.054051409309766936]
siam score:  -0.8723245
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2729030820287285, 0.5935462304309281, 0.07949927823057643, 0.054051409309766936]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2729030820287285, 0.5935462304309281, 0.07949927823057643, 0.054051409309766936]
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2729030820287285, 0.5935462304309281, 0.07949927823057643, 0.054051409309766936]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.290260998774965
maxi score, test score, baseline:  0.042100000000000005 0.85 0.85
probs:  [0.2729030820287285, 0.5935462304309281, 0.07949927823057643, 0.054051409309766936]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.2729030820287285, 0.5935462304309281, 0.07949927823057643, 0.054051409309766936]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.222]
 [0.102]
 [0.518]
 [0.518]] [[1.919]
 [3.514]
 [2.622]
 [1.919]
 [1.919]] [[0.518]
 [0.222]
 [0.102]
 [0.518]
 [0.518]]
actor:  0 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27007215269823254, 0.596244634044512, 0.07938670206502307, 0.05429651119223235]
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27007215269823254, 0.596244634044512, 0.07938670206502307, 0.05429651119223235]
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27007215269823254, 0.596244634044512, 0.07938670206502307, 0.05429651119223235]
maxi score, test score, baseline:  0.0441 0.85 0.85
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27007215269823254, 0.596244634044512, 0.07938670206502307, 0.05429651119223235]
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27007215269823254, 0.596244634044512, 0.07938670206502307, 0.05429651119223235]
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27007215269823254, 0.596244634044512, 0.07938670206502307, 0.05429651119223235]
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27007215269823254, 0.596244634044512, 0.07938670206502307, 0.05429651119223235]
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27007215269823254, 0.596244634044512, 0.07938670206502307, 0.05429651119223235]
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27007215269823254, 0.596244634044512, 0.07938670206502307, 0.05429651119223235]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]] [[1.979]
 [1.979]
 [1.979]
 [1.979]
 [1.979]] [[1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.443]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0441 0.85 0.85
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.2686863219976512, 0.5975655891563124, 0.07933159242145234, 0.054416496424584064]
UNIT TEST: sample policy line 217 mcts : [0.083 0.417 0.083 0.333 0.083]
1949 3547
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0441 0.85 0.85
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.2723123501601728, 0.5946018524737819, 0.07893864877200757, 0.054147148594037665]
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.2723123501601728, 0.5946018524737819, 0.07893864877200757, 0.054147148594037665]
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.2723123501601728, 0.5946018524737819, 0.07893864877200757, 0.054147148594037665]
Printing some Q and Qe and total Qs values:  [[0.998]
 [1.187]
 [1.02 ]
 [0.998]
 [0.998]] [[2.883]
 [3.614]
 [3.164]
 [2.883]
 [2.883]] [[1.97 ]
 [2.597]
 [2.137]
 [1.97 ]
 [1.97 ]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.012]
 [0.069]
 [0.06 ]
 [0.06 ]] [[2.222]
 [1.914]
 [2.175]
 [1.972]
 [2.223]] [[0.76 ]
 [0.463]
 [0.751]
 [0.597]
 [0.764]]
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0441 0.85 0.85
maxi score, test score, baseline:  0.0441 0.85 0.85
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0441 0.85 0.85
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
first move QE:  0.6876150589729836
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0441 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
using another actor
from probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.087]
 [0.12 ]
 [0.083]
 [0.088]] [[1.947]
 [1.947]
 [1.37 ]
 [1.655]
 [1.373]] [[0.464]
 [0.464]
 [0.337]
 [0.359]
 [0.275]]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
1965 3563
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0461 0.85 0.85
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
start point for exploration sampling:  20004
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]] [[2.9]
 [2.9]
 [2.9]
 [2.9]
 [2.9]] [[1.174]
 [1.174]
 [1.174]
 [1.174]
 [1.174]]
1970 3565
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27590258077197644, 0.5916673749446426, 0.07854958441406028, 0.05388045986932075]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.2759025807719765, 0.5916673749446425, 0.07854958441406025, 0.05388045986932074]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.2759025807719765, 0.5916673749446425, 0.07854958441406025, 0.05388045986932074]
first move QE:  0.6983378806624099
maxi score, test score, baseline:  0.0461 0.85 0.85
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.011]
 [0.035]
 [0.033]
 [0.03 ]] [[2.512]
 [2.136]
 [2.026]
 [2.537]
 [2.659]] [[1.117]
 [0.818]
 [0.792]
 [1.129]
 [1.204]]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.2759025807719765, 0.5916673749446425, 0.07854958441406025, 0.05388045986932074]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.2759025807719765, 0.5916673749446425, 0.07854958441406025, 0.05388045986932074]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0461 0.85 0.85
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.2759025807719765, 0.5916673749446425, 0.07854958441406025, 0.05388045986932074]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.037]
 [0.037]
 [0.04 ]
 [0.037]] [[3.03]
 [3.03]
 [3.03]
 [2.84]
 [3.03]] [[1.228]
 [1.228]
 [1.228]
 [1.107]
 [1.228]]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.2759025807719765, 0.5916673749446425, 0.07854958441406025, 0.05388045986932074]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]] [[2.753]
 [2.753]
 [2.753]
 [2.753]
 [2.753]] [[1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.101]]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.2759025807719765, 0.5916673749446425, 0.07854958441406025, 0.05388045986932074]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244317, 0.07850113783778416, 0.05400130038603904]
maxi score, test score, baseline:  0.0461 0.85 0.85
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244317, 0.07850113783778416, 0.05400130038603904]
siam score:  -0.8686847
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244317, 0.07850113783778416, 0.05400130038603904]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244317, 0.07850113783778416, 0.05400130038603904]
UNIT TEST: sample policy line 217 mcts : [0.417 0.208 0.125 0.167 0.083]
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.27449983745174505, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
using another actor
maxi score, test score, baseline:  0.0461 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.27449983745174505, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.019]
 [0.019]
 [0.017]
 [0.019]] [[2.993]
 [2.993]
 [2.993]
 [2.855]
 [2.993]] [[1.384]
 [1.384]
 [1.384]
 [1.255]
 [1.384]]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]] [[2.375]
 [2.375]
 [2.375]
 [2.375]
 [2.375]] [[0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.27449983745174505, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.27449983745174505, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
siam score:  -0.87467253
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2744998374517451, 0.5929977243244318, 0.07850113783778417, 0.05400130038603906]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1996 3585
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.002]
 [0.018]
 [0.02 ]
 [0.017]] [[2.351]
 [2.179]
 [2.351]
 [2.633]
 [2.354]] [[0.911]
 [0.764]
 [0.911]
 [1.103]
 [0.912]]
maxi score, test score, baseline:  0.048100000000000004 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.032]
 [0.003]
 [0.014]
 [0.018]] [[1.397]
 [1.43 ]
 [1.514]
 [2.656]
 [1.956]] [[0.002]
 [0.032]
 [0.003]
 [0.014]
 [0.018]]
actor:  0 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
siam score:  -0.86357546
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.27803736494929543, 0.5901054269936272, 0.07811876270214532, 0.0537384453549319]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.27803736494929543, 0.5901054269936272, 0.07811876270214532, 0.0537384453549319]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.27803736494929543, 0.5901054269936272, 0.07811876270214532, 0.0537384453549319]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.27803736494929543, 0.5901054269936272, 0.07811876270214532, 0.0537384453549319]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.27803736494929543, 0.5901054269936272, 0.07811876270214532, 0.0537384453549319]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.27803736494929543, 0.5901054269936272, 0.07811876270214532, 0.0537384453549319]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.27803736494929543, 0.5901054269936272, 0.07811876270214532, 0.0537384453549319]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.27803736494929543, 0.5901054269936272, 0.07811876270214532, 0.0537384453549319]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.27803736494929543, 0.5901054269936272, 0.07811876270214532, 0.0537384453549319]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.27803736494929543, 0.5901054269936272, 0.07811876270214532, 0.0537384453549319]
first move QE:  0.7282335340198206
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.27803736494929543, 0.5901054269936272, 0.07811876270214532, 0.0537384453549319]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[3.205]
 [3.205]
 [3.205]
 [3.205]
 [3.205]] [[1.546]
 [1.546]
 [1.546]
 [1.546]
 [1.546]]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[3.014]
 [3.014]
 [3.014]
 [3.259]
 [3.014]] [[1.088]
 [1.088]
 [1.088]
 [1.25 ]
 [1.088]]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.01 ]
 [0.011]
 [0.011]] [[3.259]
 [3.259]
 [2.904]
 [3.024]
 [3.259]] [[1.25 ]
 [1.25 ]
 [1.012]
 [1.093]
 [1.25 ]]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[2.354]
 [3.006]
 [3.006]
 [3.006]
 [3.006]] [[0.879]
 [1.313]
 [1.313]
 [1.313]
 [1.313]]
using explorer policy with actor:  1
from probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
siam score:  -0.8459399
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
first move QE:  0.7344507617514817
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
siam score:  -0.8378633
maxi score, test score, baseline:  0.050100000000000006 0.85 0.85
probs:  [0.2780373649492954, 0.5901054269936273, 0.07811876270214535, 0.05373844535493191]
using another actor
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
siam score:  -0.82589287
start point for exploration sampling:  20004
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.2835755720254217, 0.5857557202542176, 0.07732562958354522, 0.05334307813681541]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28216040957973, 0.5870887374467997, 0.07728668929404266, 0.05346416367942784]
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28216040957973, 0.5870887374467997, 0.07728668929404266, 0.05346416367942784]
maxi score, test score, baseline:  0.0541 0.85 0.85
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28216040957973, 0.5870887374467997, 0.07728668929404266, 0.05346416367942784]
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28216040957973, 0.5870887374467997, 0.07728668929404266, 0.05346416367942784]
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28216040957973, 0.5870887374467997, 0.07728668929404266, 0.05346416367942784]
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28216040957973, 0.5870887374467997, 0.07728668929404266, 0.05346416367942784]
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28216040957973, 0.5870887374467997, 0.07728668929404266, 0.05346416367942784]
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28216040957973, 0.5870887374467997, 0.07728668929404266, 0.05346416367942784]
maxi score, test score, baseline:  0.0541 0.85 0.85
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.171]
 [0.211]
 [0.179]
 [0.112]] [[1.394]
 [1.191]
 [1.736]
 [1.878]
 [1.761]] [[0.446]
 [0.421]
 [0.683]
 [0.667]
 [0.494]]
maxi score, test score, baseline:  0.0541 0.85 0.85
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28216040957973004, 0.5870887374467995, 0.07728668929404263, 0.05346416367942783]
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28216040957973004, 0.5870887374467995, 0.07728668929404263, 0.05346416367942783]
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.183]
 [0.183]
 [0.154]
 [0.183]] [[2.405]
 [2.405]
 [2.405]
 [2.61 ]
 [2.405]] [[1.04]
 [1.04]
 [1.04]
 [1.12]
 [1.04]]
maxi score, test score, baseline:  0.0541 0.85 0.85
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28216040957973004, 0.5870887374467995, 0.07728668929404263, 0.05346416367942783]
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28216040957973004, 0.5870887374467995, 0.07728668929404263, 0.05346416367942783]
Printing some Q and Qe and total Qs values:  [[0.954]
 [0.954]
 [0.764]
 [0.954]
 [0.954]] [[2.842]
 [2.842]
 [4.709]
 [2.842]
 [2.842]] [[1.631]
 [1.631]
 [2.272]
 [1.631]
 [1.631]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28076400756258185, 0.5884040831884007, 0.07724826522550174, 0.05358364402351568]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.566869866164881
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28076400756258185, 0.5884040831884007, 0.07724826522550174, 0.05358364402351568]
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.28076400756258185, 0.5884040831884007, 0.07724826522550174, 0.05358364402351568]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.27938599537748104, 0.589702106563681, 0.07721034718041139, 0.05370155087842655]
maxi score, test score, baseline:  0.0541 0.85 0.85
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.27938599537748104, 0.5897021065636809, 0.07721034718041138, 0.053701550878426534]
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.27938599537748104, 0.5897021065636809, 0.07721034718041138, 0.053701550878426534]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.049]
 [0.049]
 [0.027]
 [0.049]] [[1.685]
 [1.685]
 [1.685]
 [1.468]
 [1.685]] [[0.049]
 [0.049]
 [0.049]
 [0.027]
 [0.049]]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.025]
 [0.025]
 [0.02 ]
 [0.025]] [[1.518]
 [1.582]
 [1.582]
 [1.726]
 [1.772]] [[0.025]
 [0.025]
 [0.025]
 [0.02 ]
 [0.025]]
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.27938599537748104, 0.5897021065636809, 0.07721034718041138, 0.053701550878426534]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.04 ]
 [0.07 ]
 [0.035]
 [0.038]] [[1.423]
 [1.279]
 [1.822]
 [1.55 ]
 [2.363]] [[0.036]
 [0.04 ]
 [0.07 ]
 [0.035]
 [0.038]]
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.082]
 [0.082]
 [0.08 ]
 [0.082]] [[2.275]
 [2.275]
 [2.275]
 [2.082]
 [2.275]] [[0.082]
 [0.082]
 [0.082]
 [0.08 ]
 [0.082]]
maxi score, test score, baseline:  0.0541 0.85 0.85
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.27938599537748104, 0.5897021065636809, 0.07721034718041138, 0.053701550878426534]
maxi score, test score, baseline:  0.0541 0.85 0.85
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.27938599537748104, 0.5897021065636809, 0.07721034718041138, 0.053701550878426534]
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]] [[2.357]
 [2.357]
 [2.357]
 [2.357]
 [2.357]] [[0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]]
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.665]
 [0.178]
 [0.062]
 [0.157]] [[3.374]
 [3.142]
 [2.19 ]
 [3.009]
 [3.99 ]] [[0.135]
 [0.665]
 [0.178]
 [0.062]
 [0.157]]
maxi score, test score, baseline:  0.0541 0.85 0.85
probs:  [0.27938599537748104, 0.5897021065636809, 0.07721034718041138, 0.053701550878426534]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.056100000000000004 0.85 0.85
maxi score, test score, baseline:  0.056100000000000004 0.85 0.85
probs:  [0.27938599537748104, 0.5897021065636809, 0.07721034718041138, 0.053701550878426534]
rdn probs:  [0.27938599537748104, 0.5897021065636809, 0.07721034718041138, 0.053701550878426534]
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.078]] [[2.713]
 [2.713]
 [2.713]
 [2.713]
 [1.988]] [[1.517]
 [1.517]
 [1.517]
 [1.517]
 [1.033]]
using explorer policy with actor:  1
from probs:  [0.2793865700090136, 0.5897087493041971, 0.07720696834700005, 0.05369771233978917]
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.005]
 [0.081]
 [0.075]
 [0.071]] [[2.155]
 [0.742]
 [1.283]
 [1.748]
 [1.684]] [[0.058]
 [0.005]
 [0.081]
 [0.075]
 [0.071]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6911304099046112
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.084]
 [0.084]
 [0.084]
 [0.091]] [[2.113]
 [3.024]
 [3.024]
 [3.024]
 [3.612]] [[0.888]
 [1.459]
 [1.459]
 [1.459]
 [1.859]]
maxi score, test score, baseline:  0.056100000000000004 0.05 0.056100000000000004
probs:  [0.2793865700090136, 0.5897087493041971, 0.07720696834700005, 0.05369771233978917]
maxi score, test score, baseline:  0.056100000000000004 0.05 0.056100000000000004
probs:  [0.2793865700090136, 0.5897087493041971, 0.07720696834700005, 0.05369771233978917]
maxi score, test score, baseline:  0.056100000000000004 0.05 0.056100000000000004
probs:  [0.2793865700090136, 0.5897087493041971, 0.07720696834700005, 0.05369771233978917]
maxi score, test score, baseline:  0.056100000000000004 0.05 0.056100000000000004
probs:  [0.2793865700090136, 0.5897087493041971, 0.07720696834700005, 0.05369771233978917]
maxi score, test score, baseline:  0.056100000000000004 0.05 0.056100000000000004
maxi score, test score, baseline:  0.056100000000000004 0.05 0.056100000000000004
probs:  [0.2793865700090136, 0.5897087493041971, 0.07720696834700005, 0.05369771233978917]
maxi score, test score, baseline:  0.056100000000000004 0.05 0.056100000000000004
probs:  [0.2793865700090136, 0.5897087493041971, 0.07720696834700005, 0.05369771233978917]
maxi score, test score, baseline:  0.056100000000000004 0.05 0.056100000000000004
probs:  [0.2793865700090136, 0.5897087493041971, 0.07720696834700005, 0.05369771233978917]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.056100000000000004 0.05 0.056100000000000004
probs:  [0.2793865700090136, 0.5897087493041971, 0.07720696834700005, 0.05369771233978917]
using another actor
from probs:  [0.2793865700090136, 0.5897087493041971, 0.07720696834700005, 0.05369771233978917]
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.279386569945721, 0.5897087485725345, 0.07720696871916065, 0.053697712762583866]
siam score:  -0.8296416
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.279386569945721, 0.5897087485725345, 0.07720696871916065, 0.053697712762583866]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.279386569945721, 0.5897087485725345, 0.07720696871916065, 0.053697712762583866]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.279386569945721, 0.5897087485725345, 0.07720696871916065, 0.053697712762583866]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.279386569945721, 0.5897087485725345, 0.07720696871916065, 0.053697712762583866]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.673]
 [0.581]
 [0.565]
 [0.387]] [[3.435]
 [2.761]
 [2.852]
 [3.296]
 [3.382]] [[1.921]
 [1.731]
 [1.636]
 [1.84 ]
 [1.609]]
Printing some Q and Qe and total Qs values:  [[0.863]
 [1.17 ]
 [0.762]
 [0.671]
 [0.76 ]] [[3.176]
 [3.712]
 [4.064]
 [3.541]
 [3.888]] [[1.437]
 [2.059]
 [1.685]
 [1.346]
 [1.608]]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.27802655651766556, 0.5909897709649309, 0.07716956814106243, 0.05381410437634114]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.27802655651766556, 0.5909897709649309, 0.07716956814106243, 0.05381410437634114]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.27802655651766556, 0.5909897709649309, 0.07716956814106243, 0.05381410437634114]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.27802655651766556, 0.5909897709649309, 0.07716956814106243, 0.05381410437634114]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.27802655651766556, 0.5909897709649309, 0.07716956814106243, 0.05381410437634114]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.27802655651766556, 0.5909897709649309, 0.07716956814106243, 0.05381410437634114]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2766842212386585, 0.5922541419740982, 0.07713265371477747, 0.053928983072465714]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2800253652148593, 0.589517591275717, 0.07677673914504231, 0.05368030436438128]
siam score:  -0.842847
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2800253652148593, 0.589517591275717, 0.07677673914504231, 0.05368030436438128]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2800253652148593, 0.589517591275717, 0.07677673914504231, 0.05368030436438128]
siam score:  -0.8425546
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
siam score:  -0.84451556
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
from probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
using another actor
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
first move QE:  0.7617803285968751
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.054]
 [0.054]
 [0.056]
 [0.055]] [[2.672]
 [2.672]
 [2.672]
 [2.926]
 [2.668]] [[0.776]
 [0.776]
 [0.776]
 [0.949]
 [0.775]]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.048]
 [0.058]
 [0.053]
 [0.052]] [[1.536]
 [1.2  ]
 [1.394]
 [1.475]
 [2.244]] [[0.73 ]
 [0.498]
 [0.649]
 [0.692]
 [1.203]]
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.045]
 [0.022]
 [0.052]
 [0.049]] [[1.538]
 [1.54 ]
 [1.418]
 [1.903]
 [3.738]] [[0.311]
 [0.304]
 [0.216]
 [0.439]
 [1.045]]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
maxi score, test score, baseline:  0.058100000000000006 0.05 0.058100000000000006
probs:  [0.2833357690348911, 0.5868062181801067, 0.07642409916315322, 0.05343391362184901]
actor:  0 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2069 3631
maxi score, test score, baseline:  0.0601 0.05 0.0601
probs:  [0.2833357689642293, 0.5868062174661788, 0.07642409953108194, 0.05343391403851002]
from probs:  [0.2833357689642293, 0.5868062174661788, 0.07642409953108194, 0.05343391403851002]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2072 3634
in main func line 156:  2074
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.2833357688931149, 0.5868062167476787, 0.07642409990136691, 0.053433914457839356]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]] [[1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.412]] [[0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]]
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.2833357688931149, 0.5868062167476787, 0.07642409990136691, 0.053433914457839356]
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.2833357688931149, 0.5868062167476787, 0.07642409990136691, 0.053433914457839356]
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.2833357688931149, 0.5868062167476787, 0.07642409990136691, 0.053433914457839356]
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.2833357688931149, 0.5868062167476787, 0.07642409990136691, 0.053433914457839356]
maxi score, test score, baseline:  0.0621 0.05 0.0621
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.2833357688931149, 0.5868062167476787, 0.07642409990136691, 0.053433914457839356]
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.2833357688931149, 0.5868062167476787, 0.07642409990136691, 0.053433914457839356]
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.2833357688931149, 0.5868062167476787, 0.07642409990136691, 0.053433914457839356]
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.049]
 [0.049]
 [0.043]
 [0.049]] [[1.304]
 [1.304]
 [1.304]
 [1.563]
 [1.304]] [[0.049]
 [0.049]
 [0.049]
 [0.043]
 [0.049]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.28198032363863684, 0.5880777070370177, 0.07639252881882876, 0.05354944050551676]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.2806421130880581, 0.58933303012331, 0.07636135916767081, 0.05366349762096112]
using explorer policy with actor:  0
from probs:  [0.2806421130880581, 0.58933303012331, 0.07636135916767081, 0.05366349762096112]
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.2806421130880581, 0.58933303012331, 0.07636135916767081, 0.05366349762096112]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0621 0.05 0.0621
siam score:  -0.83918685
maxi score, test score, baseline:  0.0621 0.05 0.0621
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.27932081060495784, 0.5905724924114334, 0.07633058333986516, 0.05377611364374375]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0621 0.05 0.0621
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.27932081060495784, 0.5905724924114334, 0.07633058333986516, 0.05377611364374375]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.27801609775509206, 0.5917963926121232, 0.07630019391842918, 0.05388731571435553]
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.27932081060495784, 0.5905724924114334, 0.07633058333986516, 0.05377611364374375]
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.27932081060495784, 0.5905724924114334, 0.07633058333986516, 0.05377611364374375]
siam score:  -0.8454641
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.27932081060495784, 0.5905724924114334, 0.07633058333986516, 0.05377611364374375]
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.27932081060495784, 0.5905724924114334, 0.07633058333986516, 0.05377611364374375]
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.27932081060495784, 0.5905724924114334, 0.07633058333986516, 0.05377611364374375]
maxi score, test score, baseline:  0.0621 0.05 0.0621
probs:  [0.27932081060495784, 0.5905724924114333, 0.07633058333986514, 0.053776113643743746]
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2812379357975682, 0.5891547315164539, 0.07596007198497758, 0.05364726070100035]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2812379357975682, 0.5891547315164539, 0.07596007198497758, 0.05364726070100035]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2812379357975682, 0.5891547315164539, 0.07596007198497758, 0.05364726070100035]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.28255704910185947, 0.58791971653999, 0.07598818583488884, 0.05353504852326159]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.28255704910185947, 0.58791971653999, 0.07598818583488884, 0.05353504852326159]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.28255704910185947, 0.58791971653999, 0.07598818583488884, 0.05353504852326159]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.28255704910185947, 0.58791971653999, 0.07598818583488884, 0.05353504852326159]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.28255704910185947, 0.58791971653999, 0.07598818583488884, 0.05353504852326159]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.28255704910185947, 0.58791971653999, 0.07598818583488884, 0.05353504852326159]
siam score:  -0.8354294
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.28255704910185947, 0.58791971653999, 0.07598818583488884, 0.05353504852326159]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.28255704910185947, 0.58791971653999, 0.07598818583488884, 0.05353504852326159]
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.524]
 [0.277]
 [0.524]
 [0.524]] [[2.722]
 [2.722]
 [3.086]
 [2.722]
 [2.722]] [[0.539]
 [0.539]
 [0.434]
 [0.539]
 [0.539]]
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.81 ]
 [0.589]
 [0.81 ]
 [0.81 ]] [[3.695]
 [3.695]
 [7.825]
 [3.695]
 [3.695]] [[1.068]
 [1.068]
 [2.106]
 [1.068]
 [1.068]]
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.524]
 [0.179]
 [0.524]
 [0.524]] [[2.722]
 [2.722]
 [3.567]
 [2.722]
 [2.722]] [[0.498]
 [0.498]
 [0.444]
 [0.498]
 [0.498]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.28255704910185947, 0.58791971653999, 0.07598818583488884, 0.05353504852326159]
start point for exploration sampling:  20004
line 256 mcts: sample exp_bonus 3.787694254211047
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
first move QE:  0.7743392341368441
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2812379357975682, 0.5891547315164539, 0.07596007198497758, 0.05364726070100035]
maxi score, test score, baseline:  0.0641 0.05 0.0641
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2812379357975682, 0.5891547315164539, 0.07596007198497758, 0.05364726070100035]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2812379357975682, 0.5891547315164539, 0.07596007198497758, 0.05364726070100035]
maxi score, test score, baseline:  0.0641 0.05 0.0641
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2812379357975682, 0.5891547315164539, 0.07596007198497758, 0.05364726070100035]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2812379357975682, 0.5891547315164539, 0.07596007198497758, 0.05364726070100035]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2812379357975682, 0.5891547315164539, 0.07596007198497758, 0.05364726070100035]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2812379357975682, 0.5891547315164539, 0.07596007198497758, 0.05364726070100035]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2812379357975682, 0.5891547315164539, 0.07596007198497758, 0.05364726070100035]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.279935208288339, 0.5903744053525948, 0.0759323073603994, 0.053758078998666844]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.207]
 [1.295]
 [1.207]
 [1.207]
 [1.207]] [[2.99 ]
 [3.176]
 [2.99 ]
 [2.99 ]
 [2.99 ]] [[2.525]
 [2.804]
 [2.525]
 [2.525]
 [2.525]]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2831144147322183, 0.587767030268627, 0.0755974157436502, 0.053521139255504654]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.2831144147322183, 0.587767030268627, 0.0755974157436502, 0.053521139255504654]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.094]
 [0.094]
 [0.094]
 [0.093]
 [0.094]] [[2.454]
 [2.454]
 [2.454]
 [2.443]
 [2.454]] [[1.052]
 [1.052]
 [1.052]
 [1.042]
 [1.052]]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.28052923347330666, 0.5901828872739892, 0.07554723729539013, 0.0537406419573139]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.28052923347330666, 0.5901828872739892, 0.07554723729539013, 0.0537406419573139]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.28052923347330666, 0.5901828872739892, 0.07554723729539013, 0.0537406419573139]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8708371
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.27926018425285765, 0.5913688162833393, 0.07552260501073765, 0.053848394453065315]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.27926018425285765, 0.5913688162833393, 0.07552260501073765, 0.053848394453065315]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.27926018425285765, 0.5913688162833393, 0.07552260501073765, 0.053848394453065315]
siam score:  -0.87865204
maxi score, test score, baseline:  0.0641 0.05 0.0641
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2823709256299297, 0.5888156882599316, 0.07519700159837926, 0.05361638451175941]
line 256 mcts: sample exp_bonus 2.867414525348475
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.968]
 [1.01 ]
 [1.316]
 [1.18 ]
 [1.193]] [[2.267]
 [2.035]
 [2.335]
 [2.474]
 [2.655]] [[1.732]
 [1.663]
 [2.474]
 [2.294]
 [2.442]]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2811038828604072, 0.5899976160947962, 0.0751747273708146, 0.05372377367398203]
Printing some Q and Qe and total Qs values:  [[1.236]
 [1.236]
 [1.236]
 [1.339]
 [1.236]] [[5.035]
 [5.035]
 [5.035]
 [3.733]
 [5.035]] [[2.533]
 [2.533]
 [2.533]
 [2.085]
 [2.533]]
Printing some Q and Qe and total Qs values:  [[1.151]
 [1.25 ]
 [1.151]
 [1.151]
 [1.151]] [[4.099]
 [4.347]
 [4.099]
 [4.099]
 [4.099]] [[2.399]
 [2.707]
 [2.399]
 [2.399]
 [2.399]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.8005061662794963
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0641 0.05 0.0641
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2811038828604072, 0.5899976160947962, 0.0751747273708146, 0.05372377367398203]
siam score:  -0.89371735
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2811038828604072, 0.5899976160947962, 0.0751747273708146, 0.05372377367398203]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2811038828604072, 0.5899976160947962, 0.0751747273708146, 0.05372377367398203]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2811038828604072, 0.5899976160947962, 0.0751747273708146, 0.05372377367398203]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2811038828604072, 0.5899976160947962, 0.0751747273708146, 0.05372377367398203]
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.2811038828604072, 0.5899976160947962, 0.0751747273708146, 0.05372377367398203]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.2811038828604072, 0.5899976160947962, 0.0751747273708146, 0.05372377367398203]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0641 0.05 0.0641
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.6886],
        [0.4355],
        [0.0756],
        [0.0757],
        [0.0756],
        [0.0000],
        [0.5747],
        [0.0694],
        [0.0000],
        [0.7161]], dtype=torch.float64)
0.0 0.688562191192634
0.0 0.4355273182981158
0.0 0.07561155716101269
0.0 0.07567806380819696
0.0 0.07563258558568059
0.0 0.0
0.0 0.5746971800112717
0.0 0.06941851686449978
0.0 0.0
0.0 0.71608831906287
maxi score, test score, baseline:  0.0641 0.05 0.0641
probs:  [0.27985197476965407, 0.5911654259389034, 0.075152719206312, 0.05382988008513054]
actor:  0 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0661 0.05 0.0661
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.28290998633655356, 0.5886543755277608, 0.07483394369253749, 0.05360169444314808]
maxi score, test score, baseline:  0.0661 0.05 0.0661
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.28290998633655356, 0.5886543755277608, 0.07483394369253749, 0.05360169444314808]
maxi score, test score, baseline:  0.0661 0.05 0.0661
maxi score, test score, baseline:  0.0661 0.05 0.0661
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.28290998633655356, 0.5886543755277608, 0.07483394369253749, 0.05360169444314808]
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.28290998633655356, 0.5886543755277608, 0.07483394369253749, 0.05360169444314808]
siam score:  -0.8888866
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.2841748388927379, 0.5874765340657869, 0.07485395067471816, 0.05349467636675695]
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.28545491687070235, 0.5862845145615101, 0.07487419848713689, 0.053386370080650634]
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.28545491687070235, 0.5862845145615101, 0.07487419848713689, 0.053386370080650634]
maxi score, test score, baseline:  0.0661 0.05 0.0661
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.28545491687070235, 0.5862845145615101, 0.07487419848713689, 0.053386370080650634]
maxi score, test score, baseline:  0.0661 0.05 0.0661
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.28545491687070235, 0.5862845145615101, 0.07487419848713689, 0.053386370080650634]
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.28545491687070235, 0.5862845145615101, 0.07487419848713689, 0.053386370080650634]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.205]
 [0.303]
 [0.138]
 [0.205]] [[1.282]
 [1.136]
 [1.19 ]
 [1.542]
 [1.136]] [[0.539]
 [0.545]
 [0.759]
 [0.546]
 [0.545]]
first move QE:  0.7838137802233468
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.2867504968447249, 0.5850780594666093, 0.07489469150454606, 0.05327675218411965]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.2898209357868134, 0.5825586258952795, 0.0745726342364706, 0.05304780408143632]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.2898209357868134, 0.5825586258952795, 0.0745726342364706, 0.05304780408143632]
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.2898209357868134, 0.5825586258952795, 0.0745726342364706, 0.05304780408143632]
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.2898209357868134, 0.5825586258952795, 0.0745726342364706, 0.05304780408143632]
maxi score, test score, baseline:  0.0661 0.05 0.0661
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.2898209357868134, 0.5825586258952795, 0.0745726342364706, 0.05304780408143632]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.2898209357868134, 0.5825586258952795, 0.0745726342364706, 0.05304780408143632]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.2928650391967905, 0.580060801815287, 0.07425333929315885, 0.05282081969476359]
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.2928650391967905, 0.580060801815287, 0.07425333929315885, 0.05282081969476359]
maxi score, test score, baseline:  0.0661 0.05 0.0661
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.239]
 [0.087]
 [0.095]
 [0.092]] [[1.668]
 [1.22 ]
 [3.342]
 [1.474]
 [1.668]] [[0.558]
 [0.474]
 [1.468]
 [0.455]
 [0.558]]
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.2915440177065119, 0.5812869104288508, 0.07423684816475763, 0.05293222369987976]
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.2915440177065119, 0.5812869104288508, 0.07423684816475763, 0.05293222369987976]
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.2915440177065119, 0.5812869104288508, 0.07423684816475763, 0.05293222369987976]
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.2915440177065119, 0.5812869104288508, 0.07423684816475763, 0.05293222369987976]
siam score:  -0.87226653
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0661 0.05 0.0661
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.29753016569207835, 0.5763738044189385, 0.07361027398717565, 0.05248575590180747]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.29753016569207835, 0.5763738044189385, 0.07361027398717565, 0.05248575590180747]
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.29753016569207835, 0.5763738044189385, 0.07361027398717565, 0.05248575590180747]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.29753016569207835, 0.5763738044189385, 0.07361027398717565, 0.05248575590180747]
maxi score, test score, baseline:  0.0661 0.05 0.0661
probs:  [0.29753016569207835, 0.5763738044189385, 0.07361027398717565, 0.05248575590180747]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.06810000000000001 0.05 0.06810000000000001
probs:  [0.3004854447835804, 0.573948270694641, 0.07330094325746851, 0.05226534126431001]
Printing some Q and Qe and total Qs values:  [[1.154]
 [1.053]
 [1.154]
 [1.154]
 [1.259]] [[2.177]
 [1.587]
 [2.177]
 [2.177]
 [2.35 ]] [[2.145]
 [1.551]
 [2.145]
 [2.145]
 [2.47 ]]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.30495788315662764, 0.5704148282150556, 0.07268305623050318, 0.05194423239781349]
siam score:  -0.8687831
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.30495788315662764, 0.5704148282150556, 0.07268305623050318, 0.05194423239781349]
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.30495788315662764, 0.5704148282150556, 0.07268305623050318, 0.05194423239781349]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.3064694083136895, 0.5693088361010442, 0.07237804294057666, 0.05184371264468957]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.87031424
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.30795141597647085, 0.5682244421164103, 0.07207898603715078, 0.051745155869968]
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.30795141597647085, 0.5682244421164103, 0.07207898603715078, 0.051745155869968]
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.26 ]
 [0.597]
 [0.505]
 [0.26 ]] [[0.271]
 [0.271]
 [0.508]
 [0.408]
 [0.271]] [[0.26 ]
 [0.26 ]
 [0.597]
 [0.505]
 [0.26 ]]
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.30795141597647085, 0.5682244421164103, 0.07207898603715078, 0.051745155869968]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.30795141597647085, 0.5682244421164103, 0.07207898603715078, 0.051745155869968]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
in main func line 156:  2144
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.759]
 [1.224]
 [0.759]
 [0.759]] [[2.004]
 [2.004]
 [1.933]
 [2.004]
 [2.004]] [[1.515]
 [1.515]
 [2.061]
 [1.515]
 [1.515]]
siam score:  -0.86780465
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [1.076]
 [0.43 ]
 [0.43 ]
 [0.43 ]] [[0.661]
 [1.585]
 [0.661]
 [0.661]
 [0.661]] [[0.614]
 [1.743]
 [0.614]
 [0.614]
 [0.614]]
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.30661229342544627, 0.5694550843293038, 0.07207564923431184, 0.05185697301093818]
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.30661229342544627, 0.5694550843293038, 0.07207564923431184, 0.05185697301093818]
Printing some Q and Qe and total Qs values:  [[1.487]
 [1.497]
 [1.487]
 [1.487]
 [1.487]] [[2.458]
 [2.009]
 [2.458]
 [2.458]
 [2.458]] [[2.674]
 [2.544]
 [2.674]
 [2.674]
 [2.674]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.3094047624232168, 0.5671610197171748, 0.07178571273034941, 0.05164850512925894]
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
2147 3681
2148 3681
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.177]
 [0.047]
 [0.383]
 [0.186]] [[0.628]
 [0.642]
 [0.23 ]
 [0.477]
 [0.742]] [[0.339]
 [0.492]
 [0.094]
 [0.848]
 [0.542]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.128]
 [0.05 ]
 [0.15 ]
 [0.108]] [[1.059]
 [0.88 ]
 [0.672]
 [0.982]
 [1.207]] [[0.527]
 [0.485]
 [0.191]
 [0.597]
 [0.663]]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.31217481792554236, 0.5648853682035533, 0.07149810337505581, 0.0514417104958486]
siam score:  -0.8783602
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.31217481792554236, 0.5648853682035533, 0.07149810337505581, 0.0514417104958486]
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.31217481792554236, 0.5648853682035533, 0.07149810337505581, 0.0514417104958486]
maxi score, test score, baseline:  0.07010000000000001 0.05 0.07010000000000001
probs:  [0.31217481792554236, 0.5648853682035533, 0.07149810337505581, 0.0514417104958486]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0721 0.05 0.0721
probs:  [0.31083027101321115, 0.5661179657571791, 0.07149805719074115, 0.051553706038868656]
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0741 0.05 0.0741
probs:  [0.31083027089620646, 0.5661179651491384, 0.07149805753408278, 0.051553706420572463]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0741 0.05 0.0741
probs:  [0.31083027089620646, 0.5661179651491384, 0.07149805753408278, 0.051553706420572463]
actor:  0 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0761 0.05 0.0761
probs:  [0.3108302707784411, 0.5661179645371446, 0.07149805787965652, 0.05155370680475781]
maxi score, test score, baseline:  0.0761 0.05 0.0761
probs:  [0.3108302707784411, 0.5661179645371446, 0.07149805787965652, 0.05155370680475781]
maxi score, test score, baseline:  0.0761 0.05 0.0761
probs:  [0.3108302707784411, 0.5661179645371446, 0.07149805787965652, 0.05155370680475781]
maxi score, test score, baseline:  0.0761 0.05 0.0761
probs:  [0.3108302707784411, 0.5661179645371446, 0.07149805787965652, 0.05155370680475781]
maxi score, test score, baseline:  0.0761 0.05 0.0761
probs:  [0.3108302707784411, 0.5661179645371446, 0.07149805787965652, 0.05155370680475781]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.31356823628393077, 0.5638681666519084, 0.07121433545144452, 0.051349261612716135]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0781 0.05 0.0781
probs:  [0.3149353222858048, 0.5628701891952411, 0.07093592945429594, 0.05125855906465814]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0781 0.05 0.0781
line 256 mcts: sample exp_bonus 2.5803133647658867
Printing some Q and Qe and total Qs values:  [[0.94 ]
 [1.246]
 [0.999]
 [0.94 ]
 [0.94 ]] [[2.002]
 [1.996]
 [2.335]
 [2.002]
 [2.002]] [[1.507]
 [1.944]
 [1.752]
 [1.507]
 [1.507]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
in main func line 156:  2163
maxi score, test score, baseline:  0.0781 0.05 0.0781
maxi score, test score, baseline:  0.0781 0.05 0.0781
maxi score, test score, baseline:  0.0781 0.05 0.0781
probs:  [0.31228105461285444, 0.5652978389775754, 0.07094196798804364, 0.05147913842152664]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2165 3688
maxi score, test score, baseline:  0.0801 0.05 0.0801
probs:  [0.3149475449737601, 0.5631053287540969, 0.07066722656499089, 0.05127989970715206]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.31363263328893426, 0.5643066432150391, 0.07067166982209429, 0.051389053673932386]
Printing some Q and Qe and total Qs values:  [[1.358]
 [1.358]
 [1.313]
 [1.358]
 [1.19 ]] [[1.767]
 [1.767]
 [1.624]
 [1.767]
 [4.337]] [[2.191]
 [2.191]
 [2.054]
 [2.191]
 [2.714]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.3162693338126696, 0.5621381665089511, 0.07040050111638811, 0.05119199856199112]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.32148239305774295, 0.5578508394353466, 0.06986436949448764, 0.05080239801242283]
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.32148239305774295, 0.5578508394353466, 0.06986436949448764, 0.05080239801242283]
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
siam score:  -0.8806526
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.32148239305774295, 0.5578508394353466, 0.06986436949448764, 0.05080239801242283]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.31894511328148245, 0.5569474221435864, 0.07338717556661342, 0.05072028900831782]
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.31894511328148245, 0.5569474221435864, 0.07338717556661342, 0.05072028900831782]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.32150819972329003, 0.5548507461887627, 0.07311129542133518, 0.050529758666612024]
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.3228329875122412, 0.5536284284601224, 0.07311988747027143, 0.050418696557365084]
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.3228329875122412, 0.5536284284601224, 0.07311988747027143, 0.050418696557365084]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.32150819972329003, 0.5548507461887627, 0.07311129542133518, 0.050529758666612024]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.3201973041777825, 0.5560602462151317, 0.0731027934719881, 0.05063965613509771]
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.3201973041777825, 0.5560602462151317, 0.0731027934719881, 0.05063965613509771]
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.3201973041777825, 0.5560602462151317, 0.0731027934719881, 0.05063965613509771]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.31890008349622595, 0.5572571291047913, 0.07309438021239288, 0.05074840718658988]
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.31890008349622595, 0.5572571291047913, 0.07309438021239288, 0.05074840718658988]
from probs:  [0.31890008349622595, 0.5572571291047913, 0.07309438021239288, 0.05074840718658988]
maxi score, test score, baseline:  0.08410000000000001 0.05 0.08410000000000001
probs:  [0.31890008349622595, 0.5572571291047913, 0.07309438021239288, 0.05074840718658988]
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.3176163246852508, 0.558441590687514, 0.07308605459064516, 0.05085603003659009]
first move QE:  0.7842500967368642
siam score:  -0.9037502
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.3176163246852508, 0.558441590687514, 0.07308605459064516, 0.05085603003659009]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.31634581952663104, 0.5596138244576114, 0.07307781459565059, 0.050962541420106915]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.054]
 [0.358]
 [1.24 ]
 [1.176]
 [1.126]] [[3.64 ]
 [2.178]
 [3.077]
 [2.839]
 [2.414]] [[2.615]
 [0.902]
 [2.614]
 [2.407]
 [2.128]]
maxi score, test score, baseline:  0.0861 0.05 0.0861
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.013]
 [1.013]
 [0.62 ]
 [1.013]
 [1.013]] [[4.362]
 [4.362]
 [5.353]
 [4.362]
 [4.362]] [[2.045]
 [2.045]
 [1.968]
 [2.045]
 [2.045]]
start point for exploration sampling:  20004
siam score:  -0.90895736
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.31508836370167187, 0.5607740182375598, 0.07306965923348362, 0.05106795882728469]
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.31508836370167187, 0.5607740182375598, 0.07306965923348362, 0.05106795882728469]
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.31508836370167187, 0.5607740182375598, 0.07306965923348362, 0.05106795882728469]
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.31508836370167187, 0.5607740182375598, 0.07306965923348362, 0.05106795882728469]
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.31508836370167187, 0.5607740182375598, 0.07306965923348362, 0.05106795882728469]
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.31508836370167187, 0.5607740182375598, 0.07306965923348362, 0.05106795882728469]
maxi score, test score, baseline:  0.0861 0.05 0.0861
maxi score, test score, baseline:  0.0861 0.05 0.0861
maxi score, test score, baseline:  0.0861 0.05 0.0861
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.31508836370167187, 0.5607740182375598, 0.07306965923348362, 0.05106795882728469]
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.3150883637016718, 0.5607740182375599, 0.07306965923348362, 0.0510679588272847]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.31759060590687965, 0.5587246594125045, 0.07280300613601814, 0.05088172854459772]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.6838],
        [0.3160],
        [0.3160],
        [0.3196],
        [0.1210],
        [0.1211],
        [0.3516],
        [0.1159],
        [0.6538],
        [0.1159]], dtype=torch.float64)
0.0 0.6838395978763234
0.0 0.3159838940235604
0.0 0.3159838940235604
0.0 0.3195509290122477
0.0 0.12099683002207778
0.0 0.1210976528194073
0.0 0.3515577272742297
0.0 0.11585073446412769
0.0 0.6538185921857773
0.0 0.11585073446412769
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.31759060590687965, 0.5587246594125045, 0.07280300613601814, 0.05088172854459772]
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.3175906059068797, 0.5587246594125045, 0.07280300613601814, 0.05088172854459772]
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.3175906059068797, 0.5587246594125045, 0.07280300613601814, 0.05088172854459772]
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.3175906059068797, 0.5587246594125045, 0.07280300613601814, 0.05088172854459772]
maxi score, test score, baseline:  0.0861 0.05 0.0861
probs:  [0.3175906059068797, 0.5587246594125045, 0.07280300613601814, 0.05088172854459772]
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.867]
 [0.709]
 [0.664]
 [0.664]] [[2.78 ]
 [1.982]
 [2.725]
 [2.78 ]
 [2.78 ]] [[0.664]
 [0.867]
 [0.709]
 [0.664]
 [0.664]]
from probs:  [0.3175906059068797, 0.5587246594125045, 0.07280300613601814, 0.05088172854459772]
actor:  0 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.3188137433591264, 0.5578509571329339, 0.07253297765277929, 0.050802321855160426]
maxi score, test score, baseline:  0.0901 0.05 0.0901
maxi score, test score, baseline:  0.0901 0.05 0.0901
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.3188137433591264, 0.5578509571329339, 0.07253297765277929, 0.050802321855160426]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.3188137433591264, 0.5578509571329339, 0.07253297765277929, 0.050802321855160426]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.3188137433591264, 0.5578509571329339, 0.07253297765277929, 0.050802321855160426]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.3188137433591264, 0.5578509571329339, 0.07253297765277929, 0.050802321855160426]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9003435
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
first move QE:  0.7934446306609637
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
maxi score, test score, baseline:  0.0901 0.05 0.0901
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
maxi score, test score, baseline:  0.0901 0.05 0.0901
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
Printing some Q and Qe and total Qs values:  [[0.964]
 [1.262]
 [0.964]
 [0.964]
 [0.964]] [[3.11 ]
 [2.487]
 [3.11 ]
 [3.11 ]
 [3.11 ]] [[2.263]
 [2.422]
 [2.263]
 [2.263]
 [2.263]]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]] [[2.53]
 [2.53]
 [2.53]
 [2.53]
 [2.53]] [[0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
line 256 mcts: sample exp_bonus 1.4010514527482458
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.050619336572339774]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.050619336572339774]
siam score:  -0.90532094
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32127182086328127, 0.5558373072487639, 0.0722715353156151, 0.05061933657233978]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32001579227111165, 0.5569923199579513, 0.0722676042348703, 0.0507242835360667]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]] [[2.341]
 [2.341]
 [2.341]
 [2.341]
 [2.341]] [[2.031]
 [2.031]
 [2.031]
 [2.031]
 [2.031]]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.3187723323526571, 0.5581357748268404, 0.07226371249118481, 0.05082818032931766]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.48 ]
 [0.865]
 [0.48 ]
 [0.48 ]] [[1.972]
 [1.972]
 [1.446]
 [1.972]
 [1.972]] [[1.309]
 [1.309]
 [1.679]
 [1.309]
 [1.309]]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32360504187024336, 0.5541750525481144, 0.07175164559133822, 0.05046825999030396]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32360504187024336, 0.5541750525481144, 0.07175164559133822, 0.05046825999030396]
maxi score, test score, baseline:  0.0901 0.05 0.0901
probs:  [0.32360504187024336, 0.5541750525481144, 0.07175164559133822, 0.05046825999030396]
maxi score, test score, baseline:  0.0901 0.05 0.0901
siam score:  -0.88810533
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0901 0.05 0.0901
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
2225 3716
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.32360504173671273, 0.5541750519962949, 0.07175164591470762, 0.050468260352284654]
Printing some Q and Qe and total Qs values:  [[0.861]
 [0.861]
 [1.2  ]
 [0.861]
 [0.861]] [[2.276]
 [2.276]
 [2.455]
 [2.276]
 [2.276]] [[1.572]
 [1.572]
 [2.211]
 [1.572]
 [1.572]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3248637595975267, 0.5530199793233227, 0.07175295333922198, 0.05036330773992862]
maxi score, test score, baseline:  0.0921 0.05 0.0921
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3248637595975267, 0.5530199793233227, 0.07175295333922198, 0.05036330773992862]
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3248637595975267, 0.5530199793233227, 0.07175295333922198, 0.05036330773992862]
maxi score, test score, baseline:  0.0921 0.05 0.0921
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3248637595975267, 0.5530199793233227, 0.07175295333922198, 0.05036330773992862]
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3248637595975267, 0.5530199793233227, 0.07175295333922198, 0.05036330773992862]
maxi score, test score, baseline:  0.0921 0.05 0.0921
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3248637595975267, 0.5530199793233227, 0.07175295333922198, 0.05036330773992862]
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3248637595975267, 0.5530199793233227, 0.07175295333922198, 0.05036330773992862]
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3248637595975267, 0.5530199793233227, 0.07175295333922198, 0.05036330773992862]
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3248637595975267, 0.5530199793233227, 0.07175295333922198, 0.05036330773992862]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3248637595975267, 0.5530199793233227, 0.07175295333922198, 0.05036330773992862]
maxi score, test score, baseline:  0.0921 0.05 0.0921
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3248637595975267, 0.5530199793233227, 0.07175295333922198, 0.05036330773992862]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0921 0.05 0.0921
maxi score, test score, baseline:  0.0921 0.05 0.0921
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3272618989828056, 0.5510549856916221, 0.07149837131558692, 0.05018474400998536]
maxi score, test score, baseline:  0.0921 0.05 0.0921
from probs:  [0.32854069771001815, 0.549882663983706, 0.0714984142954131, 0.05007822401086268]
Printing some Q and Qe and total Qs values:  [[0.778]
 [0.778]
 [1.17 ]
 [0.778]
 [0.778]] [[1.31]
 [1.31]
 [1.49]
 [1.31]
 [1.31]] [[1.637]
 [1.637]
 [2.348]
 [1.637]
 [1.637]]
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[1.458]
 [1.377]
 [1.377]
 [1.377]
 [1.377]] [[0.971]
 [0.893]
 [0.893]
 [0.893]
 [0.893]]
from probs:  [0.32854069771001826, 0.549882663983706, 0.0714984142954131, 0.05007822401086268]
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3285406977100182, 0.549882663983706, 0.0714984142954131, 0.05007822401086268]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.324]
 [0.388]
 [0.324]
 [0.324]] [[1.462]
 [1.462]
 [1.485]
 [1.462]
 [1.462]] [[0.324]
 [0.324]
 [0.388]
 [0.324]
 [0.324]]
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3285406977100182, 0.549882663983706, 0.0714984142954131, 0.05007822401086268]
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3285406977100182, 0.549882663983706, 0.0714984142954131, 0.05007822401086268]
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3285406977100182, 0.549882663983706, 0.0714984142954131, 0.05007822401086268]
siam score:  -0.8907367
first move QE:  0.7949824801969804
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3285406977100182, 0.549882663983706, 0.0714984142954131, 0.05007822401086268]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3311370447466268, 0.5475024974042987, 0.07149850155742081, 0.04986195629165364]
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3311370447466268, 0.5475024974042987, 0.07149850155742081, 0.04986195629165364]
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3311370447466268, 0.5475024974042987, 0.07149850155742081, 0.04986195629165364]
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3311370447466268, 0.5475024974042987, 0.07149850155742081, 0.04986195629165364]
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3311370447466268, 0.5475024974042987, 0.07149850155742081, 0.04986195629165364]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.772]
 [1.156]
 [0.772]
 [0.772]] [[1.269]
 [1.269]
 [0.621]
 [1.269]
 [1.269]] [[1.688]
 [1.688]
 [2.209]
 [1.688]
 [1.688]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Starting evaluation
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0921 0.05 0.0921
probs:  [0.3333006875974461, 0.5459832942292235, 0.07099213941825411, 0.04972387875507638]
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8777755884499354
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn probs:  [0.33200739808064195, 0.5471665930449067, 0.07099460418956667, 0.04983140468488488]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.169]
 [1.443]
 [1.235]
 [1.169]
 [1.235]] [[1.404]
 [1.542]
 [1.281]
 [1.142]
 [1.281]] [[2.098]
 [2.587]
 [2.172]
 [2.026]
 [2.172]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.093]
 [0.093]
 [0.097]
 [0.093]] [[0.514]
 [0.473]
 [0.473]
 [0.533]
 [0.473]] [[0.349]
 [0.287]
 [0.287]
 [0.376]
 [0.287]]
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.709]
 [0.326]
 [0.326]
 [0.326]] [[0.838]
 [0.507]
 [0.838]
 [0.838]
 [0.838]] [[0.999]
 [1.323]
 [0.999]
 [0.999]
 [0.999]]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.791]
 [1.205]
 [0.791]
 [0.791]] [[1.285]
 [1.285]
 [0.589]
 [1.285]
 [1.285]] [[1.887]
 [1.887]
 [2.484]
 [1.887]
 [1.887]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
line 256 mcts: sample exp_bonus 1.722797327911587
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.079]
 [0.171]
 [0.074]
 [0.081]] [[ 0.081]
 [-0.043]
 [ 0.365]
 [-0.103]
 [ 0.01 ]] [[0.079]
 [0.079]
 [0.171]
 [0.074]
 [0.081]]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.042]
 [1.042]
 [1.094]
 [1.042]
 [1.042]] [[2.669]
 [2.669]
 [2.467]
 [2.669]
 [2.669]] [[2.415]
 [2.415]
 [2.385]
 [2.415]
 [2.415]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1321 1.0 1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7917906358982407
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1321 1.0 1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]] [[1.653]
 [1.653]
 [1.653]
 [1.653]
 [1.653]] [[1.954]
 [1.954]
 [1.954]
 [1.954]
 [1.954]]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.5178918835082498
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.74 ]
 [0.849]
 [0.74 ]
 [0.74 ]] [[2.24 ]
 [2.24 ]
 [2.546]
 [2.24 ]
 [2.24 ]] [[2.207]
 [2.207]
 [2.575]
 [2.207]
 [2.207]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  2265
maxi score, test score, baseline:  0.1361 1.0 1.0
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1361 1.0 1.0
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.134]
 [0.134]
 [0.127]
 [0.134]] [[2.245]
 [1.223]
 [1.223]
 [1.586]
 [1.223]] [[1.419]
 [0.769]
 [0.769]
 [0.996]
 [0.769]]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 2.0937371688503115
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1381 1.0 1.0
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.254]
 [0.181]
 [0.137]
 [0.18 ]] [[-0.064]
 [ 0.276]
 [ 0.   ]
 [-0.197]
 [-0.244]] [[0.15 ]
 [0.254]
 [0.181]
 [0.137]
 [0.18 ]]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.1  ]
 [0.127]
 [0.092]
 [0.095]] [[1.314]
 [1.314]
 [1.459]
 [1.608]
 [1.729]] [[0.1  ]
 [0.1  ]
 [0.127]
 [0.092]
 [0.095]]
maxi score, test score, baseline:  0.1381 1.0 1.0
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8803092
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.126]
 [0.161]
 [0.124]
 [0.121]] [[1.311]
 [1.612]
 [1.685]
 [1.631]
 [1.673]] [[0.918]
 [1.316]
 [1.482]
 [1.337]
 [1.386]]
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.123]
 [0.123]
 [0.119]
 [0.121]] [[1.87 ]
 [2.355]
 [2.355]
 [2.147]
 [2.136]] [[0.943]
 [1.585]
 [1.585]
 [1.301]
 [1.29 ]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8765593
first move QE:  0.7938154359969076
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.12 ]
 [0.12 ]
 [0.139]
 [0.12 ]] [[2.476]
 [2.476]
 [2.476]
 [3.277]
 [2.476]] [[1.227]
 [1.227]
 [1.227]
 [2.154]
 [1.227]]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.13 ]
 [0.13 ]
 [0.118]
 [0.13 ]] [[3.104]
 [3.104]
 [3.104]
 [3.034]
 [3.104]] [[1.903]
 [1.903]
 [1.903]
 [1.809]
 [1.903]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.8430483379584693
line 256 mcts: sample exp_bonus 0.7299593244546332
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.2683848613983472
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.13 ]
 [0.13 ]
 [0.125]
 [0.13 ]] [[2.903]
 [2.903]
 [2.903]
 [2.958]
 [2.903]] [[2.061]
 [2.061]
 [2.061]
 [2.106]
 [2.061]]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1381 1.0 1.0
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1381 1.0 1.0
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.458]
 [0.668]
 [0.389]
 [0.424]] [[2.09 ]
 [2.267]
 [2.499]
 [2.752]
 [2.453]] [[0.1  ]
 [0.458]
 [0.668]
 [0.389]
 [0.424]]
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.238]
 [0.238]
 [0.213]
 [0.238]] [[5.132]
 [4.57 ]
 [4.57 ]
 [5.04 ]
 [4.57 ]] [[1.65 ]
 [1.314]
 [1.314]
 [1.569]
 [1.314]]
siam score:  -0.89573044
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8972531
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89609253
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.2  ]
 [0.286]
 [0.256]
 [0.2  ]] [[2.234]
 [2.426]
 [2.333]
 [2.408]
 [2.426]] [[1.639]
 [1.926]
 [1.964]
 [1.997]
 [1.926]]
maxi score, test score, baseline:  0.1401 1.0 1.0
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1401 1.0 1.0
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 2.358715864173905
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.1112865444032882
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.3801418017087963
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
siam score:  -0.8803589
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8875658
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2322 3765
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 0.9081183265784787
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.433]
 [0.112]
 [0.143]
 [0.148]] [[1.268]
 [1.314]
 [1.403]
 [1.447]
 [1.466]] [[0.154]
 [0.433]
 [0.112]
 [0.143]
 [0.148]]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.3402308735916106
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]] [[1.873]
 [1.873]
 [1.873]
 [1.873]
 [1.873]] [[0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0979204405147476
Printing some Q and Qe and total Qs values:  [[0.825]
 [0.825]
 [0.897]
 [0.825]
 [0.825]] [[1.866]
 [1.866]
 [1.928]
 [1.866]
 [1.866]] [[2.209]
 [2.209]
 [2.394]
 [2.209]
 [2.209]]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.442]
 [0.954]
 [0.181]
 [0.635]] [[2.942]
 [2.273]
 [1.978]
 [2.818]
 [2.503]] [[2.047]
 [1.194]
 [2.023]
 [1.035]
 [1.734]]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]] [[1.782]
 [1.782]
 [1.782]
 [1.782]
 [1.782]] [[1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.451]]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
first move QE:  0.8093538025388287
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.3773],
        [0.2369],
        [0.7553],
        [0.6153],
        [0.3773],
        [0.0000],
        [0.1291],
        [0.4106],
        [0.0000],
        [0.7053]], dtype=torch.float64)
0.0 0.3772920186396808
0.0 0.2369150408500841
0.0 0.7553148952598426
0.0 0.615281971254317
0.0 0.3772920186396808
0.0 0.0
0.0 0.1290890944951752
0.0 0.41059547471578794
0.0 0.0
0.0 0.7053145237167079
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8832577
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.263]
 [0.263]
 [0.268]
 [0.263]] [[3.019]
 [2.952]
 [2.952]
 [3.26 ]
 [2.952]] [[1.608]
 [1.553]
 [1.553]
 [1.792]
 [1.553]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20004
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.246]
 [0.272]
 [0.258]
 [0.262]] [[2.856]
 [2.856]
 [2.94 ]
 [3.242]
 [3.277]] [[1.332]
 [1.332]
 [1.436]
 [1.645]
 [1.676]]
first move QE:  0.8119275291064996
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8845425
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.25 ]
 [0.25 ]
 [0.237]
 [0.25 ]] [[6.644]
 [6.067]
 [6.067]
 [6.829]
 [6.067]] [[1.291]
 [1.1  ]
 [1.1  ]
 [1.375]
 [1.1  ]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
siam score:  -0.88829786
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  0.8253258999050798
siam score:  -0.8911734
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8925967
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89573866
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.952]
 [0.952]
 [0.976]
 [0.952]
 [0.952]] [[4.402]
 [4.402]
 [4.802]
 [4.402]
 [4.402]] [[1.85 ]
 [1.85 ]
 [2.057]
 [1.85 ]
 [1.85 ]]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89567286
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.1561 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
2383 3800
siam score:  -0.8970506
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1561 1.0 1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6989233269505963
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.239318531703269
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.083]
 [1.383]
 [1.157]
 [1.117]
 [1.209]] [[3.77 ]
 [1.862]
 [3.268]
 [2.93 ]
 [3.002]] [[2.404]
 [1.903]
 [2.265]
 [2.037]
 [2.211]]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1581 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1581 1.0 1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.222]
 [0.261]
 [0.212]
 [0.254]] [[1.749]
 [1.592]
 [1.652]
 [1.895]
 [1.578]] [[0.957]
 [0.78 ]
 [0.892]
 [1.052]
 [0.812]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1581 1.0 1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Sims:  25 1 epoch:  147878 pick best:  False frame count:  147878
2400 3805
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.619]
 [0.364]
 [0.404]
 [0.364]] [[1.032]
 [0.955]
 [1.032]
 [1.227]
 [1.032]] [[0.955]
 [1.44 ]
 [0.955]
 [1.101]
 [0.955]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1601 1.0 1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.365]
 [1.365]
 [1.345]
 [1.365]
 [1.365]] [[2.685]
 [2.685]
 [2.765]
 [2.685]
 [2.685]] [[2.343]
 [2.343]
 [2.373]
 [2.343]
 [2.343]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.021]
 [0.026]
 [0.021]
 [0.021]] [[2.428]
 [1.014]
 [0.926]
 [0.913]
 [0.9  ]] [[2.009]
 [0.194]
 [0.091]
 [0.065]
 [0.047]]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89804435
siam score:  -0.89530337
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1621 1.0 1.0
first move QE:  0.8536995942556123
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]] [[5.585]
 [5.585]
 [5.585]
 [5.585]
 [5.585]] [[1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  2419
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.89292556
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89580643
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
siam score:  -0.8961539
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5906849775562677
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.057]
 [1.103]
 [1.057]
 [1.057]
 [1.057]] [[2.583]
 [4.083]
 [2.583]
 [2.583]
 [2.583]] [[1.525]
 [2.398]
 [1.525]
 [1.525]
 [1.525]]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89182955
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.305]] [[2.116]
 [2.116]
 [2.116]
 [2.116]
 [2.937]] [[1.182]
 [1.182]
 [1.182]
 [1.182]
 [2.186]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1621 1.0 1.0
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1621 1.0 1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[3.461]
 [3.461]
 [3.461]
 [3.461]
 [3.461]] [[2.096]
 [2.096]
 [2.096]
 [2.096]
 [2.096]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.8561673258617256
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1641 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
first move QE:  0.8564658678974586
Printing some Q and Qe and total Qs values:  [[1.037]
 [1.305]
 [1.037]
 [0.111]
 [1.037]] [[1.685]
 [1.686]
 [1.685]
 [2.114]
 [1.685]] [[1.84 ]
 [2.376]
 [1.84 ]
 [0.131]
 [1.84 ]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.799]
 [1.139]
 [0.882]
 [0.882]
 [0.882]] [[3.646]
 [2.33 ]
 [2.311]
 [2.311]
 [2.311]] [[2.447]
 [2.304]
 [1.921]
 [1.921]
 [1.921]]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]] [[3.038]
 [3.038]
 [3.038]
 [3.038]
 [3.038]] [[2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]]
Printing some Q and Qe and total Qs values:  [[1.017]
 [1.132]
 [1.006]
 [1.017]
 [0.977]] [[2.459]
 [2.858]
 [2.633]
 [2.459]
 [2.858]] [[1.751]
 [2.181]
 [1.833]
 [1.751]
 [1.911]]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.1641 1.0 1.0
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Starting evaluation
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3441222244920905
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.473]
 [0.701]
 [0.473]
 [0.473]] [[1.849]
 [1.849]
 [2.37 ]
 [1.849]
 [1.849]] [[0.473]
 [0.473]
 [0.701]
 [0.473]
 [0.473]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8839243
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.257]
 [0.305]
 [0.296]
 [0.277]] [[2.504]
 [2.266]
 [2.469]
 [2.655]
 [2.724]] [[1.642]
 [1.339]
 [1.653]
 [1.85 ]
 [1.895]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8986481
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
first move QE:  0.8577335577799994
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.8577335577799994
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.024]
 [1.024]
 [1.024]
 [0.961]
 [1.024]] [[2.883]
 [2.883]
 [2.883]
 [2.751]
 [2.883]] [[2.464]
 [2.464]
 [2.464]
 [2.249]
 [2.464]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.20609999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.308]
 [0.319]
 [0.306]
 [0.3  ]] [[3.006]
 [3.006]
 [2.952]
 [2.856]
 [3.191]] [[1.895]
 [1.895]
 [1.867]
 [1.754]
 [2.049]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.739]
 [0.942]
 [0.739]
 [0.739]] [[1.143]
 [1.143]
 [1.472]
 [1.143]
 [1.143]] [[1.46 ]
 [1.46 ]
 [1.931]
 [1.46 ]
 [1.46 ]]
start point for exploration sampling:  20004
in main func line 156:  2469
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.8589292754793678
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2473 3840
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[2.836]
 [2.836]
 [2.836]
 [2.836]
 [2.836]] [[2.083]
 [2.083]
 [2.083]
 [2.083]
 [2.083]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89460635
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2480 3844
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88993746
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[1.781]
 [0.868]
 [0.866]
 [1.479]
 [0.973]] [[1.247]
 [0.029]
 [0.029]
 [0.844]
 [0.17 ]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.909]
 [0.909]
 [0.941]
 [0.909]
 [0.909]] [[2.044]
 [2.044]
 [2.371]
 [2.044]
 [2.044]] [[0.909]
 [0.909]
 [0.941]
 [0.909]
 [0.909]]
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.301]
 [0.301]
 [0.289]
 [0.301]] [[3.444]
 [2.001]
 [2.001]
 [2.633]
 [2.001]] [[2.123]
 [1.046]
 [1.046]
 [1.517]
 [1.046]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89002854
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2486 3852
line 256 mcts: sample exp_bonus 2.8251049617058563
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  2489
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8945723
siam score:  -0.8959825
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8916515
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]] [[2.295]
 [2.295]
 [2.295]
 [2.295]
 [2.295]] [[0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]]
2490 3856
siam score:  -0.8892697
siam score:  -0.8896843
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.408]
 [0.408]
 [0.491]
 [0.43 ]] [[1.66 ]
 [1.66 ]
 [1.66 ]
 [1.859]
 [1.75 ]] [[1.534]
 [1.534]
 [1.534]
 [1.967]
 [1.7  ]]
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.083 0.042 0.417 0.292 0.167]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2181 1.0 1.0
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2181 1.0 1.0
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2181 1.0 1.0
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2201 1.0 1.0
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89269805
first move QE:  0.8597185648841343
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2221 1.0 1.0
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.272]
 [0.299]
 [0.308]
 [0.283]] [[4.985]
 [4.083]
 [3.539]
 [3.397]
 [2.962]] [[1.868]
 [1.37 ]
 [1.084]
 [1.012]
 [0.742]]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2241 1.0 1.0
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8997139
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.89849365
maxi score, test score, baseline:  0.2261 1.0 1.0
maxi score, test score, baseline:  0.2261 1.0 1.0
siam score:  -0.89655966
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.2261 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2523 3870
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.049]
 [0.113]
 [0.124]
 [0.124]] [[0.839]
 [0.761]
 [0.794]
 [0.839]
 [0.839]] [[0.124]
 [0.049]
 [0.113]
 [0.124]
 [0.124]]
siam score:  -0.8931343
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2261 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.843]
 [0.843]
 [0.928]
 [0.741]
 [0.752]] [[2.324]
 [2.324]
 [1.571]
 [2.223]
 [2.471]] [[0.843]
 [0.843]
 [0.928]
 [0.741]
 [0.752]]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.216]
 [0.191]
 [0.22 ]
 [0.224]] [[5.261]
 [5.261]
 [3.749]
 [5.004]
 [4.402]] [[1.921]
 [1.921]
 [1.168]
 [1.799]
 [1.51 ]]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.081]
 [0.087]
 [0.251]
 [0.227]] [[4.057]
 [2.108]
 [3.43 ]
 [3.237]
 [3.374]] [[1.496]
 [0.534]
 [1.132]
 [1.155]
 [1.201]]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2301 1.0 1.0
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2301 1.0 1.0
maxi score, test score, baseline:  0.2301 1.0 1.0
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.865]
 [1.05 ]
 [0.093]
 [0.966]] [[ 0.815]
 [ 0.755]
 [-0.438]
 [ 1.723]
 [ 0.379]] [[1.262]
 [1.312]
 [1.283]
 [0.092]
 [1.388]]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.893923
maxi score, test score, baseline:  0.2321 1.0 1.0
using explorer policy with actor:  1
siam score:  -0.89191645
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2321 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.326]
 [1.162]
 [0.835]
 [0.782]] [[5.01 ]
 [4.548]
 [4.695]
 [5.198]
 [6.181]] [[1.653]
 [0.866]
 [1.776]
 [1.7  ]
 [2.139]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.8623902228627565
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.8623902228627565
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.313]
 [0.313]
 [0.316]
 [0.313]] [[8.794]
 [8.794]
 [8.794]
 [7.629]
 [8.794]] [[2.099]
 [2.099]
 [2.099]
 [1.681]
 [2.099]]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89996344
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.    0.625 0.333]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
2556 3891
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2559 3893
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2321 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9022761
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.8676121354268739
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
siam score:  -0.90377516
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.8676121354268739
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.273]
 [0.337]
 [0.284]
 [0.282]] [[2.481]
 [1.971]
 [2.221]
 [2.844]
 [2.888]] [[1.042]
 [0.615]
 [0.888]
 [1.319]
 [1.351]]
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.319]
 [0.319]
 [0.333]
 [0.304]] [[3.19 ]
 [3.19 ]
 [3.19 ]
 [3.453]
 [2.526]] [[1.907]
 [1.907]
 [1.907]
 [2.18 ]
 [1.248]]
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.264]
 [0.211]
 [0.252]
 [0.264]] [[2.166]
 [2.166]
 [2.172]
 [2.496]
 [2.166]] [[1.416]
 [1.416]
 [1.344]
 [1.719]
 [1.416]]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2577 3899
maxi score, test score, baseline:  0.2321 1.0 1.0
siam score:  -0.8929421
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.004]
 [0.087]
 [0.24 ]
 [0.215]] [[2.343]
 [1.763]
 [1.828]
 [2.367]
 [2.343]] [[1.246]
 [0.418]
 [0.593]
 [1.303]
 [1.246]]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2341 1.0 1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.239]
 [0.239]
 [0.242]
 [0.239]] [[5.005]
 [5.005]
 [5.005]
 [5.306]
 [5.005]] [[1.793]
 [1.793]
 [1.793]
 [1.984]
 [1.793]]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.241]
 [0.241]
 [0.232]
 [0.241]] [[5.344]
 [5.344]
 [5.344]
 [5.488]
 [5.344]] [[2.009]
 [2.009]
 [2.009]
 [2.089]
 [2.009]]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.986]
 [0.986]
 [0.986]
 [1.097]
 [0.986]] [[4.23 ]
 [4.23 ]
 [4.23 ]
 [4.424]
 [4.23 ]] [[2.222]
 [2.222]
 [2.222]
 [2.509]
 [2.222]]
maxi score, test score, baseline:  0.2341 1.0 1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.8759352357831884
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]] [[4.539]
 [4.539]
 [4.539]
 [4.539]
 [4.539]] [[4.911]
 [4.911]
 [4.911]
 [4.911]
 [4.911]]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
Printing some Q and Qe and total Qs values:  [[0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]] [[6.075]
 [6.075]
 [6.075]
 [6.075]
 [6.075]] [[1.966]
 [1.966]
 [1.966]
 [1.966]
 [1.966]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
siam score:  -0.860755
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.417 0.042 0.125 0.208 0.208]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  2598
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.864]
 [0.953]
 [0.916]
 [0.864]
 [0.864]] [[4.424]
 [2.832]
 [3.813]
 [4.424]
 [4.424]] [[2.293]
 [1.665]
 [2.078]
 [2.293]
 [2.293]]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.76 ]] [[4.636]
 [4.636]
 [4.636]
 [4.636]
 [4.141]] [[2.153]
 [2.153]
 [2.153]
 [2.153]
 [1.998]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85479546
siam score:  -0.85819024
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.196]
 [0.211]
 [0.191]
 [0.194]] [[2.371]
 [2.371]
 [2.016]
 [2.186]
 [3.115]] [[1.587]
 [1.587]
 [1.333]
 [1.435]
 [2.164]]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85461026
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.38 ]] [[2.613]
 [2.613]
 [2.613]
 [2.613]
 [4.587]] [[0.919]
 [0.919]
 [0.919]
 [0.919]
 [1.969]]
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.186]
 [0.222]
 [0.181]
 [0.183]] [[1.604]
 [1.105]
 [1.349]
 [1.59 ]
 [1.626]] [[0.877]
 [0.563]
 [0.797]
 [0.876]
 [0.903]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.129]] [[2.427]
 [2.427]
 [2.427]
 [2.427]
 [2.905]] [[1.423]
 [1.423]
 [1.423]
 [1.423]
 [1.796]]
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.495]
 [0.343]
 [0.343]
 [0.343]] [[1.404]
 [1.455]
 [1.404]
 [1.404]
 [1.404]] [[0.774]
 [1.097]
 [0.774]
 [0.774]
 [0.774]]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]] [[1.941]
 [1.941]
 [1.941]
 [1.941]
 [1.941]] [[0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]]
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.175]
 [0.147]
 [0.147]
 [0.15 ]] [[1.226]
 [1.304]
 [1.226]
 [1.458]
 [1.527]] [[0.147]
 [0.175]
 [0.147]
 [0.147]
 [0.15 ]]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.505]
 [0.504]
 [0.381]
 [0.474]] [[2.194]
 [2.111]
 [2.333]
 [2.258]
 [2.09 ]] [[0.448]
 [0.505]
 [0.504]
 [0.381]
 [0.474]]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.4605927866634456
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 2.209091510087819
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8407637
maxi score, test score, baseline:  0.2801 1.0 1.0
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2801 1.0 1.0
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8441095
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.243]] [[2.617]
 [2.617]
 [2.617]
 [2.617]
 [3.13 ]] [[1.557]
 [1.557]
 [1.557]
 [1.557]
 [1.888]]
Printing some Q and Qe and total Qs values:  [[1.101]
 [1.101]
 [1.101]
 [1.199]
 [1.101]] [[4.763]
 [4.763]
 [4.763]
 [3.493]
 [4.763]] [[2.526]
 [2.526]
 [2.526]
 [2.078]
 [2.526]]
maxi score, test score, baseline:  0.2801 1.0 1.0
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.442]
 [1.474]
 [1.442]
 [1.442]
 [1.442]] [[1.481]
 [1.181]
 [1.481]
 [1.481]
 [1.481]] [[2.282]
 [2.246]
 [2.282]
 [2.282]
 [2.282]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
siam score:  -0.8508315
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8553432
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.8857275723033591
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.165]
 [0.165]
 [0.197]
 [0.165]] [[3.993]
 [2.192]
 [2.192]
 [1.899]
 [2.192]] [[2.094]
 [0.914]
 [0.914]
 [0.774]
 [0.914]]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.818]
 [1.04 ]
 [0.818]
 [0.818]] [[2.672]
 [2.672]
 [2.632]
 [2.672]
 [2.672]] [[1.776]
 [1.776]
 [2.109]
 [1.776]
 [1.776]]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.215]
 [0.336]
 [0.229]
 [0.217]] [[1.662]
 [1.982]
 [1.451]
 [1.976]
 [1.883]] [[0.986]
 [1.291]
 [1.017]
 [1.311]
 [1.201]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.166184710803895
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
in main func line 156:  2645
line 256 mcts: sample exp_bonus 0.9412351505775169
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
start point for exploration sampling:  20004
2649 3915
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.393]
 [0.267]
 [0.226]
 [0.259]] [[1.475]
 [1.145]
 [1.485]
 [1.628]
 [1.781]] [[0.275]
 [0.393]
 [0.267]
 [0.226]
 [0.259]]
line 256 mcts: sample exp_bonus 0.28088632410853837
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.114]
 [1.114]
 [1.156]
 [1.114]
 [1.114]] [[2.182]
 [2.182]
 [2.123]
 [2.182]
 [2.182]] [[2.829]
 [2.829]
 [2.851]
 [2.829]
 [2.829]]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.8894701306033151
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8648359
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.079]
 [0.564]
 [1.006]
 [0.716]
 [0.716]] [[3.253]
 [3.257]
 [2.891]
 [3.4  ]
 [3.4  ]] [[2.449]
 [1.587]
 [2.124]
 [1.923]
 [1.923]]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87934005
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 3.142234637161955
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.876279
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8742632
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.756]
 [0.732]
 [0.741]
 [0.728]] [[2.552]
 [2.126]
 [2.636]
 [2.739]
 [3.069]] [[1.677]
 [1.516]
 [1.79 ]
 [1.872]
 [2.054]]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2941 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[3.024]
 [3.024]
 [3.024]
 [3.024]
 [3.024]] [[2.314]
 [2.314]
 [2.314]
 [2.314]
 [2.314]]
siam score:  -0.88515353
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89193296
maxi score, test score, baseline:  0.2941 1.0 1.0
maxi score, test score, baseline:  0.2941 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.833 0.042]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.974]
 [0.93 ]
 [1.151]
 [0.428]
 [0.925]] [[2.893]
 [1.561]
 [1.864]
 [1.745]
 [1.611]] [[2.307]
 [1.333]
 [1.776]
 [0.938]
 [1.363]]
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.852]
 [1.042]
 [0.521]
 [0.852]] [[1.99 ]
 [1.99 ]
 [2.15 ]
 [2.088]
 [1.99 ]] [[2.203]
 [2.203]
 [2.689]
 [1.606]
 [2.203]]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8991228
siam score:  -0.8987941
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.925]
 [0.89 ]
 [0.862]
 [0.879]] [[1.996]
 [2.117]
 [1.849]
 [1.996]
 [2.185]] [[0.799]
 [0.925]
 [0.89 ]
 [0.862]
 [0.879]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2961 1.0 1.0
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89448524
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8892937
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88714373
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[2.809]
 [2.809]
 [2.809]
 [2.809]
 [2.809]] [[2.176]
 [2.176]
 [2.176]
 [2.176]
 [2.176]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.877236
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8773706
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.36 ]
 [0.503]
 [0.363]
 [0.365]] [[1.287]
 [1.184]
 [1.221]
 [1.489]
 [1.6  ]] [[1.227]
 [1.107]
 [1.43 ]
 [1.417]
 [1.533]]
siam score:  -0.8862465
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.894]
 [0.894]
 [1.113]
 [0.894]
 [0.894]] [[2.287]
 [2.287]
 [2.394]
 [2.287]
 [2.287]] [[2.087]
 [2.087]
 [2.493]
 [2.087]
 [2.087]]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.232]
 [1.306]
 [1.232]
 [1.232]
 [1.232]] [[1.865]
 [1.885]
 [1.865]
 [1.865]
 [1.865]] [[2.292]
 [2.422]
 [2.292]
 [2.292]
 [2.292]]
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.632]
 [1.036]
 [0.675]
 [0.632]] [[1.595]
 [1.595]
 [1.282]
 [1.546]
 [1.595]] [[1.275]
 [1.275]
 [1.79 ]
 [1.322]
 [1.275]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3041 1.0 1.0
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.339]
 [1.375]
 [1.401]
 [1.339]
 [1.316]] [[2.372]
 [2.379]
 [2.282]
 [2.372]
 [2.478]] [[2.364]
 [2.438]
 [2.458]
 [2.364]
 [2.353]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.255]
 [1.329]
 [1.275]
 [1.154]
 [1.208]] [[2.364]
 [2.104]
 [2.244]
 [2.183]
 [2.684]] [[2.507]
 [2.568]
 [2.505]
 [2.245]
 [2.518]]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8972284
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [1.081]
 [0.75 ]
 [0.75 ]
 [0.75 ]] [[0.749]
 [0.558]
 [0.749]
 [0.749]
 [0.749]] [[1.187]
 [1.659]
 [1.187]
 [1.187]
 [1.187]]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.333]
 [0.448]
 [0.369]
 [0.342]] [[1.672]
 [2.119]
 [1.985]
 [1.967]
 [2.523]] [[0.659]
 [0.921]
 [0.979]
 [0.874]
 [1.175]]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.895692931307874
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.345]
 [0.387]
 [0.335]
 [0.345]] [[1.597]
 [1.418]
 [1.653]
 [1.765]
 [1.418]] [[1.195]
 [1.042]
 [1.282]
 [1.254]
 [1.042]]
maxi score, test score, baseline:  0.3181 1.0 1.0
2727 3949
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2729 3950
siam score:  -0.88356924
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
Printing some Q and Qe and total Qs values:  [[1.198]
 [1.198]
 [1.198]
 [1.198]
 [1.094]] [[3.194]
 [3.194]
 [3.194]
 [3.194]
 [3.518]] [[2.459]
 [2.459]
 [2.459]
 [2.459]
 [2.464]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.2734337599582672
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.8968480993725704
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8892163
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3221 1.0 1.0
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
first move QE:  0.896240949607065
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.246]
 [1.352]
 [1.246]
 [1.246]
 [1.246]] [[1.736]
 [1.468]
 [1.736]
 [1.736]
 [1.736]] [[2.768]
 [2.891]
 [2.768]
 [2.768]
 [2.768]]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.162]
 [1.139]
 [0.962]
 [1.076]
 [1.14 ]] [[2.244]
 [1.455]
 [2.971]
 [2.601]
 [2.301]] [[1.816]
 [1.376]
 [1.881]
 [1.866]
 [1.811]]
Printing some Q and Qe and total Qs values:  [[1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.228]] [[2.45]
 [2.45]
 [2.45]
 [2.45]
 [2.45]] [[2.547]
 [2.547]
 [2.547]
 [2.547]
 [2.547]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3261 1.0 1.0
line 256 mcts: sample exp_bonus 2.889019113577254
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]] [[0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]] [[0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.459]
 [0.436]
 [0.479]
 [0.469]] [[0.269]
 [0.506]
 [0.703]
 [0.117]
 [0.319]] [[0.901]
 [0.978]
 [1.062]
 [0.758]
 [0.873]]
maxi score, test score, baseline:  0.3281 1.0 1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.338]
 [1.338]
 [1.318]
 [1.338]
 [1.338]] [[2.957]
 [2.957]
 [2.905]
 [2.957]
 [2.957]] [[2.521]
 [2.521]
 [2.458]
 [2.521]
 [2.521]]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.983]
 [0.983]
 [0.958]
 [0.983]
 [0.983]] [[2.424]
 [2.424]
 [2.43 ]
 [2.424]
 [2.424]] [[2.327]
 [2.327]
 [2.282]
 [2.327]
 [2.327]]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 1.0 1.0
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.705]] [[3.034]
 [3.034]
 [3.034]
 [3.034]
 [5.047]] [[1.245]
 [1.245]
 [1.245]
 [1.245]
 [1.926]]
maxi score, test score, baseline:  0.3321 1.0 1.0
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.642]
 [0.831]
 [0.642]
 [0.642]] [[1.204]
 [1.204]
 [1.739]
 [1.204]
 [1.204]] [[1.535]
 [1.535]
 [2.449]
 [1.535]
 [1.535]]
line 256 mcts: sample exp_bonus 9.632183089852333
siam score:  -0.8915871
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
first move QE:  0.8976946481874651
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.025]
 [1.304]
 [1.171]
 [1.153]
 [1.025]] [[1.915]
 [1.611]
 [1.935]
 [1.763]
 [1.915]] [[1.701]
 [1.8  ]
 [1.905]
 [1.736]
 [1.701]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.935]
 [0.935]
 [0.911]
 [0.935]
 [0.935]] [[2.184]
 [2.184]
 [3.875]
 [2.184]
 [2.184]] [[1.308]
 [1.308]
 [2.25 ]
 [1.308]
 [1.308]]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.070280943264246
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8838782
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
2782 3978
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
siam score:  -0.89069647
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3361 1.0 1.0
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
siam score:  -0.8974328
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3361 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.804]
 [0.872]
 [0.872]
 [0.735]
 [0.804]] [[2.415]
 [2.303]
 [2.394]
 [2.411]
 [2.415]] [[0.804]
 [0.872]
 [0.872]
 [0.735]
 [0.804]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8945081
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89758104
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8906384
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3381 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.474]
 [0.474]
 [0.456]
 [0.474]] [[2.179]
 [1.749]
 [1.749]
 [1.871]
 [1.749]] [[2.01 ]
 [1.578]
 [1.578]
 [1.679]
 [1.578]]
Starting evaluation
maxi score, test score, baseline:  0.3401 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.37 ]
 [0.259]
 [0.213]
 [0.259]] [[0.735]
 [1.064]
 [0.735]
 [0.627]
 [0.735]] [[0.259]
 [0.37 ]
 [0.259]
 [0.213]
 [0.259]]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.545]
 [0.644]
 [0.545]
 [0.545]] [[1.804]
 [1.804]
 [1.969]
 [1.804]
 [1.804]] [[0.545]
 [0.545]
 [0.644]
 [0.545]
 [0.545]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8888883
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88816017
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.3801 1.0 1.0
first move QE:  0.8993352439497082
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  0.8994787968559386
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86889285
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.78 ]
 [0.714]
 [0.688]
 [0.688]] [[2.03 ]
 [2.073]
 [2.108]
 [2.03 ]
 [2.03 ]] [[0.688]
 [0.78 ]
 [0.714]
 [0.688]
 [0.688]]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3821 1.0 1.0
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.854]
 [0.854]
 [0.908]
 [0.854]
 [0.854]] [[1.645]
 [1.645]
 [2.196]
 [1.645]
 [1.645]] [[1.946]
 [1.946]
 [2.422]
 [1.946]
 [1.946]]
siam score:  -0.8869669
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.155]
 [1.283]
 [1.181]
 [1.155]
 [1.155]] [[2.058]
 [1.801]
 [2.262]
 [2.058]
 [2.058]] [[2.445]
 [2.603]
 [2.556]
 [2.445]
 [2.445]]
2823 3992
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8853293
siam score:  -0.8831479
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87728167
first move QE:  0.8990368663929632
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
siam score:  -0.87040555
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
in main func line 156:  2832
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8772526
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8712694
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87107795
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]] [[2.023]
 [2.023]
 [2.023]
 [2.023]
 [2.023]] [[0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.181]
 [1.181]
 [1.169]
 [1.181]
 [1.181]] [[2.575]
 [2.575]
 [2.81 ]
 [2.575]
 [2.575]] [[2.513]
 [2.513]
 [2.694]
 [2.513]
 [2.513]]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.462]
 [0.462]
 [0.447]
 [0.453]] [[3.361]
 [3.361]
 [3.361]
 [3.28 ]
 [3.166]] [[2.235]
 [2.235]
 [2.235]
 [2.161]
 [2.092]]
maxi score, test score, baseline:  0.3901 1.0 1.0
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3921 1.0 1.0
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88483167
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2849 4002
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.87045264
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.837]
 [0.794]
 [0.794]
 [0.794]] [[1.073]
 [1.548]
 [1.073]
 [1.073]
 [1.073]] [[1.388]
 [1.792]
 [1.388]
 [1.388]
 [1.388]]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.58034889737331
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.766]
 [0.939]
 [0.766]
 [0.766]] [[2.491]
 [2.491]
 [2.005]
 [2.491]
 [2.491]] [[2.063]
 [2.063]
 [2.246]
 [2.063]
 [2.063]]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.88224566
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8825992
Printing some Q and Qe and total Qs values:  [[0.945]
 [0.945]
 [1.18 ]
 [0.945]
 [0.945]] [[1.621]
 [1.621]
 [1.456]
 [1.621]
 [1.621]] [[1.862]
 [1.862]
 [2.263]
 [1.862]
 [1.862]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.434]
 [0.434]
 [0.431]
 [0.434]] [[1.627]
 [1.627]
 [1.627]
 [2.106]
 [1.627]] [[1.081]
 [1.081]
 [1.081]
 [1.369]
 [1.081]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[1.801]
 [1.801]
 [1.801]
 [1.801]
 [1.801]] [[1.196]
 [1.196]
 [1.196]
 [1.196]
 [1.196]]
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.619]
 [0.619]
 [0.472]
 [0.619]] [[1.699]
 [1.699]
 [1.699]
 [1.927]
 [1.699]] [[1.735]
 [1.735]
 [1.735]
 [1.669]
 [1.735]]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3981 1.0 1.0
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4001 1.0 1.0
maxi score, test score, baseline:  0.4001 1.0 1.0
siam score:  -0.88468766
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.422]
 [0.422]
 [0.431]
 [0.433]] [[1.772]
 [1.751]
 [1.751]
 [1.907]
 [2.037]] [[0.965]
 [0.935]
 [0.935]
 [1.075]
 [1.185]]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8849109
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.88099027
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.745]
 [0.897]
 [0.745]
 [0.745]] [[1.881]
 [1.881]
 [1.868]
 [1.881]
 [1.881]] [[1.935]
 [1.935]
 [2.219]
 [1.935]
 [1.935]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.972]
 [0.972]
 [0.972]
 [0.766]
 [0.972]] [[2.799]
 [2.799]
 [2.799]
 [3.522]
 [2.799]] [[2.138]
 [2.138]
 [2.138]
 [2.378]
 [2.138]]
siam score:  -0.87430894
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
siam score:  -0.87296385
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.16 ]
 [0.218]
 [0.157]
 [0.16 ]] [[0.858]
 [0.858]
 [0.945]
 [1.051]
 [0.858]] [[0.16 ]
 [0.16 ]
 [0.218]
 [0.157]
 [0.16 ]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.448]
 [0.542]
 [0.46 ]
 [0.457]] [[0.675]
 [0.815]
 [0.833]
 [1.01 ]
 [1.334]] [[0.934]
 [1.025]
 [1.191]
 [1.202]
 [1.46 ]]
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]] [[2.696]
 [2.696]
 [2.696]
 [2.696]
 [2.696]] [[1.767]
 [1.767]
 [1.767]
 [1.767]
 [1.767]]
siam score:  -0.8736506
maxi score, test score, baseline:  0.4021 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8749329
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2891 4016
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.903480795466989
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.769]
 [0.338]
 [0.384]
 [0.409]] [[1.089]
 [1.154]
 [1.322]
 [1.243]
 [1.299]] [[0.843]
 [1.444]
 [0.953]
 [0.943]
 [1.04 ]]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8679306
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.147]
 [1.25 ]
 [1.147]
 [1.147]
 [1.147]] [[1.584]
 [1.935]
 [1.584]
 [1.584]
 [1.584]] [[2.18 ]
 [2.618]
 [2.18 ]
 [2.18 ]
 [2.18 ]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4061 1.0 1.0
maxi score, test score, baseline:  0.4061 1.0 1.0
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2900 4025
siam score:  -0.8637884
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.404]
 [0.534]
 [0.404]
 [0.421]] [[0.832]
 [1.063]
 [0.992]
 [1.063]
 [1.5  ]] [[1.272]
 [1.471]
 [1.66 ]
 [1.471]
 [1.942]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.904]
 [0.906]
 [1.071]
 [0.904]
 [0.904]] [[1.923]
 [2.061]
 [1.94 ]
 [1.923]
 [1.923]] [[1.702]
 [1.752]
 [2.042]
 [1.702]
 [1.702]]
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.143]
 [0.172]
 [0.156]
 [0.138]] [[1.224]
 [1.228]
 [1.39 ]
 [1.269]
 [1.224]] [[0.138]
 [0.143]
 [0.172]
 [0.156]
 [0.138]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
siam score:  -0.8602748
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
2909 4034
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.673]] [[2.726]
 [2.726]
 [2.726]
 [2.726]
 [2.783]] [[1.973]
 [1.973]
 [1.973]
 [1.973]
 [1.964]]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.444]] [[2.806]
 [2.806]
 [2.806]
 [2.806]
 [3.195]] [[2.066]
 [2.066]
 [2.066]
 [2.066]
 [2.393]]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 9.890469415725349
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
2913 4034
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8566234
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.9031388454751497
Printing some Q and Qe and total Qs values:  [[0.145]
 [1.405]
 [1.106]
 [1.157]
 [0.956]] [[3.074]
 [1.866]
 [3.351]
 [2.569]
 [2.648]] [[0.797]
 [1.823]
 [2.516]
 [1.984]
 [1.732]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85352564
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.85026145
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.218]
 [0.275]
 [0.229]
 [0.223]] [[0.962]
 [1.054]
 [1.078]
 [1.093]
 [1.145]] [[0.23 ]
 [0.218]
 [0.275]
 [0.229]
 [0.223]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.771]
 [0.382]
 [0.375]
 [0.363]] [[1.205]
 [1.332]
 [1.299]
 [1.376]
 [1.465]] [[0.744]
 [1.444]
 [0.823]
 [0.852]
 [0.88 ]]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.9054495689277937
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.15]
 [0.15]
 [0.15]
 [0.15]
 [0.19]] [[2.217]
 [2.217]
 [2.217]
 [2.217]
 [2.344]] [[1.311]
 [1.311]
 [1.311]
 [1.311]
 [1.517]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.9369579295916233
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 1.0 1.0
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84753615
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4241 1.0 1.0
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4241 1.0 1.0
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8480101
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]] [[0.105]
 [0.105]
 [0.105]
 [0.105]
 [0.105]] [[1.751]
 [1.751]
 [1.751]
 [1.751]
 [1.751]]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.9022392287494402
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84663683
maxi score, test score, baseline:  0.4261 1.0 1.0
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.89 ]
 [0.89 ]
 [1.052]
 [0.89 ]
 [0.89 ]] [[0.652]
 [0.652]
 [1.355]
 [0.652]
 [0.652]] [[1.677]
 [1.677]
 [2.135]
 [1.677]
 [1.677]]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84741026
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]] [[1.657]
 [1.657]
 [1.657]
 [1.657]
 [1.657]] [[0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.485]
 [0.543]
 [0.488]
 [0.447]] [[1.487]
 [1.487]
 [0.654]
 [0.538]
 [1.73 ]] [[1.874]
 [1.874]
 [1.435]
 [1.247]
 [1.96 ]]
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4281 1.0 1.0
maxi score, test score, baseline:  0.4281 1.0 1.0
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.371]
 [0.371]
 [0.427]
 [0.39 ]] [[1.999]
 [1.999]
 [1.999]
 [1.242]
 [2.105]] [[1.627]
 [1.627]
 [1.627]
 [1.043]
 [1.756]]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.523]
 [0.503]
 [0.503]
 [0.503]] [[1.466]
 [1.444]
 [1.466]
 [1.466]
 [1.466]] [[0.981]
 [1.014]
 [0.981]
 [0.981]
 [0.981]]
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]] [[0.26]
 [0.26]
 [0.26]
 [0.26]
 [0.26]] [[1.849]
 [1.849]
 [1.849]
 [1.849]
 [1.849]]
line 256 mcts: sample exp_bonus 1.501568075525785
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.9022349825850424
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 2.3363008992290704
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.514]
 [0.714]
 [0.514]
 [0.514]] [[1.293]
 [1.293]
 [1.217]
 [1.293]
 [1.293]] [[0.514]
 [0.514]
 [0.714]
 [0.514]
 [0.514]]
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.934]
 [0.766]
 [0.766]
 [0.766]] [[2.106]
 [1.603]
 [2.106]
 [2.106]
 [2.106]] [[0.766]
 [0.934]
 [0.766]
 [0.766]
 [0.766]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.478]
 [0.625]
 [0.478]
 [0.493]] [[2.431]
 [2.431]
 [1.597]
 [2.431]
 [2.518]] [[1.661]
 [1.661]
 [1.399]
 [1.661]
 [1.75 ]]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.424]
 [0.338]
 [0.338]
 [0.338]] [[0.527]
 [0.67 ]
 [0.527]
 [0.527]
 [0.527]] [[0.615]
 [0.835]
 [0.615]
 [0.615]
 [0.615]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]] [[0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]] [[1.524]
 [1.524]
 [1.524]
 [1.524]
 [1.524]]
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.039]
 [0.419]
 [0.516]
 [0.498]] [[0.557]
 [0.634]
 [0.557]
 [0.797]
 [0.875]] [[1.093]
 [0.411]
 [1.093]
 [1.527]
 [1.57 ]]
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.503]
 [0.489]
 [0.685]
 [0.517]] [[2.482]
 [2.334]
 [2.726]
 [3.424]
 [2.294]] [[1.142]
 [1.14 ]
 [1.357]
 [2.005]
 [1.133]]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.676]
 [0.553]
 [0.553]
 [0.553]] [[2.086]
 [2.044]
 [2.086]
 [2.086]
 [2.086]] [[1.986]
 [2.191]
 [1.986]
 [1.986]
 [1.986]]
siam score:  -0.85208297
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4321 1.0 1.0
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.84866315
siam score:  -0.8523399
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7804819280937934
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4321 1.0 1.0
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.931]
 [0.962]
 [0.934]
 [0.909]
 [0.913]] [[1.534]
 [1.573]
 [1.936]
 [1.973]
 [1.762]] [[0.931]
 [0.962]
 [0.934]
 [0.909]
 [0.913]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]] [[1.48]
 [1.48]
 [1.48]
 [1.48]
 [1.48]]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8534182
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
2987 4079
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.37 ]
 [0.505]
 [0.421]
 [0.408]] [[0.964]
 [1.198]
 [0.965]
 [0.921]
 [1.3  ]] [[1.403]
 [1.618]
 [1.578]
 [1.357]
 [1.824]]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4361 1.0 1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]] [[2.356]
 [1.35 ]
 [0.899]
 [1.056]
 [1.459]] [[0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.562]
 [0.713]
 [0.562]
 [0.562]] [[1.002]
 [1.002]
 [0.739]
 [1.002]
 [1.002]] [[0.562]
 [0.562]
 [0.713]
 [0.562]
 [0.562]]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.628]
 [0.643]
 [0.628]
 [0.493]] [[0.   ]
 [0.   ]
 [0.774]
 [0.   ]
 [1.026]] [[0.628]
 [0.628]
 [0.643]
 [0.628]
 [0.493]]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.841]
 [0.421]
 [0.751]
 [0.751]] [[1.383]
 [0.808]
 [2.418]
 [1.01 ]
 [1.01 ]] [[0.689]
 [0.841]
 [0.421]
 [0.751]
 [0.751]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.4556678816240673
Printing some Q and Qe and total Qs values:  [[0.945]
 [1.099]
 [1.208]
 [1.099]
 [1.115]] [[1.628]
 [1.574]
 [0.841]
 [1.574]
 [1.551]] [[2.167]
 [2.447]
 [2.421]
 [2.447]
 [2.472]]
Printing some Q and Qe and total Qs values:  [[1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]] [[1.864]
 [1.864]
 [1.864]
 [1.864]
 [1.864]] [[2.48]
 [2.48]
 [2.48]
 [2.48]
 [2.48]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.412]] [[2.186]
 [2.186]
 [2.186]
 [2.186]
 [2.186]] [[2.843]
 [2.843]
 [2.843]
 [2.843]
 [2.843]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.104]
 [1.288]
 [1.104]
 [1.104]
 [1.104]] [[0.95 ]
 [1.297]
 [0.95 ]
 [0.95 ]
 [0.95 ]] [[1.894]
 [2.307]
 [1.894]
 [1.894]
 [1.894]]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  20004
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
start point for exploration sampling:  20004
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.042 0.083 0.875]
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4861 1.0 1.0
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4861 1.0 1.0
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  3021
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
3029 4097
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.384]
 [0.19 ]
 [0.321]
 [0.299]] [[0.668]
 [0.404]
 [0.993]
 [0.454]
 [0.781]] [[0.306]
 [0.384]
 [0.19 ]
 [0.321]
 [0.299]]
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.369]
 [0.454]
 [0.413]
 [0.377]] [[0.265]
 [0.494]
 [0.353]
 [0.324]
 [0.697]] [[0.706]
 [0.764]
 [0.841]
 [0.739]
 [0.915]]
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]] [[0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.784]] [[0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.871]
 [0.546]
 [0.546]
 [0.546]] [[0.42 ]
 [0.891]
 [0.42 ]
 [0.42 ]
 [0.42 ]] [[0.912]
 [1.835]
 [0.912]
 [0.912]
 [0.912]]
maxi score, test score, baseline:  0.4901 1.0 1.0
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
in main func line 156:  3033
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8267103
maxi score, test score, baseline:  0.4921 1.0 1.0
siam score:  -0.8242239
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
3038 4108
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]] [[0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]]
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4941 1.0 1.0
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.82350975
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4941 1.0 1.0
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.8955002802273476
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5507685910787796
siam score:  -0.83214027
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4941 1.0 1.0
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.552]
 [0.701]
 [0.552]
 [0.552]] [[1.167]
 [1.167]
 [0.618]
 [1.167]
 [1.167]] [[0.552]
 [0.552]
 [0.701]
 [0.552]
 [0.552]]
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.831]
 [0.825]
 [0.679]
 [0.789]] [[2.088]
 [1.302]
 [1.152]
 [1.506]
 [1.473]] [[0.721]
 [0.831]
 [0.825]
 [0.679]
 [0.789]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.84235674
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8435584
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8454527
maxi score, test score, baseline:  0.4981 1.0 1.0
maxi score, test score, baseline:  0.4981 1.0 1.0
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.499]
 [0.499]
 [0.609]
 [0.567]] [[0.856]
 [0.856]
 [0.856]
 [0.754]
 [1.161]] [[0.937]
 [0.937]
 [0.937]
 [1.008]
 [1.387]]
3051 4115
maxi score, test score, baseline:  0.4981 1.0 1.0
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[2.482]
 [2.378]
 [2.378]
 [2.378]
 [2.378]] [[1.395]
 [1.59 ]
 [1.59 ]
 [1.59 ]
 [1.59 ]]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  0.8943554187061381
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5264581719878025
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
3058 4121
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.272]
 [0.272]
 [0.291]
 [0.272]] [[2.053]
 [2.167]
 [2.167]
 [2.488]
 [2.167]] [[1.471]
 [1.475]
 [1.475]
 [1.766]
 [1.475]]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.28593895659602
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5925162932155046
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.797]
 [0.688]
 [0.913]
 [0.688]
 [0.789]] [[2.458]
 [2.471]
 [1.58 ]
 [2.471]
 [2.592]] [[2.024]
 [1.816]
 [1.672]
 [1.816]
 [2.097]]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8428283
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20004
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.582]
 [0.483]
 [0.538]
 [0.483]] [[1.15 ]
 [1.18 ]
 [1.267]
 [1.268]
 [1.267]] [[1.382]
 [1.496]
 [1.386]
 [1.496]
 [1.386]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8552797
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.855703
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.627]
 [0.627]
 [0.627]
 [0.71 ]] [[0.291]
 [0.804]
 [0.804]
 [0.804]
 [0.912]] [[1.098]
 [1.455]
 [1.455]
 [1.455]
 [1.73 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4981 1.0 1.0
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5001 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
3083 4142
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4981 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.641]
 [0.761]
 [0.641]
 [0.641]] [[1.73 ]
 [1.73 ]
 [1.604]
 [1.73 ]
 [1.73 ]] [[0.641]
 [0.641]
 [0.761]
 [0.641]
 [0.641]]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5001 1.0 1.0
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.174]
 [1.268]
 [1.269]
 [1.174]
 [1.174]] [[1.749]
 [1.639]
 [2.003]
 [1.749]
 [1.749]] [[2.188]
 [2.338]
 [2.462]
 [2.188]
 [2.188]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 1.4076388945229117
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.    0.042 0.875]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5001 1.0 1.0
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5001 1.0 1.0
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5001 1.0 1.0
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.86413085
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.692]
 [0.811]
 [0.   ]
 [0.715]] [[1.518]
 [1.296]
 [1.439]
 [1.345]
 [1.92 ]] [[0.614]
 [0.692]
 [0.811]
 [0.   ]
 [0.715]]
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.822]
 [0.874]
 [0.807]
 [0.775]] [[1.555]
 [1.635]
 [1.622]
 [1.594]
 [1.555]] [[0.775]
 [0.822]
 [0.874]
 [0.807]
 [0.775]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[2.456]
 [2.456]
 [2.456]
 [2.456]
 [2.53 ]] [[2.287]
 [2.287]
 [2.287]
 [2.287]
 [2.347]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.5021 1.0 1.0
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]] [[4.46]
 [4.46]
 [4.46]
 [4.46]
 [4.46]] [[6.252]
 [6.252]
 [6.252]
 [6.252]
 [6.252]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
