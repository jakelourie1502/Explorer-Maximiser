dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
res_block_channels:[32, 64, 64]
res_block_ds:[False, False, False]
reward_support:[-1, 1, 41]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 41]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:32
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[11, 12]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
channels:3
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'H' 'F' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'F' 'H' 'F' 'H' 'H' 'F']
 ['H' 'F' 'F' 'H' 'F' 'F' 'F' 'H']
 ['H' 'F' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'H' 'H' 'F' 'H' 'F']
 ['H' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:205000
exp_gamma:0.95
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
timesteps_in_obs:2
store_prev_actions:True
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 1, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
state_size:[6, 6]
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:25
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'H' 'F' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'F' 'H' 'F' 'H' 'H' 'F']
 ['H' 'F' 'F' 'H' 'F' 'F' 'F' 'H']
 ['H' 'F' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'H' 'H' 'F' 'H' 'F']
 ['H' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1 82
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2 134
2 147
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.010584401834586804
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.029191679178737103
deleting a thread, now have 2 threads
Frames:  1147 train batches done:  33 episodes:  199
first move QE:  -0.029191679178737103
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.30808663
maxi score, test score, baseline:  0.0001 0.0 0.0001
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
5 239
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
5 266
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
deleting a thread, now have 2 threads
Frames:  1776 train batches done:  111 episodes:  304
siam score:  -0.31339177
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.32866806
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
7 369
7 370
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.22786509
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
7 412
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.16932565
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
7 484
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
7 520
8 528
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.17501102
9 549
9 561
9 567
9 571
9 578
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.4 0.  0.2]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.17961363
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.15148376
11 632
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
11 682
12 694
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.28420532
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
using another actor
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.2 0.2 0.2]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.012233549469568589, 0.012233549469568589, 0.012233549469568589, 0.012233549469568589, 0.9388322526521572, 0.012233549469568589]
using explorer policy with actor:  1
using another actor
from probs:  [0.012661737406083758, 0.012661737406083758, 0.012661737406083758, 0.012661737406083758, 0.9366913129695813, 0.012661737406083758]
siam score:  -0.099499054
siam score:  -0.09801438
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.012747090220329852, 0.012747090220329852, 0.012747090220329852, 0.012747090220329852, 0.9362645488983508, 0.012747090220329852]
from probs:  [0.012832348478283339, 0.012832348478283339, 0.012832348478283339, 0.012832348478283339, 0.9358382576085833, 0.012832348478283339]
from probs:  [0.012832348478283339, 0.012832348478283339, 0.012832348478283339, 0.012832348478283339, 0.9358382576085833, 0.012832348478283339]
using explorer policy with actor:  0
using another actor
from probs:  [0.01317243908278888, 0.01317243908278888, 0.01317243908278888, 0.01317243908278888, 0.9341378045860556, 0.01317243908278888]
16 916
from probs:  [0.01317243908278888, 0.01317243908278888, 0.01317243908278888, 0.01317243908278888, 0.9341378045860556, 0.01317243908278888]
16 920
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
using another actor
17 958
using explorer policy with actor:  1
from probs:  [0.013595443564252768, 0.013595443564252768, 0.013595443564252768, 0.013595443564252768, 0.9320227821787361, 0.013595443564252768]
from probs:  [0.013595443564252768, 0.013595443564252768, 0.013595443564252768, 0.013595443564252768, 0.9320227821787361, 0.013595443564252768]
19 974
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01401612299362291, 0.01401612299362291, 0.01401612299362291, 0.01401612299362291, 0.9299193850318854, 0.01401612299362291]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[-0.062]
 [-0.104]
 [-0.104]
 [-0.06 ]
 [-0.104]] [[0.319]
 [0.304]
 [0.303]
 [0.318]
 [0.304]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01401612299362291, 0.01401612299362291, 0.01401612299362291, 0.01401612299362291, 0.9299193850318854, 0.01401612299362291]
siam score:  -0.39684525
siam score:  -0.43193215
from probs:  [0.014099981559238403, 0.014099981559238403, 0.014099981559238403, 0.014099981559238403, 0.9295000922038079, 0.014099981559238403]
siam score:  -0.48336193
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.014183748040046228, 0.014183748040046228, 0.014183748040046228, 0.014183748040046228, 0.9290812597997687, 0.014183748040046228]
siam score:  -0.48500037
from probs:  [0.01426742258764018, 0.01426742258764018, 0.01426742258764018, 0.01426742258764018, 0.928662887061799, 0.01426742258764018]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.   ]
 [0.   ]
 [0.   ]] [[-0.536]
 [-0.527]
 [ 0.   ]
 [-0.253]
 [-0.086]] [[0.   ]
 [0.016]
 [0.715]
 [0.378]
 [0.599]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
27 1033
siam score:  -0.5104961
line 256 mcts: sample exp_bonus -0.06077266787094035
29 1051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.014684421609956021, 0.014684421609956021, 0.014684421609956021, 0.014684421609956021, 0.9265778919502199, 0.014684421609956021]
from probs:  [0.014684421609956021, 0.014684421609956021, 0.014684421609956021, 0.014684421609956021, 0.9265778919502199, 0.014684421609956021]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.014684421609956021, 0.014684421609956021, 0.014684421609956021, 0.014684421609956021, 0.9265778919502199, 0.014684421609956021]
31 1074
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015016381370437008, 0.015016381370437008, 0.015016381370437008, 0.015016381370437008, 0.9249180931478149, 0.015016381370437008]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015016381370437008, 0.015016381370437008, 0.015016381370437008, 0.015016381370437008, 0.9249180931478149, 0.015016381370437008]
from probs:  [0.015346894157765217, 0.015346894157765217, 0.015346894157765217, 0.015346894157765217, 0.9232655292111739, 0.015346894157765217]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.001]
 [0.001]
 [0.   ]] [[-0.359]
 [-0.149]
 [-0.118]
 [ 0.   ]
 [ 0.018]] [[0.   ]
 [0.001]
 [0.001]
 [0.001]
 [0.   ]]
siam score:  -0.5176228
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.002]
 [0.   ]
 [0.   ]] [[-0.068]
 [-0.13 ]
 [-0.022]
 [ 0.004]
 [ 0.029]] [[0.   ]
 [0.001]
 [0.002]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015429297373123093, 0.015429297373123093, 0.015429297373123093, 0.015429297373123093, 0.9228535131343845, 0.015429297373123093]
siam score:  -0.51935077
35 1140
using another actor
from probs:  [0.015429297373123093, 0.015429297373123093, 0.015429297373123093, 0.015429297373123093, 0.9228535131343845, 0.015429297373123093]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.001]
 [0.   ]] [[-0.006]
 [-0.009]
 [ 0.12 ]
 [ 0.234]
 [ 0.237]] [[0.032]
 [0.027]
 [0.243]
 [0.435]
 [0.438]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015593834854090344, 0.015593834854090344, 0.015593834854090344, 0.015593834854090344, 0.9220308257295482, 0.015593834854090344]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.0159218381045609, 0.0159218381045609, 0.0159218381045609, 0.0159218381045609, 0.9203908094771954, 0.0159218381045609]
37 1204
using another actor
from probs:  [0.0159218381045609, 0.0159218381045609, 0.0159218381045609, 0.0159218381045609, 0.9203908094771954, 0.0159218381045609]
using another actor
from probs:  [0.016003616491829867, 0.016003616491829867, 0.016003616491829867, 0.016003616491829867, 0.9199819175408506, 0.016003616491829867]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]]
from probs:  [0.016003616491829867, 0.016003616491829867, 0.016003616491829867, 0.016003616491829867, 0.9199819175408506, 0.016003616491829867]
37 1239
37 1240
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.016329844674036045, 0.016329844674036045, 0.016329844674036045, 0.016329844674036045, 0.9183507766298197, 0.016329844674036045]
line 256 mcts: sample exp_bonus 0.06618013261420767
using another actor
37 1260
from probs:  [0.016329844674036045, 0.016329844674036045, 0.016329844674036045, 0.016329844674036045, 0.9183507766298197, 0.016329844674036045]
38 1263
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.016411181096382496, 0.016411181096382496, 0.016411181096382496, 0.016411181096382496, 0.9179440945180875, 0.016411181096382496]
first move QE:  -0.13214936853556272
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]]
38 1285
38 1288
from probs:  [0.01665466315579165, 0.01665466315579165, 0.01665466315579165, 0.01665466315579165, 0.9167266842210416, 0.01665466315579165]
line 256 mcts: sample exp_bonus 0.05599187755870147
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.01689735738902249, 0.01689735738902249, 0.01689735738902249, 0.01689735738902249, 0.9155132130548874, 0.01689735738902249]
from probs:  [0.017058717749517706, 0.017058717749517706, 0.017058717749517706, 0.017058717749517706, 0.9147064112524114, 0.017058717749517706]
using explorer policy with actor:  0
Sims:  25 1 epoch:  8515 pick best:  False frame count:  8515
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.017219730787266987, 0.017219730787266987, 0.017219730787266987, 0.017219730787266987, 0.913901346063665, 0.017219730787266987]
42 1337
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01738039762246553, 0.01738039762246553, 0.01738039762246553, 0.01738039762246553, 0.9130980118876723, 0.01738039762246553]
from probs:  [0.01738039762246553, 0.01738039762246553, 0.01738039762246553, 0.01738039762246553, 0.9130980118876723, 0.01738039762246553]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.11121839022963942
from probs:  [0.01778055738223233, 0.01778055738223233, 0.01778055738223233, 0.01778055738223233, 0.9110972130888383, 0.01778055738223233]
siam score:  -0.2511829
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
using another actor
from probs:  [0.01794002126079952, 0.01794002126079952, 0.01794002126079952, 0.01794002126079952, 0.9102998936960023, 0.01794002126079952]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01825792644943065, 0.01825792644943065, 0.01825792644943065, 0.01825792644943065, 0.9087103677528466, 0.01825792644943065]
from probs:  [0.01825792644943065, 0.01825792644943065, 0.01825792644943065, 0.01825792644943065, 0.9087103677528466, 0.01825792644943065]
from probs:  [0.018337190507947385, 0.018337190507947385, 0.018337190507947385, 0.018337190507947385, 0.908314047460263, 0.018337190507947385]
42 1409
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
42 1430
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]] [[0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.018811002495022155, 0.018811002495022155, 0.018811002495022155, 0.018811002495022155, 0.9059449875248892, 0.018811002495022155]
using another actor
from probs:  [0.018811002495022155, 0.018811002495022155, 0.018811002495022155, 0.018811002495022155, 0.9059449875248892, 0.018811002495022155]
siam score:  -0.24986316
from probs:  [0.019125199051485752, 0.019125199051485752, 0.019125199051485752, 0.019125199051485752, 0.9043740047425711, 0.019125199051485752]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01928179710523795, 0.01928179710523795, 0.01928179710523795, 0.01928179710523795, 0.9035910144738102, 0.01928179710523795]
siam score:  -0.15863816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01928179710523795, 0.01928179710523795, 0.01928179710523795, 0.01928179710523795, 0.9035910144738102, 0.01928179710523795]
using explorer policy with actor:  1
from probs:  [0.01928179710523795, 0.01928179710523795, 0.01928179710523795, 0.01928179710523795, 0.9035910144738102, 0.01928179710523795]
from probs:  [0.019359971540311424, 0.019359971540311424, 0.019359971540311424, 0.019359971540311424, 0.9032001422984428, 0.019359971540311424]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.019359971540311424, 0.019359971540311424, 0.019359971540311424, 0.019359971540311424, 0.9032001422984428, 0.019359971540311424]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.135]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[ 0.141]
 [-0.127]
 [-0.127]
 [-0.127]
 [-0.127]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.019671841746014097, 0.019671841746014097, 0.019671841746014097, 0.019671841746014097, 0.9016407912699295, 0.019671841746014097]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.019671841746014097, 0.019671841746014097, 0.019671841746014097, 0.019671841746014097, 0.9016407912699295, 0.019671841746014097]
from probs:  [0.019671841746014097, 0.019671841746014097, 0.019671841746014097, 0.019671841746014097, 0.9016407912699295, 0.019671841746014097]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01990487916236675, 0.01990487916236675, 0.01990487916236675, 0.01990487916236675, 0.9004756041881662, 0.01990487916236675]
using another actor
46 1561
from probs:  [0.020368744333900398, 0.020368744333900398, 0.020368744333900398, 0.020368744333900398, 0.8981562783304979, 0.020368744333900398]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
UNIT TEST: sample policy line 217 mcts : [0.167 0.083 0.125 0.458 0.167]
46 1590
using another actor
46 1592
Starting evaluation
line 256 mcts: sample exp_bonus -0.08681976651722426
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[-0.051]
 [-0.037]
 [-0.016]
 [-0.015]
 [-0.012]] [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]]
rdn probs:  [0.02067636218086243, 0.02067636218086243, 0.02067636218086243, 0.02067636218086243, 0.8966181890956878, 0.02067636218086243]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02067636218086243, 0.02067636218086243, 0.02067636218086243, 0.02067636218086243, 0.8966181890956878, 0.02067636218086243]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.021]
 [-0.049]
 [-0.048]
 [-0.033]
 [-0.028]] [[0.015]
 [0.006]
 [0.006]
 [0.011]
 [0.012]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.02067636218086243, 0.02067636218086243, 0.02067636218086243, 0.02067636218086243, 0.8966181890956878, 0.02067636218086243]
from probs:  [0.02067636218086243, 0.02067636218086243, 0.02067636218086243, 0.02067636218086243, 0.8966181890956878, 0.02067636218086243]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.020753064616873335, 0.020753064616873335, 0.020753064616873335, 0.020753064616873335, 0.8962346769156332, 0.020753064616873335]
46 1619
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.009]] [[0.029]
 [0.029]
 [0.029]
 [0.028]
 [0.03 ]]
47 1629
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0210590700711371, 0.0210590700711371, 0.0210590700711371, 0.0210590700711371, 0.8947046496443144, 0.0210590700711371]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
siam score:  -0.27528483
siam score:  -0.29145443
from probs:  [0.021515679255089896, 0.021515679255089896, 0.021515679255089896, 0.021515679255089896, 0.8924216037245504, 0.021515679255089896]
from probs:  [0.02159150251095679, 0.02159150251095679, 0.02159150251095679, 0.02159150251095679, 0.892042487445216, 0.02159150251095679]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02166724659182695, 0.02166724659182695, 0.02166724659182695, 0.02166724659182695, 0.8916637670408651, 0.02166724659182695]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02212005532512096, 0.02212005532512096, 0.02212005532512096, 0.02212005532512096, 0.8893997233743951, 0.02212005532512096]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.022195248637114195, 0.022195248637114195, 0.022195248637114195, 0.022195248637114195, 0.8890237568144289, 0.022195248637114195]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.022570044771964327, 0.022570044771964327, 0.022570044771964327, 0.022570044771964327, 0.8871497761401783, 0.022570044771964327]
using another actor
using another actor
UNIT TEST: sample policy line 217 mcts : [0.458 0.125 0.167 0.083 0.167]
51 1773
siam score:  -0.29836872
siam score:  -0.30932063
from probs:  [0.022644770763657986, 0.022644770763657986, 0.022644770763657986, 0.022644770763657986, 0.88677614618171, 0.022644770763657986]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.053]
 [-0.055]
 [-0.04 ]
 [-0.033]
 [-0.023]] [[0.002]
 [0.   ]
 [0.016]
 [0.023]
 [0.032]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.023239800109831298, 0.023239800109831298, 0.023239800109831298, 0.023239800109831298, 0.8838009994508436, 0.023239800109831298]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
using explorer policy with actor:  1
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.02368285546865578, 0.02368285546865578, 0.02368285546865578, 0.02368285546865578, 0.8815857226567211, 0.02368285546865578]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02368285546865578, 0.02368285546865578, 0.02368285546865578, 0.02368285546865578, 0.8815857226567211, 0.02368285546865578]
using another actor
from probs:  [0.023829932934768275, 0.023829932934768275, 0.023829932934768275, 0.023829932934768275, 0.8808503353261586, 0.023829932934768275]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.08551400075662767
from probs:  [0.024123182000657124, 0.024123182000657124, 0.024123182000657124, 0.024123182000657124, 0.8793840899967142, 0.024123182000657124]
using another actor
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.35412475
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.024488054398566092, 0.024488054398566092, 0.024488054398566092, 0.024488054398566092, 0.8775597280071695, 0.024488054398566092]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.024778610143233907, 0.024778610143233907, 0.024778610143233907, 0.024778610143233907, 0.8761069492838305, 0.024778610143233907]
siam score:  -0.32471558
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.024778610143233907, 0.024778610143233907, 0.024778610143233907, 0.024778610143233907, 0.8761069492838305, 0.024778610143233907]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.02514013907643843, 0.02514013907643843, 0.02514013907643843, 0.02514013907643843, 0.8742993046178077, 0.02514013907643843]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 0.002450429120515348
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.417 0.083 0.167 0.083 0.25 ]
using another actor
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.025786269650099173, 0.025786269650099173, 0.025786269650099173, 0.025786269650099173, 0.8710686517495041, 0.025786269650099173]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.08534209143888609
using another actor
from probs:  [0.025857697939346818, 0.025857697939346818, 0.025857697939346818, 0.025857697939346818, 0.8707115103032659, 0.025857697939346818]
from probs:  [0.025857697939346818, 0.025857697939346818, 0.025857697939346818, 0.025857697939346818, 0.8707115103032659, 0.025857697939346818]
using another actor
line 256 mcts: sample exp_bonus -0.053465781425386445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.026142688259886256, 0.026142688259886256, 0.026142688259886256, 0.026142688259886256, 0.8692865587005687, 0.026142688259886256]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.2682552
60 2078
60 2084
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.031]
 [-0.042]
 [-0.059]
 [-0.031]
 [-0.027]] [[0.06 ]
 [0.046]
 [0.023]
 [0.06 ]
 [0.066]]
first move QE:  -0.08355030317149759
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.026426527298211353, 0.026426527298211353, 0.026426527298211353, 0.026426527298211353, 0.8678673635089432, 0.026426527298211353]
using another actor
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[-0.014]
 [ 0.   ]
 [-0.032]
 [-0.01 ]
 [-0.011]] [[0.069]
 [0.098]
 [0.034]
 [0.079]
 [0.078]]
from probs:  [0.026709222016552816, 0.026709222016552816, 0.026709222016552816, 0.026709222016552816, 0.8664538899172359, 0.026709222016552816]
61 2137
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
line 256 mcts: sample exp_bonus -0.03800558401080875
from probs:  [0.02685014241609211, 0.02685014241609211, 0.02685014241609211, 0.02685014241609211, 0.8657492879195394, 0.02685014241609211]
UNIT TEST: sample policy line 217 mcts : [0.125 0.375 0.208 0.167 0.125]
63 2163
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.027271206062646765, 0.027271206062646765, 0.027271206062646765, 0.027271206062646765, 0.8636439696867662, 0.027271206062646765]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.001]
 [0.001]] [[-0.001]
 [-0.003]
 [-0.003]
 [-0.001]
 [ 0.002]] [[0.032]
 [0.028]
 [0.027]
 [0.032]
 [0.038]]
from probs:  [0.027271206062646765, 0.027271206062646765, 0.027271206062646765, 0.027271206062646765, 0.8636439696867662, 0.027271206062646765]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.004194086600962005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02741099759810298, 0.02741099759810298, 0.02741099759810298, 0.02741099759810298, 0.8629450120094851, 0.02741099759810298]
from probs:  [0.027480788276973034, 0.027480788276973034, 0.027480788276973034, 0.027480788276973034, 0.8625960586151348, 0.027480788276973034]
from probs:  [0.027689741220269046, 0.027689741220269046, 0.027689741220269046, 0.027689741220269046, 0.8615512938986548, 0.027689741220269046]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.027689741220269046, 0.027689741220269046, 0.027689741220269046, 0.027689741220269046, 0.8615512938986548, 0.027689741220269046]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.   ]
 [0.   ]] [[-0.018]
 [-0.018]
 [-0.018]
 [-0.041]
 [-0.05 ]] [[0.083]
 [0.083]
 [0.083]
 [0.044]
 [0.029]]
from probs:  [0.027759252852957213, 0.027759252852957213, 0.027759252852957213, 0.027759252852957213, 0.8612037357352139, 0.027759252852957213]
using another actor
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02789806772236786, 0.02789806772236786, 0.02789806772236786, 0.02789806772236786, 0.8605096613881608, 0.02789806772236786]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
using another actor
from probs:  [0.02789806772236786, 0.02789806772236786, 0.02789806772236786, 0.02789806772236786, 0.8605096613881608, 0.02789806772236786]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.028243894099984266, 0.028243894099984266, 0.028243894099984266, 0.028243894099984266, 0.8587805295000787, 0.028243894099984266]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02831285263772125, 0.02831285263772125, 0.02831285263772125, 0.02831285263772125, 0.8584357368113937, 0.02831285263772125]
from probs:  [0.028381742503056077, 0.028381742503056077, 0.028381742503056077, 0.028381742503056077, 0.8580912874847196, 0.028381742503056077]
from probs:  [0.02845056379851888, 0.02845056379851888, 0.02845056379851888, 0.02845056379851888, 0.8577471810074055, 0.02845056379851888]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02845056379851888, 0.02845056379851888, 0.02845056379851888, 0.02845056379851888, 0.8577471810074055, 0.02845056379851888]
68 2306
maxi score, test score, baseline:  0.0001 0.0 0.0001
68 2312
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.028725165325122804, 0.028725165325122804, 0.028725165325122804, 0.028725165325122804, 0.8563741733743859, 0.028725165325122804]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
using another actor
69 2331
69 2333
siam score:  -0.32605866
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.029271107938068044, 0.029271107938068044, 0.029271107938068044, 0.029271107938068044, 0.8536444603096598, 0.029271107938068044]
line 256 mcts: sample exp_bonus -0.01059334723861329
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02933904706330108, 0.02933904706330108, 0.02933904706330108, 0.02933904706330108, 0.8533047646834946, 0.02933904706330108]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.029406919032891936, 0.029406919032891936, 0.029406919032891936, 0.029406919032891936, 0.8529654048355403, 0.029406919032891936]
using another actor
using explorer policy with actor:  1
from probs:  [0.029474723946363215, 0.029474723946363215, 0.029474723946363215, 0.029474723946363215, 0.8526263802681839, 0.029474723946363215]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.39398232
using another actor
from probs:  [0.02981274614148533, 0.02981274614148533, 0.02981274614148533, 0.02981274614148533, 0.8509362692925734, 0.02981274614148533]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02981274614148533, 0.02981274614148533, 0.02981274614148533, 0.02981274614148533, 0.8509362692925734, 0.02981274614148533]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.032]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.037]] [[0.014]
 [0.011]
 [0.011]
 [0.011]
 [0.009]]
using another actor
from probs:  [0.030081966966541255, 0.030081966966541255, 0.030081966966541255, 0.030081966966541255, 0.8495901651672937, 0.030081966966541255]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.075]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.075]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.030417007110245243, 0.030417007110245243, 0.030417007110245243, 0.030417007110245243, 0.8479149644487738, 0.030417007110245243]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.030816891971165074, 0.030816891971165074, 0.030816891971165074, 0.030816891971165074, 0.8459155401441746, 0.030816891971165074]
from probs:  [0.03088331135199388, 0.03088331135199388, 0.03088331135199388, 0.03088331135199388, 0.8455834432400307, 0.03088331135199388]
using another actor
using another actor
76 2508
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.030949665817318246, 0.030949665817318246, 0.030949665817318246, 0.030949665817318246, 0.8452516709134088, 0.030949665817318246]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.031015955462260214, 0.031015955462260214, 0.031015955462260214, 0.031015955462260214, 0.8449202226886989, 0.031015955462260214]
siam score:  -0.4130519
maxi score, test score, baseline:  0.0001 0.0 0.0001
76 2522
from probs:  [0.031015955462260214, 0.031015955462260214, 0.031015955462260214, 0.031015955462260214, 0.8449202226886989, 0.031015955462260214]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03114834067055676, 0.03114834067055676, 0.03114834067055676, 0.03114834067055676, 0.8442582966472162, 0.03114834067055676]
siam score:  -0.43828443
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0]
 [0]
 [0]
 [0]
 [0]] [[3.]
 [3.]
 [3.]
 [3.]
 [3.]]
77 2551
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.002]] [[0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.076]]
in main func line 156:  78
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03154395044158328, 0.03154395044158328, 0.03154395044158328, 0.03154395044158328, 0.8422802477920837, 0.03154395044158328]
79 2567
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
79 2584
using another actor
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0318064095839559, 0.0318064095839559, 0.0318064095839559, 0.0318064095839559, 0.8409679520802206, 0.0318064095839559]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.0318064095839559, 0.0318064095839559, 0.0318064095839559, 0.0318064095839559, 0.8409679520802206, 0.0318064095839559]
using another actor
80 2608
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03206785111415306, 0.03206785111415306, 0.03206785111415306, 0.03206785111415306, 0.8396607444292347, 0.03206785111415306]
80 2640
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03232828093898516, 0.03232828093898516, 0.03232828093898516, 0.03232828093898516, 0.8383585953050742, 0.03232828093898516]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03232828093898516, 0.03232828093898516, 0.03232828093898516, 0.03232828093898516, 0.8383585953050742, 0.03232828093898516]
from probs:  [0.03232828093898516, 0.03232828093898516, 0.03232828093898516, 0.03232828093898516, 0.8383585953050742, 0.03232828093898516]
using explorer policy with actor:  1
81 2678
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.066]
 [-0.068]
 [-0.071]
 [-0.069]
 [-0.068]] [[0.005]
 [0.004]
 [0.   ]
 [0.002]
 [0.003]]
using explorer policy with actor:  1
81 2694
line 256 mcts: sample exp_bonus -0.06405746745932853
81 2706
from probs:  [0.03258770491963546, 0.03258770491963546, 0.03258770491963546, 0.03258770491963546, 0.8370614754018227, 0.03258770491963546]
using another actor
from probs:  [0.03258770491963546, 0.03258770491963546, 0.03258770491963546, 0.03258770491963546, 0.8370614754018227, 0.03258770491963546]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[-0.006]
 [-0.039]
 [-0.019]
 [-0.011]
 [-0.006]] [[0.066]
 [0.032]
 [0.052]
 [0.061]
 [0.066]]
from probs:  [0.03265240443419331, 0.03265240443419331, 0.03265240443419331, 0.03265240443419331, 0.8367379778290335, 0.03265240443419331]
using another actor
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.006660607264790337
82 2747
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03297496764301214, 0.03297496764301214, 0.03297496764301214, 0.03297496764301214, 0.8351251617849393, 0.03297496764301214]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03297496764301214, 0.03297496764301214, 0.03297496764301214, 0.03297496764301214, 0.8351251617849393, 0.03297496764301214]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03323190236041077, 0.03323190236041077, 0.03323190236041077, 0.03323190236041077, 0.8338404881979461, 0.03323190236041077]
using explorer policy with actor:  1
siam score:  -0.41399002
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.033359999733120003, 0.033359999733120003, 0.033359999733120003, 0.033359999733120003, 0.8332000013344, 0.033359999733120003]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.033423956233616214, 0.033423956233616214, 0.033423956233616214, 0.033423956233616214, 0.8328802188319189, 0.033423956233616214]
using explorer policy with actor:  1
using another actor
from probs:  [0.03348785139475822, 0.03348785139475822, 0.03348785139475822, 0.03348785139475822, 0.8325607430262089, 0.03348785139475822]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03348785139475822, 0.03348785139475822, 0.03348785139475822, 0.03348785139475822, 0.8325607430262089, 0.03348785139475822]
siam score:  -0.37970752
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03355168530474768, 0.03355168530474768, 0.03355168530474768, 0.03355168530474768, 0.8322415734762616, 0.03355168530474768]
84 2824
siam score:  -0.37403035
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.03361545805161723, 0.03361545805161723, 0.03361545805161723, 0.03361545805161723, 0.8319227097419138, 0.03361545805161723]
85 2841
85 2852
siam score:  -0.35735905
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0339334074085279, 0.0339334074085279, 0.0339334074085279, 0.0339334074085279, 0.8303329629573605, 0.0339334074085279]
86 2863
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
using explorer policy with actor:  1
from probs:  [0.034249840795853144, 0.034249840795853144, 0.034249840795853144, 0.034249840795853144, 0.8287507960207343, 0.034249840795853144]
86 2880
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.062]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
86 2898
from probs:  [0.034375992105620895, 0.034375992105620895, 0.034375992105620895, 0.034375992105620895, 0.8281200394718955, 0.034375992105620895]
using explorer policy with actor:  1
from probs:  [0.034375992105620895, 0.034375992105620895, 0.034375992105620895, 0.034375992105620895, 0.8281200394718955, 0.034375992105620895]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.034690321283935364, 0.034690321283935364, 0.034690321283935364, 0.034690321283935364, 0.8265483935803232, 0.034690321283935364]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03475300795794009, 0.03475300795794009, 0.03475300795794009, 0.03475300795794009, 0.8262349602102995, 0.03475300795794009]
using another actor
using explorer policy with actor:  1
using explorer policy with actor:  1
88 2968
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.06595663354538214
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus -0.06567101441414963
from probs:  [0.03500316027968426, 0.03500316027968426, 0.03500316027968426, 0.03500316027968426, 0.8249841986015787, 0.03500316027968426]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.   ]
 [0.   ]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.003]
 [-0.004]] [[0.009]
 [0.009]
 [0.009]
 [0.008]
 [0.007]]
from probs:  [0.035127880998359014, 0.035127880998359014, 0.035127880998359014, 0.035127880998359014, 0.8243605950082049, 0.035127880998359014]
88 3009
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03525236565258387, 0.03525236565258387, 0.03525236565258387, 0.03525236565258387, 0.8237381717370806, 0.03525236565258387]
from probs:  [0.03525236565258387, 0.03525236565258387, 0.03525236565258387, 0.03525236565258387, 0.8237381717370806, 0.03525236565258387]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.108]
 [-0.069]
 [-0.05 ]
 [ 0.043]
 [ 0.019]] [[0.342]
 [0.42 ]
 [0.456]
 [0.643]
 [0.595]]
Starting evaluation
STARTED EXPV TRAINING ON FRAME NO.  20084
91 3033
92 3033
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.212]
 [-0.373]
 [ 0.146]
 [ 0.166]
 [ 0.52 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.03525236565258387, 0.03525236565258387, 0.03525236565258387, 0.03525236565258387, 0.8237381717370806, 0.03525236565258387]
rdn probs:  [0.03525236565258387, 0.03525236565258387, 0.03525236565258387, 0.03525236565258387, 0.8237381717370806, 0.03525236565258387]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]] [[0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]]
from probs:  [0.03531451966487057, 0.03531451966487057, 0.03531451966487057, 0.03531451966487057, 0.8234274016756472, 0.03531451966487057]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.035376614911939684, 0.035376614911939684, 0.035376614911939684, 0.035376614911939684, 0.8231169254403016, 0.035376614911939684]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[-0.464]
 [-0.199]
 [ 0.072]
 [ 0.453]
 [ 0.462]] [[0.   ]
 [0.267]
 [0.537]
 [0.917]
 [0.926]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.035376614911939684, 0.035376614911939684, 0.035376614911939684, 0.035376614911939684, 0.8231169254403016, 0.035376614911939684]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
101 3054
from probs:  [0.03550062944347741, 0.03550062944347741, 0.03550062944347741, 0.03550062944347741, 0.822496852782613, 0.03550062944347741]
102 3063
from probs:  [0.03556254889407908, 0.03556254889407908, 0.03556254889407908, 0.03556254889407908, 0.8221872555296046, 0.03556254889407908]
siam score:  -0.44132698
siam score:  -0.44531262
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.035932841393905214, 0.035932841393905214, 0.035932841393905214, 0.035932841393905214, 0.8203357930304739, 0.035932841393905214]
maxi score, test score, baseline:  0.0001 0.0 0.0001
108 3084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03611720435508302, 0.03611720435508302, 0.03611720435508302, 0.03611720435508302, 0.819413978224585, 0.03611720435508302]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.023]
 [0.011]
 [0.012]
 [0.013]] [[-0.359]
 [ 0.072]
 [-0.004]
 [ 0.052]
 [ 0.112]] [[0.464]
 [1.339]
 [1.163]
 [1.277]
 [1.399]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03611720435508302, 0.03611720435508302, 0.03611720435508302, 0.03611720435508302, 0.819413978224585, 0.03611720435508302]
first move QE:  -0.06333705992881439
from probs:  [0.036239824387603134, 0.036239824387603134, 0.036239824387603134, 0.036239824387603134, 0.8188008780619843, 0.036239824387603134]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.036239824387603134, 0.036239824387603134, 0.036239824387603134, 0.036239824387603134, 0.8188008780619843, 0.036239824387603134]
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.036239824387603134, 0.036239824387603134, 0.036239824387603134, 0.036239824387603134, 0.8188008780619843, 0.036239824387603134]
from probs:  [0.036239824387603134, 0.036239824387603134, 0.036239824387603134, 0.036239824387603134, 0.8188008780619843, 0.036239824387603134]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03630104806523981, 0.03630104806523981, 0.03630104806523981, 0.03630104806523981, 0.8184947596738009, 0.03630104806523981]
siam score:  -0.44891393
from probs:  [0.03636221429181878, 0.03636221429181878, 0.03636221429181878, 0.03636221429181878, 0.8181889285409061, 0.03636221429181878]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.61 ]
 [-0.317]
 [-0.008]
 [ 0.006]
 [ 0.013]] [[0.001]
 [0.586]
 [1.205]
 [1.233]
 [1.247]]
from probs:  [0.03636221429181878, 0.03636221429181878, 0.03636221429181878, 0.03636221429181878, 0.8181889285409061, 0.03636221429181878]
siam score:  -0.4495546
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03636221429181878, 0.03636221429181878, 0.03636221429181878, 0.03636221429181878, 0.8181889285409061, 0.03636221429181878]
from probs:  [0.03636221429181878, 0.03636221429181878, 0.03636221429181878, 0.03636221429181878, 0.8181889285409061, 0.03636221429181878]
from probs:  [0.03636221429181878, 0.03636221429181878, 0.03636221429181878, 0.03636221429181878, 0.8181889285409061, 0.03636221429181878]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03636221429181878, 0.03636221429181878, 0.03636221429181878, 0.03636221429181878, 0.8181889285409061, 0.03636221429181878]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.05826874052161767
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0364233231481685, 0.0364233231481685, 0.0364233231481685, 0.0364233231481685, 0.8178833842591575, 0.0364233231481685]
using explorer policy with actor:  1
start point for exploration sampling:  20084
siam score:  -0.4688367
131 3152
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
using explorer policy with actor:  1
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.164]
 [1.781]
 [1.781]
 [1.033]
 [1.254]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.15 ]
 [0.15 ]
 [0.171]
 [0.387]
 [0.84 ]] [[0.092]
 [0.092]
 [0.12 ]
 [0.408]
 [1.012]]
142 3166
start point for exploration sampling:  20084
143 3171
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.033231886541634614
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.037031274023406925, 0.037031274023406925, 0.037031274023406925, 0.037031274023406925, 0.8148436298829654, 0.037031274023406925]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
start point for exploration sampling:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.037031274023406925, 0.037031274023406925, 0.037031274023406925, 0.037031274023406925, 0.8148436298829654, 0.037031274023406925]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.03207839256535619
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.752]
 [2.072]
 [2.072]
 [2.072]
 [2.935]] [[0.26 ]
 [1.128]
 [1.128]
 [1.128]
 [1.697]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.037272868168404506, 0.037272868168404506, 0.037272868168404506, 0.037272868168404506, 0.8136356591579775, 0.037272868168404506]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03733312606851035, 0.03733312606851035, 0.03733312606851035, 0.03733312606851035, 0.8133343696574482, 0.03733312606851035]
from probs:  [0.03733312606851035, 0.03733312606851035, 0.03733312606851035, 0.03733312606851035, 0.8133343696574482, 0.03733312606851035]
siam score:  -0.5109966
line 256 mcts: sample exp_bonus 3.0694242508381167
from probs:  [0.037513563497802174, 0.037513563497802174, 0.037513563497802174, 0.037513563497802174, 0.8124321825109891, 0.037513563497802174]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.088]
 [1.739]
 [1.477]
 [1.723]
 [2.132]] [[0.855]
 [1.362]
 [1.158]
 [1.35 ]
 [1.67 ]]
using explorer policy with actor:  1
in main func line 156:  173
using explorer policy with actor:  1
siam score:  -0.5216405
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.8115325091986648, 0.03769349816026704]
using another actor
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.8115325091986648, 0.03769349816026704]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.8115325091986648, 0.03769349816026704]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.8115325091986648, 0.03769349816026704]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]] [[0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]]
from probs:  [0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.8115325091986648, 0.03769349816026704]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.8115325091986648, 0.03769349816026704]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.8115325091986648, 0.03769349816026704]
using another actor
from probs:  [0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.03769349816026704, 0.8115325091986648, 0.03769349816026704]
from probs:  [0.03775336501815438, 0.03775336501815438, 0.03775336501815438, 0.03775336501815438, 0.8112331749092281, 0.03775336501815438]
maxi score, test score, baseline:  0.0001 0.0 0.0001
185 3274
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03781317632374593, 0.03781317632374593, 0.03781317632374593, 0.03781317632374593, 0.8109341183812704, 0.03781317632374593]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.358]
 [1.234]
 [1.234]
 [1.095]
 [1.249]] [[0.   ]
 [1.752]
 [1.752]
 [1.473]
 [1.782]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
using another actor
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.64 ]
 [0.585]
 [1.076]
 [1.48 ]
 [1.424]] [[0.592]
 [0.526]
 [1.112]
 [1.593]
 [1.527]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.135]
 [ 0.001]
 [ 0.287]
 [ 0.634]
 [ 1.09 ]] [[0.   ]
 [0.09 ]
 [0.281]
 [0.512]
 [0.815]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.099]
 [1.294]
 [1.294]
 [1.45 ]
 [1.542]] [[0.064]
 [1.537]
 [1.537]
 [1.729]
 [1.842]]
in main func line 156:  193
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03799227769890133, 0.03799227769890133, 0.03799227769890133, 0.03799227769890133, 0.8100386115054934, 0.03799227769890133]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.297]
 [-0.123]
 [ 0.193]
 [ 0.689]
 [ 0.793]] [[0.   ]
 [0.116]
 [0.326]
 [0.657]
 [0.726]]
194 3295
from probs:  [0.03799227769890133, 0.03799227769890133, 0.03799227769890133, 0.03799227769890133, 0.8100386115054934, 0.03799227769890133]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03799227769890133, 0.03799227769890133, 0.03799227769890133, 0.03799227769890133, 0.8100386115054934, 0.03799227769890133]
using another actor
siam score:  -0.52634746
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03805186756675002, 0.03805186756675002, 0.03805186756675002, 0.03805186756675002, 0.8097406621662498, 0.03805186756675002]
siam score:  -0.5321124
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03805186756675002, 0.03805186756675002, 0.03805186756675002, 0.03805186756675002, 0.8097406621662498, 0.03805186756675002]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.   ]] [[3.044]
 [3.044]
 [3.044]
 [2.391]
 [2.843]] [[1.992]
 [1.992]
 [1.992]
 [1.278]
 [1.772]]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
201 3317
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.052]
 [-0.049]
 [ 0.052]
 [ 0.837]
 [ 1.071]] [[0.   ]
 [0.004]
 [0.14 ]
 [1.187]
 [1.499]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.002]
 [0.001]] [[1.909]
 [1.909]
 [1.909]
 [2.14 ]
 [1.909]] [[1.088]
 [1.088]
 [1.088]
 [1.396]
 [1.088]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0382303064727123, 0.0382303064727123, 0.0382303064727123, 0.0382303064727123, 0.8088484676364385, 0.0382303064727123]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0382303064727123, 0.0382303064727123, 0.0382303064727123, 0.0382303064727123, 0.8088484676364385, 0.0382303064727123]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[1.682]
 [1.327]
 [1.936]
 [1.71 ]
 [2.513]] [[1.005]
 [0.737]
 [1.197]
 [1.027]
 [1.633]]
siam score:  -0.5320561
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5731978951279957
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.768]
 [1.101]
 [1.624]
 [2.56 ]
 [2.82 ]] [[0.176]
 [0.287]
 [0.462]
 [0.774]
 [0.861]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
211 3348
using another actor
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.564]
 [3.564]
 [3.564]
 [3.564]
 [2.772]] [[1.246]
 [1.246]
 [1.246]
 [1.246]
 [0.719]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.45069587
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.524]
 [1.358]
 [2.179]
 [2.221]
 [2.515]] [[0.772]
 [0.614]
 [1.393]
 [1.433]
 [1.712]]
227 3365
228 3365
siam score:  -0.49530753
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.019]
 [0.027]
 [0.042]
 [0.049]
 [0.048]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
231 3376
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.5735449151509693
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.411]
 [2.254]
 [1.515]
 [1.882]
 [2.117]] [[0.368]
 [0.929]
 [0.437]
 [0.682]
 [0.838]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.806]
 [0.452]
 [0.929]
 [2.251]
 [1.708]] [[0.423]
 [0.07 ]
 [0.546]
 [1.869]
 [1.325]]
siam score:  -0.48475656
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.1  ]
 [1.158]
 [1.693]
 [1.832]
 [2.962]] [[0.429]
 [0.468]
 [0.829]
 [0.923]
 [1.686]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.665]
 [1.665]
 [1.665]
 [1.665]
 [1.665]] [[0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.984]
 [3.423]
 [1.113]
 [1.665]
 [1.908]] [[0.689]
 [3.942]
 [0.861]
 [1.598]
 [1.922]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
253 3395
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.596]
 [3.596]
 [3.596]
 [3.596]
 [4.67 ]] [[0.924]
 [0.924]
 [0.924]
 [0.924]
 [1.25 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.829]
 [2.829]
 [2.829]
 [2.586]
 [4.117]] [[1.184]
 [1.184]
 [1.184]
 [1.042]
 [1.94 ]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.751]
 [2.751]
 [2.751]
 [2.751]
 [3.71 ]] [[1.307]
 [1.307]
 [1.307]
 [1.307]
 [1.932]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.494]
 [2.829]
 [1.969]
 [2.172]
 [2.378]] [[0.202]
 [2.   ]
 [0.843]
 [1.115]
 [1.393]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.568]
 [1.633]
 [1.941]
 [2.396]
 [2.076]] [[1.063]
 [1.102]
 [1.286]
 [1.559]
 [1.367]]
siam score:  -0.45933518
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.001]
 [2.368]
 [2.368]
 [1.767]
 [2.645]] [[0.675]
 [1.525]
 [1.525]
 [1.152]
 [1.697]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
261 3417
siam score:  -0.49078956
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.001]
 [0.   ]
 [0.001]] [[-0.21 ]
 [-0.034]
 [ 0.183]
 [ 0.623]
 [ 1.01 ]] [[0.   ]
 [0.268]
 [0.601]
 [1.269]
 [1.86 ]]
265 3431
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
siam score:  -0.512722
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Starting evaluation
rdn probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.52143025
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.   ]] [[1.949]
 [1.949]
 [1.949]
 [1.949]
 [2.626]] [[0.711]
 [0.711]
 [0.711]
 [0.711]
 [1.387]]
269 3452
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
271 3464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
first move QE:  0.08053294138142374
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
276 3469
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
using another actor
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.676]
 [0.624]
 [1.291]
 [1.972]
 [1.508]] [[0.236]
 [0.202]
 [0.646]
 [1.099]
 [0.791]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
siam score:  -0.4155465
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
line 256 mcts: sample exp_bonus 0.22197440044456684
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using another actor
siam score:  -0.44582665
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.881]
 [0.626]
 [0.412]
 [1.077]
 [0.754]] [[0.247]
 [0.163]
 [0.091]
 [0.312]
 [0.204]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.45588142
using explorer policy with actor:  1
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.47909084
using another actor
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.027]
 [0.156]
 [0.307]
 [0.412]
 [0.376]] [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.471]
 [2.471]
 [2.471]
 [2.471]
 [2.471]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.10155965722790367
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
first move QE:  0.10155965722790367
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]] [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]]
using explorer policy with actor:  1
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.319]
 [3.064]
 [2.991]
 [2.907]
 [4.517]] [[0.246]
 [0.597]
 [0.562]
 [0.523]
 [1.279]]
using explorer policy with actor:  1
siam score:  -0.49227357
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.    0.875]
first move QE:  0.10754937221603364
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.819]
 [3.819]
 [3.819]
 [3.819]
 [4.96 ]] [[1.203]
 [1.203]
 [1.203]
 [1.203]
 [1.915]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.436]
 [2.436]
 [2.436]
 [2.436]
 [2.436]] [[1.829]
 [1.829]
 [1.829]
 [1.829]
 [1.829]]
306 3574
siam score:  -0.4963489
start point for exploration sampling:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  0
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
314 3590
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.788]
 [1.788]
 [1.049]
 [1.444]
 [1.851]] [[1.523]
 [1.523]
 [0.674]
 [1.128]
 [1.594]]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.002]
 [0.   ]
 [0.   ]
 [0.   ]] [[1.162]
 [1.45 ]
 [1.256]
 [3.604]
 [3.682]] [[0.122]
 [0.222]
 [0.153]
 [0.937]
 [0.963]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.981]
 [1.508]
 [1.508]
 [1.508]
 [2.377]] [[0.539]
 [1.09 ]
 [1.09 ]
 [1.09 ]
 [2.   ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.532]
 [2.532]
 [2.532]
 [2.532]
 [2.532]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.43622994
using another actor
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.442]
 [2.442]
 [2.442]
 [2.442]
 [2.442]] [[0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.565]
 [1.565]
 [1.565]
 [3.064]
 [1.565]] [[0.785]
 [0.785]
 [0.785]
 [1.991]
 [0.785]]
siam score:  -0.38044244
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
siam score:  -0.37517327
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
line 256 mcts: sample exp_bonus 4.488781609199632
using explorer policy with actor:  1
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
333 3612
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.21 ]
 [2.21 ]
 [2.21 ]
 [1.853]
 [3.594]] [[0.52 ]
 [0.52 ]
 [0.52 ]
 [0.388]
 [1.031]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.383]
 [0.638]
 [0.72 ]
 [1.418]
 [3.185]] [[0.   ]
 [0.155]
 [0.205]
 [0.627]
 [1.698]]
first move QE:  0.13847479431968474
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
siam score:  -0.48766366
340 3616
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.191]
 [2.191]
 [1.434]
 [1.658]
 [2.01 ]] [[1.927]
 [1.927]
 [1.211]
 [1.423]
 [1.756]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.063]
 [0.793]
 [1.902]
 [1.83 ]
 [2.128]] [[0.277]
 [0.007]
 [1.116]
 [1.044]
 [1.342]]
siam score:  -0.53523535
line 256 mcts: sample exp_bonus 2.276978136775057
348 3636
using another actor
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
348 3637
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.823]
 [3.433]
 [3.027]
 [3.978]
 [4.664]] [[0.591]
 [0.861]
 [0.681]
 [1.101]
 [1.404]]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.40341705
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.4457746
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7990468322263344
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.25  0.042 0.083 0.125 0.5  ]
373 3670
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.167]
 [3.167]
 [3.167]
 [2.697]
 [3.394]] [[1.372]
 [1.372]
 [1.372]
 [1.042]
 [1.532]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.825]
 [0.908]
 [2.065]
 [1.946]
 [2.15 ]] [[0.928]
 [0.317]
 [1.088]
 [1.009]
 [1.145]]
373 3673
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
first move QE:  0.17524245880802458
start point for exploration sampling:  20084
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.899]
 [1.276]
 [2.517]
 [2.456]
 [2.591]] [[0.415]
 [0.   ]
 [0.827]
 [0.786]
 [0.876]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.67 ]
 [2.342]
 [3.002]
 [3.622]
 [3.792]] [[0.984]
 [0.791]
 [1.18 ]
 [1.544]
 [1.645]]
using another actor
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  386
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.5104116
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
in main func line 156:  390
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using another actor
using another actor
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
in main func line 156:  396
start point for exploration sampling:  20084
start point for exploration sampling:  20084
siam score:  -0.44258654
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.43856156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.616]
 [3.616]
 [3.616]
 [3.616]
 [3.019]] [[1.953]
 [1.953]
 [1.953]
 [1.953]
 [1.629]]
using explorer policy with actor:  1
first move QE:  0.1896024919086736
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0001],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0002]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.00010802541069391138
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.00017138495791417022
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.4338391
siam score:  -0.43484324
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.757]
 [5.757]
 [5.757]
 [5.478]
 [5.403]] [[1.903]
 [1.903]
 [1.903]
 [1.785]
 [1.754]]
using explorer policy with actor:  1
407 3720
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 0.969961334715886
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]] [[0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]]
siam score:  -0.49568567
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.765]
 [2.765]
 [2.765]
 [2.765]
 [2.765]] [[1.799]
 [1.799]
 [1.799]
 [1.799]
 [1.799]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.20185084090133845
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.359]
 [2.387]
 [2.026]
 [2.896]
 [4.273]] [[0.725]
 [0.741]
 [0.528]
 [1.042]
 [1.857]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
line 256 mcts: sample exp_bonus 3.5097605506802343
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
line 256 mcts: sample exp_bonus 0.3772107668962217
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 2.0463704463256764
siam score:  -0.5404691
in main func line 156:  423
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.995]
 [0.995]
 [0.995]
 [0.995]
 [1.181]] [[0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.712]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
427 3752
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Starting evaluation
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.065]
 [3.065]
 [3.065]
 [3.065]
 [3.984]] [[0.967]
 [0.967]
 [0.967]
 [0.967]
 [1.519]]
430 3764
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.806]
 [2.615]
 [2.325]
 [2.638]
 [3.424]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.621]
 [2.621]
 [2.621]
 [2.621]
 [3.855]] [[0.661]
 [0.661]
 [0.661]
 [0.661]
 [1.268]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.765]
 [4.765]
 [4.765]
 [4.765]
 [4.765]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.135]
 [3.135]
 [3.135]
 [3.135]
 [2.641]] [[1.253]
 [1.253]
 [1.253]
 [1.253]
 [0.923]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
siam score:  -0.5427618
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.936]
 [5.936]
 [5.936]
 [5.936]
 [6.273]] [[1.77 ]
 [1.77 ]
 [1.77 ]
 [1.77 ]
 [1.903]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.917887116757128
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.499]
 [5.499]
 [5.499]
 [4.92 ]
 [5.489]] [[1.732]
 [1.732]
 [1.732]
 [1.385]
 [1.726]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.248]
 [2.248]
 [2.248]
 [2.248]
 [2.266]] [[1.979]
 [1.979]
 [1.979]
 [1.979]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.152]
 [4.152]
 [4.152]
 [4.152]
 [4.73 ]] [[1.605]
 [1.605]
 [1.605]
 [1.605]
 [1.886]]
line 256 mcts: sample exp_bonus 4.799318131982281
using explorer policy with actor:  1
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.441]
 [5.441]
 [5.441]
 [5.441]
 [5.894]] [[1.518]
 [1.518]
 [1.518]
 [1.518]
 [1.695]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[8.084]
 [8.084]
 [8.084]
 [6.328]
 [7.654]] [[2.   ]
 [2.   ]
 [2.   ]
 [1.36 ]
 [1.843]]
457 3812
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.945]
 [4.945]
 [4.945]
 [4.945]
 [7.491]] [[0.873]
 [0.873]
 [0.873]
 [0.873]
 [1.467]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[8.351]
 [8.351]
 [8.351]
 [8.351]
 [9.745]] [[1.662]
 [1.662]
 [1.662]
 [1.662]
 [1.998]]
siam score:  -0.48179984
siam score:  -0.4885135
siam score:  -0.49780735
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
460 3824
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.391]
 [1.899]
 [2.078]
 [1.857]
 [3.012]] [[0.181]
 [0.351]
 [0.411]
 [0.337]
 [0.723]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
in main func line 156:  464
464 3828
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
line 256 mcts: sample exp_bonus 5.3500134585965045
466 3832
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.485]
 [4.485]
 [4.485]
 [4.485]
 [4.341]] [[2.   ]
 [2.   ]
 [2.   ]
 [2.   ]
 [1.914]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
siam score:  -0.5443928
line 256 mcts: sample exp_bonus 5.079812976211921
472 3840
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.543]
 [2.086]
 [2.543]
 [2.329]
 [3.205]] [[0.878]
 [0.601]
 [0.878]
 [0.748]
 [1.279]]
first move QE:  0.22052499377834067
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.846]
 [4.846]
 [4.846]
 [4.846]
 [5.362]] [[1.647]
 [1.647]
 [1.647]
 [1.647]
 [1.874]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
siam score:  -0.55582273
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.187]
 [2.187]
 [2.187]
 [2.187]
 [2.187]] [[1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using another actor
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.478]
 [4.478]
 [4.478]
 [6.761]
 [5.671]] [[0.903]
 [0.903]
 [0.903]
 [1.78 ]
 [1.362]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5586545
line 256 mcts: sample exp_bonus 3.364098997125438
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.262]
 [1.262]
 [1.262]
 [1.262]
 [1.262]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
siam score:  -0.57167894
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.042 0.833]
using explorer policy with actor:  1
500 3884
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.24364869813841675
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
start point for exploration sampling:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
513 3895
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.409]
 [2.409]
 [2.409]
 [2.409]
 [2.341]] [[2.   ]
 [2.   ]
 [2.   ]
 [2.   ]
 [1.937]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.313]
 [1.739]
 [1.739]
 [1.838]
 [1.999]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.731]
 [2.789]
 [1.847]
 [2.298]
 [2.918]] [[0.536]
 [1.349]
 [0.625]
 [0.972]
 [1.448]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.433]
 [1.947]
 [1.947]
 [2.11 ]
 [2.585]] [[0.348]
 [0.819]
 [0.819]
 [0.968]
 [1.404]]
start point for exploration sampling:  20084
using explorer policy with actor:  1
start point for exploration sampling:  20084
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
start point for exploration sampling:  20084
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.99]
 [3.99]
 [3.99]
 [3.99]
 [3.99]] [[5.322]
 [5.322]
 [5.322]
 [5.322]
 [5.322]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
siam score:  -0.51842797
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.663]
 [7.925]
 [7.925]
 [5.875]
 [9.076]] [[0.063]
 [1.588]
 [1.588]
 [0.854]
 [2.   ]]
start point for exploration sampling:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
siam score:  -0.51633173
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
siam score:  -0.5277825
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
siam score:  -0.5404229
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.749]
 [5.749]
 [5.749]
 [5.749]
 [5.433]] [[1.633]
 [1.633]
 [1.633]
 [1.633]
 [1.501]]
siam score:  -0.5498857
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.844]
 [1.434]
 [1.201]
 [2.047]
 [2.329]] [[1.19 ]
 [0.862]
 [0.676]
 [1.353]
 [1.578]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.399]
 [4.399]
 [4.399]
 [4.399]
 [5.793]] [[1.172]
 [1.172]
 [1.172]
 [1.172]
 [2.   ]]
556 3950
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.53336424
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.658]
 [1.658]
 [1.658]
 [1.658]
 [2.049]] [[0.765]
 [0.765]
 [0.765]
 [0.765]
 [1.109]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using another actor
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.955]
 [1.955]
 [1.955]
 [1.955]
 [2.523]] [[0.865]
 [0.865]
 [0.865]
 [0.865]
 [1.343]]
565 3959
siam score:  -0.5454785
first move QE:  0.28713609803256174
571 3962
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.352]
 [2.739]
 [3.3  ]
 [3.313]
 [3.273]] [[0.561]
 [0.871]
 [1.32 ]
 [1.33 ]
 [1.298]]
siam score:  -0.5543148
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.042 0.833]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.457]
 [1.97 ]
 [1.036]
 [2.146]
 [2.59 ]] [[0.398]
 [0.736]
 [0.121]
 [0.851]
 [1.143]]
first move QE:  0.29125998686272553
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.565]
 [1.107]
 [2.571]
 [2.813]
 [2.934]] [[0.153]
 [0.   ]
 [0.489]
 [0.57 ]
 [0.61 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
line 256 mcts: sample exp_bonus 4.457987910774627
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746, 0.03846745623582508]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.248162914226574
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.5784301
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.356]
 [1.356]
 [1.269]
 [1.749]
 [1.978]] [[0.97 ]
 [0.97 ]
 [0.892]
 [1.325]
 [1.532]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.229]
 [2.229]
 [2.229]
 [2.229]
 [2.229]] [[1.679]
 [1.679]
 [1.679]
 [1.679]
 [1.679]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
608 4025
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.494]
 [2.494]
 [2.494]
 [2.494]
 [2.473]] [[1.889]
 [1.889]
 [1.889]
 [1.889]
 [1.871]]
UNIT TEST: sample policy line 217 mcts : [0.    0.042 0.083 0.333 0.542]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.392]
 [2.392]
 [2.392]
 [2.169]
 [3.631]] [[0.526]
 [0.526]
 [0.526]
 [0.452]
 [0.936]]
siam score:  -0.5875171
siam score:  -0.58700585
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.82 ]
 [1.82 ]
 [1.82 ]
 [1.921]
 [1.938]] [[0.6  ]
 [0.6  ]
 [0.6  ]
 [0.667]
 [0.679]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.31 ]
 [1.533]
 [1.533]
 [1.533]
 [1.533]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.989]
 [4.989]
 [4.989]
 [4.989]
 [4.989]] [[1.686]
 [1.686]
 [1.686]
 [1.686]
 [1.686]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.042 0.833]
start point for exploration sampling:  20084
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.227]
 [4.566]
 [5.111]
 [3.988]
 [6.375]] [[0.428]
 [0.863]
 [1.04 ]
 [0.676]
 [1.45 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.386]
 [0.344]
 [2.908]
 [1.11 ]
 [2.62 ]] [[0.025]
 [0.   ]
 [1.522]
 [0.455]
 [1.35 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20084
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.513]
 [4.383]
 [3.513]
 [3.513]
 [4.608]] [[0.964]
 [1.295]
 [0.964]
 [0.964]
 [1.381]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.977]
 [4.977]
 [4.977]
 [4.977]
 [6.339]] [[1.173]
 [1.173]
 [1.173]
 [1.173]
 [1.963]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.32339593936997046
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.18 ]
 [5.18 ]
 [5.18 ]
 [5.18 ]
 [4.928]] [[1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.385]]
siam score:  -0.5522008
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
626 4051
631 4052
siam score:  -0.5587952
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.32547204157253307
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.002]
 [0.002]
 [0.   ]] [[1.434]
 [1.127]
 [1.127]
 [1.127]
 [2.527]] [[0.003]
 [0.002]
 [0.002]
 [0.002]
 [0.   ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
636 4058
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.679]
 [2.679]
 [2.679]
 [2.679]
 [2.679]] [[1.464]
 [1.464]
 [1.464]
 [1.464]
 [1.464]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
639 4064
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.   ]] [[1.881]
 [1.881]
 [1.881]
 [1.881]
 [2.399]] [[1.14 ]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.889]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.924]
 [3.924]
 [3.924]
 [3.924]
 [4.118]] [[1.852]
 [1.852]
 [1.852]
 [1.852]
 [2.   ]]
siam score:  -0.62248814
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0002],
        [0.0002],
        [0.0001],
        [0.0001],
        [0.0001],
        [0.0000],
        [0.0002],
        [0.0000],
        [0.0000],
        [0.0000]], dtype=torch.float64)
0.0 0.0001682186132461203
0.0 0.00017523317993053497
0.0 0.00014033174579715555
0.0 0.00014073320688791604
0.0 0.00014920267753896902
0.0 0.0
0.0 0.00015125268677313356
0.0 0.0
0.0 0.0
0.0 0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.5   0.167 0.292]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.295]
 [2.821]
 [3.721]
 [3.478]
 [4.981]] [[0.363]
 [0.659]
 [1.167]
 [1.029]
 [1.877]]
644 4076
first move QE:  0.3331585233287551
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.495]
 [2.258]
 [2.179]
 [1.37 ]
 [2.186]] [[0.44 ]
 [0.695]
 [0.668]
 [0.398]
 [0.67 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.736]
 [2.401]
 [2.011]
 [1.965]
 [2.547]] [[0.   ]
 [1.811]
 [1.387]
 [1.337]
 [1.97 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5558398
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
start point for exploration sampling:  20084
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.56071615
siam score:  -0.5730954
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.58362436
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.589]
 [0.429]
 [1.619]
 [1.275]
 [1.316]] [[0.373]
 [0.218]
 [1.369]
 [1.037]
 [1.076]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.   ]] [[0.961]
 [0.956]
 [1.77 ]
 [1.029]
 [1.674]] [[0.437]
 [0.432]
 [1.135]
 [0.495]
 [1.052]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  667
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[0.445]
 [1.002]
 [0.931]
 [0.959]
 [1.228]] [[0.312]
 [1.055]
 [0.961]
 [0.998]
 [1.357]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6522339
siam score:  -0.65740657
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.941]
 [2.941]
 [2.941]
 [2.941]
 [3.068]] [[1.669]
 [1.669]
 [1.669]
 [1.669]
 [1.769]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.171]
 [2.716]
 [2.911]
 [2.831]
 [3.433]] [[0.745]
 [1.043]
 [1.149]
 [1.105]
 [1.434]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.9883511887511127
start point for exploration sampling:  20084
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.021]
 [2.071]
 [2.071]
 [2.201]
 [2.662]] [[0.   ]
 [0.639]
 [0.639]
 [0.717]
 [0.997]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.675586
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.067]
 [3.184]
 [2.564]
 [3.184]
 [4.063]] [[0.   ]
 [1.142]
 [0.808]
 [1.142]
 [1.616]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.2160757831138906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.3292789712765871
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.213]
 [2.213]
 [2.213]
 [2.213]
 [2.868]] [[1.363]
 [1.363]
 [1.363]
 [1.363]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.911]
 [0.773]
 [0.711]
 [1.329]
 [1.854]] [[0.541]
 [0.41 ]
 [0.352]
 [0.935]
 [1.429]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.104]
 [0.284]
 [0.344]
 [0.353]
 [0.433]] [[0.05 ]
 [0.23 ]
 [0.291]
 [0.299]
 [0.38 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.265]
 [1.457]
 [2.163]
 [1.457]
 [1.457]] [[0.093]
 [0.887]
 [1.357]
 [0.887]
 [0.887]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.846]
 [2.846]
 [2.574]
 [2.375]
 [2.672]] [[1.556]
 [1.556]
 [1.375]
 [1.242]
 [1.44 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
siam score:  -0.7048738
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.216]
 [3.216]
 [3.216]
 [3.216]
 [4.087]] [[1.514]
 [1.514]
 [1.514]
 [1.514]
 [1.982]]
siam score:  -0.70954466
using explorer policy with actor:  1
701 4148
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6955478
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.331]
 [1.331]
 [0.795]
 [0.911]
 [1.745]] [[0.501]
 [0.501]
 [0.145]
 [0.222]
 [0.777]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.949]
 [5.949]
 [5.949]
 [5.949]
 [5.949]] [[1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.618]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.607]
 [5.607]
 [5.607]
 [3.965]
 [6.949]] [[1.47 ]
 [1.47 ]
 [1.47 ]
 [0.822]
 [2.   ]]
siam score:  -0.6682521
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6858641
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2541853136053802
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 1.1926322444564468
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.845]
 [3.845]
 [3.845]
 [3.845]
 [3.575]] [[1.839]
 [1.839]
 [1.839]
 [1.839]
 [1.659]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.498000356308911
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4273738133980287
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.282]
 [0.282]
 [0.282]
 [0.359]
 [0.35 ]] [[0.251]
 [0.251]
 [0.251]
 [0.354]
 [0.342]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.72544754
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
722 4168
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.74205434
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.73970413
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 4.281245534396963
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
730 4182
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
736 4194
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.53 ]
 [6.53 ]
 [6.53 ]
 [6.53 ]
 [6.536]] [[1.998]
 [1.998]
 [1.998]
 [1.998]
 [2.   ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.641]
 [1.982]
 [3.374]
 [3.824]
 [3.877]] [[0.761]
 [0.528]
 [1.02 ]
 [1.179]
 [1.198]]
siam score:  -0.6893985
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
740 4203
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.971]
 [1.971]
 [1.971]
 [1.971]
 [3.012]] [[0.841]
 [0.841]
 [0.841]
 [0.841]
 [1.404]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.803]
 [5.144]
 [5.528]
 [5.803]
 [5.331]] [[1.341]
 [1.121]
 [1.249]
 [1.341]
 [1.183]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.399]
 [4.399]
 [4.399]
 [4.399]
 [4.399]] [[1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.019]
 [4.019]
 [4.019]
 [4.019]
 [5.198]] [[0.922]
 [0.922]
 [0.922]
 [0.922]
 [1.284]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
755 4221
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.72177416
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.352]
 [2.711]
 [1.262]
 [0.575]
 [3.172]] [[0.355]
 [0.974]
 [0.313]
 [0.   ]
 [1.184]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.75093305
siam score:  -0.7542003
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 1.8398104807167005e-07
0.0 2.9626602789137903e-06
0.0 3.212772774082227e-06
0.0 0.0
0.0 0.0
0.0 3.6169767653217545e-06
0.0 8.329193944898878e-07
0.0 8.922660815311828e-07
0.0 4.555222707881862e-07
0.0 1.933672292498064e-07
siam score:  -0.7663086
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.131]
 [0.911]
 [2.131]
 [2.131]
 [2.131]] [[1.131]
 [0.318]
 [1.131]
 [1.131]
 [1.131]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.016]
 [2.99 ]
 [4.181]
 [3.105]
 [3.71 ]] [[1.645]
 [1.088]
 [1.734]
 [1.15 ]
 [1.478]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
769 4243
using explorer policy with actor:  1
771 4245
siam score:  -0.77214795
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.922]
 [2.848]
 [2.808]
 [2.75 ]
 [3.099]] [[1.29 ]
 [0.701]
 [0.68 ]
 [0.647]
 [0.839]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.308]
 [3.308]
 [3.308]
 [3.308]
 [2.882]] [[1.34 ]
 [1.34 ]
 [1.34 ]
 [1.34 ]
 [0.991]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
line 256 mcts: sample exp_bonus 1.1444597226209248
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.77855116
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
781 4267
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.125]
 [1.125]
 [1.125]
 [1.125]
 [2.62 ]] [[-0.81 ]
 [-0.81 ]
 [-0.81 ]
 [-0.81 ]
 [ 0.518]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.671]
 [0.557]
 [3.634]
 [2.572]
 [4.469]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.8957318143872932
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7806001
siam score:  -0.78285384
first move QE:  0.3747778633904235
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.956]
 [2.956]
 [2.956]
 [2.956]
 [3.4  ]] [[1.247]
 [1.247]
 [1.247]
 [1.247]
 [1.542]]
siam score:  -0.7751046
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.7567203
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7541066
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.47 ]
 [0.323]
 [0.974]
 [0.516]
 [0.769]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.592]
 [4.592]
 [4.592]
 [4.592]
 [5.171]] [[1.702]
 [1.702]
 [1.702]
 [1.702]
 [1.988]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
811 4309
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.207]
 [4.207]
 [4.207]
 [4.207]
 [6.057]] [[1.137]
 [1.137]
 [1.137]
 [1.137]
 [1.992]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.558]
 [2.7  ]
 [2.558]
 [2.558]
 [2.558]] [[1.864]
 [2.   ]
 [1.864]
 [1.864]
 [1.864]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.186]
 [3.186]
 [3.186]
 [3.186]
 [3.186]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
822 4320
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 3.9839381557482345
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.141]
 [3.903]
 [3.747]
 [3.177]
 [4.544]] [[0.299]
 [0.572]
 [0.516]
 [0.312]
 [0.802]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.293]
 [4.293]
 [4.293]
 [4.293]
 [3.947]] [[1.963]
 [1.963]
 [1.963]
 [1.963]
 [1.738]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.79364896
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.79186785
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.3783236673338424
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.393]
 [2.393]
 [2.393]
 [2.393]
 [2.753]] [[1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.886]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.278]
 [2.27 ]
 [2.27 ]
 [2.27 ]
 [2.868]] [[0.793]
 [0.788]
 [0.788]
 [0.788]
 [1.184]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[8.692]
 [8.692]
 [8.692]
 [8.692]
 [8.844]] [[1.963]
 [1.963]
 [1.963]
 [1.963]
 [1.999]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.426]
 [0.944]
 [3.167]
 [4.065]
 [5.513]] [[0.318]
 [0.191]
 [0.777]
 [1.013]
 [1.394]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.601]
 [5.601]
 [5.601]
 [5.601]
 [5.616]] [[1.169]
 [1.169]
 [1.169]
 [1.169]
 [1.174]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.41 ]
 [4.41 ]
 [4.41 ]
 [4.41 ]
 [4.249]] [[1.408]
 [1.408]
 [1.408]
 [1.408]
 [1.326]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
857 4358
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.021]
 [5.021]
 [5.021]
 [5.021]
 [4.983]] [[1.936]
 [1.936]
 [1.936]
 [1.936]
 [1.918]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.674]
 [0.489]
 [0.46 ]
 [0.467]
 [0.555]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.12 ]
 [2.973]
 [3.024]
 [3.024]
 [3.547]] [[0.943]
 [0.861]
 [0.889]
 [0.889]
 [1.182]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.945]
 [2.974]
 [3.524]
 [0.846]
 [3.225]] [[1.08 ]
 [1.092]
 [1.326]
 [0.189]
 [1.199]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.184]
 [3.184]
 [3.184]
 [3.184]
 [3.542]] [[1.692]
 [1.692]
 [1.692]
 [1.692]
 [2.   ]]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7474635
871 4375
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.083 0.    0.917]
first move QE:  0.4010317535123544
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.696]
 [7.696]
 [7.942]
 [7.696]
 [7.303]] [[1.682]
 [1.682]
 [1.784]
 [1.682]
 [1.518]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.861]
 [3.982]
 [4.871]
 [4.871]
 [5.063]] [[1.058]
 [1.123]
 [1.601]
 [1.601]
 [1.704]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[8.076]
 [8.076]
 [8.076]
 [7.636]
 [8.392]] [[1.582]
 [1.582]
 [1.582]
 [1.417]
 [1.7  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.77808535
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.17 ]
 [0.409]
 [0.406]
 [0.254]
 [0.417]] [[0.033]
 [0.272]
 [0.27 ]
 [0.117]
 [0.281]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.42133499922108814
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7819977
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.826]
 [2.826]
 [2.826]
 [2.826]
 [2.826]] [[1.796]
 [1.796]
 [1.796]
 [1.796]
 [1.796]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.79277635
start point for exploration sampling:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.385]
 [2.484]
 [2.146]
 [2.445]
 [2.648]] [[0.415]
 [1.149]
 [0.924]
 [1.123]
 [1.259]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.449]
 [1.293]
 [1.077]
 [1.323]
 [3.304]] [[0.528]
 [0.142]
 [0.069]
 [0.152]
 [0.813]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.175]
 [2.175]
 [2.175]
 [2.175]
 [2.475]] [[1.364]
 [1.364]
 [1.364]
 [1.364]
 [1.613]]
start point for exploration sampling:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.892]
 [3.892]
 [3.892]
 [3.892]
 [4.255]] [[1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.581]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
using explorer policy with actor:  1
siam score:  -0.7768511
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.798]
 [1.798]
 [1.798]
 [1.798]
 [1.798]] [[1.902]
 [1.902]
 [1.902]
 [1.902]
 [1.902]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.161]
 [0.122]
 [2.565]
 [2.325]
 [2.717]] [[0.917]
 [0.   ]
 [1.099]
 [0.991]
 [1.167]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.869]
 [2.048]
 [2.148]
 [2.381]
 [2.435]] [[0.577]
 [0.709]
 [0.783]
 [0.955]
 [0.995]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8166755
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.248]
 [2.176]
 [2.248]
 [1.584]
 [2.949]] [[0.488]
 [0.455]
 [0.488]
 [0.178]
 [0.816]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.813]
 [3.813]
 [3.813]
 [3.813]
 [4.102]] [[1.523]
 [1.523]
 [1.523]
 [1.523]
 [1.716]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.581]
 [4.581]
 [4.581]
 [4.581]
 [4.581]] [[1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8224361
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.495]
 [3.495]
 [2.521]
 [3.495]
 [3.666]] [[1.111]
 [1.111]
 [0.462]
 [1.111]
 [1.224]]
using explorer policy with actor:  1
siam score:  -0.8242398
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.407]
 [1.694]
 [1.694]
 [1.694]
 [2.588]] [[1.082]
 [0.735]
 [0.735]
 [0.735]
 [1.171]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.81554675
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.79697174
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
922 4507
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.79843354
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.908]
 [3.908]
 [3.908]
 [3.908]
 [3.703]] [[1.341]
 [1.341]
 [1.341]
 [1.341]
 [1.228]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.038467456235825086, 0.038467456235825086, 0.038467456235825086, 0.8076627188208747, 0.038467456235825086, 0.038467456235825086]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.038467456235825086, 0.038467456235825086, 0.038467456235825086, 0.8076627188208747, 0.038467456235825086, 0.038467456235825086]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.038467456235825086, 0.038467456235825086, 0.038467456235825086, 0.8076627188208747, 0.038467456235825086, 0.038467456235825086]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.038467456235825086, 0.038467456235825086, 0.038467456235825086, 0.8076627188208747, 0.038467456235825086, 0.038467456235825086]
siam score:  -0.7936277
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.038467456235825086, 0.038467456235825086, 0.038467456235825086, 0.8076627188208747, 0.038467456235825086, 0.038467456235825086]
actor:  1 policy actor:  1  step number:  97 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.724]
 [3.724]
 [3.724]
 [3.724]
 [3.826]] [[1.752]
 [1.752]
 [1.752]
 [1.752]
 [1.817]]
line 256 mcts: sample exp_bonus 5.339347039712338
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02174291162164801, 0.02174291162164801, 0.02174291162164801, 0.45651417675670397, 0.45651417675670397, 0.02174291162164801]
line 256 mcts: sample exp_bonus 2.7470891935184296
936 4531
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02174291162164801, 0.02174291162164801, 0.02174291162164801, 0.45651417675670397, 0.45651417675670397, 0.02174291162164801]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02174291162164801, 0.02174291162164801, 0.02174291162164801, 0.45651417675670397, 0.45651417675670397, 0.02174291162164801]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 7.29635110407734
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.001]] [[3.011]
 [3.011]
 [3.011]
 [3.011]
 [3.043]] [[1.439]
 [1.439]
 [1.439]
 [1.439]
 [1.459]]
using explorer policy with actor:  1
Starting evaluation
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.971]
 [2.971]
 [2.705]
 [2.034]
 [1.015]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.614]
 [0.4  ]
 [1.614]
 [2.04 ]
 [1.745]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.141]
 [2.109]
 [2.109]
 [2.109]
 [3.13 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.74237996
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.862]
 [4.862]
 [4.862]
 [4.862]
 [4.862]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.   ]] [[3.028]
 [3.186]
 [3.414]
 [2.224]
 [3.345]] [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.   ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.26 ]
 [2.732]
 [2.732]
 [2.449]
 [3.3  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.0227838370684457
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.863]
 [2.863]
 [2.863]
 [2.885]
 [2.8  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn probs:  [0.02174291162164801, 0.02174291162164801, 0.02174291162164801, 0.45651417675670397, 0.45651417675670397, 0.02174291162164801]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.134]
 [3.537]
 [4.252]
 [3.537]
 [4.862]] [[1.593]
 [1.34 ]
 [1.643]
 [1.34 ]
 [1.902]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.18 ]
 [0.7  ]
 [2.208]
 [2.443]
 [2.554]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02174291162164801, 0.02174291162164801, 0.02174291162164801, 0.45651417675670397, 0.45651417675670397, 0.02174291162164801]
siam score:  -0.7603351
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02174291162164801, 0.02174291162164801, 0.02174291162164801, 0.45651417675670397, 0.45651417675670397, 0.02174291162164801]
using explorer policy with actor:  1
from probs:  [0.02174291162164801, 0.02174291162164801, 0.02174291162164801, 0.45651417675670397, 0.45651417675670397, 0.02174291162164801]
using another actor
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.706]
 [2.389]
 [2.389]
 [1.726]
 [2.975]] [[0.569]
 [1.003]
 [1.003]
 [0.582]
 [1.376]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.403]
 [1.403]
 [2.074]
 [1.403]
 [1.433]] [[0.507]
 [0.507]
 [1.023]
 [0.507]
 [0.53 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02174291162164801, 0.02174291162164801, 0.02174291162164801, 0.45651417675670397, 0.45651417675670397, 0.02174291162164801]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.068]
 [3.068]
 [3.068]
 [3.068]
 [3.068]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02174291162164801, 0.02174291162164801, 0.02174291162164801, 0.45651417675670397, 0.45651417675670397, 0.02174291162164801]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20084
from probs:  [0.02174291162164801, 0.02174291162164801, 0.02174291162164801, 0.45651417675670397, 0.45651417675670397, 0.02174291162164801]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.02174291162164801, 0.02174291162164801, 0.02174291162164801, 0.45651417675670397, 0.45651417675670397, 0.02174291162164801]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.02174291162164801, 0.02174291162164801, 0.02174291162164801, 0.45651417675670397, 0.45651417675670397, 0.02174291162164801]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015154270335628262, 0.015154270335628262, 0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.323]
 [1.452]
 [1.452]
 [1.363]
 [2.   ]] [[0.223]
 [0.309]
 [0.309]
 [0.249]
 [0.674]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[4.921]
 [4.921]
 [4.921]
 [4.921]
 [4.921]] [[1.271]
 [1.271]
 [1.271]
 [1.271]
 [1.271]]
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01163007060262309, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.01163007060262309]
siam score:  -0.7485482
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01163007060262309, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.01163007060262309]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01163007060262309, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.01163007060262309]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
976 4570
from probs:  [0.01163007060262309, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.01163007060262309]
siam score:  -0.72503
from probs:  [0.01163007060262309, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.01163007060262309]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]]
983 4582
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.422]
 [1.422]
 [1.422]
 [1.422]
 [1.422]] [[0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.002]
 [0.002]
 [0.002]] [[1.244]
 [1.39 ]
 [1.507]
 [1.113]
 [1.714]] [[0.783]
 [0.977]
 [1.132]
 [0.606]
 [1.409]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5848133669422704
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01163007060262309, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.01163007060262309]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01163007060262309, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.01163007060262309]
993 4592
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.01 ]
 [0.002]
 [0.001]
 [0.001]] [[0.409]
 [0.428]
 [0.366]
 [0.425]
 [0.4  ]] [[0.   ]
 [0.01 ]
 [0.002]
 [0.001]
 [0.001]]
from probs:  [0.01163007060262309, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.01163007060262309]
using another actor
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.001]
 [0.002]
 [0.001]] [[0.319]
 [0.814]
 [0.305]
 [0.572]
 [0.691]] [[0.235]
 [0.897]
 [0.215]
 [0.572]
 [0.73 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.78833216
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01163007060262309, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.01163007060262309]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
using another actor
start point for exploration sampling:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01163007060262309, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.01163007060262309]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01163007060262309, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.01163007060262309]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01163007060262309, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.01163007060262309]
1009 4618
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[1.659]
 [2.064]
 [2.492]
 [1.808]
 [1.534]] [[0.586]
 [0.827]
 [1.082]
 [0.674]
 [0.511]]
from probs:  [0.01163007060262309, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.01163007060262309]
1014 4637
1014 4639
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.01163007060262309, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.2441849646986885, 0.01163007060262309]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.035]] [[4.343]
 [4.343]
 [4.343]
 [4.343]
 [5.027]] [[1.411]
 [1.411]
 [1.411]
 [1.411]
 [1.69 ]]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.083]
 [0.085]
 [0.054]
 [0.104]] [[2.638]
 [3.147]
 [2.593]
 [3.322]
 [4.323]] [[0.745]
 [1.143]
 [0.8  ]
 [1.198]
 [1.922]]
siam score:  -0.8424302
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.005]
 [0.008]
 [0.004]
 [0.002]] [[0.912]
 [0.867]
 [0.884]
 [0.925]
 [1.111]] [[0.054]
 [0.022]
 [0.037]
 [0.064]
 [0.202]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.009435742503908828]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.018]
 [0.016]
 [0.012]
 [0.013]] [[1.77 ]
 [2.116]
 [1.82 ]
 [1.863]
 [2.468]] [[1.08 ]
 [1.556]
 [1.161]
 [1.21 ]
 [2.008]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 0.3434871926993811
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.035]
 [0.012]
 [0.012]
 [0.009]] [[2.028]
 [3.064]
 [1.99 ]
 [1.99 ]
 [2.037]] [[0.683]
 [1.42 ]
 [0.658]
 [0.658]
 [0.684]]
using explorer policy with actor:  1
using another actor
from probs:  [0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.009435742503908828]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.009435742503908828]
1032 4679
first move QE:  0.4579205999301781
from probs:  [0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.009435742503908828]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.009435742503908828]
from probs:  [0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.009435742503908828]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.009435742503908828]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.141]] [[1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.007]] [[1.355]
 [1.355]
 [1.355]
 [1.355]
 [1.374]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.007]]
using another actor
from probs:  [0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.009435742503908828]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.009435742503908828]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]] [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.009435742503908828]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.009435742503908828]
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.01 ]
 [0.005]
 [0.01 ]
 [0.005]] [[1.532]
 [1.048]
 [1.532]
 [1.328]
 [1.672]] [[0.878]
 [0.444]
 [0.878]
 [0.698]
 [1.006]]
from probs:  [0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.19811285149921826, 0.009435742503908828]
using another actor
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.32539531346864187, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.007938019864691498]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.32539531346864187, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.007938019864691498]
siam score:  -0.8784029
start point for exploration sampling:  20084
using explorer policy with actor:  1
1046 4773
from probs:  [0.16666666666666666, 0.32539531346864187, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.007938019864691498]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
start point for exploration sampling:  20084
siam score:  -0.8833373
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.066]
 [0.047]
 [0.064]
 [0.077]] [[6.086]
 [3.685]
 [6.086]
 [3.102]
 [3.694]] [[4.125]
 [2.007]
 [4.125]
 [1.488]
 [2.026]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[1.347]
 [1.347]
 [1.347]
 [1.347]
 [1.347]] [[2.006]
 [2.006]
 [2.006]
 [2.006]
 [2.006]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.32539531346864187, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.007938019864691498]
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[2.747]
 [2.747]
 [2.747]
 [2.747]
 [2.747]] [[0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.32539531346864187, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.007938019864691498]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.181]] [[1.947]
 [1.947]
 [1.947]
 [1.947]
 [1.897]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.01 ]] [[1.248]
 [1.248]
 [1.248]
 [1.248]
 [1.282]] [[1.258]
 [1.258]
 [1.258]
 [1.258]
 [1.305]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.32539531346864187, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.007938019864691498]
first move QE:  0.4560946376221298
maxi score, test score, baseline:  0.0001 0.0 0.0001
1067 4822
line 256 mcts: sample exp_bonus 0.4561983585478946
maxi score, test score, baseline:  0.0001 0.0 0.0001
1069 4828
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.32539531346864187, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.007938019864691498]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.32539531346864187, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.007938019864691498]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]] [[0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[1.504]
 [1.504]
 [1.939]
 [1.504]
 [1.504]] [[0.874]
 [0.874]
 [1.163]
 [0.874]
 [0.874]]
using another actor
from probs:  [0.16666666666666666, 0.32539531346864187, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.007938019864691498]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.054]
 [0.038]
 [0.024]
 [0.018]] [[1.518]
 [4.188]
 [2.684]
 [1.919]
 [2.083]] [[0.123]
 [1.847]
 [0.879]
 [0.377]
 [0.468]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.32539531346864187, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.007938019864691498]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.   ]
 [0.003]
 [0.003]
 [0.   ]] [[0.492]
 [0.43 ]
 [0.492]
 [0.492]
 [0.325]] [[0.603]
 [0.495]
 [0.603]
 [0.603]
 [0.319]]
from probs:  [0.16666666666666666, 0.32539531346864187, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.007938019864691498]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
from probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[1.08]
 [1.08]
 [1.08]
 [1.08]
 [1.08]] [[1.2]
 [1.2]
 [1.2]
 [1.2]
 [1.2]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]] [[1.249]
 [1.249]
 [1.249]
 [1.249]
 [1.249]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.008]
 [0.009]] [[0.815]
 [0.815]
 [0.815]
 [1.175]
 [1.013]] [[0.509]
 [0.509]
 [0.509]
 [1.112]
 [0.842]]
siam score:  -0.8728636
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.007]] [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [1.057]] [[0.165]
 [0.165]
 [0.165]
 [0.165]
 [1.088]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
1081 4891
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
using another actor
from probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.02 ]
 [0.016]
 [0.012]] [[1.89 ]
 [1.89 ]
 [3.38 ]
 [1.89 ]
 [2.194]] [[0.719]
 [0.719]
 [1.305]
 [0.719]
 [0.837]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.075]] [[7.008]
 [7.008]
 [7.008]
 [7.008]
 [6.541]] [[1.928]
 [1.928]
 [1.928]
 [1.928]
 [1.706]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.009]
 [0.006]
 [0.006]
 [0.01 ]] [[3.362]
 [3.547]
 [3.362]
 [3.362]
 [3.113]] [[1.616]
 [1.761]
 [1.616]
 [1.616]
 [1.43 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
siam score:  -0.86954427
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.106]
 [0.112]
 [0.1  ]
 [0.108]] [[4.701]
 [4.534]
 [5.203]
 [4.851]
 [4.872]] [[1.611]
 [1.537]
 [1.883]
 [1.696]
 [1.711]]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.009]
 [0.008]] [[0.407]
 [0.407]
 [0.407]
 [0.577]
 [0.407]] [[0.107]
 [0.107]
 [0.107]
 [0.166]
 [0.107]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.005]
 [0.003]
 [0.003]
 [0.005]] [[1.924]
 [2.427]
 [2.551]
 [2.551]
 [2.553]] [[1.055]
 [1.487]
 [1.592]
 [1.592]
 [1.595]]
from probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
siam score:  -0.8700888
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
using another actor
from probs:  [0.1438358041174326, 0.28082097941283696, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.   ]
 [0.003]
 [0.003]] [[0.366]
 [0.7  ]
 [0.209]
 [0.421]
 [0.52 ]] [[0.004]
 [0.004]
 [0.   ]
 [0.003]
 [0.003]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.001]
 [0.005]
 [0.004]
 [0.005]] [[0.503]
 [0.44 ]
 [0.513]
 [0.409]
 [0.404]] [[0.119]
 [0.091]
 [0.121]
 [0.086]
 [0.086]]
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.054]] [[3.519]
 [3.519]
 [3.519]
 [3.519]
 [4.195]] [[1.058]
 [1.058]
 [1.058]
 [1.058]
 [1.506]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
from probs:  [0.12650631445573782, 0.3674684277213109, 0.12650631445573782, 0.12650631445573782, 0.12650631445573782, 0.12650631445573782]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.732]
 [0.732]
 [0.732]
 [0.584]
 [0.834]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.005]
 [0.005]
 [0.003]
 [0.005]] [[0.54 ]
 [0.843]
 [0.931]
 [0.54 ]
 [0.64 ]] [[0.003]
 [0.005]
 [0.005]
 [0.003]
 [0.005]]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.02 ]
 [0.012]
 [0.012]
 [0.013]] [[3.569]
 [3.977]
 [3.569]
 [3.569]
 [3.488]] [[1.632]
 [1.9  ]
 [1.632]
 [1.632]
 [1.582]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.005]
 [0.006]] [[0.478]
 [0.478]
 [0.478]
 [0.51 ]
 [0.339]] [[0.006]
 [0.006]
 [0.006]
 [0.005]
 [0.006]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.005]
 [0.007]] [[0.603]
 [0.603]
 [0.603]
 [0.624]
 [0.593]] [[0.315]
 [0.315]
 [0.315]
 [0.334]
 [0.307]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[1.339]
 [1.339]
 [1.339]
 [1.339]
 [1.433]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.787]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12650631445573782, 0.3674684277213109, 0.12650631445573782, 0.12650631445573782, 0.12650631445573782, 0.12650631445573782]
rdn probs:  [0.12650631445573782, 0.3674684277213109, 0.12650631445573782, 0.12650631445573782, 0.12650631445573782, 0.12650631445573782]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12650631445573782, 0.3674684277213109, 0.12650631445573782, 0.12650631445573782, 0.12650631445573782, 0.12650631445573782]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.002]] [[3.282]
 [3.282]
 [3.282]
 [3.282]
 [3.578]] [[1.108]
 [1.108]
 [1.108]
 [1.108]
 [1.3  ]]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[4.362]
 [4.362]
 [4.362]
 [4.362]
 [4.998]] [[0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.989]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12650631445573782, 0.3674684277213109, 0.12650631445573782, 0.12650631445573782, 0.12650631445573782, 0.12650631445573782]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.003]
 [0.007]
 [0.01 ]
 [0.009]] [[1.633]
 [2.148]
 [2.527]
 [4.321]
 [3.839]] [[0.427]
 [0.629]
 [0.782]
 [1.494]
 [1.303]]
siam score:  -0.8413268
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.107]] [[6.486]
 [6.486]
 [6.486]
 [6.486]
 [6.183]] [[1.388]
 [1.388]
 [1.388]
 [1.388]
 [1.245]]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 3.8387073087261863
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.002]] [[1.186]
 [1.186]
 [1.186]
 [1.186]
 [1.425]] [[1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.428]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.005]
 [0.004]
 [0.005]
 [0.006]] [[1.914]
 [3.411]
 [1.914]
 [2.924]
 [3.663]] [[0.004]
 [0.005]
 [0.004]
 [0.005]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.009]] [[3.673]
 [3.673]
 [3.673]
 [3.673]
 [4.495]] [[1.002]
 [1.002]
 [1.002]
 [1.002]
 [1.493]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19902893766722723, 0.39320256367059064, 0.10194212466554547, 0.10194212466554547, 0.10194212466554547, 0.10194212466554547]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19902893766722723, 0.39320256367059064, 0.10194212466554547, 0.10194212466554547, 0.10194212466554547, 0.10194212466554547]
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[1.825]
 [1.825]
 [1.825]
 [1.825]
 [1.825]] [[0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]]
in main func line 156:  1131
from probs:  [0.19902893766722723, 0.39320256367059064, 0.10194212466554547, 0.10194212466554547, 0.10194212466554547, 0.10194212466554547]
using another actor
from probs:  [0.19902893766722723, 0.39320256367059064, 0.10194212466554547, 0.10194212466554547, 0.10194212466554547, 0.10194212466554547]
from probs:  [0.19902893766722723, 0.39320256367059064, 0.10194212466554547, 0.10194212466554547, 0.10194212466554547, 0.10194212466554547]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.9794275701696713
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18141585087753867, 0.4469011666732347, 0.09292074561230662, 0.09292074561230662, 0.09292074561230662, 0.09292074561230662]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20084
first move QE:  0.46380746821259144
from probs:  [0.18141585087753867, 0.4469011666732347, 0.09292074561230662, 0.09292074561230662, 0.09292074561230662, 0.09292074561230662]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18141585087753867, 0.4469011666732347, 0.09292074561230662, 0.09292074561230662, 0.09292074561230662, 0.09292074561230662]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[1.991]
 [1.991]
 [1.991]
 [1.991]
 [2.134]] [[0.987]
 [0.987]
 [0.987]
 [0.987]
 [1.118]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.730749570181636
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.24796708302790377, 0.410567915750378, 0.08536625030542948, 0.08536625030542948, 0.08536625030542948, 0.08536625030542948]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3045106562496137, 0.3796982869312212, 0.07894776420479119, 0.07894776420479119, 0.07894776420479119, 0.07894776420479119]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3045106562496137, 0.3796982869312212, 0.07894776420479119, 0.07894776420479119, 0.07894776420479119, 0.07894776420479119]
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]] [[1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.01 ]
 [0.01 ]
 [0.009]
 [0.011]] [[0.808]
 [0.808]
 [0.808]
 [0.868]
 [1.142]] [[0.71 ]
 [0.71 ]
 [0.71 ]
 [0.806]
 [1.256]]
using explorer policy with actor:  1
siam score:  -0.84618264
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.84720016
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[1.874]
 [1.874]
 [1.874]
 [1.874]
 [1.874]] [[1.989]
 [1.989]
 [1.989]
 [1.989]
 [1.989]]
using explorer policy with actor:  1
siam score:  -0.84755844
from probs:  [0.28321629412400456, 0.3531460705984073, 0.07342696470079628, 0.14335674117519906, 0.07342696470079628, 0.07342696470079628]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8489209
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1156 4989
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[4.64 ]
 [4.64 ]
 [4.64 ]
 [4.64 ]
 [5.044]] [[1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.76 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3300647186043205, 0.3300647186043205, 0.0686278355040743, 0.13398705627913587, 0.0686278355040743, 0.0686278355040743]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.007]
 [0.006]
 [0.005]
 [0.005]] [[2.56 ]
 [1.555]
 [0.992]
 [1.514]
 [1.529]] [[2.   ]
 [0.964]
 [0.38 ]
 [0.919]
 [0.936]]
using another actor
UNIT TEST: sample policy line 217 mcts : [0.125 0.    0.    0.042 0.833]
in main func line 156:  1163
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3300647186043205, 0.3300647186043205, 0.0686278355040743, 0.13398705627913587, 0.0686278355040743, 0.0686278355040743]
using explorer policy with actor:  1
start point for exploration sampling:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.3300647186043205, 0.3300647186043205, 0.0686278355040743, 0.13398705627913587, 0.0686278355040743, 0.0686278355040743]
1168 5010
from probs:  [0.3300647186043205, 0.3300647186043205, 0.0686278355040743, 0.13398705627913587, 0.0686278355040743, 0.0686278355040743]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3300647186043205, 0.3300647186043205, 0.0686278355040743, 0.13398705627913587, 0.0686278355040743, 0.0686278355040743]
first move QE:  0.46890139707192297
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.82281744
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2919070800258893, 0.34971034773014587, 0.06069400920886291, 0.1763005446173761, 0.06069400920886291, 0.06069400920886291]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.077]
 [0.022]
 [0.022]
 [0.022]] [[1.853]
 [3.226]
 [1.853]
 [1.853]
 [1.853]] [[0.744]
 [1.456]
 [0.744]
 [0.744]
 [0.744]]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.006]] [[1.692]
 [1.692]
 [1.692]
 [1.692]
 [1.629]] [[0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.556]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.26165773573475765, 0.4170976669370883, 0.05440449413165007, 0.15803111493320388, 0.05440449413165007, 0.05440449413165007]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.439]
 [0.754]
 [0.754]
 [0.754]
 [1.495]] [[1.819]
 [2.866]
 [2.866]
 [2.866]
 [1.443]] [[2.05 ]
 [1.893]
 [1.893]
 [1.893]
 [1.922]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.27130016686314623, 0.3609860241744144, 0.09192845224060983, 0.13677138089624394, 0.04708552358497572, 0.09192845224060983]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.27130016686314623, 0.3609860241744144, 0.09192845224060983, 0.13677138089624394, 0.04708552358497572, 0.09192845224060983]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.4450811064661042
from probs:  [0.302574757265718, 0.3454931016654183, 0.08798303526721593, 0.13090137966691634, 0.045064690867515515, 0.08798303526721593]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.302574757265718, 0.3454931016654183, 0.08798303526721593, 0.13090137966691634, 0.045064690867515515, 0.08798303526721593]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]] [[1.925]
 [1.925]
 [1.925]
 [1.925]
 [1.925]] [[1.205]
 [1.205]
 [1.205]
 [1.205]
 [1.205]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8146622
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.81364256
from probs:  [0.33127531366253105, 0.33127531366253105, 0.08436234316873448, 0.12551450491770058, 0.043210181419768366, 0.08436234316873448]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.33127531366253105, 0.33127531366253105, 0.08436234316873448, 0.12551450491770058, 0.043210181419768366, 0.08436234316873448]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8234294
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3181814588042947, 0.3181814588042947, 0.12055346905956249, 0.12055346905956249, 0.041502273161669566, 0.08102787111061603]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.008]] [[-0.276]
 [-0.276]
 [-0.276]
 [-0.276]
 [-0.166]] [[0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.105]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.2948715130606307, 0.3315014691731919, 0.11172173249782494, 0.14835168861038606, 0.0384618202727026, 0.07509177638526376]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31978766117887125, 0.31978766117887125, 0.10777397646966491, 0.14310959058786596, 0.03710274823326278, 0.07243836235146385]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[1.65 ]
 [1.681]
 [1.681]
 [1.681]
 [2.585]] [[0.244]
 [0.262]
 [0.262]
 [0.262]
 [0.865]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.539036052301528
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31978766117887125, 0.31978766117887125, 0.10777397646966491, 0.14310959058786596, 0.03710274823326278, 0.07243836235146385]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.027]
 [0.011]
 [0.011]
 [0.011]] [[4.968]
 [4.158]
 [4.12 ]
 [4.12 ]
 [3.469]] [[1.931]
 [1.435]
 [1.39 ]
 [1.39 ]
 [0.976]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[3.634]
 [3.634]
 [3.634]
 [3.634]
 [3.564]] [[1.428]
 [1.428]
 [1.428]
 [1.428]
 [1.381]]
siam score:  -0.80685306
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.34300305181671165, 0.3088734288844449, 0.10409569129084426, 0.13822531422311102, 0.0358364454263107, 0.06996606835857748]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.034]
 [0.034]
 [0.079]
 [0.081]] [[4.063]
 [4.063]
 [4.063]
 [4.615]
 [4.551]] [[1.094]
 [1.094]
 [1.094]
 [1.468]
 [1.429]]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.3316828415031078, 0.3316828415031078, 0.10066019673209023, 0.13366343169937844, 0.03465372679751376, 0.067656961764802]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.009]
 [0.008]] [[2.978]
 [2.978]
 [2.978]
 [3.762]
 [3.718]] [[0.9  ]
 [0.9  ]
 [0.9  ]
 [1.613]
 [1.572]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3316828415031078, 0.3316828415031078, 0.10066019673209023, 0.13366343169937844, 0.03465372679751376, 0.067656961764802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3316828415031078, 0.3316828415031078, 0.10066019673209023, 0.13366343169937844, 0.03465372679751376, 0.067656961764802]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3316828415031078, 0.3316828415031078, 0.10066019673209023, 0.13366343169937844, 0.03465372679751376, 0.067656961764802]
siam score:  -0.80864453
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3316828415031078, 0.3316828415031078, 0.10066019673209023, 0.13366343169937844, 0.03465372679751376, 0.067656961764802]
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.001]
 [0.004]
 [0.004]
 [0.004]] [[1.646]
 [0.935]
 [1.465]
 [1.646]
 [1.845]] [[0.807]
 [0.137]
 [0.638]
 [0.807]
 [0.993]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3530347864611909, 0.3210859659249868, 0.09744422217155768, 0.1293930427077618, 0.03354658109914935, 0.06549540163535351]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3530347864611909, 0.3210859659249868, 0.09744422217155768, 0.1293930427077618, 0.03354658109914935, 0.06549540163535351]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  0.47726545109242186
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.127]
 [0.13 ]
 [0.094]
 [0.094]] [[2.239]
 [2.81 ]
 [2.935]
 [2.358]
 [2.358]] [[1.114]
 [1.529]
 [1.607]
 [1.231]
 [1.231]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.006]
 [0.008]
 [0.005]
 [0.006]] [[4.241]
 [4.176]
 [4.953]
 [4.146]
 [4.308]] [[1.564]
 [1.532]
 [1.927]
 [1.516]
 [1.599]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.33183153419283085, 0.33183153419283085, 0.12162170279589463, 0.12162170279589463, 0.031531775054350536, 0.061561750968198575]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 10.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.313030912658011, 0.36968804013853135, 0.11473096647618966, 0.11473096647618966, 0.02974527525540911, 0.0580738389956693]
siam score:  -0.8490164
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]] [[3.641]
 [3.641]
 [3.641]
 [3.641]
 [3.641]] [[2.061]
 [2.061]
 [2.061]
 [2.061]
 [2.061]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.313030912658011, 0.36968804013853135, 0.11473096647618966, 0.11473096647618966, 0.02974527525540911, 0.0580738389956693]
from probs:  [0.313030912658011, 0.36968804013853135, 0.11473096647618966, 0.11473096647618966, 0.02974527525540911, 0.0580738389956693]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.313030912658011, 0.36968804013853135, 0.11473096647618966, 0.11473096647618966, 0.02974527525540911, 0.0580738389956693]
using another actor
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.042]
 [0.017]
 [0.023]] [[0.619]
 [0.619]
 [1.219]
 [0.619]
 [1.167]] [[0.415]
 [0.415]
 [1.187]
 [0.415]
 [1.09 ]]
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1354718374846415
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.304407485793692, 0.35950381344450216, 0.11157033901585653, 0.1391185028412616, 0.02892584753964133, 0.0564740113650464]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.84068954
siam score:  -0.83925784
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2962464403229814, 0.34986565700835304, 0.13538879026686654, 0.13538879026686654, 0.028150356896123302, 0.05495996523880912]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.143]
 [0.102]
 [0.102]
 [0.102]] [[3.976]
 [3.869]
 [3.634]
 [3.634]
 [3.634]] [[1.081]
 [0.884]
 [0.723]
 [0.723]
 [0.723]]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31462117810437457, 0.34073079776985243, 0.13185384044602952, 0.13185384044602952, 0.02741536178411806, 0.053524981449595936]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31462117810437457, 0.34073079776985243, 0.13185384044602952, 0.13185384044602952, 0.02741536178411806, 0.053524981449595936]
first move QE:  0.4779089548419097
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31462117810437457, 0.34073079776985243, 0.13185384044602952, 0.13185384044602952, 0.02741536178411806, 0.053524981449595936]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31462117810437457, 0.34073079776985243, 0.13185384044602952, 0.13185384044602952, 0.02741536178411806, 0.053524981449595936]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31462117810437457, 0.34073079776985243, 0.13185384044602952, 0.13185384044602952, 0.02741536178411806, 0.053524981449595936]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31462117810437457, 0.34073079776985243, 0.13185384044602952, 0.13185384044602952, 0.02741536178411806, 0.053524981449595936]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.089]
 [0.033]
 [0.033]] [[0.262]
 [0.262]
 [1.04 ]
 [0.262]
 [0.262]] [[-0.068]
 [-0.068]
 [ 0.543]
 [-0.068]
 [-0.068]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.3320608161542566, 0.33206081615425664, 0.12849878601568437, 0.12849878601568437, 0.026717770946398246, 0.05216302471371978]
first move QE:  0.4770515395748984
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3238211059384383, 0.32382110593843844, 0.15012409411174335, 0.12531023527935833, 0.026054799949818328, 0.050868658782203334]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3238211059384383, 0.32382110593843844, 0.15012409411174335, 0.12531023527935833, 0.026054799949818328, 0.050868658782203334]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3238211059384383, 0.32382110593843844, 0.15012409411174335, 0.12531023527935833, 0.026054799949818328, 0.050868658782203334]
siam score:  -0.86676997
siam score:  -0.8773552
maxi score, test score, baseline:  0.0001 0.0 0.0001
1260 5146
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.033]] [[0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.633]] [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.033]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3238211059384383, 0.3238211059384384, 0.15012409411174335, 0.12531023527935833, 0.026054799949818328, 0.050868658782203334]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3238211059384383, 0.3238211059384384, 0.15012409411174335, 0.12531023527935833, 0.026054799949818328, 0.050868658782203334]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.34019345246563926, 0.3159804125867129, 0.146489133434228, 0.12227609355530157, 0.025423934039595922, 0.04963697391852234]
siam score:  -0.89675766
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.082]
 [0.148]
 [0.107]
 [0.107]] [[0.901]
 [1.081]
 [1.706]
 [0.901]
 [0.901]] [[0.724]
 [0.793]
 [1.342]
 [0.724]
 [0.724]]
from probs:  [0.34019345246563926, 0.31598041258671294, 0.146489133434228, 0.12227609355530157, 0.025423934039595922, 0.04963697391852234]
Printing some Q and Qe and total Qs values:  [[0.06 ]
 [0.055]
 [0.06 ]
 [0.06 ]
 [0.093]] [[6.368]
 [5.97 ]
 [6.368]
 [6.368]
 [5.912]] [[1.706]
 [1.543]
 [1.706]
 [1.706]
 [1.55 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.34019345246563926, 0.31598041258671294, 0.146489133434228, 0.12227609355530157, 0.025423934039595922, 0.04963697391852234]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.34019345246563926, 0.31598041258671294, 0.146489133434228, 0.12227609355530157, 0.025423934039595922, 0.04963697391852234]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.34019345246563926, 0.31598041258671294, 0.146489133434228, 0.12227609355530157, 0.025423934039595922, 0.04963697391852234]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.01 ]
 [0.043]
 [0.045]
 [0.038]] [[2.377]
 [2.65 ]
 [1.867]
 [2.27 ]
 [3.211]] [[0.8  ]
 [1.027]
 [0.357]
 [0.719]
 [1.553]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1108999661705588
from probs:  [0.34019345246563926, 0.31598041258671294, 0.146489133434228, 0.12227609355530157, 0.025423934039595922, 0.04963697391852234]
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]] [[2.371]
 [2.371]
 [2.371]
 [2.371]
 [2.371]] [[1.601]
 [1.601]
 [1.601]
 [1.601]
 [1.601]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.34019345246563926, 0.3159804125867129, 0.146489133434228, 0.12227609355530157, 0.025423934039595922, 0.04963697391852234]
using another actor
from probs:  [0.34019345246563926, 0.3159804125867129, 0.146489133434228, 0.12227609355530157, 0.025423934039595922, 0.04963697391852234]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3321510654715488, 0.30851043707085135, 0.16666666666666669, 0.11938540986527178, 0.024822896262481995, 0.04846352466317945]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.3321510654715488, 0.30851043707085135, 0.16666666666666669, 0.11938540986527178, 0.024822896262481995, 0.04846352466317945]
first move QE:  0.47533397170108355
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.3321510654715488, 0.30851043707085135, 0.16666666666666669, 0.11938540986527178, 0.024822896262481995, 0.04846352466317945]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]] [[1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]] [[1.75]
 [1.75]
 [1.75]
 [1.75]
 [1.75]]
start point for exploration sampling:  20084
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20084
from probs:  [0.33972888520788075, 0.2945822195014771, 0.1817155552354679, 0.11399555667586236, 0.02370222526305498, 0.04627555811625683]
Starting evaluation
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 0.5856258748229486
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.051]
 [0.045]
 [0.047]
 [0.046]] [[0.356]
 [0.356]
 [0.564]
 [0.685]
 [0.71 ]] [[0.051]
 [0.051]
 [0.045]
 [0.047]
 [0.046]]
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0121 0.0 0.0121
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 2.3804871516697954
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0301 0.0 0.0301
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [0.33965527961943304, 0.29452781537088524, 0.18170915474951588, 0.1140179583766943, 0.023763029879598826, 0.046326762003872694]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [0.33965527961943304, 0.29452781537088524, 0.18170915474951588, 0.1140179583766943, 0.023763029879598826, 0.046326762003872694]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.037]] [[2.827]
 [2.827]
 [2.827]
 [2.827]
 [2.54 ]] [[0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.037]]
actor:  0 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.3396501301696524, 0.29452400925583, 0.18170870697127411, 0.1140195256005406, 0.02376728377289593, 0.046330344229807106]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
from probs:  [0.3251793868232225, 0.304044357469015, 0.17723418134377034, 0.11382909328114808, 0.029288975864318334, 0.050424005218525775]
line 256 mcts: sample exp_bonus 0.9069572405941039
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.034100000000000005 0.64 0.64
maxi score, test score, baseline:  0.034100000000000005 0.64 0.64
probs:  [0.3251793868232225, 0.304044357469015, 0.17723418134377034, 0.11382909328114808, 0.029288975864318334, 0.050424005218525775]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]] [[2.448]
 [2.448]
 [2.448]
 [2.448]
 [2.448]] [[2.803]
 [2.803]
 [2.803]
 [2.803]
 [2.803]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.034100000000000005 0.64 0.64
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.034100000000000005 0.64 0.64
probs:  [0.3391465440143095, 0.29775137345087516, 0.17356586176057237, 0.11147310591542096, 0.02868276478855245, 0.049380350070269585]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.091]] [[3.622]
 [3.622]
 [3.622]
 [3.622]
 [3.946]] [[1.551]
 [1.551]
 [1.551]
 [1.551]
 [1.807]]
using explorer policy with actor:  0
siam score:  -0.8884398
maxi score, test score, baseline:  0.034100000000000005 0.64 0.64
probs:  [0.3391465440143095, 0.2977513734508751, 0.17356586176057237, 0.11147310591542096, 0.02868276478855245, 0.049380350070269585]
maxi score, test score, baseline:  0.034100000000000005 0.64 0.64
probs:  [0.3391465440143095, 0.2977513734508751, 0.17356586176057237, 0.11147310591542096, 0.02868276478855245, 0.049380350070269585]
first move QE:  0.4740148866827286
maxi score, test score, baseline:  0.034100000000000005 0.64 0.64
probs:  [0.3391465440143095, 0.2977513734508751, 0.17356586176057237, 0.11147310591542096, 0.02868276478855245, 0.049380350070269585]
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.459]
 [0.376]
 [0.31 ]
 [0.341]] [[3.206]
 [3.35 ]
 [4.224]
 [3.206]
 [3.572]] [[1.118]
 [1.465]
 [1.591]
 [1.118]
 [1.302]]
